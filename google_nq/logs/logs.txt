[2024-03-02 19:18:10,122 INFO main.py <module> l.93] MAIN STARTS
[2024-03-02 19:20:57,209 INFO main.py <module> l.93] MAIN STARTS
[2024-03-02 19:20:57,209 INFO generators.py generate l.475] (1/10) *** AnsGenerator for question "Qui est considéré comme l'inventeur du télégraphe électrique ? A) Samuel Morse B) Charles Wheatstone C) Alexander Graham Bell D) Thomas Edison"
[2024-03-02 19:20:57,209 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "gpt-4"
[2024-03-02 19:20:57,209 DEBUG generators.py generate l.349] (1/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:20:57,209 DEBUG generators.py generate l.358] (1/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:20:57,383 ERROR generators.py complete l.400] (1/10) The following exception occurred with prompt meta={} user="\n        La question est 'Qui est considéré comme l'inventeur du télégraphe électrique ? A) Samuel Morse B) Charles Wheatstone C) Alexander Graham Bell D) Thomas Edison'" system='Tu es un expert qui doit répondre à des questions à l\'aide de paragraphes qui te sont fournis.\n        Tu dois utiliser uniquement ces paragraphes pour répondre aux questions.\n        Tu dois inclure les titres exacts de ces paragraphes dans la réponse que tu renvoies.\n        Tu dois justifier tes réponses et expliquer comment tu les as construites.\n        Tu dois détailler les phrases et les mots qui te permettent de générer ta réponse.\n\n        Les paragraphes sont présentés ainsi :\n        - Titre (Page X)\n        Contenu\n\n        La réponse générée doit indiquer clairement la source avec le Titre et la Page.\n\n        La réponse doit utiliser le format JSON suivant :\n        {\n        "q_ok": 0 ou 1,\n        "chunks_ok": 0 ou 1,\n        "answer": une chaîne de caractères contenant la réponse\n        }\n\n        Le champ "q_ok" vaut 0 si la question n\'est pas claire, 1 sinon.\n        Le champ "chunks_ok" vaut 0 si les paragraphes fournis ne permettent pas de répondre à la question, 1 sinon.\n        Le champ "answer" contient la réponse.'
OpenAIException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 881, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 855, in completion
    response = openai_chat_completions.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 373, in completion
    raise openaiError(status_code=500, message=traceback.format_exc())
litellm.llms.openai.openaiError: Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7750, in exception_type
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 6568, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: openaiException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = OpenAI(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise OpenAIError(
openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 881, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 855, in completion
    response = openai_chat_completions.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 373, in completion
    raise OpenAIError(status_code=500, message=traceback.format_exc())
litellm.llms.openai.OpenAIError: Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = OpenAI(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise OpenAIError(
openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7750, in exception_type
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 6568, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: OpenAIException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2592, in wrapper
    return litellm.completion_with_retries(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1940, in completion_with_retries
    return retryer(original_function, *args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\concurrent\futures\_base.py", line 451, in result
    return self.__get_result()
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\concurrent\futures\_base.py", line 403, in __get_result
    raise self._exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7750, in exception_type
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 6568, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: OpenAIException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 881, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 855, in completion
    response = openai_chat_completions.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 373, in completion
    raise openaiError(status_code=500, message=traceback.format_exc())
litellm.llms.openai.openaiError: Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7750, in exception_type
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 6568, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: openaiException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

[2024-03-02 19:20:57,431 DEBUG generators.py generate l.373] (1/10) Reuse post-processing
[2024-03-02 19:20:57,434 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-02 19:20:57,434 DEBUG generators.py generate l.349] (1/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:20:57,434 DEBUG generators.py generate l.358] (1/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:20:57,481 ERROR generators.py complete l.400] (1/10) The following exception occurred with prompt meta={} user="\n        La question est 'Qui est considéré comme l'inventeur du télégraphe électrique ? A) Samuel Morse B) Charles Wheatstone C) Alexander Graham Bell D) Thomas Edison'" system='Tu es un expert qui doit répondre à des questions à l\'aide de paragraphes qui te sont fournis.\n        Tu dois utiliser uniquement ces paragraphes pour répondre aux questions.\n        Tu dois inclure les titres exacts de ces paragraphes dans la réponse que tu renvoies.\n        Tu dois justifier tes réponses et expliquer comment tu les as construites.\n        Tu dois détailler les phrases et les mots qui te permettent de générer ta réponse.\n\n        Les paragraphes sont présentés ainsi :\n        - Titre (Page X)\n        Contenu\n\n        La réponse générée doit indiquer clairement la source avec le Titre et la Page.\n\n        La réponse doit utiliser le format JSON suivant :\n        {\n        "q_ok": 0 ou 1,\n        "chunks_ok": 0 ou 1,\n        "answer": une chaîne de caractères contenant la réponse\n        }\n\n        Le champ "q_ok" vaut 0 si la question n\'est pas claire, 1 sinon.\n        Le champ "chunks_ok" vaut 0 si les paragraphes fournis ne permettent pas de répondre à la question, 1 sinon.\n        Le champ "answer" contient la réponse.'
OpenAIException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 881, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 855, in completion
    response = openai_chat_completions.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 373, in completion
    raise openaiError(status_code=500, message=traceback.format_exc())
litellm.llms.openai.openaiError: Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7750, in exception_type
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 6568, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: openaiException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = OpenAI(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise OpenAIError(
openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 881, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 855, in completion
    response = openai_chat_completions.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 373, in completion
    raise OpenAIError(status_code=500, message=traceback.format_exc())
litellm.llms.openai.OpenAIError: Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = OpenAI(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise OpenAIError(
openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7750, in exception_type
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 6568, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: OpenAIException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2592, in wrapper
    return litellm.completion_with_retries(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1940, in completion_with_retries
    return retryer(original_function, *args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\concurrent\futures\_base.py", line 451, in result
    return self.__get_result()
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\concurrent\futures\_base.py", line 403, in __get_result
    raise self._exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7750, in exception_type
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 6568, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: OpenAIException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 881, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 855, in completion
    response = openai_chat_completions.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 373, in completion
    raise openaiError(status_code=500, message=traceback.format_exc())
litellm.llms.openai.openaiError: Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7750, in exception_type
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 6568, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: openaiException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

[2024-03-02 19:20:57,500 DEBUG generators.py generate l.373] (1/10) Reuse post-processing
[2024-03-02 19:20:57,500 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "gemini-pro"
[2024-03-02 19:20:57,500 DEBUG generators.py generate l.349] (1/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:20:57,501 DEBUG generators.py generate l.358] (1/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:21:15,135 DEBUG generators.py generate l.370] (1/10) Post-process Answer
[2024-03-02 19:21:16,210 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "claude-2.1"
[2024-03-02 19:21:16,210 DEBUG generators.py generate l.349] (1/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:21:16,212 DEBUG generators.py generate l.358] (1/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:21:16,256 ERROR generators.py complete l.400] (1/10) The following exception occurred with prompt meta={} user="\n        La question est 'Qui est considéré comme l'inventeur du télégraphe électrique ? A) Samuel Morse B) Charles Wheatstone C) Alexander Graham Bell D) Thomas Edison'" system='Tu es un expert qui doit répondre à des questions à l\'aide de paragraphes qui te sont fournis.\n        Tu dois utiliser uniquement ces paragraphes pour répondre aux questions.\n        Tu dois inclure les titres exacts de ces paragraphes dans la réponse que tu renvoies.\n        Tu dois justifier tes réponses et expliquer comment tu les as construites.\n        Tu dois détailler les phrases et les mots qui te permettent de générer ta réponse.\n\n        Les paragraphes sont présentés ainsi :\n        - Titre (Page X)\n        Contenu\n\n        La réponse générée doit indiquer clairement la source avec le Titre et la Page.\n\n        La réponse doit utiliser le format JSON suivant :\n        {\n        "q_ok": 0 ou 1,\n        "chunks_ok": 0 ou 1,\n        "answer": une chaîne de caractères contenant la réponse\n        }\n\n        Le champ "q_ok" vaut 0 si la question n\'est pas claire, 1 sinon.\n        Le champ "chunks_ok" vaut 0 si les paragraphes fournis ne permettent pas de répondre à la question, 1 sinon.\n        Le champ "answer" contient la réponse.'
Missing Anthropic API Key - A call is being made to anthropic but no key is set either in the environment variables or via params
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1028, in completion
    response = anthropic.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\anthropic.py", line 112, in completion
    headers = validate_environment(api_key, headers)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\anthropic.py", line 83, in validate_environment
    raise ValueError(
ValueError: Missing Anthropic API Key - A call is being made to anthropic but no key is set either in the environment variables or via params

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7750, in exception_type
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7725, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: Missing Anthropic API Key - A call is being made to anthropic but no key is set either in the environment variables or via params

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2592, in wrapper
    return litellm.completion_with_retries(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1940, in completion_with_retries
    return retryer(original_function, *args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\concurrent\futures\_base.py", line 451, in result
    return self.__get_result()
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\concurrent\futures\_base.py", line 403, in __get_result
    raise self._exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7750, in exception_type
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7725, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: Missing Anthropic API Key - A call is being made to anthropic but no key is set either in the environment variables or via params
[2024-03-02 19:21:16,271 DEBUG generators.py generate l.373] (1/10) Reuse post-processing
[2024-03-02 19:21:16,274 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-02 19:21:16,276 DEBUG generators.py generate l.349] (1/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:21:16,276 DEBUG generators.py generate l.358] (1/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:21:16,325 ERROR generators.py complete l.400] (1/10) The following exception occurred with prompt meta={} user="\n        La question est 'Qui est considéré comme l'inventeur du télégraphe électrique ? A) Samuel Morse B) Charles Wheatstone C) Alexander Graham Bell D) Thomas Edison'" system='Tu es un expert qui doit répondre à des questions à l\'aide de paragraphes qui te sont fournis.\n        Tu dois utiliser uniquement ces paragraphes pour répondre aux questions.\n        Tu dois inclure les titres exacts de ces paragraphes dans la réponse que tu renvoies.\n        Tu dois justifier tes réponses et expliquer comment tu les as construites.\n        Tu dois détailler les phrases et les mots qui te permettent de générer ta réponse.\n\n        Les paragraphes sont présentés ainsi :\n        - Titre (Page X)\n        Contenu\n\n        La réponse générée doit indiquer clairement la source avec le Titre et la Page.\n\n        La réponse doit utiliser le format JSON suivant :\n        {\n        "q_ok": 0 ou 1,\n        "chunks_ok": 0 ou 1,\n        "answer": une chaîne de caractères contenant la réponse\n        }\n\n        Le champ "q_ok" vaut 0 si la question n\'est pas claire, 1 sinon.\n        Le champ "chunks_ok" vaut 0 si les paragraphes fournis ne permettent pas de répondre à la question, 1 sinon.\n        Le champ "answer" contient la réponse.'
MistralException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\mistral.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\mistral.py", line 301, in completion
    mistral_client = mistral(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\mistral\_client.py", line 92, in __init__
    raise mistralError(
mistral.mistralError: The api_key client option must be set either by passing api_key to the client or by setting the MISTRAL_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 881, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 855, in completion
    response = mistral_chat_completions.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\mistral.py", line 373, in completion
    raise mistralError(status_code=500, message=traceback.format_exc())
litellm.llms.mistral.mistralError: Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\mistral.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\mistral.py", line 301, in completion
    mistral_client = mistral(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\mistral\_client.py", line 92, in __init__
    raise mistralError(
mistral.mistralError: The api_key client option must be set either by passing api_key to the client or by setting the MISTRAL_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7750, in exception_type
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 6568, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: MistralException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\mistral.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\mistral.py", line 301, in completion
    mistral_client = mistral(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\mistral\_client.py", line 92, in __init__
    raise mistralError(
mistral.mistralError: The api_key client option must be set either by passing api_key to the client or by setting the MISTRAL_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\mistral.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\mistral.py", line 301, in completion
    mistral_client = mistral(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\mistral\_client.py", line 92, in __init__
    raise mistralError(
mistral.mistralError: The api_key client option must be set either by passing api_key to the client or by setting the MISTRAL_API_KEY environment variable
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = OpenAI(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise OpenAIError(
openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 881, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 855, in completion
    response = openai_chat_completions.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 373, in completion
    raise OpenAIError(status_code=500, message=traceback.format_exc())
litellm.llms.openai.OpenAIError: Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = OpenAI(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise OpenAIError(
openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7750, in exception_type
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 6568, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: MistralException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\mistral.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\mistral.py", line 301, in completion
    mistral_client = mistral(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\mistral\_client.py", line 92, in __init__
    raise mistralError(
mistral.mistralError: The api_key client option must be set either by passing api_key to the client or by setting the MISTRAL_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2592, in wrapper
    return litellm.completion_with_retries(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1940, in completion_with_retries
    return retryer(original_function, *args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\concurrent\futures\_base.py", line 451, in result
    return self.__get_result()
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\concurrent\futures\_base.py", line 403, in __get_result
    raise self._exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7750, in exception_type
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 6568, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: MistralException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\mistral.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\mistral.py", line 301, in completion
    mistral_client = mistral(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\mistral\_client.py", line 92, in __init__
    raise mistralError(
mistral.mistralError: The api_key client option must be set either by passing api_key to the client or by setting the MISTRAL_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 881, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 855, in completion
    response = mistral_chat_completions.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\mistral.py", line 373, in completion
    raise mistralError(status_code=500, message=traceback.format_exc())
litellm.llms.mistral.mistralError: Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\mistral.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\mistral.py", line 301, in completion
    mistral_client = mistral(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\mistral\_client.py", line 92, in __init__
    raise mistralError(
mistral.mistralError: The api_key client option must be set either by passing api_key to the client or by setting the MISTRAL_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7750, in exception_type
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 6568, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: MistralException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\mistral.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\mistral.py", line 301, in completion
    mistral_client = mistral(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\mistral\_client.py", line 92, in __init__
    raise mistralError(
mistral.mistralError: The api_key client option must be set either by passing api_key to the client or by setting the MISTRAL_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\mistral.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\mistral.py", line 301, in completion
    mistral_client = mistral(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\mistral\_client.py", line 92, in __init__
    raise mistralError(
mistral.mistralError: The api_key client option must be set either by passing api_key to the client or by setting the MISTRAL_API_KEY environment variable

[2024-03-02 19:21:16,347 DEBUG generators.py generate l.373] (1/10) Reuse post-processing
[2024-03-02 19:21:16,348 INFO generators.py generate l.477] (1/10) End question "Qui est considéré comme l'inventeur du télégraphe électrique ? A) Samuel Morse B) Charles Wheatstone C) Alexander Graham Bell D) Thomas Edison"
[2024-03-02 19:21:16,349 INFO generators.py generate l.475] (2/10) *** AnsGenerator for question "Qui sont les pionniers du cinématographe ? A) Thomas Edison B) Les frères Lumière C) George Eastman D) William Friese-Greene"
[2024-03-02 19:21:16,350 INFO generators.py gen_for_qa l.548] (2/10) * Start with LLM "gpt-4"
[2024-03-02 19:21:16,352 DEBUG generators.py generate l.349] (2/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:21:16,352 DEBUG generators.py generate l.358] (2/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:21:16,407 ERROR generators.py complete l.400] (2/10) The following exception occurred with prompt meta={} user="\n        La question est 'Qui sont les pionniers du cinématographe ? A) Thomas Edison B) Les frères Lumière C) George Eastman D) William Friese-Greene'" system='Tu es un expert qui doit répondre à des questions à l\'aide de paragraphes qui te sont fournis.\n        Tu dois utiliser uniquement ces paragraphes pour répondre aux questions.\n        Tu dois inclure les titres exacts de ces paragraphes dans la réponse que tu renvoies.\n        Tu dois justifier tes réponses et expliquer comment tu les as construites.\n        Tu dois détailler les phrases et les mots qui te permettent de générer ta réponse.\n\n        Les paragraphes sont présentés ainsi :\n        - Titre (Page X)\n        Contenu\n\n        La réponse générée doit indiquer clairement la source avec le Titre et la Page.\n\n        La réponse doit utiliser le format JSON suivant :\n        {\n        "q_ok": 0 ou 1,\n        "chunks_ok": 0 ou 1,\n        "answer": une chaîne de caractères contenant la réponse\n        }\n\n        Le champ "q_ok" vaut 0 si la question n\'est pas claire, 1 sinon.\n        Le champ "chunks_ok" vaut 0 si les paragraphes fournis ne permettent pas de répondre à la question, 1 sinon.\n        Le champ "answer" contient la réponse.'
OpenAIException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 881, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 855, in completion
    response = openai_chat_completions.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 373, in completion
    raise openaiError(status_code=500, message=traceback.format_exc())
litellm.llms.openai.openaiError: Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7750, in exception_type
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 6568, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: openaiException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = OpenAI(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise OpenAIError(
openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 881, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 855, in completion
    response = openai_chat_completions.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 373, in completion
    raise OpenAIError(status_code=500, message=traceback.format_exc())
litellm.llms.openai.OpenAIError: Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = OpenAI(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise OpenAIError(
openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7750, in exception_type
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 6568, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: OpenAIException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2592, in wrapper
    return litellm.completion_with_retries(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1940, in completion_with_retries
    return retryer(original_function, *args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\concurrent\futures\_base.py", line 451, in result
    return self.__get_result()
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\concurrent\futures\_base.py", line 403, in __get_result
    raise self._exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7750, in exception_type
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 6568, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: OpenAIException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 881, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 855, in completion
    response = openai_chat_completions.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 373, in completion
    raise openaiError(status_code=500, message=traceback.format_exc())
litellm.llms.openai.openaiError: Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7750, in exception_type
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 6568, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: openaiException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

[2024-03-02 19:21:16,421 DEBUG generators.py generate l.373] (2/10) Reuse post-processing
[2024-03-02 19:21:16,426 INFO generators.py gen_for_qa l.548] (2/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-02 19:21:16,426 DEBUG generators.py generate l.349] (2/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:21:16,428 DEBUG generators.py generate l.358] (2/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:21:16,493 ERROR generators.py complete l.400] (2/10) The following exception occurred with prompt meta={} user="\n        La question est 'Qui sont les pionniers du cinématographe ? A) Thomas Edison B) Les frères Lumière C) George Eastman D) William Friese-Greene'" system='Tu es un expert qui doit répondre à des questions à l\'aide de paragraphes qui te sont fournis.\n        Tu dois utiliser uniquement ces paragraphes pour répondre aux questions.\n        Tu dois inclure les titres exacts de ces paragraphes dans la réponse que tu renvoies.\n        Tu dois justifier tes réponses et expliquer comment tu les as construites.\n        Tu dois détailler les phrases et les mots qui te permettent de générer ta réponse.\n\n        Les paragraphes sont présentés ainsi :\n        - Titre (Page X)\n        Contenu\n\n        La réponse générée doit indiquer clairement la source avec le Titre et la Page.\n\n        La réponse doit utiliser le format JSON suivant :\n        {\n        "q_ok": 0 ou 1,\n        "chunks_ok": 0 ou 1,\n        "answer": une chaîne de caractères contenant la réponse\n        }\n\n        Le champ "q_ok" vaut 0 si la question n\'est pas claire, 1 sinon.\n        Le champ "chunks_ok" vaut 0 si les paragraphes fournis ne permettent pas de répondre à la question, 1 sinon.\n        Le champ "answer" contient la réponse.'
OpenAIException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 881, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 855, in completion
    response = openai_chat_completions.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 373, in completion
    raise openaiError(status_code=500, message=traceback.format_exc())
litellm.llms.openai.openaiError: Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7750, in exception_type
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 6568, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: openaiException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = OpenAI(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise OpenAIError(
openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 881, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 855, in completion
    response = openai_chat_completions.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 373, in completion
    raise OpenAIError(status_code=500, message=traceback.format_exc())
litellm.llms.openai.OpenAIError: Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = OpenAI(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise OpenAIError(
openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7750, in exception_type
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 6568, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: OpenAIException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2592, in wrapper
    return litellm.completion_with_retries(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1940, in completion_with_retries
    return retryer(original_function, *args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\concurrent\futures\_base.py", line 451, in result
    return self.__get_result()
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\concurrent\futures\_base.py", line 403, in __get_result
    raise self._exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7750, in exception_type
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 6568, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: OpenAIException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 881, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 855, in completion
    response = openai_chat_completions.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 373, in completion
    raise openaiError(status_code=500, message=traceback.format_exc())
litellm.llms.openai.openaiError: Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7750, in exception_type
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 6568, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: openaiException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

[2024-03-02 19:21:16,516 DEBUG generators.py generate l.373] (2/10) Reuse post-processing
[2024-03-02 19:21:16,516 INFO generators.py gen_for_qa l.548] (2/10) * Start with LLM "gemini-pro"
[2024-03-02 19:21:16,517 DEBUG generators.py generate l.349] (2/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:21:16,517 DEBUG generators.py generate l.358] (2/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:23:03,189 INFO main.py <module> l.93] MAIN STARTS
[2024-03-02 19:23:03,192 INFO generators.py generate l.475] (1/10) *** AnsGenerator for question "Qui est considéré comme l'inventeur du télégraphe électrique ? A) Samuel Morse B) Charles Wheatstone C) Alexander Graham Bell D) Thomas Edison"
[2024-03-02 19:23:03,193 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "gpt-4"
[2024-03-02 19:23:03,194 DEBUG generators.py generate l.349] (1/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:23:03,195 DEBUG generators.py generate l.358] (1/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:23:03,298 ERROR generators.py complete l.400] (1/10) The following exception occurred with prompt meta={} user="\n        La question est 'Qui est considéré comme l'inventeur du télégraphe électrique ? A) Samuel Morse B) Charles Wheatstone C) Alexander Graham Bell D) Thomas Edison'" system='Tu es un expert qui doit répondre à des questions à l\'aide de paragraphes qui te sont fournis.\n        Tu dois utiliser uniquement ces paragraphes pour répondre aux questions.\n        Tu dois inclure les titres exacts de ces paragraphes dans la réponse que tu renvoies.\n        Tu dois justifier tes réponses et expliquer comment tu les as construites.\n        Tu dois détailler les phrases et les mots qui te permettent de générer ta réponse.\n\n        Les paragraphes sont présentés ainsi :\n        - Titre (Page X)\n        Contenu\n\n        La réponse générée doit indiquer clairement la source avec le Titre et la Page.\n\n        La réponse doit utiliser le format JSON suivant :\n        {\n        "q_ok": 0 ou 1,\n        "chunks_ok": 0 ou 1,\n        "answer": une chaîne de caractères contenant la réponse\n        }\n\n        Le champ "q_ok" vaut 0 si la question n\'est pas claire, 1 sinon.\n        Le champ "chunks_ok" vaut 0 si les paragraphes fournis ne permettent pas de répondre à la question, 1 sinon.\n        Le champ "answer" contient la réponse.'
OpenAIException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 881, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 855, in completion
    response = openai_chat_completions.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 373, in completion
    raise openaiError(status_code=500, message=traceback.format_exc())
litellm.llms.openai.openaiError: Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7750, in exception_type
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 6568, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: openaiException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = OpenAI(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise OpenAIError(
openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 881, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 855, in completion
    response = openai_chat_completions.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 373, in completion
    raise OpenAIError(status_code=500, message=traceback.format_exc())
litellm.llms.openai.OpenAIError: Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = OpenAI(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise OpenAIError(
openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7750, in exception_type
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 6568, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: OpenAIException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2592, in wrapper
    return litellm.completion_with_retries(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1940, in completion_with_retries
    return retryer(original_function, *args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\concurrent\futures\_base.py", line 451, in result
    return self.__get_result()
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\concurrent\futures\_base.py", line 403, in __get_result
    raise self._exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7750, in exception_type
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 6568, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: OpenAIException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 881, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 855, in completion
    response = openai_chat_completions.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 373, in completion
    raise openaiError(status_code=500, message=traceback.format_exc())
litellm.llms.openai.openaiError: Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7750, in exception_type
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 6568, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: openaiException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

[2024-03-02 19:23:03,323 DEBUG generators.py generate l.373] (1/10) Reuse post-processing
[2024-03-02 19:23:03,323 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-02 19:23:03,324 DEBUG generators.py generate l.349] (1/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:23:03,324 DEBUG generators.py generate l.358] (1/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:23:03,369 ERROR generators.py complete l.400] (1/10) The following exception occurred with prompt meta={} user="\n        La question est 'Qui est considéré comme l'inventeur du télégraphe électrique ? A) Samuel Morse B) Charles Wheatstone C) Alexander Graham Bell D) Thomas Edison'" system='Tu es un expert qui doit répondre à des questions à l\'aide de paragraphes qui te sont fournis.\n        Tu dois utiliser uniquement ces paragraphes pour répondre aux questions.\n        Tu dois inclure les titres exacts de ces paragraphes dans la réponse que tu renvoies.\n        Tu dois justifier tes réponses et expliquer comment tu les as construites.\n        Tu dois détailler les phrases et les mots qui te permettent de générer ta réponse.\n\n        Les paragraphes sont présentés ainsi :\n        - Titre (Page X)\n        Contenu\n\n        La réponse générée doit indiquer clairement la source avec le Titre et la Page.\n\n        La réponse doit utiliser le format JSON suivant :\n        {\n        "q_ok": 0 ou 1,\n        "chunks_ok": 0 ou 1,\n        "answer": une chaîne de caractères contenant la réponse\n        }\n\n        Le champ "q_ok" vaut 0 si la question n\'est pas claire, 1 sinon.\n        Le champ "chunks_ok" vaut 0 si les paragraphes fournis ne permettent pas de répondre à la question, 1 sinon.\n        Le champ "answer" contient la réponse.'
OpenAIException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 881, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 855, in completion
    response = openai_chat_completions.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 373, in completion
    raise openaiError(status_code=500, message=traceback.format_exc())
litellm.llms.openai.openaiError: Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7750, in exception_type
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 6568, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: openaiException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = OpenAI(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise OpenAIError(
openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 881, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 855, in completion
    response = openai_chat_completions.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 373, in completion
    raise OpenAIError(status_code=500, message=traceback.format_exc())
litellm.llms.openai.OpenAIError: Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = OpenAI(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise OpenAIError(
openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7750, in exception_type
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 6568, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: OpenAIException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2592, in wrapper
    return litellm.completion_with_retries(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1940, in completion_with_retries
    return retryer(original_function, *args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\concurrent\futures\_base.py", line 451, in result
    return self.__get_result()
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\concurrent\futures\_base.py", line 403, in __get_result
    raise self._exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7750, in exception_type
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 6568, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: OpenAIException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 881, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 855, in completion
    response = openai_chat_completions.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 373, in completion
    raise openaiError(status_code=500, message=traceback.format_exc())
litellm.llms.openai.openaiError: Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7750, in exception_type
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 6568, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: openaiException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

[2024-03-02 19:23:03,383 DEBUG generators.py generate l.373] (1/10) Reuse post-processing
[2024-03-02 19:23:03,386 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "gemini-pro"
[2024-03-02 19:23:03,387 DEBUG generators.py generate l.349] (1/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:23:03,388 DEBUG generators.py generate l.358] (1/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:23:15,623 DEBUG generators.py generate l.370] (1/10) Post-process Answer
[2024-03-02 19:23:16,184 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "claude-2.1"
[2024-03-02 19:23:16,184 DEBUG generators.py generate l.349] (1/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:23:16,184 DEBUG generators.py generate l.358] (1/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:23:16,232 ERROR generators.py complete l.400] (1/10) The following exception occurred with prompt meta={} user="\n        La question est 'Qui est considéré comme l'inventeur du télégraphe électrique ? A) Samuel Morse B) Charles Wheatstone C) Alexander Graham Bell D) Thomas Edison'" system='Tu es un expert qui doit répondre à des questions à l\'aide de paragraphes qui te sont fournis.\n        Tu dois utiliser uniquement ces paragraphes pour répondre aux questions.\n        Tu dois inclure les titres exacts de ces paragraphes dans la réponse que tu renvoies.\n        Tu dois justifier tes réponses et expliquer comment tu les as construites.\n        Tu dois détailler les phrases et les mots qui te permettent de générer ta réponse.\n\n        Les paragraphes sont présentés ainsi :\n        - Titre (Page X)\n        Contenu\n\n        La réponse générée doit indiquer clairement la source avec le Titre et la Page.\n\n        La réponse doit utiliser le format JSON suivant :\n        {\n        "q_ok": 0 ou 1,\n        "chunks_ok": 0 ou 1,\n        "answer": une chaîne de caractères contenant la réponse\n        }\n\n        Le champ "q_ok" vaut 0 si la question n\'est pas claire, 1 sinon.\n        Le champ "chunks_ok" vaut 0 si les paragraphes fournis ne permettent pas de répondre à la question, 1 sinon.\n        Le champ "answer" contient la réponse.'
Missing Anthropic API Key - A call is being made to anthropic but no key is set either in the environment variables or via params
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1028, in completion
    response = anthropic.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\anthropic.py", line 112, in completion
    headers = validate_environment(api_key, headers)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\anthropic.py", line 83, in validate_environment
    raise ValueError(
ValueError: Missing Anthropic API Key - A call is being made to anthropic but no key is set either in the environment variables or via params

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7750, in exception_type
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7725, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: Missing Anthropic API Key - A call is being made to anthropic but no key is set either in the environment variables or via params

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2592, in wrapper
    return litellm.completion_with_retries(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1940, in completion_with_retries
    return retryer(original_function, *args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\concurrent\futures\_base.py", line 451, in result
    return self.__get_result()
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\concurrent\futures\_base.py", line 403, in __get_result
    raise self._exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7750, in exception_type
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7725, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: Missing Anthropic API Key - A call is being made to anthropic but no key is set either in the environment variables or via params
[2024-03-02 19:23:16,239 DEBUG generators.py generate l.373] (1/10) Reuse post-processing
[2024-03-02 19:23:16,240 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-02 19:23:16,241 DEBUG generators.py generate l.349] (1/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:23:16,242 DEBUG generators.py generate l.358] (1/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:23:16,286 ERROR generators.py complete l.400] (1/10) The following exception occurred with prompt meta={} user="\n        La question est 'Qui est considéré comme l'inventeur du télégraphe électrique ? A) Samuel Morse B) Charles Wheatstone C) Alexander Graham Bell D) Thomas Edison'" system='Tu es un expert qui doit répondre à des questions à l\'aide de paragraphes qui te sont fournis.\n        Tu dois utiliser uniquement ces paragraphes pour répondre aux questions.\n        Tu dois inclure les titres exacts de ces paragraphes dans la réponse que tu renvoies.\n        Tu dois justifier tes réponses et expliquer comment tu les as construites.\n        Tu dois détailler les phrases et les mots qui te permettent de générer ta réponse.\n\n        Les paragraphes sont présentés ainsi :\n        - Titre (Page X)\n        Contenu\n\n        La réponse générée doit indiquer clairement la source avec le Titre et la Page.\n\n        La réponse doit utiliser le format JSON suivant :\n        {\n        "q_ok": 0 ou 1,\n        "chunks_ok": 0 ou 1,\n        "answer": une chaîne de caractères contenant la réponse\n        }\n\n        Le champ "q_ok" vaut 0 si la question n\'est pas claire, 1 sinon.\n        Le champ "chunks_ok" vaut 0 si les paragraphes fournis ne permettent pas de répondre à la question, 1 sinon.\n        Le champ "answer" contient la réponse.'
MistralException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\mistral.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\mistral.py", line 301, in completion
    mistral_client = mistral(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\mistral\_client.py", line 92, in __init__
    raise mistralError(
mistral.mistralError: The api_key client option must be set either by passing api_key to the client or by setting the MISTRAL_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 881, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 855, in completion
    response = mistral_chat_completions.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\mistral.py", line 373, in completion
    raise mistralError(status_code=500, message=traceback.format_exc())
litellm.llms.mistral.mistralError: Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\mistral.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\mistral.py", line 301, in completion
    mistral_client = mistral(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\mistral\_client.py", line 92, in __init__
    raise mistralError(
mistral.mistralError: The api_key client option must be set either by passing api_key to the client or by setting the MISTRAL_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7750, in exception_type
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 6568, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: MistralException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\mistral.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\mistral.py", line 301, in completion
    mistral_client = mistral(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\mistral\_client.py", line 92, in __init__
    raise mistralError(
mistral.mistralError: The api_key client option must be set either by passing api_key to the client or by setting the MISTRAL_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\mistral.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\mistral.py", line 301, in completion
    mistral_client = mistral(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\mistral\_client.py", line 92, in __init__
    raise mistralError(
mistral.mistralError: The api_key client option must be set either by passing api_key to the client or by setting the MISTRAL_API_KEY environment variable
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = OpenAI(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise OpenAIError(
openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 881, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 855, in completion
    response = openai_chat_completions.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 373, in completion
    raise OpenAIError(status_code=500, message=traceback.format_exc())
litellm.llms.openai.OpenAIError: Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = OpenAI(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise OpenAIError(
openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7750, in exception_type
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 6568, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: MistralException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\mistral.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\mistral.py", line 301, in completion
    mistral_client = mistral(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\mistral\_client.py", line 92, in __init__
    raise mistralError(
mistral.mistralError: The api_key client option must be set either by passing api_key to the client or by setting the MISTRAL_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2592, in wrapper
    return litellm.completion_with_retries(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1940, in completion_with_retries
    return retryer(original_function, *args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\concurrent\futures\_base.py", line 451, in result
    return self.__get_result()
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\concurrent\futures\_base.py", line 403, in __get_result
    raise self._exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7750, in exception_type
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 6568, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: MistralException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\mistral.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\mistral.py", line 301, in completion
    mistral_client = mistral(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\mistral\_client.py", line 92, in __init__
    raise mistralError(
mistral.mistralError: The api_key client option must be set either by passing api_key to the client or by setting the MISTRAL_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 881, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 855, in completion
    response = mistral_chat_completions.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\mistral.py", line 373, in completion
    raise mistralError(status_code=500, message=traceback.format_exc())
litellm.llms.mistral.mistralError: Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\mistral.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\mistral.py", line 301, in completion
    mistral_client = mistral(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\mistral\_client.py", line 92, in __init__
    raise mistralError(
mistral.mistralError: The api_key client option must be set either by passing api_key to the client or by setting the MISTRAL_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7750, in exception_type
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 6568, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: MistralException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\mistral.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\mistral.py", line 301, in completion
    mistral_client = mistral(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\mistral\_client.py", line 92, in __init__
    raise mistralError(
mistral.mistralError: The api_key client option must be set either by passing api_key to the client or by setting the MISTRAL_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\mistral.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\mistral.py", line 301, in completion
    mistral_client = mistral(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\mistral\_client.py", line 92, in __init__
    raise mistralError(
mistral.mistralError: The api_key client option must be set either by passing api_key to the client or by setting the MISTRAL_API_KEY environment variable

[2024-03-02 19:23:16,302 DEBUG generators.py generate l.373] (1/10) Reuse post-processing
[2024-03-02 19:23:16,302 INFO generators.py generate l.477] (1/10) End question "Qui est considéré comme l'inventeur du télégraphe électrique ? A) Samuel Morse B) Charles Wheatstone C) Alexander Graham Bell D) Thomas Edison"
[2024-03-02 19:23:16,303 INFO generators.py generate l.475] (2/10) *** AnsGenerator for question "Qui sont les pionniers du cinématographe ? A) Thomas Edison B) Les frères Lumière C) George Eastman D) William Friese-Greene"
[2024-03-02 19:23:16,303 INFO generators.py gen_for_qa l.548] (2/10) * Start with LLM "gpt-4"
[2024-03-02 19:23:16,304 DEBUG generators.py generate l.349] (2/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:23:16,304 DEBUG generators.py generate l.358] (2/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:23:16,353 ERROR generators.py complete l.400] (2/10) The following exception occurred with prompt meta={} user="\n        La question est 'Qui sont les pionniers du cinématographe ? A) Thomas Edison B) Les frères Lumière C) George Eastman D) William Friese-Greene'" system='Tu es un expert qui doit répondre à des questions à l\'aide de paragraphes qui te sont fournis.\n        Tu dois utiliser uniquement ces paragraphes pour répondre aux questions.\n        Tu dois inclure les titres exacts de ces paragraphes dans la réponse que tu renvoies.\n        Tu dois justifier tes réponses et expliquer comment tu les as construites.\n        Tu dois détailler les phrases et les mots qui te permettent de générer ta réponse.\n\n        Les paragraphes sont présentés ainsi :\n        - Titre (Page X)\n        Contenu\n\n        La réponse générée doit indiquer clairement la source avec le Titre et la Page.\n\n        La réponse doit utiliser le format JSON suivant :\n        {\n        "q_ok": 0 ou 1,\n        "chunks_ok": 0 ou 1,\n        "answer": une chaîne de caractères contenant la réponse\n        }\n\n        Le champ "q_ok" vaut 0 si la question n\'est pas claire, 1 sinon.\n        Le champ "chunks_ok" vaut 0 si les paragraphes fournis ne permettent pas de répondre à la question, 1 sinon.\n        Le champ "answer" contient la réponse.'
OpenAIException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 881, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 855, in completion
    response = openai_chat_completions.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 373, in completion
    raise openaiError(status_code=500, message=traceback.format_exc())
litellm.llms.openai.openaiError: Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7750, in exception_type
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 6568, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: openaiException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = OpenAI(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise OpenAIError(
openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 881, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 855, in completion
    response = openai_chat_completions.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 373, in completion
    raise OpenAIError(status_code=500, message=traceback.format_exc())
litellm.llms.openai.OpenAIError: Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = OpenAI(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise OpenAIError(
openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7750, in exception_type
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 6568, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: OpenAIException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2592, in wrapper
    return litellm.completion_with_retries(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1940, in completion_with_retries
    return retryer(original_function, *args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\concurrent\futures\_base.py", line 451, in result
    return self.__get_result()
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\concurrent\futures\_base.py", line 403, in __get_result
    raise self._exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7750, in exception_type
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 6568, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: OpenAIException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 881, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 855, in completion
    response = openai_chat_completions.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 373, in completion
    raise openaiError(status_code=500, message=traceback.format_exc())
litellm.llms.openai.openaiError: Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7750, in exception_type
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 6568, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: openaiException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

[2024-03-02 19:23:16,368 DEBUG generators.py generate l.373] (2/10) Reuse post-processing
[2024-03-02 19:23:16,368 INFO generators.py gen_for_qa l.548] (2/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-02 19:23:16,369 DEBUG generators.py generate l.349] (2/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:23:16,369 DEBUG generators.py generate l.358] (2/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:23:16,416 ERROR generators.py complete l.400] (2/10) The following exception occurred with prompt meta={} user="\n        La question est 'Qui sont les pionniers du cinématographe ? A) Thomas Edison B) Les frères Lumière C) George Eastman D) William Friese-Greene'" system='Tu es un expert qui doit répondre à des questions à l\'aide de paragraphes qui te sont fournis.\n        Tu dois utiliser uniquement ces paragraphes pour répondre aux questions.\n        Tu dois inclure les titres exacts de ces paragraphes dans la réponse que tu renvoies.\n        Tu dois justifier tes réponses et expliquer comment tu les as construites.\n        Tu dois détailler les phrases et les mots qui te permettent de générer ta réponse.\n\n        Les paragraphes sont présentés ainsi :\n        - Titre (Page X)\n        Contenu\n\n        La réponse générée doit indiquer clairement la source avec le Titre et la Page.\n\n        La réponse doit utiliser le format JSON suivant :\n        {\n        "q_ok": 0 ou 1,\n        "chunks_ok": 0 ou 1,\n        "answer": une chaîne de caractères contenant la réponse\n        }\n\n        Le champ "q_ok" vaut 0 si la question n\'est pas claire, 1 sinon.\n        Le champ "chunks_ok" vaut 0 si les paragraphes fournis ne permettent pas de répondre à la question, 1 sinon.\n        Le champ "answer" contient la réponse.'
OpenAIException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 881, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 855, in completion
    response = openai_chat_completions.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 373, in completion
    raise openaiError(status_code=500, message=traceback.format_exc())
litellm.llms.openai.openaiError: Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7750, in exception_type
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 6568, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: openaiException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = OpenAI(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise OpenAIError(
openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 881, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 855, in completion
    response = openai_chat_completions.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 373, in completion
    raise OpenAIError(status_code=500, message=traceback.format_exc())
litellm.llms.openai.OpenAIError: Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = OpenAI(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise OpenAIError(
openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7750, in exception_type
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 6568, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: OpenAIException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2592, in wrapper
    return litellm.completion_with_retries(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1940, in completion_with_retries
    return retryer(original_function, *args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\concurrent\futures\_base.py", line 451, in result
    return self.__get_result()
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\concurrent\futures\_base.py", line 403, in __get_result
    raise self._exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tenacity\__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7750, in exception_type
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 6568, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: OpenAIException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 881, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 855, in completion
    response = openai_chat_completions.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 373, in completion
    raise openaiError(status_code=500, message=traceback.format_exc())
litellm.llms.openai.openaiError: Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7750, in exception_type
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 6568, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: openaiException - Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 365, in completion
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\openai.py", line 301, in completion
    openai_client = openai(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_client.py", line 92, in __init__
    raise openaiError(
openai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

[2024-03-02 19:23:16,433 DEBUG generators.py generate l.373] (2/10) Reuse post-processing
[2024-03-02 19:23:16,433 INFO generators.py gen_for_qa l.548] (2/10) * Start with LLM "gemini-pro"
[2024-03-02 19:23:16,433 DEBUG generators.py generate l.349] (2/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:23:16,435 DEBUG generators.py generate l.358] (2/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:25:13,569 INFO main.py <module> l.94] MAIN STARTS
[2024-03-02 19:25:13,581 INFO generators.py generate l.475] (1/10) *** AnsGenerator for question "Qui est considéré comme l'inventeur du télégraphe électrique ? A) Samuel Morse B) Charles Wheatstone C) Alexander Graham Bell D) Thomas Edison"
[2024-03-02 19:25:13,585 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "gpt-4"
[2024-03-02 19:25:13,585 DEBUG generators.py generate l.349] (1/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:25:13,587 DEBUG generators.py generate l.358] (1/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:25:30,098 DEBUG generators.py generate l.370] (1/10) Post-process Answer
[2024-03-02 19:25:30,588 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-02 19:25:30,588 DEBUG generators.py generate l.349] (1/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:25:30,590 DEBUG generators.py generate l.358] (1/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:25:35,324 DEBUG generators.py generate l.370] (1/10) Post-process Answer
[2024-03-02 19:25:35,429 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "gemini-pro"
[2024-03-02 19:25:35,430 DEBUG generators.py generate l.349] (1/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:25:35,432 DEBUG generators.py generate l.358] (1/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:25:47,373 DEBUG generators.py generate l.370] (1/10) Post-process Answer
[2024-03-02 19:25:47,400 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "claude-2.1"
[2024-03-02 19:25:47,406 DEBUG generators.py generate l.349] (1/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:25:47,408 DEBUG generators.py generate l.358] (1/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:26:01,115 DEBUG generators.py generate l.370] (1/10) Post-process Answer
[2024-03-02 19:26:01,150 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-02 19:26:01,152 DEBUG generators.py generate l.349] (1/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:26:01,153 DEBUG generators.py generate l.358] (1/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:26:07,750 DEBUG generators.py generate l.370] (1/10) Post-process Answer
[2024-03-02 19:26:07,768 INFO generators.py generate l.477] (1/10) End question "Qui est considéré comme l'inventeur du télégraphe électrique ? A) Samuel Morse B) Charles Wheatstone C) Alexander Graham Bell D) Thomas Edison"
[2024-03-02 19:26:07,769 INFO generators.py generate l.475] (2/10) *** AnsGenerator for question "Qui sont les pionniers du cinématographe ? A) Thomas Edison B) Les frères Lumière C) George Eastman D) William Friese-Greene"
[2024-03-02 19:26:07,770 INFO generators.py gen_for_qa l.548] (2/10) * Start with LLM "gpt-4"
[2024-03-02 19:26:07,771 DEBUG generators.py generate l.349] (2/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:26:07,773 DEBUG generators.py generate l.358] (2/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:26:35,839 DEBUG generators.py generate l.370] (2/10) Post-process Answer
[2024-03-02 19:26:35,891 INFO generators.py gen_for_qa l.548] (2/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-02 19:26:35,893 DEBUG generators.py generate l.349] (2/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:26:35,894 DEBUG generators.py generate l.358] (2/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:26:38,733 DEBUG generators.py generate l.370] (2/10) Post-process Answer
[2024-03-02 19:26:38,775 INFO generators.py gen_for_qa l.548] (2/10) * Start with LLM "gemini-pro"
[2024-03-02 19:26:38,788 DEBUG generators.py generate l.349] (2/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:26:38,789 DEBUG generators.py generate l.358] (2/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:26:45,381 DEBUG generators.py generate l.370] (2/10) Post-process Answer
[2024-03-02 19:26:45,396 INFO generators.py gen_for_qa l.548] (2/10) * Start with LLM "claude-2.1"
[2024-03-02 19:26:45,402 DEBUG generators.py generate l.349] (2/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:26:45,404 DEBUG generators.py generate l.358] (2/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:26:56,368 DEBUG generators.py generate l.370] (2/10) Post-process Answer
[2024-03-02 19:26:56,403 INFO generators.py gen_for_qa l.548] (2/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-02 19:26:56,403 DEBUG generators.py generate l.349] (2/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:26:56,406 DEBUG generators.py generate l.358] (2/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:27:01,540 DEBUG generators.py generate l.370] (2/10) Post-process Answer
[2024-03-02 19:27:01,561 INFO generators.py generate l.477] (2/10) End question "Qui sont les pionniers du cinématographe ? A) Thomas Edison B) Les frères Lumière C) George Eastman D) William Friese-Greene"
[2024-03-02 19:27:01,563 INFO generators.py generate l.475] (3/10) *** AnsGenerator for question "Qui a apporté des contributions cruciales au développement du moteur à vapeur ? A) James Watt B) Thomas Newcomen C) Denis Papin"
[2024-03-02 19:27:01,565 INFO generators.py gen_for_qa l.548] (3/10) * Start with LLM "gpt-4"
[2024-03-02 19:27:01,565 DEBUG generators.py generate l.349] (3/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:27:01,567 DEBUG generators.py generate l.358] (3/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:27:22,713 DEBUG generators.py generate l.370] (3/10) Post-process Answer
[2024-03-02 19:27:22,823 INFO generators.py gen_for_qa l.548] (3/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-02 19:27:22,823 DEBUG generators.py generate l.349] (3/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:27:22,823 DEBUG generators.py generate l.358] (3/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:27:28,839 DEBUG generators.py generate l.370] (3/10) Post-process Answer
[2024-03-02 19:27:28,905 INFO generators.py gen_for_qa l.548] (3/10) * Start with LLM "gemini-pro"
[2024-03-02 19:27:28,920 DEBUG generators.py generate l.349] (3/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:27:28,920 DEBUG generators.py generate l.358] (3/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:27:33,588 DEBUG generators.py generate l.370] (3/10) Post-process Answer
[2024-03-02 19:27:33,654 INFO generators.py gen_for_qa l.548] (3/10) * Start with LLM "claude-2.1"
[2024-03-02 19:27:33,669 DEBUG generators.py generate l.349] (3/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:27:33,670 DEBUG generators.py generate l.358] (3/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:27:44,603 DEBUG generators.py generate l.370] (3/10) Post-process Answer
[2024-03-02 19:27:44,685 INFO generators.py gen_for_qa l.548] (3/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-02 19:27:44,685 DEBUG generators.py generate l.349] (3/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:27:44,685 DEBUG generators.py generate l.358] (3/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:27:52,532 DEBUG generators.py generate l.370] (3/10) Post-process Answer
[2024-03-02 19:27:52,599 INFO generators.py generate l.477] (3/10) End question "Qui a apporté des contributions cruciales au développement du moteur à vapeur ? A) James Watt B) Thomas Newcomen C) Denis Papin"
[2024-03-02 19:27:52,599 INFO generators.py generate l.475] (4/10) *** AnsGenerator for question "Qui est reconnu pour avoir inventé le stéthoscope ? A) René Laennec B) Hippocrate C) Antonie van Leeuwenhoek D) Edward Jenner"
[2024-03-02 19:27:52,599 INFO generators.py gen_for_qa l.548] (4/10) * Start with LLM "gpt-4"
[2024-03-02 19:27:52,599 DEBUG generators.py generate l.349] (4/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:27:52,599 DEBUG generators.py generate l.358] (4/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:28:05,592 DEBUG generators.py generate l.370] (4/10) Post-process Answer
[2024-03-02 19:28:05,660 INFO generators.py gen_for_qa l.548] (4/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-02 19:28:05,660 DEBUG generators.py generate l.349] (4/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:28:05,660 DEBUG generators.py generate l.358] (4/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:28:07,931 DEBUG generators.py generate l.370] (4/10) Post-process Answer
[2024-03-02 19:28:07,992 INFO generators.py gen_for_qa l.548] (4/10) * Start with LLM "gemini-pro"
[2024-03-02 19:28:07,992 DEBUG generators.py generate l.349] (4/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:28:07,992 DEBUG generators.py generate l.358] (4/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:28:11,707 DEBUG generators.py generate l.370] (4/10) Post-process Answer
[2024-03-02 19:28:11,742 INFO generators.py gen_for_qa l.548] (4/10) * Start with LLM "claude-2.1"
[2024-03-02 19:28:11,742 DEBUG generators.py generate l.349] (4/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:28:11,758 DEBUG generators.py generate l.358] (4/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:28:21,462 DEBUG generators.py generate l.370] (4/10) Post-process Answer
[2024-03-02 19:28:21,538 INFO generators.py gen_for_qa l.548] (4/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-02 19:28:21,538 DEBUG generators.py generate l.349] (4/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:28:21,538 DEBUG generators.py generate l.358] (4/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:28:29,073 DEBUG generators.py generate l.370] (4/10) Post-process Answer
[2024-03-02 19:28:29,130 INFO generators.py generate l.477] (4/10) End question "Qui est reconnu pour avoir inventé le stéthoscope ? A) René Laennec B) Hippocrate C) Antonie van Leeuwenhoek D) Edward Jenner"
[2024-03-02 19:28:29,130 INFO generators.py generate l.475] (5/10) *** AnsGenerator for question "Quel scientifique est crédité pour l'invention de la photographie ? A) Nicéphore Niépce B) Louis Daguerre C) William Henry Fox Talbot"
[2024-03-02 19:28:29,134 INFO generators.py gen_for_qa l.548] (5/10) * Start with LLM "gpt-4"
[2024-03-02 19:28:29,136 DEBUG generators.py generate l.349] (5/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:28:29,138 DEBUG generators.py generate l.358] (5/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:28:40,467 DEBUG generators.py generate l.370] (5/10) Post-process Answer
[2024-03-02 19:28:40,514 INFO generators.py gen_for_qa l.548] (5/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-02 19:28:40,514 DEBUG generators.py generate l.349] (5/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:28:40,514 DEBUG generators.py generate l.358] (5/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:28:44,142 DEBUG generators.py generate l.370] (5/10) Post-process Answer
[2024-03-02 19:28:44,180 INFO generators.py gen_for_qa l.548] (5/10) * Start with LLM "gemini-pro"
[2024-03-02 19:28:44,180 DEBUG generators.py generate l.349] (5/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:28:44,195 DEBUG generators.py generate l.358] (5/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:28:49,146 DEBUG generators.py generate l.370] (5/10) Post-process Answer
[2024-03-02 19:28:49,192 INFO generators.py gen_for_qa l.548] (5/10) * Start with LLM "claude-2.1"
[2024-03-02 19:28:49,195 DEBUG generators.py generate l.349] (5/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:28:49,197 DEBUG generators.py generate l.358] (5/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:29:03,069 DEBUG generators.py generate l.370] (5/10) Post-process Answer
[2024-03-02 19:29:03,126 INFO generators.py gen_for_qa l.548] (5/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-02 19:29:03,126 DEBUG generators.py generate l.349] (5/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:29:03,137 DEBUG generators.py generate l.358] (5/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:29:13,521 DEBUG generators.py generate l.370] (5/10) Post-process Answer
[2024-03-02 19:29:13,606 INFO generators.py generate l.477] (5/10) End question "Quel scientifique est crédité pour l'invention de la photographie ? A) Nicéphore Niépce B) Louis Daguerre C) William Henry Fox Talbot"
[2024-03-02 19:29:13,606 INFO generators.py generate l.475] (6/10) *** AnsGenerator for question "Qui est associé à l'invention de la méthode champenoise pour la production de champagne ? A) Dom Pérignon B) Louis Pasteur C) Veuve Clicquot D) John Pemberton"
[2024-03-02 19:29:13,606 INFO generators.py gen_for_qa l.548] (6/10) * Start with LLM "gpt-4"
[2024-03-02 19:29:13,606 DEBUG generators.py generate l.349] (6/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:29:13,606 DEBUG generators.py generate l.358] (6/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:29:38,595 DEBUG generators.py generate l.370] (6/10) Post-process Answer
[2024-03-02 19:29:38,711 INFO generators.py gen_for_qa l.548] (6/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-02 19:29:38,711 DEBUG generators.py generate l.349] (6/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:29:38,711 DEBUG generators.py generate l.358] (6/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:29:42,369 DEBUG generators.py generate l.370] (6/10) Post-process Answer
[2024-03-02 19:29:42,476 INFO generators.py gen_for_qa l.548] (6/10) * Start with LLM "gemini-pro"
[2024-03-02 19:29:42,476 DEBUG generators.py generate l.349] (6/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:29:42,476 DEBUG generators.py generate l.358] (6/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:29:55,187 DEBUG generators.py generate l.370] (6/10) Post-process Answer
[2024-03-02 19:29:55,221 INFO generators.py gen_for_qa l.548] (6/10) * Start with LLM "claude-2.1"
[2024-03-02 19:29:55,221 DEBUG generators.py generate l.349] (6/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:29:55,223 DEBUG generators.py generate l.358] (6/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:30:06,303 DEBUG generators.py generate l.370] (6/10) Post-process Answer
[2024-03-02 19:30:06,336 INFO generators.py gen_for_qa l.548] (6/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-02 19:30:06,336 DEBUG generators.py generate l.349] (6/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:30:06,336 DEBUG generators.py generate l.358] (6/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:30:13,405 DEBUG generators.py generate l.370] (6/10) Post-process Answer
[2024-03-02 19:30:13,432 INFO generators.py generate l.477] (6/10) End question "Qui est associé à l'invention de la méthode champenoise pour la production de champagne ? A) Dom Pérignon B) Louis Pasteur C) Veuve Clicquot D) John Pemberton"
[2024-03-02 19:30:13,432 INFO generators.py generate l.475] (7/10) *** AnsGenerator for question "La découverte du calcul infinitésimal est attribuée à : A) Isaac Newton B) Gottfried Wilhelm Leibniz C) Albert Einstein"
[2024-03-02 19:30:13,433 INFO generators.py gen_for_qa l.548] (7/10) * Start with LLM "gpt-4"
[2024-03-02 19:30:13,434 DEBUG generators.py generate l.349] (7/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:30:13,435 DEBUG generators.py generate l.358] (7/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:30:35,894 DEBUG generators.py generate l.370] (7/10) Post-process Answer
[2024-03-02 19:30:35,958 INFO generators.py gen_for_qa l.548] (7/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-02 19:30:35,958 DEBUG generators.py generate l.349] (7/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:30:35,958 DEBUG generators.py generate l.358] (7/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:30:40,506 DEBUG generators.py generate l.370] (7/10) Post-process Answer
[2024-03-02 19:30:40,585 INFO generators.py gen_for_qa l.548] (7/10) * Start with LLM "gemini-pro"
[2024-03-02 19:30:40,601 DEBUG generators.py generate l.349] (7/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:30:40,601 DEBUG generators.py generate l.358] (7/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:30:45,048 DEBUG generators.py generate l.370] (7/10) Post-process Answer
[2024-03-02 19:30:45,079 INFO generators.py gen_for_qa l.548] (7/10) * Start with LLM "claude-2.1"
[2024-03-02 19:30:45,081 DEBUG generators.py generate l.349] (7/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:30:45,081 DEBUG generators.py generate l.358] (7/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:30:55,238 DEBUG generators.py generate l.370] (7/10) Post-process Answer
[2024-03-02 19:30:55,265 INFO generators.py gen_for_qa l.548] (7/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-02 19:30:55,267 DEBUG generators.py generate l.349] (7/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:30:55,268 DEBUG generators.py generate l.358] (7/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:31:02,212 DEBUG generators.py generate l.370] (7/10) Post-process Answer
[2024-03-02 19:31:25,604 INFO expe.py save_to_json l.283] (7/10) Expe saved as JSON to expe\Questions\Stopped_at_7_of_10_culture--10Q_0C_0F_5M_35A_0HE_0AE_2024-03-02_19,31,25.json
[2024-03-02 19:36:55,245 INFO main.py <module> l.109] MAIN STARTS
[2024-03-02 19:36:55,249 INFO generators.py generate l.475] (1/10) *** AnsGenerator for question "Qui est considéré comme l'inventeur du télégraphe électrique ? A) Samuel Morse B) Charles Wheatstone C) Alexander Graham Bell D) Thomas Edison"
[2024-03-02 19:36:55,249 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "gpt-4"
[2024-03-02 19:36:55,249 DEBUG generators.py generate l.349] (1/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:36:55,249 DEBUG generators.py generate l.358] (1/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:36:59,307 DEBUG generators.py generate l.370] (1/10) Post-process Answer
[2024-03-02 19:36:59,309 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-02 19:36:59,311 DEBUG generators.py generate l.349] (1/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:36:59,312 DEBUG generators.py generate l.358] (1/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:37:03,473 DEBUG generators.py generate l.370] (1/10) Post-process Answer
[2024-03-02 19:37:03,480 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "gemini-pro"
[2024-03-02 19:37:03,482 DEBUG generators.py generate l.349] (1/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:37:03,483 DEBUG generators.py generate l.358] (1/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:37:15,777 DEBUG generators.py generate l.370] (1/10) Post-process Answer
[2024-03-02 19:37:15,781 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "claude-2.1"
[2024-03-02 19:37:15,785 DEBUG generators.py generate l.349] (1/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:37:15,789 DEBUG generators.py generate l.358] (1/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:37:18,720 DEBUG generators.py generate l.370] (1/10) Post-process Answer
[2024-03-02 19:37:18,722 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-02 19:37:18,722 DEBUG generators.py generate l.349] (1/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:37:18,724 DEBUG generators.py generate l.358] (1/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:37:20,561 DEBUG generators.py generate l.370] (1/10) Post-process Answer
[2024-03-02 19:45:23,042 INFO generators.py generate l.477] (1/10) End question "Qui est considéré comme l'inventeur du télégraphe électrique ? A) Samuel Morse B) Charles Wheatstone C) Alexander Graham Bell D) Thomas Edison"
[2024-03-02 19:45:23,046 INFO generators.py generate l.475] (2/10) *** AnsGenerator for question "Qui sont les pionniers du cinématographe ? A) Thomas Edison B) Les frères Lumière C) George Eastman D) William Friese-Greene"
[2024-03-02 19:45:23,046 INFO generators.py gen_for_qa l.548] (2/10) * Start with LLM "gpt-4"
[2024-03-02 19:45:23,046 DEBUG generators.py generate l.349] (2/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:45:23,057 DEBUG generators.py generate l.358] (2/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:45:28,102 DEBUG generators.py generate l.370] (2/10) Post-process Answer
[2024-03-02 19:45:28,104 INFO generators.py gen_for_qa l.548] (2/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-02 19:45:28,107 DEBUG generators.py generate l.349] (2/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:45:28,108 DEBUG generators.py generate l.358] (2/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:45:32,935 DEBUG generators.py generate l.370] (2/10) Post-process Answer
[2024-03-02 19:45:32,935 INFO generators.py gen_for_qa l.548] (2/10) * Start with LLM "gemini-pro"
[2024-03-02 19:45:32,950 DEBUG generators.py generate l.349] (2/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:45:32,953 DEBUG generators.py generate l.358] (2/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:45:37,935 DEBUG generators.py generate l.370] (2/10) Post-process Answer
[2024-03-02 19:45:37,947 INFO generators.py gen_for_qa l.548] (2/10) * Start with LLM "claude-2.1"
[2024-03-02 19:45:37,950 DEBUG generators.py generate l.349] (2/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:45:37,955 DEBUG generators.py generate l.358] (2/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:45:40,483 DEBUG generators.py generate l.370] (2/10) Post-process Answer
[2024-03-02 19:45:40,499 INFO generators.py gen_for_qa l.548] (2/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-02 19:45:40,499 DEBUG generators.py generate l.349] (2/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:45:40,499 DEBUG generators.py generate l.358] (2/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:45:44,022 DEBUG generators.py generate l.370] (2/10) Post-process Answer
[2024-03-02 19:45:44,027 INFO generators.py generate l.477] (2/10) End question "Qui sont les pionniers du cinématographe ? A) Thomas Edison B) Les frères Lumière C) George Eastman D) William Friese-Greene"
[2024-03-02 19:45:44,028 INFO generators.py generate l.475] (3/10) *** AnsGenerator for question "Qui a apporté des contributions cruciales au développement du moteur à vapeur ? A) James Watt B) Thomas Newcomen C) Denis Papin"
[2024-03-02 19:45:44,028 INFO generators.py gen_for_qa l.548] (3/10) * Start with LLM "gpt-4"
[2024-03-02 19:45:44,033 DEBUG generators.py generate l.349] (3/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:45:44,033 DEBUG generators.py generate l.358] (3/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:45:47,166 DEBUG generators.py generate l.370] (3/10) Post-process Answer
[2024-03-02 19:45:47,166 INFO generators.py gen_for_qa l.548] (3/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-02 19:45:47,181 DEBUG generators.py generate l.349] (3/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:45:47,181 DEBUG generators.py generate l.358] (3/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:45:53,801 DEBUG generators.py generate l.370] (3/10) Post-process Answer
[2024-03-02 19:45:53,801 INFO generators.py gen_for_qa l.548] (3/10) * Start with LLM "gemini-pro"
[2024-03-02 19:45:53,817 DEBUG generators.py generate l.349] (3/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:45:53,817 DEBUG generators.py generate l.358] (3/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:45:57,096 DEBUG generators.py generate l.370] (3/10) Post-process Answer
[2024-03-02 19:45:57,116 INFO generators.py gen_for_qa l.548] (3/10) * Start with LLM "claude-2.1"
[2024-03-02 19:45:57,116 DEBUG generators.py generate l.349] (3/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:45:57,116 DEBUG generators.py generate l.358] (3/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:45:58,669 DEBUG generators.py generate l.370] (3/10) Post-process Answer
[2024-03-02 19:45:58,669 INFO generators.py gen_for_qa l.548] (3/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-02 19:45:58,669 DEBUG generators.py generate l.349] (3/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:45:58,682 DEBUG generators.py generate l.358] (3/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:46:02,511 DEBUG generators.py generate l.370] (3/10) Post-process Answer
[2024-03-02 19:46:02,511 INFO generators.py generate l.477] (3/10) End question "Qui a apporté des contributions cruciales au développement du moteur à vapeur ? A) James Watt B) Thomas Newcomen C) Denis Papin"
[2024-03-02 19:46:02,511 INFO generators.py generate l.475] (4/10) *** AnsGenerator for question "Qui est reconnu pour avoir inventé le stéthoscope ? A) René Laennec B) Hippocrate C) Antonie van Leeuwenhoek D) Edward Jenner"
[2024-03-02 19:46:02,522 INFO generators.py gen_for_qa l.548] (4/10) * Start with LLM "gpt-4"
[2024-03-02 19:46:02,522 DEBUG generators.py generate l.349] (4/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:46:02,522 DEBUG generators.py generate l.358] (4/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:46:07,197 DEBUG generators.py generate l.370] (4/10) Post-process Answer
[2024-03-02 19:46:07,199 INFO generators.py gen_for_qa l.548] (4/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-02 19:46:07,201 DEBUG generators.py generate l.349] (4/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:46:07,204 DEBUG generators.py generate l.358] (4/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:46:11,265 DEBUG generators.py generate l.370] (4/10) Post-process Answer
[2024-03-02 19:46:11,267 INFO generators.py gen_for_qa l.548] (4/10) * Start with LLM "gemini-pro"
[2024-03-02 19:46:11,271 DEBUG generators.py generate l.349] (4/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:46:11,272 DEBUG generators.py generate l.358] (4/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:46:15,416 DEBUG generators.py generate l.370] (4/10) Post-process Answer
[2024-03-02 19:46:15,418 INFO generators.py gen_for_qa l.548] (4/10) * Start with LLM "claude-2.1"
[2024-03-02 19:46:15,420 DEBUG generators.py generate l.349] (4/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:46:15,420 DEBUG generators.py generate l.358] (4/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:46:16,729 DEBUG generators.py generate l.370] (4/10) Post-process Answer
[2024-03-02 19:46:16,740 INFO generators.py gen_for_qa l.548] (4/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-02 19:46:16,744 DEBUG generators.py generate l.349] (4/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:46:16,745 DEBUG generators.py generate l.358] (4/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:46:17,685 DEBUG generators.py generate l.370] (4/10) Post-process Answer
[2024-03-02 19:46:17,688 INFO generators.py generate l.477] (4/10) End question "Qui est reconnu pour avoir inventé le stéthoscope ? A) René Laennec B) Hippocrate C) Antonie van Leeuwenhoek D) Edward Jenner"
[2024-03-02 19:46:17,691 INFO generators.py generate l.475] (5/10) *** AnsGenerator for question "Quel scientifique est crédité pour l'invention de la photographie ? A) Nicéphore Niépce B) Louis Daguerre C) William Henry Fox Talbot"
[2024-03-02 19:46:17,693 INFO generators.py gen_for_qa l.548] (5/10) * Start with LLM "gpt-4"
[2024-03-02 19:46:17,695 DEBUG generators.py generate l.349] (5/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:46:17,696 DEBUG generators.py generate l.358] (5/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:46:21,265 DEBUG generators.py generate l.370] (5/10) Post-process Answer
[2024-03-02 19:46:21,269 INFO generators.py gen_for_qa l.548] (5/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-02 19:46:21,272 DEBUG generators.py generate l.349] (5/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:46:21,274 DEBUG generators.py generate l.358] (5/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:46:26,138 DEBUG generators.py generate l.370] (5/10) Post-process Answer
[2024-03-02 19:46:26,140 INFO generators.py gen_for_qa l.548] (5/10) * Start with LLM "gemini-pro"
[2024-03-02 19:46:26,145 DEBUG generators.py generate l.349] (5/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:46:26,148 DEBUG generators.py generate l.358] (5/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:46:29,055 DEBUG generators.py generate l.370] (5/10) Post-process Answer
[2024-03-02 19:46:29,057 INFO generators.py gen_for_qa l.548] (5/10) * Start with LLM "claude-2.1"
[2024-03-02 19:46:29,058 DEBUG generators.py generate l.349] (5/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:46:29,058 DEBUG generators.py generate l.358] (5/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:46:30,652 DEBUG generators.py generate l.370] (5/10) Post-process Answer
[2024-03-02 19:46:30,655 INFO generators.py gen_for_qa l.548] (5/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-02 19:46:30,659 DEBUG generators.py generate l.349] (5/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:46:30,662 DEBUG generators.py generate l.358] (5/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:46:31,629 DEBUG generators.py generate l.370] (5/10) Post-process Answer
[2024-03-02 19:46:31,633 INFO generators.py generate l.477] (5/10) End question "Quel scientifique est crédité pour l'invention de la photographie ? A) Nicéphore Niépce B) Louis Daguerre C) William Henry Fox Talbot"
[2024-03-02 19:46:31,635 INFO generators.py generate l.475] (6/10) *** AnsGenerator for question "Qui est associé à l'invention de la méthode champenoise pour la production de champagne ? A) Dom Pérignon B) Louis Pasteur C) Veuve Clicquot D) John Pemberton"
[2024-03-02 19:46:31,636 INFO generators.py gen_for_qa l.548] (6/10) * Start with LLM "gpt-4"
[2024-03-02 19:46:31,638 DEBUG generators.py generate l.349] (6/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:46:31,639 DEBUG generators.py generate l.358] (6/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:46:35,415 DEBUG generators.py generate l.370] (6/10) Post-process Answer
[2024-03-02 19:46:35,420 INFO generators.py gen_for_qa l.548] (6/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-02 19:46:35,422 DEBUG generators.py generate l.349] (6/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:46:35,423 DEBUG generators.py generate l.358] (6/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:46:42,375 DEBUG generators.py generate l.370] (6/10) Post-process Answer
[2024-03-02 19:46:42,378 INFO generators.py gen_for_qa l.548] (6/10) * Start with LLM "gemini-pro"
[2024-03-02 19:46:42,379 DEBUG generators.py generate l.349] (6/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:46:42,380 DEBUG generators.py generate l.358] (6/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:46:45,389 DEBUG generators.py generate l.370] (6/10) Post-process Answer
[2024-03-02 19:46:45,389 INFO generators.py gen_for_qa l.548] (6/10) * Start with LLM "claude-2.1"
[2024-03-02 19:46:45,391 DEBUG generators.py generate l.349] (6/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:46:45,392 DEBUG generators.py generate l.358] (6/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:46:46,677 DEBUG generators.py generate l.370] (6/10) Post-process Answer
[2024-03-02 19:46:46,678 INFO generators.py gen_for_qa l.548] (6/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-02 19:46:46,686 DEBUG generators.py generate l.349] (6/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:46:46,688 DEBUG generators.py generate l.358] (6/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:46:47,418 DEBUG generators.py generate l.370] (6/10) Post-process Answer
[2024-03-02 19:46:47,425 INFO generators.py generate l.477] (6/10) End question "Qui est associé à l'invention de la méthode champenoise pour la production de champagne ? A) Dom Pérignon B) Louis Pasteur C) Veuve Clicquot D) John Pemberton"
[2024-03-02 19:46:47,425 INFO generators.py generate l.475] (7/10) *** AnsGenerator for question "La découverte du calcul infinitésimal est attribuée à : A) Isaac Newton B) Gottfried Wilhelm Leibniz C) Albert Einstein"
[2024-03-02 19:46:47,427 INFO generators.py gen_for_qa l.548] (7/10) * Start with LLM "gpt-4"
[2024-03-02 19:46:47,429 DEBUG generators.py generate l.349] (7/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:46:47,430 DEBUG generators.py generate l.358] (7/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:46:52,395 DEBUG generators.py generate l.370] (7/10) Post-process Answer
[2024-03-02 19:46:52,396 INFO generators.py gen_for_qa l.548] (7/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-02 19:46:52,396 DEBUG generators.py generate l.349] (7/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:46:52,401 DEBUG generators.py generate l.358] (7/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:46:58,030 DEBUG generators.py generate l.370] (7/10) Post-process Answer
[2024-03-02 19:46:58,039 INFO generators.py gen_for_qa l.548] (7/10) * Start with LLM "gemini-pro"
[2024-03-02 19:46:58,042 DEBUG generators.py generate l.349] (7/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:46:58,043 DEBUG generators.py generate l.358] (7/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:47:01,151 DEBUG generators.py generate l.370] (7/10) Post-process Answer
[2024-03-02 19:47:01,152 INFO generators.py gen_for_qa l.548] (7/10) * Start with LLM "claude-2.1"
[2024-03-02 19:47:01,152 DEBUG generators.py generate l.349] (7/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:47:01,156 DEBUG generators.py generate l.358] (7/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:47:02,462 DEBUG generators.py generate l.370] (7/10) Post-process Answer
[2024-03-02 19:47:02,463 INFO generators.py gen_for_qa l.548] (7/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-02 19:47:02,465 DEBUG generators.py generate l.349] (7/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:47:02,465 DEBUG generators.py generate l.358] (7/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:47:05,071 DEBUG generators.py generate l.370] (7/10) Post-process Answer
[2024-03-02 19:47:05,075 INFO generators.py generate l.477] (7/10) End question "La découverte du calcul infinitésimal est attribuée à : A) Isaac Newton B) Gottfried Wilhelm Leibniz C) Albert Einstein"
[2024-03-02 19:47:05,077 INFO generators.py generate l.475] (8/10) *** AnsGenerator for question "Qui est considéré comme l'un des pionniers de la radio ? A) Nikola Tesla B) Guglielmo Marconi C) Édouard Branly"
[2024-03-02 19:47:05,079 INFO generators.py gen_for_qa l.548] (8/10) * Start with LLM "gpt-4"
[2024-03-02 19:47:05,081 DEBUG generators.py generate l.349] (8/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:47:05,082 DEBUG generators.py generate l.358] (8/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:47:10,167 DEBUG generators.py generate l.370] (8/10) Post-process Answer
[2024-03-02 19:47:10,179 INFO generators.py gen_for_qa l.548] (8/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-02 19:47:10,181 DEBUG generators.py generate l.349] (8/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:47:10,182 DEBUG generators.py generate l.358] (8/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:47:16,665 DEBUG generators.py generate l.370] (8/10) Post-process Answer
[2024-03-02 19:47:16,671 INFO generators.py gen_for_qa l.548] (8/10) * Start with LLM "gemini-pro"
[2024-03-02 19:47:16,673 DEBUG generators.py generate l.349] (8/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:47:16,674 DEBUG generators.py generate l.358] (8/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:47:24,235 DEBUG generators.py generate l.370] (8/10) Post-process Answer
[2024-03-02 19:47:24,237 INFO generators.py gen_for_qa l.548] (8/10) * Start with LLM "claude-2.1"
[2024-03-02 19:47:24,239 DEBUG generators.py generate l.349] (8/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:47:24,239 DEBUG generators.py generate l.358] (8/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:47:26,033 DEBUG generators.py generate l.370] (8/10) Post-process Answer
[2024-03-02 19:47:26,033 INFO generators.py gen_for_qa l.548] (8/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-02 19:47:26,049 DEBUG generators.py generate l.349] (8/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:47:26,051 DEBUG generators.py generate l.358] (8/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:47:26,886 DEBUG generators.py generate l.370] (8/10) Post-process Answer
[2024-03-02 19:47:26,887 INFO generators.py generate l.477] (8/10) End question "Qui est considéré comme l'un des pionniers de la radio ? A) Nikola Tesla B) Guglielmo Marconi C) Édouard Branly"
[2024-03-02 19:47:26,889 INFO generators.py generate l.475] (9/10) *** AnsGenerator for question "Qui a réalisé le premier vol contrôlé d'un aéroplane ? A) Les frères Wright B) Clément Ader C) Alberto Santos-Dumont D) Louis Blériot"
[2024-03-02 19:47:26,890 INFO generators.py gen_for_qa l.548] (9/10) * Start with LLM "gpt-4"
[2024-03-02 19:47:26,891 DEBUG generators.py generate l.349] (9/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:47:26,892 DEBUG generators.py generate l.358] (9/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:47:31,891 DEBUG generators.py generate l.370] (9/10) Post-process Answer
[2024-03-02 19:47:31,891 INFO generators.py gen_for_qa l.548] (9/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-02 19:47:31,898 DEBUG generators.py generate l.349] (9/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:47:31,901 DEBUG generators.py generate l.358] (9/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:47:36,825 DEBUG generators.py generate l.370] (9/10) Post-process Answer
[2024-03-02 19:47:36,825 INFO generators.py gen_for_qa l.548] (9/10) * Start with LLM "gemini-pro"
[2024-03-02 19:47:36,825 DEBUG generators.py generate l.349] (9/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:47:36,830 DEBUG generators.py generate l.358] (9/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:47:40,087 DEBUG generators.py generate l.370] (9/10) Post-process Answer
[2024-03-02 19:47:40,092 INFO generators.py gen_for_qa l.548] (9/10) * Start with LLM "claude-2.1"
[2024-03-02 19:47:40,096 DEBUG generators.py generate l.349] (9/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:47:40,099 DEBUG generators.py generate l.358] (9/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:47:42,041 DEBUG generators.py generate l.370] (9/10) Post-process Answer
[2024-03-02 19:47:42,046 INFO generators.py gen_for_qa l.548] (9/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-02 19:47:42,050 DEBUG generators.py generate l.349] (9/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:47:42,053 DEBUG generators.py generate l.358] (9/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:47:42,741 DEBUG generators.py generate l.370] (9/10) Post-process Answer
[2024-03-02 19:47:42,742 INFO generators.py generate l.477] (9/10) End question "Qui a réalisé le premier vol contrôlé d'un aéroplane ? A) Les frères Wright B) Clément Ader C) Alberto Santos-Dumont D) Louis Blériot"
[2024-03-02 19:47:42,745 INFO generators.py generate l.475] (10/10) *** AnsGenerator for question "L'invention de la lampe incandescente est souvent attribuée à : A) Thomas Edison B) Joseph Swan C) Humphry Davy"
[2024-03-02 19:47:42,746 INFO generators.py gen_for_qa l.548] (10/10) * Start with LLM "gpt-4"
[2024-03-02 19:47:42,747 DEBUG generators.py generate l.349] (10/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:47:42,748 DEBUG generators.py generate l.358] (10/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:47:49,241 DEBUG generators.py generate l.370] (10/10) Post-process Answer
[2024-03-02 19:47:49,249 INFO generators.py gen_for_qa l.548] (10/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-02 19:47:49,250 DEBUG generators.py generate l.349] (10/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:47:49,250 DEBUG generators.py generate l.358] (10/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:47:53,050 DEBUG generators.py generate l.370] (10/10) Post-process Answer
[2024-03-02 19:47:53,053 INFO generators.py gen_for_qa l.548] (10/10) * Start with LLM "gemini-pro"
[2024-03-02 19:47:53,056 DEBUG generators.py generate l.349] (10/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:47:53,058 DEBUG generators.py generate l.358] (10/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:47:56,383 DEBUG generators.py generate l.370] (10/10) Post-process Answer
[2024-03-02 19:47:56,383 INFO generators.py gen_for_qa l.548] (10/10) * Start with LLM "claude-2.1"
[2024-03-02 19:47:56,389 DEBUG generators.py generate l.349] (10/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:47:56,392 DEBUG generators.py generate l.358] (10/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:47:57,922 DEBUG generators.py generate l.370] (10/10) Post-process Answer
[2024-03-02 19:47:57,924 INFO generators.py gen_for_qa l.548] (10/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-02 19:47:57,926 DEBUG generators.py generate l.349] (10/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-02 19:47:57,927 DEBUG generators.py generate l.358] (10/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-02 19:47:59,603 DEBUG generators.py generate l.370] (10/10) Post-process Answer
[2024-03-02 19:47:59,613 INFO generators.py generate l.477] (10/10) End question "L'invention de la lampe incandescente est souvent attribuée à : A) Thomas Edison B) Joseph Swan C) Humphry Davy"
[2024-03-02 19:47:59,623 INFO expe.py save_to_json l.283] (10/10) Expe saved as JSON to expe\Answers\culture--10Q_0C_0F_5M_0A_0HE_0AE_2024-03-02_19,47,59.json
[2024-03-02 19:47:59,625 INFO main.py <module> l.120] (10/10) MAIN ENDS
[2024-03-03 17:40:34,305 INFO main.py <module> l.109] MAIN STARTS
[2024-03-03 17:40:34,397 INFO expe.py save_to_html l.296] Expe saved as HTML to expe\Answers\culture--10Q_0C_0F_5M_0A_0HE_0AE_2024-03-03_17,40,34.html
[2024-03-03 17:40:34,805 INFO expe.py save_to_spreadsheet l.376] Expe saved as Spreadsheet to expe\Answers\culture--10Q_0C_0F_5M_0A_0HE_0AE_2024-03-03_17,40,34.xlsx
[2024-03-03 17:40:34,806 INFO main.py <module> l.120] MAIN ENDS
[2024-03-03 17:57:23,431 INFO main.py <module> l.109] MAIN STARTS
[2024-03-03 17:57:23,431 INFO expe.py save_to_json l.283] Expe saved as JSON to expe\Answers\culture--10Q_0C_0F_5M_50A_0HE_0AE_2024-03-03_17,57,23.json
[2024-03-03 17:57:23,503 INFO expe.py save_to_html l.296] Expe saved as HTML to expe\Answers\culture--10Q_0C_0F_5M_0A_0HE_0AE_2024-03-03_17,57,23.html
[2024-03-03 17:57:23,883 INFO expe.py save_to_spreadsheet l.376] Expe saved as Spreadsheet to expe\Answers\culture--10Q_0C_0F_5M_0A_0HE_0AE_2024-03-03_17,57,23.xlsx
[2024-03-03 17:57:23,883 INFO main.py <module> l.128] MAIN ENDS
[2024-03-03 18:01:11,174 INFO main.py <module> l.109] MAIN STARTS
[2024-03-03 18:09:52,097 INFO main.py <module> l.110] MAIN STARTS
[2024-03-03 18:21:27,747 INFO expe.py save_to_html l.296] Expe saved as HTML to expe\Answers\culture--10Q_0C_0F_5M_50A_0HE_0AE_2024-03-03_18,21,12.html
[2024-03-03 18:21:31,776 INFO main.py <module> l.122] MAIN ENDS
[2024-03-03 18:44:26,798 INFO main.py <module> l.87] MAIN STARTS
[2024-03-03 18:44:26,835 INFO generators.py generate l.475] (1/10) *** AnsGenerator for question "Qui est considéré comme l'inventeur du télégraphe électrique ? A) Samuel Morse B) Charles Wheatstone C) Alexander Graham Bell D) Thomas Edison"
[2024-03-03 18:44:26,841 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "gpt-4"
[2024-03-03 18:44:26,843 DEBUG generators.py generate l.349] (1/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:44:26,844 DEBUG generators.py generate l.358] (1/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:44:30,545 DEBUG generators.py generate l.370] (1/10) Post-process Answer
[2024-03-03 18:44:30,562 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-03 18:44:30,563 DEBUG generators.py generate l.349] (1/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:44:30,564 DEBUG generators.py generate l.358] (1/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:44:33,575 DEBUG generators.py generate l.370] (1/10) Post-process Answer
[2024-03-03 18:44:33,576 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "gemini-pro"
[2024-03-03 18:44:33,578 DEBUG generators.py generate l.349] (1/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:44:33,580 DEBUG generators.py generate l.358] (1/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:44:51,753 DEBUG generators.py generate l.370] (1/10) Post-process Answer
[2024-03-03 18:44:51,755 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "claude-2.1"
[2024-03-03 18:44:51,756 DEBUG generators.py generate l.349] (1/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:44:51,758 DEBUG generators.py generate l.358] (1/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:44:55,821 DEBUG generators.py generate l.370] (1/10) Post-process Answer
[2024-03-03 18:44:55,827 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-03 18:44:55,829 DEBUG generators.py generate l.349] (1/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:44:55,832 DEBUG generators.py generate l.358] (1/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:44:59,754 DEBUG generators.py generate l.370] (1/10) Post-process Answer
[2024-03-03 18:44:59,760 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "mistral/mistral-small"
[2024-03-03 18:44:59,762 DEBUG generators.py generate l.349] (1/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:44:59,765 DEBUG generators.py generate l.358] (1/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:45:02,121 DEBUG generators.py generate l.370] (1/10) Post-process Answer
[2024-03-03 18:45:02,121 INFO generators.py generate l.477] (1/10) End question "Qui est considéré comme l'inventeur du télégraphe électrique ? A) Samuel Morse B) Charles Wheatstone C) Alexander Graham Bell D) Thomas Edison"
[2024-03-03 18:45:02,130 INFO generators.py generate l.475] (2/10) *** AnsGenerator for question "Qui sont les pionniers du cinématographe ? A) Thomas Edison B) Les frères Lumière C) George Eastman D) William Friese-Greene"
[2024-03-03 18:45:02,130 INFO generators.py gen_for_qa l.548] (2/10) * Start with LLM "gpt-4"
[2024-03-03 18:45:02,133 DEBUG generators.py generate l.349] (2/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:45:02,134 DEBUG generators.py generate l.358] (2/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:45:10,815 DEBUG generators.py generate l.370] (2/10) Post-process Answer
[2024-03-03 18:45:10,820 INFO generators.py gen_for_qa l.548] (2/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-03 18:45:10,822 DEBUG generators.py generate l.349] (2/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:45:10,826 DEBUG generators.py generate l.358] (2/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:45:15,350 DEBUG generators.py generate l.370] (2/10) Post-process Answer
[2024-03-03 18:45:15,352 INFO generators.py gen_for_qa l.548] (2/10) * Start with LLM "gemini-pro"
[2024-03-03 18:45:15,354 DEBUG generators.py generate l.349] (2/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:45:15,355 DEBUG generators.py generate l.358] (2/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:45:19,158 DEBUG generators.py generate l.370] (2/10) Post-process Answer
[2024-03-03 18:45:19,160 INFO generators.py gen_for_qa l.548] (2/10) * Start with LLM "claude-2.1"
[2024-03-03 18:45:19,161 DEBUG generators.py generate l.349] (2/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:45:19,162 DEBUG generators.py generate l.358] (2/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:45:21,857 DEBUG generators.py generate l.370] (2/10) Post-process Answer
[2024-03-03 18:45:21,858 INFO generators.py gen_for_qa l.548] (2/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-03 18:45:21,860 DEBUG generators.py generate l.349] (2/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:45:21,862 DEBUG generators.py generate l.358] (2/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:45:24,324 DEBUG generators.py generate l.370] (2/10) Post-process Answer
[2024-03-03 18:45:24,326 INFO generators.py gen_for_qa l.548] (2/10) * Start with LLM "mistral/mistral-small"
[2024-03-03 18:45:24,327 DEBUG generators.py generate l.349] (2/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:45:24,327 DEBUG generators.py generate l.358] (2/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:45:29,980 DEBUG generators.py generate l.370] (2/10) Post-process Answer
[2024-03-03 18:45:29,980 INFO generators.py generate l.477] (2/10) End question "Qui sont les pionniers du cinématographe ? A) Thomas Edison B) Les frères Lumière C) George Eastman D) William Friese-Greene"
[2024-03-03 18:45:29,987 INFO generators.py generate l.475] (3/10) *** AnsGenerator for question "Qui a apporté des contributions cruciales au développement du moteur à vapeur ? A) James Watt B) Thomas Newcomen C) Denis Papin"
[2024-03-03 18:45:29,990 INFO generators.py gen_for_qa l.548] (3/10) * Start with LLM "gpt-4"
[2024-03-03 18:45:29,993 DEBUG generators.py generate l.349] (3/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:45:29,995 DEBUG generators.py generate l.358] (3/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:45:34,632 DEBUG generators.py generate l.370] (3/10) Post-process Answer
[2024-03-03 18:45:34,634 INFO generators.py gen_for_qa l.548] (3/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-03 18:45:34,637 DEBUG generators.py generate l.349] (3/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:45:34,637 DEBUG generators.py generate l.358] (3/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:45:38,708 DEBUG generators.py generate l.370] (3/10) Post-process Answer
[2024-03-03 18:45:38,713 INFO generators.py gen_for_qa l.548] (3/10) * Start with LLM "gemini-pro"
[2024-03-03 18:45:38,715 DEBUG generators.py generate l.349] (3/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:45:38,718 DEBUG generators.py generate l.358] (3/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:45:42,401 DEBUG generators.py generate l.370] (3/10) Post-process Answer
[2024-03-03 18:45:42,407 INFO generators.py gen_for_qa l.548] (3/10) * Start with LLM "claude-2.1"
[2024-03-03 18:45:42,411 DEBUG generators.py generate l.349] (3/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:45:42,414 DEBUG generators.py generate l.358] (3/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:45:43,864 DEBUG generators.py generate l.370] (3/10) Post-process Answer
[2024-03-03 18:45:43,869 INFO generators.py gen_for_qa l.548] (3/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-03 18:45:43,872 DEBUG generators.py generate l.349] (3/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:45:43,876 DEBUG generators.py generate l.358] (3/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:45:47,010 DEBUG generators.py generate l.370] (3/10) Post-process Answer
[2024-03-03 18:45:47,015 INFO generators.py gen_for_qa l.548] (3/10) * Start with LLM "mistral/mistral-small"
[2024-03-03 18:45:47,018 DEBUG generators.py generate l.349] (3/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:45:47,022 DEBUG generators.py generate l.358] (3/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:45:48,602 DEBUG generators.py generate l.370] (3/10) Post-process Answer
[2024-03-03 18:45:48,603 INFO generators.py generate l.477] (3/10) End question "Qui a apporté des contributions cruciales au développement du moteur à vapeur ? A) James Watt B) Thomas Newcomen C) Denis Papin"
[2024-03-03 18:45:48,605 INFO generators.py generate l.475] (4/10) *** AnsGenerator for question "Qui est reconnu pour avoir inventé le stéthoscope ? A) René Laennec B) Hippocrate C) Antonie van Leeuwenhoek D) Edward Jenner"
[2024-03-03 18:45:48,607 INFO generators.py gen_for_qa l.548] (4/10) * Start with LLM "gpt-4"
[2024-03-03 18:45:48,608 DEBUG generators.py generate l.349] (4/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:45:48,610 DEBUG generators.py generate l.358] (4/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:45:49,910 DEBUG generators.py generate l.370] (4/10) Post-process Answer
[2024-03-03 18:45:49,918 INFO generators.py gen_for_qa l.548] (4/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-03 18:45:49,922 DEBUG generators.py generate l.349] (4/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:45:49,923 DEBUG generators.py generate l.358] (4/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:45:56,267 DEBUG generators.py generate l.370] (4/10) Post-process Answer
[2024-03-03 18:45:56,269 INFO generators.py gen_for_qa l.548] (4/10) * Start with LLM "gemini-pro"
[2024-03-03 18:45:56,269 DEBUG generators.py generate l.349] (4/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:45:56,270 DEBUG generators.py generate l.358] (4/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:45:59,840 DEBUG generators.py generate l.370] (4/10) Post-process Answer
[2024-03-03 18:45:59,848 INFO generators.py gen_for_qa l.548] (4/10) * Start with LLM "claude-2.1"
[2024-03-03 18:45:59,852 DEBUG generators.py generate l.349] (4/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:45:59,855 DEBUG generators.py generate l.358] (4/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:46:01,824 DEBUG generators.py generate l.370] (4/10) Post-process Answer
[2024-03-03 18:46:01,828 INFO generators.py gen_for_qa l.548] (4/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-03 18:46:01,831 DEBUG generators.py generate l.349] (4/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:46:01,832 DEBUG generators.py generate l.358] (4/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:46:05,728 DEBUG generators.py generate l.370] (4/10) Post-process Answer
[2024-03-03 18:46:05,732 INFO generators.py gen_for_qa l.548] (4/10) * Start with LLM "mistral/mistral-small"
[2024-03-03 18:46:05,736 DEBUG generators.py generate l.349] (4/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:46:05,738 DEBUG generators.py generate l.358] (4/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:46:07,079 DEBUG generators.py generate l.370] (4/10) Post-process Answer
[2024-03-03 18:46:07,080 INFO generators.py generate l.477] (4/10) End question "Qui est reconnu pour avoir inventé le stéthoscope ? A) René Laennec B) Hippocrate C) Antonie van Leeuwenhoek D) Edward Jenner"
[2024-03-03 18:46:07,083 INFO generators.py generate l.475] (5/10) *** AnsGenerator for question "Quel scientifique est crédité pour l'invention de la photographie ? A) Nicéphore Niépce B) Louis Daguerre C) William Henry Fox Talbot"
[2024-03-03 18:46:07,084 INFO generators.py gen_for_qa l.548] (5/10) * Start with LLM "gpt-4"
[2024-03-03 18:46:07,086 DEBUG generators.py generate l.349] (5/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:46:07,086 DEBUG generators.py generate l.358] (5/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:46:13,132 DEBUG generators.py generate l.370] (5/10) Post-process Answer
[2024-03-03 18:46:13,137 INFO generators.py gen_for_qa l.548] (5/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-03 18:46:13,140 DEBUG generators.py generate l.349] (5/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:46:13,143 DEBUG generators.py generate l.358] (5/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:46:18,226 DEBUG generators.py generate l.370] (5/10) Post-process Answer
[2024-03-03 18:46:18,230 INFO generators.py gen_for_qa l.548] (5/10) * Start with LLM "gemini-pro"
[2024-03-03 18:46:18,233 DEBUG generators.py generate l.349] (5/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:46:18,236 DEBUG generators.py generate l.358] (5/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:46:21,832 DEBUG generators.py generate l.370] (5/10) Post-process Answer
[2024-03-03 18:46:21,851 INFO generators.py gen_for_qa l.548] (5/10) * Start with LLM "claude-2.1"
[2024-03-03 18:46:21,855 DEBUG generators.py generate l.349] (5/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:46:21,859 DEBUG generators.py generate l.358] (5/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:46:24,901 DEBUG generators.py generate l.370] (5/10) Post-process Answer
[2024-03-03 18:46:24,903 INFO generators.py gen_for_qa l.548] (5/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-03 18:46:24,903 DEBUG generators.py generate l.349] (5/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:46:24,912 DEBUG generators.py generate l.358] (5/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:46:26,012 DEBUG generators.py generate l.370] (5/10) Post-process Answer
[2024-03-03 18:46:26,015 INFO generators.py gen_for_qa l.548] (5/10) * Start with LLM "mistral/mistral-small"
[2024-03-03 18:46:26,018 DEBUG generators.py generate l.349] (5/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:46:26,020 DEBUG generators.py generate l.358] (5/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:46:27,359 DEBUG generators.py generate l.370] (5/10) Post-process Answer
[2024-03-03 18:46:27,361 INFO generators.py generate l.477] (5/10) End question "Quel scientifique est crédité pour l'invention de la photographie ? A) Nicéphore Niépce B) Louis Daguerre C) William Henry Fox Talbot"
[2024-03-03 18:46:27,363 INFO generators.py generate l.475] (6/10) *** AnsGenerator for question "Qui est associé à l'invention de la méthode champenoise pour la production de champagne ? A) Dom Pérignon B) Louis Pasteur C) Veuve Clicquot D) John Pemberton"
[2024-03-03 18:46:27,364 INFO generators.py gen_for_qa l.548] (6/10) * Start with LLM "gpt-4"
[2024-03-03 18:46:27,367 DEBUG generators.py generate l.349] (6/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:46:27,369 DEBUG generators.py generate l.358] (6/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:46:33,246 DEBUG generators.py generate l.370] (6/10) Post-process Answer
[2024-03-03 18:46:33,249 INFO generators.py gen_for_qa l.548] (6/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-03 18:46:33,251 DEBUG generators.py generate l.349] (6/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:46:33,252 DEBUG generators.py generate l.358] (6/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:46:36,320 DEBUG generators.py generate l.370] (6/10) Post-process Answer
[2024-03-03 18:46:36,335 INFO generators.py gen_for_qa l.548] (6/10) * Start with LLM "gemini-pro"
[2024-03-03 18:46:36,336 DEBUG generators.py generate l.349] (6/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:46:36,338 DEBUG generators.py generate l.358] (6/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:46:40,755 DEBUG generators.py generate l.370] (6/10) Post-process Answer
[2024-03-03 18:46:40,758 INFO generators.py gen_for_qa l.548] (6/10) * Start with LLM "claude-2.1"
[2024-03-03 18:46:40,759 DEBUG generators.py generate l.349] (6/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:46:40,759 DEBUG generators.py generate l.358] (6/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:46:42,583 DEBUG generators.py generate l.370] (6/10) Post-process Answer
[2024-03-03 18:46:42,583 INFO generators.py gen_for_qa l.548] (6/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-03 18:46:42,593 DEBUG generators.py generate l.349] (6/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:46:42,596 DEBUG generators.py generate l.358] (6/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:46:43,453 DEBUG generators.py generate l.370] (6/10) Post-process Answer
[2024-03-03 18:46:43,454 INFO generators.py gen_for_qa l.548] (6/10) * Start with LLM "mistral/mistral-small"
[2024-03-03 18:46:43,457 DEBUG generators.py generate l.349] (6/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:46:43,458 DEBUG generators.py generate l.358] (6/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:46:48,000 DEBUG generators.py generate l.370] (6/10) Post-process Answer
[2024-03-03 18:46:48,008 INFO generators.py generate l.477] (6/10) End question "Qui est associé à l'invention de la méthode champenoise pour la production de champagne ? A) Dom Pérignon B) Louis Pasteur C) Veuve Clicquot D) John Pemberton"
[2024-03-03 18:46:48,008 INFO generators.py generate l.475] (7/10) *** AnsGenerator for question "La découverte du calcul infinitésimal est attribuée à : A) Isaac Newton B) Gottfried Wilhelm Leibniz C) Albert Einstein"
[2024-03-03 18:46:48,009 INFO generators.py gen_for_qa l.548] (7/10) * Start with LLM "gpt-4"
[2024-03-03 18:46:48,011 DEBUG generators.py generate l.349] (7/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:46:48,013 DEBUG generators.py generate l.358] (7/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:46:56,302 DEBUG generators.py generate l.370] (7/10) Post-process Answer
[2024-03-03 18:46:56,302 INFO generators.py gen_for_qa l.548] (7/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-03 18:46:56,311 DEBUG generators.py generate l.349] (7/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:46:56,311 DEBUG generators.py generate l.358] (7/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:46:58,997 DEBUG generators.py generate l.370] (7/10) Post-process Answer
[2024-03-03 18:46:59,000 INFO generators.py gen_for_qa l.548] (7/10) * Start with LLM "gemini-pro"
[2024-03-03 18:46:59,000 DEBUG generators.py generate l.349] (7/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:46:59,000 DEBUG generators.py generate l.358] (7/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:47:03,105 DEBUG generators.py generate l.370] (7/10) Post-process Answer
[2024-03-03 18:47:03,105 INFO generators.py gen_for_qa l.548] (7/10) * Start with LLM "claude-2.1"
[2024-03-03 18:47:03,116 DEBUG generators.py generate l.349] (7/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:47:03,116 DEBUG generators.py generate l.358] (7/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:47:06,062 DEBUG generators.py generate l.370] (7/10) Post-process Answer
[2024-03-03 18:47:06,070 INFO generators.py gen_for_qa l.548] (7/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-03 18:47:06,070 DEBUG generators.py generate l.349] (7/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:47:06,080 DEBUG generators.py generate l.358] (7/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:47:07,186 DEBUG generators.py generate l.370] (7/10) Post-process Answer
[2024-03-03 18:47:07,186 INFO generators.py gen_for_qa l.548] (7/10) * Start with LLM "mistral/mistral-small"
[2024-03-03 18:47:07,186 DEBUG generators.py generate l.349] (7/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:47:07,186 DEBUG generators.py generate l.358] (7/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:47:09,819 DEBUG generators.py generate l.370] (7/10) Post-process Answer
[2024-03-03 18:47:09,819 INFO generators.py generate l.477] (7/10) End question "La découverte du calcul infinitésimal est attribuée à : A) Isaac Newton B) Gottfried Wilhelm Leibniz C) Albert Einstein"
[2024-03-03 18:47:09,819 INFO generators.py generate l.475] (8/10) *** AnsGenerator for question "Qui est considéré comme l'un des pionniers de la radio ? A) Nikola Tesla B) Guglielmo Marconi C) Édouard Branly"
[2024-03-03 18:47:09,834 INFO generators.py gen_for_qa l.548] (8/10) * Start with LLM "gpt-4"
[2024-03-03 18:47:09,834 DEBUG generators.py generate l.349] (8/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:47:09,834 DEBUG generators.py generate l.358] (8/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:47:14,279 DEBUG generators.py generate l.370] (8/10) Post-process Answer
[2024-03-03 18:47:14,294 INFO generators.py gen_for_qa l.548] (8/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-03 18:47:14,294 DEBUG generators.py generate l.349] (8/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:47:14,294 DEBUG generators.py generate l.358] (8/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:47:20,696 DEBUG generators.py generate l.370] (8/10) Post-process Answer
[2024-03-03 18:47:20,696 INFO generators.py gen_for_qa l.548] (8/10) * Start with LLM "gemini-pro"
[2024-03-03 18:47:20,696 DEBUG generators.py generate l.349] (8/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:47:20,705 DEBUG generators.py generate l.358] (8/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:47:24,362 DEBUG generators.py generate l.370] (8/10) Post-process Answer
[2024-03-03 18:47:24,379 INFO generators.py gen_for_qa l.548] (8/10) * Start with LLM "claude-2.1"
[2024-03-03 18:47:24,379 DEBUG generators.py generate l.349] (8/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:47:24,379 DEBUG generators.py generate l.358] (8/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:47:25,612 DEBUG generators.py generate l.370] (8/10) Post-process Answer
[2024-03-03 18:47:25,612 INFO generators.py gen_for_qa l.548] (8/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-03 18:47:25,612 DEBUG generators.py generate l.349] (8/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:47:25,623 DEBUG generators.py generate l.358] (8/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:47:26,978 DEBUG generators.py generate l.370] (8/10) Post-process Answer
[2024-03-03 18:47:26,978 INFO generators.py gen_for_qa l.548] (8/10) * Start with LLM "mistral/mistral-small"
[2024-03-03 18:47:26,978 DEBUG generators.py generate l.349] (8/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:47:26,978 DEBUG generators.py generate l.358] (8/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:47:30,647 DEBUG generators.py generate l.370] (8/10) Post-process Answer
[2024-03-03 18:47:30,647 INFO generators.py generate l.477] (8/10) End question "Qui est considéré comme l'un des pionniers de la radio ? A) Nikola Tesla B) Guglielmo Marconi C) Édouard Branly"
[2024-03-03 18:47:30,655 INFO generators.py generate l.475] (9/10) *** AnsGenerator for question "Qui a réalisé le premier vol contrôlé d'un aéroplane ? A) Les frères Wright B) Clément Ader C) Alberto Santos-Dumont D) Louis Blériot"
[2024-03-03 18:47:30,655 INFO generators.py gen_for_qa l.548] (9/10) * Start with LLM "gpt-4"
[2024-03-03 18:47:30,655 DEBUG generators.py generate l.349] (9/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:47:30,662 DEBUG generators.py generate l.358] (9/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:47:36,533 DEBUG generators.py generate l.370] (9/10) Post-process Answer
[2024-03-03 18:47:36,533 INFO generators.py gen_for_qa l.548] (9/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-03 18:47:36,533 DEBUG generators.py generate l.349] (9/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:47:36,549 DEBUG generators.py generate l.358] (9/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:47:40,953 DEBUG generators.py generate l.370] (9/10) Post-process Answer
[2024-03-03 18:47:40,969 INFO generators.py gen_for_qa l.548] (9/10) * Start with LLM "gemini-pro"
[2024-03-03 18:47:40,969 DEBUG generators.py generate l.349] (9/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:47:40,969 DEBUG generators.py generate l.358] (9/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:47:44,647 DEBUG generators.py generate l.370] (9/10) Post-process Answer
[2024-03-03 18:47:44,647 INFO generators.py gen_for_qa l.548] (9/10) * Start with LLM "claude-2.1"
[2024-03-03 18:47:44,662 DEBUG generators.py generate l.349] (9/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:47:44,662 DEBUG generators.py generate l.358] (9/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:47:46,612 DEBUG generators.py generate l.370] (9/10) Post-process Answer
[2024-03-03 18:47:46,612 INFO generators.py gen_for_qa l.548] (9/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-03 18:47:46,612 DEBUG generators.py generate l.349] (9/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:47:46,612 DEBUG generators.py generate l.358] (9/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:47:47,964 DEBUG generators.py generate l.370] (9/10) Post-process Answer
[2024-03-03 18:47:47,972 INFO generators.py gen_for_qa l.548] (9/10) * Start with LLM "mistral/mistral-small"
[2024-03-03 18:47:47,974 DEBUG generators.py generate l.349] (9/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:47:47,974 DEBUG generators.py generate l.358] (9/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:47:53,258 DEBUG generators.py generate l.370] (9/10) Post-process Answer
[2024-03-03 18:47:53,258 INFO generators.py generate l.477] (9/10) End question "Qui a réalisé le premier vol contrôlé d'un aéroplane ? A) Les frères Wright B) Clément Ader C) Alberto Santos-Dumont D) Louis Blériot"
[2024-03-03 18:47:53,266 INFO generators.py generate l.475] (10/10) *** AnsGenerator for question "L'invention de la lampe incandescente est souvent attribuée à : A) Thomas Edison B) Joseph Swan C) Humphry Davy"
[2024-03-03 18:47:53,266 INFO generators.py gen_for_qa l.548] (10/10) * Start with LLM "gpt-4"
[2024-03-03 18:47:53,266 DEBUG generators.py generate l.349] (10/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:47:53,266 DEBUG generators.py generate l.358] (10/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:47:57,607 DEBUG generators.py generate l.370] (10/10) Post-process Answer
[2024-03-03 18:47:57,607 INFO generators.py gen_for_qa l.548] (10/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-03 18:47:57,622 DEBUG generators.py generate l.349] (10/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:47:57,622 DEBUG generators.py generate l.358] (10/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:47:59,031 DEBUG generators.py generate l.370] (10/10) Post-process Answer
[2024-03-03 18:47:59,034 INFO generators.py gen_for_qa l.548] (10/10) * Start with LLM "gemini-pro"
[2024-03-03 18:47:59,037 DEBUG generators.py generate l.349] (10/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:47:59,038 DEBUG generators.py generate l.358] (10/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:48:02,738 DEBUG generators.py generate l.370] (10/10) Post-process Answer
[2024-03-03 18:48:02,745 INFO generators.py gen_for_qa l.548] (10/10) * Start with LLM "claude-2.1"
[2024-03-03 18:48:02,745 DEBUG generators.py generate l.349] (10/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:48:02,745 DEBUG generators.py generate l.358] (10/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:48:04,674 DEBUG generators.py generate l.370] (10/10) Post-process Answer
[2024-03-03 18:48:04,685 INFO generators.py gen_for_qa l.548] (10/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-03 18:48:04,685 DEBUG generators.py generate l.349] (10/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:48:04,695 DEBUG generators.py generate l.358] (10/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:48:05,787 DEBUG generators.py generate l.370] (10/10) Post-process Answer
[2024-03-03 18:48:05,787 INFO generators.py gen_for_qa l.548] (10/10) * Start with LLM "mistral/mistral-small"
[2024-03-03 18:48:05,787 DEBUG generators.py generate l.349] (10/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:48:05,802 DEBUG generators.py generate l.358] (10/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:48:09,477 DEBUG generators.py generate l.370] (10/10) Post-process Answer
[2024-03-03 18:48:09,481 INFO generators.py generate l.477] (10/10) End question "L'invention de la lampe incandescente est souvent attribuée à : A) Thomas Edison B) Joseph Swan C) Humphry Davy"
[2024-03-03 18:48:09,484 INFO expe.py save_to_json l.283] (10/10) Expe saved as JSON to expe\Answers\culture--10Q_0C_0F_6M_60A_0HE_0AE_2024-03-03_18,48,09.json
[2024-03-03 18:48:09,484 INFO main.py <module> l.99] (10/10) MAIN ENDS
[2024-03-03 18:52:10,214 INFO main.py <module> l.87] MAIN STARTS
[2024-03-03 18:52:10,214 INFO generators.py generate l.475] (1/10) *** AnsGenerator for question "Qui est considéré comme l'inventeur du télégraphe électrique ? A) Samuel Morse B) Charles Wheatstone C) Alexander Graham Bell D) Thomas Edison"
[2024-03-03 18:52:10,214 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "gpt-4"
[2024-03-03 18:52:10,214 DEBUG generators.py generate l.349] (1/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:52:10,214 DEBUG generators.py generate l.358] (1/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:52:11,617 DEBUG generators.py generate l.370] (1/10) Post-process Answer
[2024-03-03 18:55:03,232 INFO main.py <module> l.87] MAIN STARTS
[2024-03-03 18:55:03,242 INFO generators.py generate l.475] (1/10) *** AnsGenerator for question "Qui est considéré comme l'inventeur du télégraphe électrique ? A) Samuel Morse B) Charles Wheatstone C) Alexander Graham Bell D) Thomas Edison"
[2024-03-03 18:55:03,242 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "gpt-4"
[2024-03-03 18:55:03,242 DEBUG generators.py generate l.349] (1/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:55:03,247 DEBUG generators.py generate l.358] (1/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:55:10,029 DEBUG generators.py generate l.370] (1/10) Post-process Answer
[2024-03-03 18:55:10,029 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-03 18:55:10,032 DEBUG generators.py generate l.349] (1/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 18:55:10,032 DEBUG generators.py generate l.358] (1/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 18:55:38,538 INFO main.py <module> l.87] MAIN STARTS
[2024-03-03 18:55:43,822 INFO generators.py generate l.475] (1/10) *** AnsGenerator for question "Qui est considéré comme l'inventeur du télégraphe électrique ? A) Samuel Morse B) Charles Wheatstone C) Alexander Graham Bell D) Thomas Edison"
[2024-03-03 18:55:49,762 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "gpt-4"
[2024-03-03 18:56:47,083 INFO main.py <module> l.87] MAIN STARTS
[2024-03-03 18:56:47,083 INFO generators.py generate l.475] (1/10) *** AnsGenerator for question "Qui est considéré comme l'inventeur du télégraphe électrique ? A) Samuel Morse B) Charles Wheatstone C) Alexander Graham Bell D) Thomas Edison"
[2024-03-03 18:56:47,090 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "gpt-4"
[2024-03-03 18:56:47,091 DEBUG generators.py gen_for_qa l.554] (1/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,092 DEBUG generators.py generate l.352] (1/10) Reuse existing Prompt
[2024-03-03 18:56:47,093 DEBUG generators.py generate l.365] (1/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,095 DEBUG generators.py generate l.373] (1/10) Reuse post-processing
[2024-03-03 18:56:47,096 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-03 18:56:47,097 DEBUG generators.py gen_for_qa l.554] (1/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,097 DEBUG generators.py generate l.352] (1/10) Reuse existing Prompt
[2024-03-03 18:56:47,097 DEBUG generators.py generate l.365] (1/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,097 DEBUG generators.py generate l.373] (1/10) Reuse post-processing
[2024-03-03 18:56:47,100 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "gemini-pro"
[2024-03-03 18:56:47,100 DEBUG generators.py gen_for_qa l.554] (1/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,100 DEBUG generators.py generate l.352] (1/10) Reuse existing Prompt
[2024-03-03 18:56:47,100 DEBUG generators.py generate l.365] (1/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,100 DEBUG generators.py generate l.373] (1/10) Reuse post-processing
[2024-03-03 18:56:47,100 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "claude-2.1"
[2024-03-03 18:56:47,104 DEBUG generators.py gen_for_qa l.554] (1/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,105 DEBUG generators.py generate l.352] (1/10) Reuse existing Prompt
[2024-03-03 18:56:47,106 DEBUG generators.py generate l.365] (1/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,106 DEBUG generators.py generate l.373] (1/10) Reuse post-processing
[2024-03-03 18:56:47,106 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-03 18:56:47,106 DEBUG generators.py gen_for_qa l.554] (1/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,108 DEBUG generators.py generate l.352] (1/10) Reuse existing Prompt
[2024-03-03 18:56:47,108 DEBUG generators.py generate l.365] (1/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,108 DEBUG generators.py generate l.373] (1/10) Reuse post-processing
[2024-03-03 18:56:47,110 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "mistral/mistral-small"
[2024-03-03 18:56:47,111 DEBUG generators.py gen_for_qa l.554] (1/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,112 DEBUG generators.py generate l.352] (1/10) Reuse existing Prompt
[2024-03-03 18:56:47,113 DEBUG generators.py generate l.365] (1/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,115 DEBUG generators.py generate l.373] (1/10) Reuse post-processing
[2024-03-03 18:56:47,115 INFO generators.py generate l.477] (1/10) End question "Qui est considéré comme l'inventeur du télégraphe électrique ? A) Samuel Morse B) Charles Wheatstone C) Alexander Graham Bell D) Thomas Edison"
[2024-03-03 18:56:47,116 INFO generators.py generate l.475] (2/10) *** AnsGenerator for question "Qui sont les pionniers du cinématographe ? A) Thomas Edison B) Les frères Lumière C) George Eastman D) William Friese-Greene"
[2024-03-03 18:56:47,116 INFO generators.py gen_for_qa l.548] (2/10) * Start with LLM "gpt-4"
[2024-03-03 18:56:47,116 DEBUG generators.py gen_for_qa l.554] (2/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,116 DEBUG generators.py generate l.352] (2/10) Reuse existing Prompt
[2024-03-03 18:56:47,116 DEBUG generators.py generate l.365] (2/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,116 DEBUG generators.py generate l.373] (2/10) Reuse post-processing
[2024-03-03 18:56:47,120 INFO generators.py gen_for_qa l.548] (2/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-03 18:56:47,120 DEBUG generators.py gen_for_qa l.554] (2/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,121 DEBUG generators.py generate l.352] (2/10) Reuse existing Prompt
[2024-03-03 18:56:47,122 DEBUG generators.py generate l.365] (2/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,122 DEBUG generators.py generate l.373] (2/10) Reuse post-processing
[2024-03-03 18:56:47,122 INFO generators.py gen_for_qa l.548] (2/10) * Start with LLM "gemini-pro"
[2024-03-03 18:56:47,122 DEBUG generators.py gen_for_qa l.554] (2/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,122 DEBUG generators.py generate l.352] (2/10) Reuse existing Prompt
[2024-03-03 18:56:47,125 DEBUG generators.py generate l.365] (2/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,125 DEBUG generators.py generate l.373] (2/10) Reuse post-processing
[2024-03-03 18:56:47,125 INFO generators.py gen_for_qa l.548] (2/10) * Start with LLM "claude-2.1"
[2024-03-03 18:56:47,126 DEBUG generators.py gen_for_qa l.554] (2/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,126 DEBUG generators.py generate l.352] (2/10) Reuse existing Prompt
[2024-03-03 18:56:47,128 DEBUG generators.py generate l.365] (2/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,129 DEBUG generators.py generate l.373] (2/10) Reuse post-processing
[2024-03-03 18:56:47,130 INFO generators.py gen_for_qa l.548] (2/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-03 18:56:47,132 DEBUG generators.py gen_for_qa l.554] (2/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,132 DEBUG generators.py generate l.352] (2/10) Reuse existing Prompt
[2024-03-03 18:56:47,132 DEBUG generators.py generate l.365] (2/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,132 DEBUG generators.py generate l.373] (2/10) Reuse post-processing
[2024-03-03 18:56:47,134 INFO generators.py gen_for_qa l.548] (2/10) * Start with LLM "mistral/mistral-small"
[2024-03-03 18:56:47,135 DEBUG generators.py gen_for_qa l.554] (2/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,136 DEBUG generators.py generate l.352] (2/10) Reuse existing Prompt
[2024-03-03 18:56:47,136 DEBUG generators.py generate l.365] (2/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,137 DEBUG generators.py generate l.373] (2/10) Reuse post-processing
[2024-03-03 18:56:47,137 INFO generators.py generate l.477] (2/10) End question "Qui sont les pionniers du cinématographe ? A) Thomas Edison B) Les frères Lumière C) George Eastman D) William Friese-Greene"
[2024-03-03 18:56:47,138 INFO generators.py generate l.475] (3/10) *** AnsGenerator for question "Qui a apporté des contributions cruciales au développement du moteur à vapeur ? A) James Watt B) Thomas Newcomen C) Denis Papin"
[2024-03-03 18:56:47,138 INFO generators.py gen_for_qa l.548] (3/10) * Start with LLM "gpt-4"
[2024-03-03 18:56:47,138 DEBUG generators.py gen_for_qa l.554] (3/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,140 DEBUG generators.py generate l.352] (3/10) Reuse existing Prompt
[2024-03-03 18:56:47,141 DEBUG generators.py generate l.365] (3/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,141 DEBUG generators.py generate l.373] (3/10) Reuse post-processing
[2024-03-03 18:56:47,141 INFO generators.py gen_for_qa l.548] (3/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-03 18:56:47,141 DEBUG generators.py gen_for_qa l.554] (3/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,141 DEBUG generators.py generate l.352] (3/10) Reuse existing Prompt
[2024-03-03 18:56:47,141 DEBUG generators.py generate l.365] (3/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,145 DEBUG generators.py generate l.373] (3/10) Reuse post-processing
[2024-03-03 18:56:47,147 INFO generators.py gen_for_qa l.548] (3/10) * Start with LLM "gemini-pro"
[2024-03-03 18:56:47,147 DEBUG generators.py gen_for_qa l.554] (3/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,148 DEBUG generators.py generate l.352] (3/10) Reuse existing Prompt
[2024-03-03 18:56:47,150 DEBUG generators.py generate l.365] (3/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,151 DEBUG generators.py generate l.373] (3/10) Reuse post-processing
[2024-03-03 18:56:47,151 INFO generators.py gen_for_qa l.548] (3/10) * Start with LLM "claude-2.1"
[2024-03-03 18:56:47,151 DEBUG generators.py gen_for_qa l.554] (3/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,151 DEBUG generators.py generate l.352] (3/10) Reuse existing Prompt
[2024-03-03 18:56:47,154 DEBUG generators.py generate l.365] (3/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,154 DEBUG generators.py generate l.373] (3/10) Reuse post-processing
[2024-03-03 18:56:47,155 INFO generators.py gen_for_qa l.548] (3/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-03 18:56:47,155 DEBUG generators.py gen_for_qa l.554] (3/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,155 DEBUG generators.py generate l.352] (3/10) Reuse existing Prompt
[2024-03-03 18:56:47,156 DEBUG generators.py generate l.365] (3/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,157 DEBUG generators.py generate l.373] (3/10) Reuse post-processing
[2024-03-03 18:56:47,157 INFO generators.py gen_for_qa l.548] (3/10) * Start with LLM "mistral/mistral-small"
[2024-03-03 18:56:47,157 DEBUG generators.py gen_for_qa l.554] (3/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,157 DEBUG generators.py generate l.352] (3/10) Reuse existing Prompt
[2024-03-03 18:56:47,160 DEBUG generators.py generate l.365] (3/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,160 DEBUG generators.py generate l.373] (3/10) Reuse post-processing
[2024-03-03 18:56:47,162 INFO generators.py generate l.477] (3/10) End question "Qui a apporté des contributions cruciales au développement du moteur à vapeur ? A) James Watt B) Thomas Newcomen C) Denis Papin"
[2024-03-03 18:56:47,162 INFO generators.py generate l.475] (4/10) *** AnsGenerator for question "Qui est reconnu pour avoir inventé le stéthoscope ? A) René Laennec B) Hippocrate C) Antonie van Leeuwenhoek D) Edward Jenner"
[2024-03-03 18:56:47,164 INFO generators.py gen_for_qa l.548] (4/10) * Start with LLM "gpt-4"
[2024-03-03 18:56:47,165 DEBUG generators.py gen_for_qa l.554] (4/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,165 DEBUG generators.py generate l.352] (4/10) Reuse existing Prompt
[2024-03-03 18:56:47,167 DEBUG generators.py generate l.365] (4/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,167 DEBUG generators.py generate l.373] (4/10) Reuse post-processing
[2024-03-03 18:56:47,167 INFO generators.py gen_for_qa l.548] (4/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-03 18:56:47,167 DEBUG generators.py gen_for_qa l.554] (4/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,167 DEBUG generators.py generate l.352] (4/10) Reuse existing Prompt
[2024-03-03 18:56:47,167 DEBUG generators.py generate l.365] (4/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,167 DEBUG generators.py generate l.373] (4/10) Reuse post-processing
[2024-03-03 18:56:47,171 INFO generators.py gen_for_qa l.548] (4/10) * Start with LLM "gemini-pro"
[2024-03-03 18:56:47,172 DEBUG generators.py gen_for_qa l.554] (4/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,172 DEBUG generators.py generate l.352] (4/10) Reuse existing Prompt
[2024-03-03 18:56:47,172 DEBUG generators.py generate l.365] (4/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,174 DEBUG generators.py generate l.373] (4/10) Reuse post-processing
[2024-03-03 18:56:47,174 INFO generators.py gen_for_qa l.548] (4/10) * Start with LLM "claude-2.1"
[2024-03-03 18:56:47,176 DEBUG generators.py gen_for_qa l.554] (4/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,176 DEBUG generators.py generate l.352] (4/10) Reuse existing Prompt
[2024-03-03 18:56:47,176 DEBUG generators.py generate l.365] (4/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,177 DEBUG generators.py generate l.373] (4/10) Reuse post-processing
[2024-03-03 18:56:47,178 INFO generators.py gen_for_qa l.548] (4/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-03 18:56:47,179 DEBUG generators.py gen_for_qa l.554] (4/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,180 DEBUG generators.py generate l.352] (4/10) Reuse existing Prompt
[2024-03-03 18:56:47,181 DEBUG generators.py generate l.365] (4/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,183 DEBUG generators.py generate l.373] (4/10) Reuse post-processing
[2024-03-03 18:56:47,183 INFO generators.py gen_for_qa l.548] (4/10) * Start with LLM "mistral/mistral-small"
[2024-03-03 18:56:47,183 DEBUG generators.py gen_for_qa l.554] (4/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,184 DEBUG generators.py generate l.352] (4/10) Reuse existing Prompt
[2024-03-03 18:56:47,185 DEBUG generators.py generate l.365] (4/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,185 DEBUG generators.py generate l.373] (4/10) Reuse post-processing
[2024-03-03 18:56:47,185 INFO generators.py generate l.477] (4/10) End question "Qui est reconnu pour avoir inventé le stéthoscope ? A) René Laennec B) Hippocrate C) Antonie van Leeuwenhoek D) Edward Jenner"
[2024-03-03 18:56:47,187 INFO generators.py generate l.475] (5/10) *** AnsGenerator for question "Quel scientifique est crédité pour l'invention de la photographie ? A) Nicéphore Niépce B) Louis Daguerre C) William Henry Fox Talbot"
[2024-03-03 18:56:47,187 INFO generators.py gen_for_qa l.548] (5/10) * Start with LLM "gpt-4"
[2024-03-03 18:56:47,189 DEBUG generators.py gen_for_qa l.554] (5/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,189 DEBUG generators.py generate l.352] (5/10) Reuse existing Prompt
[2024-03-03 18:56:47,189 DEBUG generators.py generate l.365] (5/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,189 DEBUG generators.py generate l.373] (5/10) Reuse post-processing
[2024-03-03 18:56:47,191 INFO generators.py gen_for_qa l.548] (5/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-03 18:56:47,191 DEBUG generators.py gen_for_qa l.554] (5/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,191 DEBUG generators.py generate l.352] (5/10) Reuse existing Prompt
[2024-03-03 18:56:47,191 DEBUG generators.py generate l.365] (5/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,193 DEBUG generators.py generate l.373] (5/10) Reuse post-processing
[2024-03-03 18:56:47,195 INFO generators.py gen_for_qa l.548] (5/10) * Start with LLM "gemini-pro"
[2024-03-03 18:56:47,195 DEBUG generators.py gen_for_qa l.554] (5/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,197 DEBUG generators.py generate l.352] (5/10) Reuse existing Prompt
[2024-03-03 18:56:47,198 DEBUG generators.py generate l.365] (5/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,198 DEBUG generators.py generate l.373] (5/10) Reuse post-processing
[2024-03-03 18:56:47,198 INFO generators.py gen_for_qa l.548] (5/10) * Start with LLM "claude-2.1"
[2024-03-03 18:56:47,198 DEBUG generators.py gen_for_qa l.554] (5/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,201 DEBUG generators.py generate l.352] (5/10) Reuse existing Prompt
[2024-03-03 18:56:47,202 DEBUG generators.py generate l.365] (5/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,202 DEBUG generators.py generate l.373] (5/10) Reuse post-processing
[2024-03-03 18:56:47,202 INFO generators.py gen_for_qa l.548] (5/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-03 18:56:47,204 DEBUG generators.py gen_for_qa l.554] (5/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,204 DEBUG generators.py generate l.352] (5/10) Reuse existing Prompt
[2024-03-03 18:56:47,205 DEBUG generators.py generate l.365] (5/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,206 DEBUG generators.py generate l.373] (5/10) Reuse post-processing
[2024-03-03 18:56:47,206 INFO generators.py gen_for_qa l.548] (5/10) * Start with LLM "mistral/mistral-small"
[2024-03-03 18:56:47,206 DEBUG generators.py gen_for_qa l.554] (5/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,208 DEBUG generators.py generate l.352] (5/10) Reuse existing Prompt
[2024-03-03 18:56:47,208 DEBUG generators.py generate l.365] (5/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,208 DEBUG generators.py generate l.373] (5/10) Reuse post-processing
[2024-03-03 18:56:47,210 INFO generators.py generate l.477] (5/10) End question "Quel scientifique est crédité pour l'invention de la photographie ? A) Nicéphore Niépce B) Louis Daguerre C) William Henry Fox Talbot"
[2024-03-03 18:56:47,210 INFO generators.py generate l.475] (6/10) *** AnsGenerator for question "Qui est associé à l'invention de la méthode champenoise pour la production de champagne ? A) Dom Pérignon B) Louis Pasteur C) Veuve Clicquot D) John Pemberton"
[2024-03-03 18:56:47,212 INFO generators.py gen_for_qa l.548] (6/10) * Start with LLM "gpt-4"
[2024-03-03 18:56:47,213 DEBUG generators.py gen_for_qa l.554] (6/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,214 DEBUG generators.py generate l.352] (6/10) Reuse existing Prompt
[2024-03-03 18:56:47,215 DEBUG generators.py generate l.365] (6/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,215 DEBUG generators.py generate l.373] (6/10) Reuse post-processing
[2024-03-03 18:56:47,215 INFO generators.py gen_for_qa l.548] (6/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-03 18:56:47,217 DEBUG generators.py gen_for_qa l.554] (6/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,217 DEBUG generators.py generate l.352] (6/10) Reuse existing Prompt
[2024-03-03 18:56:47,217 DEBUG generators.py generate l.365] (6/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,219 DEBUG generators.py generate l.373] (6/10) Reuse post-processing
[2024-03-03 18:56:47,219 INFO generators.py gen_for_qa l.548] (6/10) * Start with LLM "gemini-pro"
[2024-03-03 18:56:47,220 DEBUG generators.py gen_for_qa l.554] (6/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,221 DEBUG generators.py generate l.352] (6/10) Reuse existing Prompt
[2024-03-03 18:56:47,222 DEBUG generators.py generate l.365] (6/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,222 DEBUG generators.py generate l.373] (6/10) Reuse post-processing
[2024-03-03 18:56:47,222 INFO generators.py gen_for_qa l.548] (6/10) * Start with LLM "claude-2.1"
[2024-03-03 18:56:47,223 DEBUG generators.py gen_for_qa l.554] (6/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,223 DEBUG generators.py generate l.352] (6/10) Reuse existing Prompt
[2024-03-03 18:56:47,223 DEBUG generators.py generate l.365] (6/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,223 DEBUG generators.py generate l.373] (6/10) Reuse post-processing
[2024-03-03 18:56:47,226 INFO generators.py gen_for_qa l.548] (6/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-03 18:56:47,226 DEBUG generators.py gen_for_qa l.554] (6/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,226 DEBUG generators.py generate l.352] (6/10) Reuse existing Prompt
[2024-03-03 18:56:47,227 DEBUG generators.py generate l.365] (6/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,227 DEBUG generators.py generate l.373] (6/10) Reuse post-processing
[2024-03-03 18:56:47,230 INFO generators.py gen_for_qa l.548] (6/10) * Start with LLM "mistral/mistral-small"
[2024-03-03 18:56:47,231 DEBUG generators.py gen_for_qa l.554] (6/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,231 DEBUG generators.py generate l.352] (6/10) Reuse existing Prompt
[2024-03-03 18:56:47,233 DEBUG generators.py generate l.365] (6/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,233 DEBUG generators.py generate l.373] (6/10) Reuse post-processing
[2024-03-03 18:56:47,234 INFO generators.py generate l.477] (6/10) End question "Qui est associé à l'invention de la méthode champenoise pour la production de champagne ? A) Dom Pérignon B) Louis Pasteur C) Veuve Clicquot D) John Pemberton"
[2024-03-03 18:56:47,234 INFO generators.py generate l.475] (7/10) *** AnsGenerator for question "La découverte du calcul infinitésimal est attribuée à : A) Isaac Newton B) Gottfried Wilhelm Leibniz C) Albert Einstein"
[2024-03-03 18:56:47,236 INFO generators.py gen_for_qa l.548] (7/10) * Start with LLM "gpt-4"
[2024-03-03 18:56:47,236 DEBUG generators.py gen_for_qa l.554] (7/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,236 DEBUG generators.py generate l.352] (7/10) Reuse existing Prompt
[2024-03-03 18:56:47,236 DEBUG generators.py generate l.365] (7/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,238 DEBUG generators.py generate l.373] (7/10) Reuse post-processing
[2024-03-03 18:56:47,238 INFO generators.py gen_for_qa l.548] (7/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-03 18:56:47,239 DEBUG generators.py gen_for_qa l.554] (7/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,239 DEBUG generators.py generate l.352] (7/10) Reuse existing Prompt
[2024-03-03 18:56:47,241 DEBUG generators.py generate l.365] (7/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,241 DEBUG generators.py generate l.373] (7/10) Reuse post-processing
[2024-03-03 18:56:47,241 INFO generators.py gen_for_qa l.548] (7/10) * Start with LLM "gemini-pro"
[2024-03-03 18:56:47,243 DEBUG generators.py gen_for_qa l.554] (7/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,243 DEBUG generators.py generate l.352] (7/10) Reuse existing Prompt
[2024-03-03 18:56:47,244 DEBUG generators.py generate l.365] (7/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,244 DEBUG generators.py generate l.373] (7/10) Reuse post-processing
[2024-03-03 18:56:47,245 INFO generators.py gen_for_qa l.548] (7/10) * Start with LLM "claude-2.1"
[2024-03-03 18:56:47,247 DEBUG generators.py gen_for_qa l.554] (7/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,249 DEBUG generators.py generate l.352] (7/10) Reuse existing Prompt
[2024-03-03 18:56:47,250 DEBUG generators.py generate l.365] (7/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,251 DEBUG generators.py generate l.373] (7/10) Reuse post-processing
[2024-03-03 18:56:47,251 INFO generators.py gen_for_qa l.548] (7/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-03 18:56:47,252 DEBUG generators.py gen_for_qa l.554] (7/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,252 DEBUG generators.py generate l.352] (7/10) Reuse existing Prompt
[2024-03-03 18:56:47,252 DEBUG generators.py generate l.365] (7/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,252 DEBUG generators.py generate l.373] (7/10) Reuse post-processing
[2024-03-03 18:56:47,254 INFO generators.py gen_for_qa l.548] (7/10) * Start with LLM "mistral/mistral-small"
[2024-03-03 18:56:47,254 DEBUG generators.py gen_for_qa l.554] (7/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,255 DEBUG generators.py generate l.352] (7/10) Reuse existing Prompt
[2024-03-03 18:56:47,256 DEBUG generators.py generate l.365] (7/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,257 DEBUG generators.py generate l.373] (7/10) Reuse post-processing
[2024-03-03 18:56:47,257 INFO generators.py generate l.477] (7/10) End question "La découverte du calcul infinitésimal est attribuée à : A) Isaac Newton B) Gottfried Wilhelm Leibniz C) Albert Einstein"
[2024-03-03 18:56:47,258 INFO generators.py generate l.475] (8/10) *** AnsGenerator for question "Qui est considéré comme l'un des pionniers de la radio ? A) Nikola Tesla B) Guglielmo Marconi C) Édouard Branly"
[2024-03-03 18:56:47,258 INFO generators.py gen_for_qa l.548] (8/10) * Start with LLM "gpt-4"
[2024-03-03 18:56:47,259 DEBUG generators.py gen_for_qa l.554] (8/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,259 DEBUG generators.py generate l.352] (8/10) Reuse existing Prompt
[2024-03-03 18:56:47,261 DEBUG generators.py generate l.365] (8/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,261 DEBUG generators.py generate l.373] (8/10) Reuse post-processing
[2024-03-03 18:56:47,262 INFO generators.py gen_for_qa l.548] (8/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-03 18:56:47,263 DEBUG generators.py gen_for_qa l.554] (8/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,263 DEBUG generators.py generate l.352] (8/10) Reuse existing Prompt
[2024-03-03 18:56:47,266 DEBUG generators.py generate l.365] (8/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,267 DEBUG generators.py generate l.373] (8/10) Reuse post-processing
[2024-03-03 18:56:47,267 INFO generators.py gen_for_qa l.548] (8/10) * Start with LLM "gemini-pro"
[2024-03-03 18:56:47,268 DEBUG generators.py gen_for_qa l.554] (8/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,268 DEBUG generators.py generate l.352] (8/10) Reuse existing Prompt
[2024-03-03 18:56:47,268 DEBUG generators.py generate l.365] (8/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,268 DEBUG generators.py generate l.373] (8/10) Reuse post-processing
[2024-03-03 18:56:47,268 INFO generators.py gen_for_qa l.548] (8/10) * Start with LLM "claude-2.1"
[2024-03-03 18:56:47,271 DEBUG generators.py gen_for_qa l.554] (8/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,272 DEBUG generators.py generate l.352] (8/10) Reuse existing Prompt
[2024-03-03 18:56:47,272 DEBUG generators.py generate l.365] (8/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,272 DEBUG generators.py generate l.373] (8/10) Reuse post-processing
[2024-03-03 18:56:47,273 INFO generators.py gen_for_qa l.548] (8/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-03 18:56:47,273 DEBUG generators.py gen_for_qa l.554] (8/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,273 DEBUG generators.py generate l.352] (8/10) Reuse existing Prompt
[2024-03-03 18:56:47,275 DEBUG generators.py generate l.365] (8/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,275 DEBUG generators.py generate l.373] (8/10) Reuse post-processing
[2024-03-03 18:56:47,275 INFO generators.py gen_for_qa l.548] (8/10) * Start with LLM "mistral/mistral-small"
[2024-03-03 18:56:47,277 DEBUG generators.py gen_for_qa l.554] (8/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,278 DEBUG generators.py generate l.352] (8/10) Reuse existing Prompt
[2024-03-03 18:56:47,278 DEBUG generators.py generate l.365] (8/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,280 DEBUG generators.py generate l.373] (8/10) Reuse post-processing
[2024-03-03 18:56:47,282 INFO generators.py generate l.477] (8/10) End question "Qui est considéré comme l'un des pionniers de la radio ? A) Nikola Tesla B) Guglielmo Marconi C) Édouard Branly"
[2024-03-03 18:56:47,282 INFO generators.py generate l.475] (9/10) *** AnsGenerator for question "Qui a réalisé le premier vol contrôlé d'un aéroplane ? A) Les frères Wright B) Clément Ader C) Alberto Santos-Dumont D) Louis Blériot"
[2024-03-03 18:56:47,282 INFO generators.py gen_for_qa l.548] (9/10) * Start with LLM "gpt-4"
[2024-03-03 18:56:47,282 DEBUG generators.py gen_for_qa l.554] (9/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,284 DEBUG generators.py generate l.352] (9/10) Reuse existing Prompt
[2024-03-03 18:56:47,284 DEBUG generators.py generate l.365] (9/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,285 DEBUG generators.py generate l.373] (9/10) Reuse post-processing
[2024-03-03 18:56:47,285 INFO generators.py gen_for_qa l.548] (9/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-03 18:56:47,287 DEBUG generators.py gen_for_qa l.554] (9/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,288 DEBUG generators.py generate l.352] (9/10) Reuse existing Prompt
[2024-03-03 18:56:47,288 DEBUG generators.py generate l.365] (9/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,288 DEBUG generators.py generate l.373] (9/10) Reuse post-processing
[2024-03-03 18:56:47,289 INFO generators.py gen_for_qa l.548] (9/10) * Start with LLM "gemini-pro"
[2024-03-03 18:56:47,289 DEBUG generators.py gen_for_qa l.554] (9/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,290 DEBUG generators.py generate l.352] (9/10) Reuse existing Prompt
[2024-03-03 18:56:47,291 DEBUG generators.py generate l.365] (9/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,291 DEBUG generators.py generate l.373] (9/10) Reuse post-processing
[2024-03-03 18:56:47,291 INFO generators.py gen_for_qa l.548] (9/10) * Start with LLM "claude-2.1"
[2024-03-03 18:56:47,293 DEBUG generators.py gen_for_qa l.554] (9/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,294 DEBUG generators.py generate l.352] (9/10) Reuse existing Prompt
[2024-03-03 18:56:47,294 DEBUG generators.py generate l.365] (9/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,295 DEBUG generators.py generate l.373] (9/10) Reuse post-processing
[2024-03-03 18:56:47,296 INFO generators.py gen_for_qa l.548] (9/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-03 18:56:47,298 DEBUG generators.py gen_for_qa l.554] (9/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,298 DEBUG generators.py generate l.352] (9/10) Reuse existing Prompt
[2024-03-03 18:56:47,298 DEBUG generators.py generate l.365] (9/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,298 DEBUG generators.py generate l.373] (9/10) Reuse post-processing
[2024-03-03 18:56:47,301 INFO generators.py gen_for_qa l.548] (9/10) * Start with LLM "mistral/mistral-small"
[2024-03-03 18:56:47,302 DEBUG generators.py gen_for_qa l.554] (9/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,302 DEBUG generators.py generate l.352] (9/10) Reuse existing Prompt
[2024-03-03 18:56:47,302 DEBUG generators.py generate l.365] (9/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,303 DEBUG generators.py generate l.373] (9/10) Reuse post-processing
[2024-03-03 18:56:47,303 INFO generators.py generate l.477] (9/10) End question "Qui a réalisé le premier vol contrôlé d'un aéroplane ? A) Les frères Wright B) Clément Ader C) Alberto Santos-Dumont D) Louis Blériot"
[2024-03-03 18:56:47,305 INFO generators.py generate l.475] (10/10) *** AnsGenerator for question "L'invention de la lampe incandescente est souvent attribuée à : A) Thomas Edison B) Joseph Swan C) Humphry Davy"
[2024-03-03 18:56:47,305 INFO generators.py gen_for_qa l.548] (10/10) * Start with LLM "gpt-4"
[2024-03-03 18:56:47,305 DEBUG generators.py gen_for_qa l.554] (10/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,306 DEBUG generators.py generate l.352] (10/10) Reuse existing Prompt
[2024-03-03 18:56:47,307 DEBUG generators.py generate l.365] (10/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,307 DEBUG generators.py generate l.373] (10/10) Reuse post-processing
[2024-03-03 18:56:47,308 INFO generators.py gen_for_qa l.548] (10/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-03 18:56:47,308 DEBUG generators.py gen_for_qa l.554] (10/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,309 DEBUG generators.py generate l.352] (10/10) Reuse existing Prompt
[2024-03-03 18:56:47,310 DEBUG generators.py generate l.365] (10/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,310 DEBUG generators.py generate l.373] (10/10) Reuse post-processing
[2024-03-03 18:56:47,312 INFO generators.py gen_for_qa l.548] (10/10) * Start with LLM "gemini-pro"
[2024-03-03 18:56:47,313 DEBUG generators.py gen_for_qa l.554] (10/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,314 DEBUG generators.py generate l.352] (10/10) Reuse existing Prompt
[2024-03-03 18:56:47,315 DEBUG generators.py generate l.365] (10/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,316 DEBUG generators.py generate l.373] (10/10) Reuse post-processing
[2024-03-03 18:56:47,317 INFO generators.py gen_for_qa l.548] (10/10) * Start with LLM "claude-2.1"
[2024-03-03 18:56:47,318 DEBUG generators.py gen_for_qa l.554] (10/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,318 DEBUG generators.py generate l.352] (10/10) Reuse existing Prompt
[2024-03-03 18:56:47,319 DEBUG generators.py generate l.365] (10/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,319 DEBUG generators.py generate l.373] (10/10) Reuse post-processing
[2024-03-03 18:56:47,320 INFO generators.py gen_for_qa l.548] (10/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-03 18:56:47,321 DEBUG generators.py gen_for_qa l.554] (10/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,322 DEBUG generators.py generate l.352] (10/10) Reuse existing Prompt
[2024-03-03 18:56:47,322 DEBUG generators.py generate l.365] (10/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,322 DEBUG generators.py generate l.373] (10/10) Reuse post-processing
[2024-03-03 18:56:47,324 INFO generators.py gen_for_qa l.548] (10/10) * Start with LLM "mistral/mistral-small"
[2024-03-03 18:56:47,324 DEBUG generators.py gen_for_qa l.554] (10/10) An Answer has already been generated with this LLM
[2024-03-03 18:56:47,324 DEBUG generators.py generate l.352] (10/10) Reuse existing Prompt
[2024-03-03 18:56:47,324 DEBUG generators.py generate l.365] (10/10) Reuse existing LLMAnswer in Answer
[2024-03-03 18:56:47,324 DEBUG generators.py generate l.373] (10/10) Reuse post-processing
[2024-03-03 18:56:47,326 INFO generators.py generate l.477] (10/10) End question "L'invention de la lampe incandescente est souvent attribuée à : A) Thomas Edison B) Joseph Swan C) Humphry Davy"
[2024-03-03 18:56:47,332 INFO expe.py save_to_json l.283] (10/10) Expe saved as JSON to expe\Answers\culture--10Q_0C_0F_6M_60A_0HE_0AE_2024-03-03_18,56,47.json
[2024-03-03 18:56:47,333 INFO main.py <module> l.99] (10/10) MAIN ENDS
[2024-03-03 18:57:15,200 INFO main.py <module> l.87] MAIN STARTS
[2024-03-03 18:57:20,263 INFO generators.py generate l.475] (1/10) *** AnsGenerator for question "Qui est considéré comme l'inventeur du télégraphe électrique ? A) Samuel Morse B) Charles Wheatstone C) Alexander Graham Bell D) Thomas Edison"
[2024-03-03 18:57:24,699 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "gpt-4"
[2024-03-03 18:57:37,956 DEBUG generators.py gen_for_qa l.554] (1/10) An Answer has already been generated with this LLM
[2024-03-03 18:59:52,773 DEBUG generators.py generate l.352] (1/10) Reuse existing Prompt
[2024-03-03 18:59:55,977 DEBUG generators.py generate l.365] (1/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,151 INFO main.py <module> l.87] MAIN STARTS
[2024-03-03 19:01:25,158 INFO generators.py generate l.475] (1/10) *** AnsGenerator for question "Qui est considéré comme l'inventeur du télégraphe électrique ? A) Samuel Morse B) Charles Wheatstone C) Alexander Graham Bell D) Thomas Edison"
[2024-03-03 19:01:25,167 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "gpt-4"
[2024-03-03 19:01:25,168 DEBUG generators.py gen_for_qa l.554] (1/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,168 DEBUG generators.py generate l.352] (1/10) Reuse existing Prompt
[2024-03-03 19:01:25,172 DEBUG generators.py generate l.365] (1/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,172 DEBUG generators.py generate l.370] (1/10) Post-process Answer
[2024-03-03 19:01:25,175 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-03 19:01:25,175 DEBUG generators.py gen_for_qa l.554] (1/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,179 DEBUG generators.py generate l.352] (1/10) Reuse existing Prompt
[2024-03-03 19:01:25,180 DEBUG generators.py generate l.365] (1/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,182 DEBUG generators.py generate l.370] (1/10) Post-process Answer
[2024-03-03 19:01:25,183 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "gemini-pro"
[2024-03-03 19:01:25,185 DEBUG generators.py gen_for_qa l.554] (1/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,187 DEBUG generators.py generate l.352] (1/10) Reuse existing Prompt
[2024-03-03 19:01:25,190 DEBUG generators.py generate l.365] (1/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,190 DEBUG generators.py generate l.370] (1/10) Post-process Answer
[2024-03-03 19:01:25,193 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "claude-2.1"
[2024-03-03 19:01:25,193 DEBUG generators.py gen_for_qa l.554] (1/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,196 DEBUG generators.py generate l.352] (1/10) Reuse existing Prompt
[2024-03-03 19:01:25,197 DEBUG generators.py generate l.365] (1/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,199 DEBUG generators.py generate l.370] (1/10) Post-process Answer
[2024-03-03 19:01:25,201 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-03 19:01:25,203 DEBUG generators.py gen_for_qa l.554] (1/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,205 DEBUG generators.py generate l.352] (1/10) Reuse existing Prompt
[2024-03-03 19:01:25,208 DEBUG generators.py generate l.365] (1/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,208 DEBUG generators.py generate l.370] (1/10) Post-process Answer
[2024-03-03 19:01:25,211 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "mistral/mistral-small"
[2024-03-03 19:01:25,211 DEBUG generators.py gen_for_qa l.554] (1/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,212 DEBUG generators.py generate l.352] (1/10) Reuse existing Prompt
[2024-03-03 19:01:25,214 DEBUG generators.py generate l.365] (1/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,215 DEBUG generators.py generate l.370] (1/10) Post-process Answer
[2024-03-03 19:01:25,215 INFO generators.py generate l.477] (1/10) End question "Qui est considéré comme l'inventeur du télégraphe électrique ? A) Samuel Morse B) Charles Wheatstone C) Alexander Graham Bell D) Thomas Edison"
[2024-03-03 19:01:25,217 INFO generators.py generate l.475] (2/10) *** AnsGenerator for question "Qui sont les pionniers du cinématographe ? A) Thomas Edison B) Les frères Lumière C) George Eastman D) William Friese-Greene"
[2024-03-03 19:01:25,218 INFO generators.py gen_for_qa l.548] (2/10) * Start with LLM "gpt-4"
[2024-03-03 19:01:25,218 DEBUG generators.py gen_for_qa l.554] (2/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,222 DEBUG generators.py generate l.352] (2/10) Reuse existing Prompt
[2024-03-03 19:01:25,223 DEBUG generators.py generate l.365] (2/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,223 DEBUG generators.py generate l.370] (2/10) Post-process Answer
[2024-03-03 19:01:25,225 INFO generators.py gen_for_qa l.548] (2/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-03 19:01:25,227 DEBUG generators.py gen_for_qa l.554] (2/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,228 DEBUG generators.py generate l.352] (2/10) Reuse existing Prompt
[2024-03-03 19:01:25,228 DEBUG generators.py generate l.365] (2/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,228 DEBUG generators.py generate l.370] (2/10) Post-process Answer
[2024-03-03 19:01:25,230 INFO generators.py gen_for_qa l.548] (2/10) * Start with LLM "gemini-pro"
[2024-03-03 19:01:25,231 DEBUG generators.py gen_for_qa l.554] (2/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,232 DEBUG generators.py generate l.352] (2/10) Reuse existing Prompt
[2024-03-03 19:01:25,234 DEBUG generators.py generate l.365] (2/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,235 DEBUG generators.py generate l.370] (2/10) Post-process Answer
[2024-03-03 19:01:25,236 INFO generators.py gen_for_qa l.548] (2/10) * Start with LLM "claude-2.1"
[2024-03-03 19:01:25,236 DEBUG generators.py gen_for_qa l.554] (2/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,238 DEBUG generators.py generate l.352] (2/10) Reuse existing Prompt
[2024-03-03 19:01:25,238 DEBUG generators.py generate l.365] (2/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,238 DEBUG generators.py generate l.370] (2/10) Post-process Answer
[2024-03-03 19:01:25,241 INFO generators.py gen_for_qa l.548] (2/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-03 19:01:25,241 DEBUG generators.py gen_for_qa l.554] (2/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,242 DEBUG generators.py generate l.352] (2/10) Reuse existing Prompt
[2024-03-03 19:01:25,242 DEBUG generators.py generate l.365] (2/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,242 DEBUG generators.py generate l.370] (2/10) Post-process Answer
[2024-03-03 19:01:25,242 INFO generators.py gen_for_qa l.548] (2/10) * Start with LLM "mistral/mistral-small"
[2024-03-03 19:01:25,242 DEBUG generators.py gen_for_qa l.554] (2/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,246 DEBUG generators.py generate l.352] (2/10) Reuse existing Prompt
[2024-03-03 19:01:25,246 DEBUG generators.py generate l.365] (2/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,247 DEBUG generators.py generate l.370] (2/10) Post-process Answer
[2024-03-03 19:01:25,247 INFO generators.py generate l.477] (2/10) End question "Qui sont les pionniers du cinématographe ? A) Thomas Edison B) Les frères Lumière C) George Eastman D) William Friese-Greene"
[2024-03-03 19:01:25,249 INFO generators.py generate l.475] (3/10) *** AnsGenerator for question "Qui a apporté des contributions cruciales au développement du moteur à vapeur ? A) James Watt B) Thomas Newcomen C) Denis Papin"
[2024-03-03 19:01:25,250 INFO generators.py gen_for_qa l.548] (3/10) * Start with LLM "gpt-4"
[2024-03-03 19:01:25,251 DEBUG generators.py gen_for_qa l.554] (3/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,253 DEBUG generators.py generate l.352] (3/10) Reuse existing Prompt
[2024-03-03 19:01:25,253 DEBUG generators.py generate l.365] (3/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,255 DEBUG generators.py generate l.370] (3/10) Post-process Answer
[2024-03-03 19:01:25,255 INFO generators.py gen_for_qa l.548] (3/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-03 19:01:25,257 DEBUG generators.py gen_for_qa l.554] (3/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,257 DEBUG generators.py generate l.352] (3/10) Reuse existing Prompt
[2024-03-03 19:01:25,258 DEBUG generators.py generate l.365] (3/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,258 DEBUG generators.py generate l.370] (3/10) Post-process Answer
[2024-03-03 19:01:25,260 INFO generators.py gen_for_qa l.548] (3/10) * Start with LLM "gemini-pro"
[2024-03-03 19:01:25,260 DEBUG generators.py gen_for_qa l.554] (3/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,260 DEBUG generators.py generate l.352] (3/10) Reuse existing Prompt
[2024-03-03 19:01:25,260 DEBUG generators.py generate l.365] (3/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,262 DEBUG generators.py generate l.370] (3/10) Post-process Answer
[2024-03-03 19:01:25,263 INFO generators.py gen_for_qa l.548] (3/10) * Start with LLM "claude-2.1"
[2024-03-03 19:01:25,263 DEBUG generators.py gen_for_qa l.554] (3/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,263 DEBUG generators.py generate l.352] (3/10) Reuse existing Prompt
[2024-03-03 19:01:25,265 DEBUG generators.py generate l.365] (3/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,265 DEBUG generators.py generate l.370] (3/10) Post-process Answer
[2024-03-03 19:01:25,266 INFO generators.py gen_for_qa l.548] (3/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-03 19:01:25,267 DEBUG generators.py gen_for_qa l.554] (3/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,268 DEBUG generators.py generate l.352] (3/10) Reuse existing Prompt
[2024-03-03 19:01:25,270 DEBUG generators.py generate l.365] (3/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,271 DEBUG generators.py generate l.370] (3/10) Post-process Answer
[2024-03-03 19:01:25,271 INFO generators.py gen_for_qa l.548] (3/10) * Start with LLM "mistral/mistral-small"
[2024-03-03 19:01:25,272 DEBUG generators.py gen_for_qa l.554] (3/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,273 DEBUG generators.py generate l.352] (3/10) Reuse existing Prompt
[2024-03-03 19:01:25,273 DEBUG generators.py generate l.365] (3/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,274 DEBUG generators.py generate l.370] (3/10) Post-process Answer
[2024-03-03 19:01:25,274 INFO generators.py generate l.477] (3/10) End question "Qui a apporté des contributions cruciales au développement du moteur à vapeur ? A) James Watt B) Thomas Newcomen C) Denis Papin"
[2024-03-03 19:01:25,275 INFO generators.py generate l.475] (4/10) *** AnsGenerator for question "Qui est reconnu pour avoir inventé le stéthoscope ? A) René Laennec B) Hippocrate C) Antonie van Leeuwenhoek D) Edward Jenner"
[2024-03-03 19:01:25,277 INFO generators.py gen_for_qa l.548] (4/10) * Start with LLM "gpt-4"
[2024-03-03 19:01:25,277 DEBUG generators.py gen_for_qa l.554] (4/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,277 DEBUG generators.py generate l.352] (4/10) Reuse existing Prompt
[2024-03-03 19:01:25,278 DEBUG generators.py generate l.365] (4/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,278 DEBUG generators.py generate l.370] (4/10) Post-process Answer
[2024-03-03 19:01:25,280 INFO generators.py gen_for_qa l.548] (4/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-03 19:01:25,280 DEBUG generators.py gen_for_qa l.554] (4/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,281 DEBUG generators.py generate l.352] (4/10) Reuse existing Prompt
[2024-03-03 19:01:25,281 DEBUG generators.py generate l.365] (4/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,282 DEBUG generators.py generate l.370] (4/10) Post-process Answer
[2024-03-03 19:01:25,282 INFO generators.py gen_for_qa l.548] (4/10) * Start with LLM "gemini-pro"
[2024-03-03 19:01:25,283 DEBUG generators.py gen_for_qa l.554] (4/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,285 DEBUG generators.py generate l.352] (4/10) Reuse existing Prompt
[2024-03-03 19:01:25,285 DEBUG generators.py generate l.365] (4/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,287 DEBUG generators.py generate l.370] (4/10) Post-process Answer
[2024-03-03 19:01:25,287 INFO generators.py gen_for_qa l.548] (4/10) * Start with LLM "claude-2.1"
[2024-03-03 19:01:25,289 DEBUG generators.py gen_for_qa l.554] (4/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,289 DEBUG generators.py generate l.352] (4/10) Reuse existing Prompt
[2024-03-03 19:01:25,289 DEBUG generators.py generate l.365] (4/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,290 DEBUG generators.py generate l.370] (4/10) Post-process Answer
[2024-03-03 19:01:25,290 INFO generators.py gen_for_qa l.548] (4/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-03 19:01:25,291 DEBUG generators.py gen_for_qa l.554] (4/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,292 DEBUG generators.py generate l.352] (4/10) Reuse existing Prompt
[2024-03-03 19:01:25,292 DEBUG generators.py generate l.365] (4/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,293 DEBUG generators.py generate l.370] (4/10) Post-process Answer
[2024-03-03 19:01:25,293 INFO generators.py gen_for_qa l.548] (4/10) * Start with LLM "mistral/mistral-small"
[2024-03-03 19:01:25,293 DEBUG generators.py gen_for_qa l.554] (4/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,295 DEBUG generators.py generate l.352] (4/10) Reuse existing Prompt
[2024-03-03 19:01:25,295 DEBUG generators.py generate l.365] (4/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,296 DEBUG generators.py generate l.370] (4/10) Post-process Answer
[2024-03-03 19:01:25,297 INFO generators.py generate l.477] (4/10) End question "Qui est reconnu pour avoir inventé le stéthoscope ? A) René Laennec B) Hippocrate C) Antonie van Leeuwenhoek D) Edward Jenner"
[2024-03-03 19:01:25,297 INFO generators.py generate l.475] (5/10) *** AnsGenerator for question "Quel scientifique est crédité pour l'invention de la photographie ? A) Nicéphore Niépce B) Louis Daguerre C) William Henry Fox Talbot"
[2024-03-03 19:01:25,298 INFO generators.py gen_for_qa l.548] (5/10) * Start with LLM "gpt-4"
[2024-03-03 19:01:25,298 DEBUG generators.py gen_for_qa l.554] (5/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,299 DEBUG generators.py generate l.352] (5/10) Reuse existing Prompt
[2024-03-03 19:01:25,300 DEBUG generators.py generate l.365] (5/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,301 DEBUG generators.py generate l.370] (5/10) Post-process Answer
[2024-03-03 19:01:25,303 INFO generators.py gen_for_qa l.548] (5/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-03 19:01:25,303 DEBUG generators.py gen_for_qa l.554] (5/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,303 DEBUG generators.py generate l.352] (5/10) Reuse existing Prompt
[2024-03-03 19:01:25,306 DEBUG generators.py generate l.365] (5/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,306 DEBUG generators.py generate l.370] (5/10) Post-process Answer
[2024-03-03 19:01:25,306 INFO generators.py gen_for_qa l.548] (5/10) * Start with LLM "gemini-pro"
[2024-03-03 19:01:25,306 DEBUG generators.py gen_for_qa l.554] (5/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,308 DEBUG generators.py generate l.352] (5/10) Reuse existing Prompt
[2024-03-03 19:01:25,308 DEBUG generators.py generate l.365] (5/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,308 DEBUG generators.py generate l.370] (5/10) Post-process Answer
[2024-03-03 19:01:25,310 INFO generators.py gen_for_qa l.548] (5/10) * Start with LLM "claude-2.1"
[2024-03-03 19:01:25,310 DEBUG generators.py gen_for_qa l.554] (5/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,310 DEBUG generators.py generate l.352] (5/10) Reuse existing Prompt
[2024-03-03 19:01:25,312 DEBUG generators.py generate l.365] (5/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,312 DEBUG generators.py generate l.370] (5/10) Post-process Answer
[2024-03-03 19:01:25,312 INFO generators.py gen_for_qa l.548] (5/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-03 19:01:25,312 DEBUG generators.py gen_for_qa l.554] (5/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,314 DEBUG generators.py generate l.352] (5/10) Reuse existing Prompt
[2024-03-03 19:01:25,314 DEBUG generators.py generate l.365] (5/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,314 DEBUG generators.py generate l.370] (5/10) Post-process Answer
[2024-03-03 19:01:25,314 INFO generators.py gen_for_qa l.548] (5/10) * Start with LLM "mistral/mistral-small"
[2024-03-03 19:01:25,318 DEBUG generators.py gen_for_qa l.554] (5/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,319 DEBUG generators.py generate l.352] (5/10) Reuse existing Prompt
[2024-03-03 19:01:25,319 DEBUG generators.py generate l.365] (5/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,319 DEBUG generators.py generate l.370] (5/10) Post-process Answer
[2024-03-03 19:01:25,319 INFO generators.py generate l.477] (5/10) End question "Quel scientifique est crédité pour l'invention de la photographie ? A) Nicéphore Niépce B) Louis Daguerre C) William Henry Fox Talbot"
[2024-03-03 19:01:25,322 INFO generators.py generate l.475] (6/10) *** AnsGenerator for question "Qui est associé à l'invention de la méthode champenoise pour la production de champagne ? A) Dom Pérignon B) Louis Pasteur C) Veuve Clicquot D) John Pemberton"
[2024-03-03 19:01:25,322 INFO generators.py gen_for_qa l.548] (6/10) * Start with LLM "gpt-4"
[2024-03-03 19:01:25,324 DEBUG generators.py gen_for_qa l.554] (6/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,324 DEBUG generators.py generate l.352] (6/10) Reuse existing Prompt
[2024-03-03 19:01:25,324 DEBUG generators.py generate l.365] (6/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,324 DEBUG generators.py generate l.370] (6/10) Post-process Answer
[2024-03-03 19:01:25,324 INFO generators.py gen_for_qa l.548] (6/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-03 19:01:25,327 DEBUG generators.py gen_for_qa l.554] (6/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,327 DEBUG generators.py generate l.352] (6/10) Reuse existing Prompt
[2024-03-03 19:01:25,327 DEBUG generators.py generate l.365] (6/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,329 DEBUG generators.py generate l.370] (6/10) Post-process Answer
[2024-03-03 19:01:25,329 INFO generators.py gen_for_qa l.548] (6/10) * Start with LLM "gemini-pro"
[2024-03-03 19:01:25,330 DEBUG generators.py gen_for_qa l.554] (6/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,330 DEBUG generators.py generate l.352] (6/10) Reuse existing Prompt
[2024-03-03 19:01:25,331 DEBUG generators.py generate l.365] (6/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,332 DEBUG generators.py generate l.370] (6/10) Post-process Answer
[2024-03-03 19:01:25,333 INFO generators.py gen_for_qa l.548] (6/10) * Start with LLM "claude-2.1"
[2024-03-03 19:01:25,333 DEBUG generators.py gen_for_qa l.554] (6/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,335 DEBUG generators.py generate l.352] (6/10) Reuse existing Prompt
[2024-03-03 19:01:25,336 DEBUG generators.py generate l.365] (6/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,337 DEBUG generators.py generate l.370] (6/10) Post-process Answer
[2024-03-03 19:01:25,337 INFO generators.py gen_for_qa l.548] (6/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-03 19:01:25,339 DEBUG generators.py gen_for_qa l.554] (6/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,339 DEBUG generators.py generate l.352] (6/10) Reuse existing Prompt
[2024-03-03 19:01:25,339 DEBUG generators.py generate l.365] (6/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,339 DEBUG generators.py generate l.370] (6/10) Post-process Answer
[2024-03-03 19:01:25,341 INFO generators.py gen_for_qa l.548] (6/10) * Start with LLM "mistral/mistral-small"
[2024-03-03 19:01:25,342 DEBUG generators.py gen_for_qa l.554] (6/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,342 DEBUG generators.py generate l.352] (6/10) Reuse existing Prompt
[2024-03-03 19:01:25,342 DEBUG generators.py generate l.365] (6/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,344 DEBUG generators.py generate l.370] (6/10) Post-process Answer
[2024-03-03 19:01:25,344 INFO generators.py generate l.477] (6/10) End question "Qui est associé à l'invention de la méthode champenoise pour la production de champagne ? A) Dom Pérignon B) Louis Pasteur C) Veuve Clicquot D) John Pemberton"
[2024-03-03 19:01:25,344 INFO generators.py generate l.475] (7/10) *** AnsGenerator for question "La découverte du calcul infinitésimal est attribuée à : A) Isaac Newton B) Gottfried Wilhelm Leibniz C) Albert Einstein"
[2024-03-03 19:01:25,344 INFO generators.py gen_for_qa l.548] (7/10) * Start with LLM "gpt-4"
[2024-03-03 19:01:25,347 DEBUG generators.py gen_for_qa l.554] (7/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,347 DEBUG generators.py generate l.352] (7/10) Reuse existing Prompt
[2024-03-03 19:01:25,347 DEBUG generators.py generate l.365] (7/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,347 DEBUG generators.py generate l.370] (7/10) Post-process Answer
[2024-03-03 19:01:25,349 INFO generators.py gen_for_qa l.548] (7/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-03 19:01:25,350 DEBUG generators.py gen_for_qa l.554] (7/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,351 DEBUG generators.py generate l.352] (7/10) Reuse existing Prompt
[2024-03-03 19:01:25,352 DEBUG generators.py generate l.365] (7/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,353 DEBUG generators.py generate l.370] (7/10) Post-process Answer
[2024-03-03 19:01:25,353 INFO generators.py gen_for_qa l.548] (7/10) * Start with LLM "gemini-pro"
[2024-03-03 19:01:25,353 DEBUG generators.py gen_for_qa l.554] (7/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,356 DEBUG generators.py generate l.352] (7/10) Reuse existing Prompt
[2024-03-03 19:01:25,356 DEBUG generators.py generate l.365] (7/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,357 DEBUG generators.py generate l.370] (7/10) Post-process Answer
[2024-03-03 19:01:25,357 INFO generators.py gen_for_qa l.548] (7/10) * Start with LLM "claude-2.1"
[2024-03-03 19:01:25,358 DEBUG generators.py gen_for_qa l.554] (7/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,358 DEBUG generators.py generate l.352] (7/10) Reuse existing Prompt
[2024-03-03 19:01:25,359 DEBUG generators.py generate l.365] (7/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,359 DEBUG generators.py generate l.370] (7/10) Post-process Answer
[2024-03-03 19:01:25,359 INFO generators.py gen_for_qa l.548] (7/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-03 19:01:25,359 DEBUG generators.py gen_for_qa l.554] (7/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,362 DEBUG generators.py generate l.352] (7/10) Reuse existing Prompt
[2024-03-03 19:01:25,362 DEBUG generators.py generate l.365] (7/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,363 DEBUG generators.py generate l.370] (7/10) Post-process Answer
[2024-03-03 19:01:25,363 INFO generators.py gen_for_qa l.548] (7/10) * Start with LLM "mistral/mistral-small"
[2024-03-03 19:01:25,365 DEBUG generators.py gen_for_qa l.554] (7/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,365 DEBUG generators.py generate l.352] (7/10) Reuse existing Prompt
[2024-03-03 19:01:25,365 DEBUG generators.py generate l.365] (7/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,367 DEBUG generators.py generate l.370] (7/10) Post-process Answer
[2024-03-03 19:01:25,368 INFO generators.py generate l.477] (7/10) End question "La découverte du calcul infinitésimal est attribuée à : A) Isaac Newton B) Gottfried Wilhelm Leibniz C) Albert Einstein"
[2024-03-03 19:01:25,369 INFO generators.py generate l.475] (8/10) *** AnsGenerator for question "Qui est considéré comme l'un des pionniers de la radio ? A) Nikola Tesla B) Guglielmo Marconi C) Édouard Branly"
[2024-03-03 19:01:25,369 INFO generators.py gen_for_qa l.548] (8/10) * Start with LLM "gpt-4"
[2024-03-03 19:01:25,372 DEBUG generators.py gen_for_qa l.554] (8/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,372 DEBUG generators.py generate l.352] (8/10) Reuse existing Prompt
[2024-03-03 19:01:25,372 DEBUG generators.py generate l.365] (8/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,372 DEBUG generators.py generate l.370] (8/10) Post-process Answer
[2024-03-03 19:01:25,374 INFO generators.py gen_for_qa l.548] (8/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-03 19:01:25,375 DEBUG generators.py gen_for_qa l.554] (8/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,376 DEBUG generators.py generate l.352] (8/10) Reuse existing Prompt
[2024-03-03 19:01:25,377 DEBUG generators.py generate l.365] (8/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,377 DEBUG generators.py generate l.370] (8/10) Post-process Answer
[2024-03-03 19:01:25,377 INFO generators.py gen_for_qa l.548] (8/10) * Start with LLM "gemini-pro"
[2024-03-03 19:01:25,379 DEBUG generators.py gen_for_qa l.554] (8/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,379 DEBUG generators.py generate l.352] (8/10) Reuse existing Prompt
[2024-03-03 19:01:25,379 DEBUG generators.py generate l.365] (8/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,379 DEBUG generators.py generate l.370] (8/10) Post-process Answer
[2024-03-03 19:01:25,379 INFO generators.py gen_for_qa l.548] (8/10) * Start with LLM "claude-2.1"
[2024-03-03 19:01:25,381 DEBUG generators.py gen_for_qa l.554] (8/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,382 DEBUG generators.py generate l.352] (8/10) Reuse existing Prompt
[2024-03-03 19:01:25,383 DEBUG generators.py generate l.365] (8/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,383 DEBUG generators.py generate l.370] (8/10) Post-process Answer
[2024-03-03 19:01:25,385 INFO generators.py gen_for_qa l.548] (8/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-03 19:01:25,385 DEBUG generators.py gen_for_qa l.554] (8/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,387 DEBUG generators.py generate l.352] (8/10) Reuse existing Prompt
[2024-03-03 19:01:25,387 DEBUG generators.py generate l.365] (8/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,387 DEBUG generators.py generate l.370] (8/10) Post-process Answer
[2024-03-03 19:01:25,389 INFO generators.py gen_for_qa l.548] (8/10) * Start with LLM "mistral/mistral-small"
[2024-03-03 19:01:25,389 DEBUG generators.py gen_for_qa l.554] (8/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,389 DEBUG generators.py generate l.352] (8/10) Reuse existing Prompt
[2024-03-03 19:01:25,391 DEBUG generators.py generate l.365] (8/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,391 DEBUG generators.py generate l.370] (8/10) Post-process Answer
[2024-03-03 19:01:25,392 INFO generators.py generate l.477] (8/10) End question "Qui est considéré comme l'un des pionniers de la radio ? A) Nikola Tesla B) Guglielmo Marconi C) Édouard Branly"
[2024-03-03 19:01:25,392 INFO generators.py generate l.475] (9/10) *** AnsGenerator for question "Qui a réalisé le premier vol contrôlé d'un aéroplane ? A) Les frères Wright B) Clément Ader C) Alberto Santos-Dumont D) Louis Blériot"
[2024-03-03 19:01:25,394 INFO generators.py gen_for_qa l.548] (9/10) * Start with LLM "gpt-4"
[2024-03-03 19:01:25,394 DEBUG generators.py gen_for_qa l.554] (9/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,395 DEBUG generators.py generate l.352] (9/10) Reuse existing Prompt
[2024-03-03 19:01:25,395 DEBUG generators.py generate l.365] (9/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,395 DEBUG generators.py generate l.370] (9/10) Post-process Answer
[2024-03-03 19:01:25,397 INFO generators.py gen_for_qa l.548] (9/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-03 19:01:25,397 DEBUG generators.py gen_for_qa l.554] (9/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,398 DEBUG generators.py generate l.352] (9/10) Reuse existing Prompt
[2024-03-03 19:01:25,398 DEBUG generators.py generate l.365] (9/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,399 DEBUG generators.py generate l.370] (9/10) Post-process Answer
[2024-03-03 19:01:25,400 INFO generators.py gen_for_qa l.548] (9/10) * Start with LLM "gemini-pro"
[2024-03-03 19:01:25,401 DEBUG generators.py gen_for_qa l.554] (9/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,403 DEBUG generators.py generate l.352] (9/10) Reuse existing Prompt
[2024-03-03 19:01:25,403 DEBUG generators.py generate l.365] (9/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,403 DEBUG generators.py generate l.370] (9/10) Post-process Answer
[2024-03-03 19:01:25,406 INFO generators.py gen_for_qa l.548] (9/10) * Start with LLM "claude-2.1"
[2024-03-03 19:01:25,406 DEBUG generators.py gen_for_qa l.554] (9/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,407 DEBUG generators.py generate l.352] (9/10) Reuse existing Prompt
[2024-03-03 19:01:25,407 DEBUG generators.py generate l.365] (9/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,407 DEBUG generators.py generate l.370] (9/10) Post-process Answer
[2024-03-03 19:01:25,407 INFO generators.py gen_for_qa l.548] (9/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-03 19:01:25,407 DEBUG generators.py gen_for_qa l.554] (9/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,411 DEBUG generators.py generate l.352] (9/10) Reuse existing Prompt
[2024-03-03 19:01:25,411 DEBUG generators.py generate l.365] (9/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,411 DEBUG generators.py generate l.370] (9/10) Post-process Answer
[2024-03-03 19:01:25,412 INFO generators.py gen_for_qa l.548] (9/10) * Start with LLM "mistral/mistral-small"
[2024-03-03 19:01:25,412 DEBUG generators.py gen_for_qa l.554] (9/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,413 DEBUG generators.py generate l.352] (9/10) Reuse existing Prompt
[2024-03-03 19:01:25,414 DEBUG generators.py generate l.365] (9/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,414 DEBUG generators.py generate l.370] (9/10) Post-process Answer
[2024-03-03 19:01:25,414 INFO generators.py generate l.477] (9/10) End question "Qui a réalisé le premier vol contrôlé d'un aéroplane ? A) Les frères Wright B) Clément Ader C) Alberto Santos-Dumont D) Louis Blériot"
[2024-03-03 19:01:25,415 INFO generators.py generate l.475] (10/10) *** AnsGenerator for question "L'invention de la lampe incandescente est souvent attribuée à : A) Thomas Edison B) Joseph Swan C) Humphry Davy"
[2024-03-03 19:01:25,415 INFO generators.py gen_for_qa l.548] (10/10) * Start with LLM "gpt-4"
[2024-03-03 19:01:25,418 DEBUG generators.py gen_for_qa l.554] (10/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,418 DEBUG generators.py generate l.352] (10/10) Reuse existing Prompt
[2024-03-03 19:01:25,419 DEBUG generators.py generate l.365] (10/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,419 DEBUG generators.py generate l.370] (10/10) Post-process Answer
[2024-03-03 19:01:25,419 INFO generators.py gen_for_qa l.548] (10/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-03 19:01:25,422 DEBUG generators.py gen_for_qa l.554] (10/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,423 DEBUG generators.py generate l.352] (10/10) Reuse existing Prompt
[2024-03-03 19:01:25,423 DEBUG generators.py generate l.365] (10/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,423 DEBUG generators.py generate l.370] (10/10) Post-process Answer
[2024-03-03 19:01:25,425 INFO generators.py gen_for_qa l.548] (10/10) * Start with LLM "gemini-pro"
[2024-03-03 19:01:25,425 DEBUG generators.py gen_for_qa l.554] (10/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,425 DEBUG generators.py generate l.352] (10/10) Reuse existing Prompt
[2024-03-03 19:01:25,425 DEBUG generators.py generate l.365] (10/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,425 DEBUG generators.py generate l.370] (10/10) Post-process Answer
[2024-03-03 19:01:25,428 INFO generators.py gen_for_qa l.548] (10/10) * Start with LLM "claude-2.1"
[2024-03-03 19:01:25,428 DEBUG generators.py gen_for_qa l.554] (10/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,428 DEBUG generators.py generate l.352] (10/10) Reuse existing Prompt
[2024-03-03 19:01:25,430 DEBUG generators.py generate l.365] (10/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,430 DEBUG generators.py generate l.370] (10/10) Post-process Answer
[2024-03-03 19:01:25,431 INFO generators.py gen_for_qa l.548] (10/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-03 19:01:25,431 DEBUG generators.py gen_for_qa l.554] (10/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,431 DEBUG generators.py generate l.352] (10/10) Reuse existing Prompt
[2024-03-03 19:01:25,432 DEBUG generators.py generate l.365] (10/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,432 DEBUG generators.py generate l.370] (10/10) Post-process Answer
[2024-03-03 19:01:25,434 INFO generators.py gen_for_qa l.548] (10/10) * Start with LLM "mistral/mistral-small"
[2024-03-03 19:01:25,435 DEBUG generators.py gen_for_qa l.554] (10/10) An Answer has already been generated with this LLM
[2024-03-03 19:01:25,436 DEBUG generators.py generate l.352] (10/10) Reuse existing Prompt
[2024-03-03 19:01:25,437 DEBUG generators.py generate l.365] (10/10) Reuse existing LLMAnswer in Answer
[2024-03-03 19:01:25,437 DEBUG generators.py generate l.370] (10/10) Post-process Answer
[2024-03-03 19:01:25,438 INFO generators.py generate l.477] (10/10) End question "L'invention de la lampe incandescente est souvent attribuée à : A) Thomas Edison B) Joseph Swan C) Humphry Davy"
[2024-03-03 19:01:25,441 INFO expe.py save_to_json l.283] (10/10) Expe saved as JSON to expe\Answers\culture--10Q_0C_0F_6M_60A_0HE_0AE_2024-03-03_19,01,25.json
[2024-03-03 19:01:25,442 INFO main.py <module> l.99] (10/10) MAIN ENDS
[2024-03-03 19:01:53,204 INFO main.py <module> l.87] MAIN STARTS
[2024-03-03 19:01:53,276 INFO expe.py save_to_html l.296] Expe saved as HTML to expe\Answers\culture--10Q_0C_0F_5M_50A_0HE_0AE_2024-03-03_19,01,53.html
[2024-03-03 19:01:53,278 INFO main.py <module> l.99] MAIN ENDS
[2024-03-03 19:02:29,699 INFO main.py <module> l.87] MAIN STARTS
[2024-03-03 19:02:29,763 INFO expe.py save_to_html l.296] Expe saved as HTML to expe\Answers\culture--10Q_0C_0F_6M_60A_0HE_0AE_2024-03-03_19,02,29.html
[2024-03-03 19:02:29,763 INFO main.py <module> l.99] MAIN ENDS
[2024-03-03 19:18:18,178 INFO main.py <module> l.87] MAIN STARTS
[2024-03-03 19:18:18,178 INFO generators.py generate l.475] (1/8) *** AnsGenerator for question "Who made crucial contributions to the development of the steam engine? A) James Watt B) Thomas Newcomen C) Denis Papin"
[2024-03-03 19:18:18,178 INFO generators.py gen_for_qa l.548] (1/8) * Start with LLM "gpt-4"
[2024-03-03 19:18:18,178 DEBUG generators.py generate l.349] (1/8) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:18:18,178 DEBUG generators.py generate l.358] (1/8) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:18:20,988 DEBUG generators.py generate l.370] (1/8) Post-process Answer
[2024-03-03 19:18:20,990 INFO generators.py gen_for_qa l.548] (1/8) * Start with LLM "gpt-3.5-turbo"
[2024-03-03 19:18:20,991 DEBUG generators.py generate l.349] (1/8) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:18:20,993 DEBUG generators.py generate l.358] (1/8) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:18:22,145 DEBUG generators.py generate l.370] (1/8) Post-process Answer
[2024-03-03 19:18:22,149 INFO generators.py gen_for_qa l.548] (1/8) * Start with LLM "gemini-pro"
[2024-03-03 19:18:22,152 DEBUG generators.py generate l.349] (1/8) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:18:22,155 DEBUG generators.py generate l.358] (1/8) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:21:10,953 INFO main.py <module> l.87] MAIN STARTS
[2024-03-03 19:21:10,953 INFO generators.py generate l.475] (1/10) *** AnsGenerator for question "Who is considered the inventor of the electric telegraph? A) Samuel Morse B) Charles Wheatstone C) Alexander Graham Bell D) Thomas Edison"
[2024-03-03 19:21:10,953 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "gpt-4"
[2024-03-03 19:21:10,964 DEBUG generators.py generate l.349] (1/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:21:10,964 DEBUG generators.py generate l.358] (1/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:21:17,200 DEBUG generators.py generate l.370] (1/10) Post-process Answer
[2024-03-03 19:21:17,205 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-03 19:21:17,205 DEBUG generators.py generate l.349] (1/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:21:17,206 DEBUG generators.py generate l.358] (1/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:21:21,618 DEBUG generators.py generate l.370] (1/10) Post-process Answer
[2024-03-03 19:21:21,618 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "gemini-pro"
[2024-03-03 19:21:21,618 DEBUG generators.py generate l.349] (1/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:21:21,624 DEBUG generators.py generate l.358] (1/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:21:35,701 DEBUG generators.py generate l.370] (1/10) Post-process Answer
[2024-03-03 19:21:35,704 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "claude-2.1"
[2024-03-03 19:21:35,708 DEBUG generators.py generate l.349] (1/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:21:35,710 DEBUG generators.py generate l.358] (1/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:21:36,998 DEBUG generators.py generate l.370] (1/10) Post-process Answer
[2024-03-03 19:21:36,998 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-03 19:21:36,998 DEBUG generators.py generate l.349] (1/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:21:36,998 DEBUG generators.py generate l.358] (1/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:21:37,551 DEBUG generators.py generate l.370] (1/10) Post-process Answer
[2024-03-03 19:21:37,553 INFO generators.py gen_for_qa l.548] (1/10) * Start with LLM "mistral/mistral-small"
[2024-03-03 19:21:37,557 DEBUG generators.py generate l.349] (1/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:21:37,559 DEBUG generators.py generate l.358] (1/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:21:38,471 DEBUG generators.py generate l.370] (1/10) Post-process Answer
[2024-03-03 19:21:38,477 INFO generators.py generate l.477] (1/10) End question "Who is considered the inventor of the electric telegraph? A) Samuel Morse B) Charles Wheatstone C) Alexander Graham Bell D) Thomas Edison"
[2024-03-03 19:21:38,479 INFO generators.py generate l.475] (2/10) *** AnsGenerator for question "Who are the pioneers of the cinematograph? A) Thomas Edison B) Les frères Lumière C) George Eastman D) William Friese-Greene"
[2024-03-03 19:21:38,480 INFO generators.py gen_for_qa l.548] (2/10) * Start with LLM "gpt-4"
[2024-03-03 19:21:38,480 DEBUG generators.py generate l.349] (2/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:21:38,487 DEBUG generators.py generate l.358] (2/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:21:41,058 DEBUG generators.py generate l.370] (2/10) Post-process Answer
[2024-03-03 19:21:41,058 INFO generators.py gen_for_qa l.548] (2/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-03 19:21:41,058 DEBUG generators.py generate l.349] (2/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:21:41,074 DEBUG generators.py generate l.358] (2/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:21:41,737 DEBUG generators.py generate l.370] (2/10) Post-process Answer
[2024-03-03 19:21:41,738 INFO generators.py gen_for_qa l.548] (2/10) * Start with LLM "gemini-pro"
[2024-03-03 19:21:41,740 DEBUG generators.py generate l.349] (2/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:21:41,741 DEBUG generators.py generate l.358] (2/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:21:44,614 DEBUG generators.py generate l.370] (2/10) Post-process Answer
[2024-03-03 19:21:44,630 INFO generators.py gen_for_qa l.548] (2/10) * Start with LLM "claude-2.1"
[2024-03-03 19:21:44,630 DEBUG generators.py generate l.349] (2/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:21:44,630 DEBUG generators.py generate l.358] (2/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:21:46,002 DEBUG generators.py generate l.370] (2/10) Post-process Answer
[2024-03-03 19:21:46,018 INFO generators.py gen_for_qa l.548] (2/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-03 19:21:46,018 DEBUG generators.py generate l.349] (2/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:21:46,018 DEBUG generators.py generate l.358] (2/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:21:46,985 DEBUG generators.py generate l.370] (2/10) Post-process Answer
[2024-03-03 19:21:46,989 INFO generators.py gen_for_qa l.548] (2/10) * Start with LLM "mistral/mistral-small"
[2024-03-03 19:21:46,989 DEBUG generators.py generate l.349] (2/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:21:46,989 DEBUG generators.py generate l.358] (2/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:21:48,951 DEBUG generators.py generate l.370] (2/10) Post-process Answer
[2024-03-03 19:21:48,951 INFO generators.py generate l.477] (2/10) End question "Who are the pioneers of the cinematograph? A) Thomas Edison B) Les frères Lumière C) George Eastman D) William Friese-Greene"
[2024-03-03 19:21:48,957 INFO generators.py generate l.475] (3/10) *** AnsGenerator for question "Who made crucial contributions to the development of the steam engine? A) James Watt B) Thomas Newcomen C) Denis Papin"
[2024-03-03 19:21:48,957 INFO generators.py gen_for_qa l.548] (3/10) * Start with LLM "gpt-4"
[2024-03-03 19:21:48,965 DEBUG generators.py generate l.349] (3/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:21:48,965 DEBUG generators.py generate l.358] (3/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:21:55,934 DEBUG generators.py generate l.370] (3/10) Post-process Answer
[2024-03-03 19:21:55,940 INFO generators.py gen_for_qa l.548] (3/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-03 19:21:55,942 DEBUG generators.py generate l.349] (3/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:21:55,942 DEBUG generators.py generate l.358] (3/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:21:59,546 DEBUG generators.py generate l.370] (3/10) Post-process Answer
[2024-03-03 19:21:59,546 INFO generators.py gen_for_qa l.548] (3/10) * Start with LLM "gemini-pro"
[2024-03-03 19:21:59,554 DEBUG generators.py generate l.349] (3/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:21:59,561 DEBUG generators.py generate l.358] (3/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:22:03,753 DEBUG generators.py generate l.370] (3/10) Post-process Answer
[2024-03-03 19:22:03,769 INFO generators.py gen_for_qa l.548] (3/10) * Start with LLM "claude-2.1"
[2024-03-03 19:22:03,769 DEBUG generators.py generate l.349] (3/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:22:03,769 DEBUG generators.py generate l.358] (3/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:22:05,248 DEBUG generators.py generate l.370] (3/10) Post-process Answer
[2024-03-03 19:22:05,248 INFO generators.py gen_for_qa l.548] (3/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-03 19:22:05,248 DEBUG generators.py generate l.349] (3/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:22:05,248 DEBUG generators.py generate l.358] (3/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:22:09,643 DEBUG generators.py generate l.370] (3/10) Post-process Answer
[2024-03-03 19:22:09,643 INFO generators.py gen_for_qa l.548] (3/10) * Start with LLM "mistral/mistral-small"
[2024-03-03 19:22:09,643 DEBUG generators.py generate l.349] (3/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:22:09,657 DEBUG generators.py generate l.358] (3/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:22:11,272 DEBUG generators.py generate l.370] (3/10) Post-process Answer
[2024-03-03 19:22:11,276 INFO generators.py generate l.477] (3/10) End question "Who made crucial contributions to the development of the steam engine? A) James Watt B) Thomas Newcomen C) Denis Papin"
[2024-03-03 19:22:11,277 INFO generators.py generate l.475] (4/10) *** AnsGenerator for question "Who is recognized for having invented the stethoscope? A) René Laennec B) Hippocrate C) Antonie van Leeuwenhoek D) Edward Jenner"
[2024-03-03 19:22:11,279 INFO generators.py gen_for_qa l.548] (4/10) * Start with LLM "gpt-4"
[2024-03-03 19:22:11,279 DEBUG generators.py generate l.349] (4/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:22:11,285 DEBUG generators.py generate l.358] (4/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:22:15,687 DEBUG generators.py generate l.370] (4/10) Post-process Answer
[2024-03-03 19:22:15,695 INFO generators.py gen_for_qa l.548] (4/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-03 19:22:15,700 DEBUG generators.py generate l.349] (4/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:22:15,707 DEBUG generators.py generate l.358] (4/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:22:18,447 DEBUG generators.py generate l.370] (4/10) Post-process Answer
[2024-03-03 19:22:18,459 INFO generators.py gen_for_qa l.548] (4/10) * Start with LLM "gemini-pro"
[2024-03-03 19:22:18,465 DEBUG generators.py generate l.349] (4/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:22:18,471 DEBUG generators.py generate l.358] (4/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:22:22,323 DEBUG generators.py generate l.370] (4/10) Post-process Answer
[2024-03-03 19:22:22,328 INFO generators.py gen_for_qa l.548] (4/10) * Start with LLM "claude-2.1"
[2024-03-03 19:22:22,336 DEBUG generators.py generate l.349] (4/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:22:22,341 DEBUG generators.py generate l.358] (4/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:22:24,034 DEBUG generators.py generate l.370] (4/10) Post-process Answer
[2024-03-03 19:22:24,041 INFO generators.py gen_for_qa l.548] (4/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-03 19:22:24,044 DEBUG generators.py generate l.349] (4/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:22:24,048 DEBUG generators.py generate l.358] (4/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:22:26,042 DEBUG generators.py generate l.370] (4/10) Post-process Answer
[2024-03-03 19:22:26,042 INFO generators.py gen_for_qa l.548] (4/10) * Start with LLM "mistral/mistral-small"
[2024-03-03 19:22:26,042 DEBUG generators.py generate l.349] (4/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:22:26,042 DEBUG generators.py generate l.358] (4/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:22:26,994 DEBUG generators.py generate l.370] (4/10) Post-process Answer
[2024-03-03 19:22:26,994 INFO generators.py generate l.477] (4/10) End question "Who is recognized for having invented the stethoscope? A) René Laennec B) Hippocrate C) Antonie van Leeuwenhoek D) Edward Jenner"
[2024-03-03 19:22:26,994 INFO generators.py generate l.475] (5/10) *** AnsGenerator for question "Which scientist is credited with the invention of photography? A) Nicéphore Niépce B) Louis Daguerre C) William Henry Fox Talbot"
[2024-03-03 19:22:26,994 INFO generators.py gen_for_qa l.548] (5/10) * Start with LLM "gpt-4"
[2024-03-03 19:22:27,008 DEBUG generators.py generate l.349] (5/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:22:27,008 DEBUG generators.py generate l.358] (5/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:22:31,289 DEBUG generators.py generate l.370] (5/10) Post-process Answer
[2024-03-03 19:22:31,289 INFO generators.py gen_for_qa l.548] (5/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-03 19:22:31,289 DEBUG generators.py generate l.349] (5/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:22:31,289 DEBUG generators.py generate l.358] (5/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:22:35,242 DEBUG generators.py generate l.370] (5/10) Post-process Answer
[2024-03-03 19:22:35,242 INFO generators.py gen_for_qa l.548] (5/10) * Start with LLM "gemini-pro"
[2024-03-03 19:22:35,242 DEBUG generators.py generate l.349] (5/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:22:35,250 DEBUG generators.py generate l.358] (5/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:22:38,388 DEBUG generators.py generate l.370] (5/10) Post-process Answer
[2024-03-03 19:22:38,398 INFO generators.py gen_for_qa l.548] (5/10) * Start with LLM "claude-2.1"
[2024-03-03 19:22:38,398 DEBUG generators.py generate l.349] (5/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:22:38,403 DEBUG generators.py generate l.358] (5/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:22:41,414 DEBUG generators.py generate l.370] (5/10) Post-process Answer
[2024-03-03 19:22:41,416 INFO generators.py gen_for_qa l.548] (5/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-03 19:22:41,422 DEBUG generators.py generate l.349] (5/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:22:41,423 DEBUG generators.py generate l.358] (5/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:22:42,319 DEBUG generators.py generate l.370] (5/10) Post-process Answer
[2024-03-03 19:22:42,319 INFO generators.py gen_for_qa l.548] (5/10) * Start with LLM "mistral/mistral-small"
[2024-03-03 19:22:42,333 DEBUG generators.py generate l.349] (5/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:22:42,333 DEBUG generators.py generate l.358] (5/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:22:43,954 DEBUG generators.py generate l.370] (5/10) Post-process Answer
[2024-03-03 19:22:43,954 INFO generators.py generate l.477] (5/10) End question "Which scientist is credited with the invention of photography? A) Nicéphore Niépce B) Louis Daguerre C) William Henry Fox Talbot"
[2024-03-03 19:22:43,960 INFO generators.py generate l.475] (6/10) *** AnsGenerator for question "Who is associated with the invention of the Champagne method for producing champagne? A) Dom Pérignon B) Louis Pasteur C) Veuve Clicquot D) John Pemberton"
[2024-03-03 19:22:43,963 INFO generators.py gen_for_qa l.548] (6/10) * Start with LLM "gpt-4"
[2024-03-03 19:22:43,966 DEBUG generators.py generate l.349] (6/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:22:43,967 DEBUG generators.py generate l.358] (6/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:22:45,171 DEBUG generators.py generate l.370] (6/10) Post-process Answer
[2024-03-03 19:22:45,187 INFO generators.py gen_for_qa l.548] (6/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-03 19:22:45,187 DEBUG generators.py generate l.349] (6/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:22:45,187 DEBUG generators.py generate l.358] (6/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:22:46,347 DEBUG generators.py generate l.370] (6/10) Post-process Answer
[2024-03-03 19:22:46,347 INFO generators.py gen_for_qa l.548] (6/10) * Start with LLM "gemini-pro"
[2024-03-03 19:22:46,363 DEBUG generators.py generate l.349] (6/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:22:46,363 DEBUG generators.py generate l.358] (6/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:22:50,017 DEBUG generators.py generate l.370] (6/10) Post-process Answer
[2024-03-03 19:22:50,025 INFO generators.py gen_for_qa l.548] (6/10) * Start with LLM "claude-2.1"
[2024-03-03 19:22:50,028 DEBUG generators.py generate l.349] (6/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:22:50,034 DEBUG generators.py generate l.358] (6/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:22:51,446 DEBUG generators.py generate l.370] (6/10) Post-process Answer
[2024-03-03 19:22:51,446 INFO generators.py gen_for_qa l.548] (6/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-03 19:22:51,453 DEBUG generators.py generate l.349] (6/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:22:51,453 DEBUG generators.py generate l.358] (6/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:22:52,268 DEBUG generators.py generate l.370] (6/10) Post-process Answer
[2024-03-03 19:22:52,278 INFO generators.py gen_for_qa l.548] (6/10) * Start with LLM "mistral/mistral-small"
[2024-03-03 19:22:52,281 DEBUG generators.py generate l.349] (6/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:22:52,288 DEBUG generators.py generate l.358] (6/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:22:54,205 DEBUG generators.py generate l.370] (6/10) Post-process Answer
[2024-03-03 19:22:54,205 INFO generators.py generate l.477] (6/10) End question "Who is associated with the invention of the Champagne method for producing champagne? A) Dom Pérignon B) Louis Pasteur C) Veuve Clicquot D) John Pemberton"
[2024-03-03 19:22:54,205 INFO generators.py generate l.475] (7/10) *** AnsGenerator for question "The discovery of infinitesimal calculus is attributed to: A) Isaac Newton B) Gottfried Wilhelm Leibniz C) Albert Einstein"
[2024-03-03 19:22:54,221 INFO generators.py gen_for_qa l.548] (7/10) * Start with LLM "gpt-4"
[2024-03-03 19:22:54,221 DEBUG generators.py generate l.349] (7/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:22:54,221 DEBUG generators.py generate l.358] (7/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:22:56,896 DEBUG generators.py generate l.370] (7/10) Post-process Answer
[2024-03-03 19:22:56,912 INFO generators.py gen_for_qa l.548] (7/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-03 19:22:56,912 DEBUG generators.py generate l.349] (7/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:22:56,912 DEBUG generators.py generate l.358] (7/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:23:02,622 DEBUG generators.py generate l.370] (7/10) Post-process Answer
[2024-03-03 19:23:02,622 INFO generators.py gen_for_qa l.548] (7/10) * Start with LLM "gemini-pro"
[2024-03-03 19:23:02,638 DEBUG generators.py generate l.349] (7/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:23:02,638 DEBUG generators.py generate l.358] (7/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:23:06,191 DEBUG generators.py generate l.370] (7/10) Post-process Answer
[2024-03-03 19:23:06,194 INFO generators.py gen_for_qa l.548] (7/10) * Start with LLM "claude-2.1"
[2024-03-03 19:23:06,195 DEBUG generators.py generate l.349] (7/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:23:06,195 DEBUG generators.py generate l.358] (7/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:23:07,902 DEBUG generators.py generate l.370] (7/10) Post-process Answer
[2024-03-03 19:23:07,902 INFO generators.py gen_for_qa l.548] (7/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-03 19:23:07,902 DEBUG generators.py generate l.349] (7/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:23:07,902 DEBUG generators.py generate l.358] (7/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:23:09,889 DEBUG generators.py generate l.370] (7/10) Post-process Answer
[2024-03-03 19:23:09,889 INFO generators.py gen_for_qa l.548] (7/10) * Start with LLM "mistral/mistral-small"
[2024-03-03 19:23:09,898 DEBUG generators.py generate l.349] (7/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:23:09,902 DEBUG generators.py generate l.358] (7/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:23:11,955 DEBUG generators.py generate l.370] (7/10) Post-process Answer
[2024-03-03 19:23:11,970 INFO generators.py generate l.477] (7/10) End question "The discovery of infinitesimal calculus is attributed to: A) Isaac Newton B) Gottfried Wilhelm Leibniz C) Albert Einstein"
[2024-03-03 19:23:11,970 INFO generators.py generate l.475] (8/10) *** AnsGenerator for question "Who is considered one of the pioneers of radio? A) Nikola Tesla B) Guglielmo Marconi C) Édouard Branly"
[2024-03-03 19:23:11,970 INFO generators.py gen_for_qa l.548] (8/10) * Start with LLM "gpt-4"
[2024-03-03 19:23:11,970 DEBUG generators.py generate l.349] (8/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:23:11,970 DEBUG generators.py generate l.358] (8/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:23:17,128 DEBUG generators.py generate l.370] (8/10) Post-process Answer
[2024-03-03 19:23:17,133 INFO generators.py gen_for_qa l.548] (8/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-03 19:23:17,135 DEBUG generators.py generate l.349] (8/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:23:17,137 DEBUG generators.py generate l.358] (8/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:23:22,545 DEBUG generators.py generate l.370] (8/10) Post-process Answer
[2024-03-03 19:23:22,545 INFO generators.py gen_for_qa l.548] (8/10) * Start with LLM "gemini-pro"
[2024-03-03 19:23:22,549 DEBUG generators.py generate l.349] (8/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:23:22,551 DEBUG generators.py generate l.358] (8/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:23:26,524 DEBUG generators.py generate l.370] (8/10) Post-process Answer
[2024-03-03 19:23:26,524 INFO generators.py gen_for_qa l.548] (8/10) * Start with LLM "claude-2.1"
[2024-03-03 19:23:26,524 DEBUG generators.py generate l.349] (8/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:23:26,524 DEBUG generators.py generate l.358] (8/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:23:27,890 DEBUG generators.py generate l.370] (8/10) Post-process Answer
[2024-03-03 19:23:27,893 INFO generators.py gen_for_qa l.548] (8/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-03 19:23:27,898 DEBUG generators.py generate l.349] (8/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:23:27,899 DEBUG generators.py generate l.358] (8/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:23:29,097 DEBUG generators.py generate l.370] (8/10) Post-process Answer
[2024-03-03 19:23:29,100 INFO generators.py gen_for_qa l.548] (8/10) * Start with LLM "mistral/mistral-small"
[2024-03-03 19:23:29,101 DEBUG generators.py generate l.349] (8/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:23:29,102 DEBUG generators.py generate l.358] (8/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:23:30,510 DEBUG generators.py generate l.370] (8/10) Post-process Answer
[2024-03-03 19:23:30,520 INFO generators.py generate l.477] (8/10) End question "Who is considered one of the pioneers of radio? A) Nikola Tesla B) Guglielmo Marconi C) Édouard Branly"
[2024-03-03 19:23:30,522 INFO generators.py generate l.475] (9/10) *** AnsGenerator for question "Who achieved the first controlled flight of an airplane? A) Les frères Wright B) Clément Ader C) Alberto Santos-Dumont D) Louis Blériot"
[2024-03-03 19:23:30,522 INFO generators.py gen_for_qa l.548] (9/10) * Start with LLM "gpt-4"
[2024-03-03 19:23:30,522 DEBUG generators.py generate l.349] (9/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:23:30,522 DEBUG generators.py generate l.358] (9/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:23:34,664 DEBUG generators.py generate l.370] (9/10) Post-process Answer
[2024-03-03 19:23:34,679 INFO generators.py gen_for_qa l.548] (9/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-03 19:23:34,679 DEBUG generators.py generate l.349] (9/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:23:34,679 DEBUG generators.py generate l.358] (9/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:23:37,502 DEBUG generators.py generate l.370] (9/10) Post-process Answer
[2024-03-03 19:23:37,502 INFO generators.py gen_for_qa l.548] (9/10) * Start with LLM "gemini-pro"
[2024-03-03 19:23:37,516 DEBUG generators.py generate l.349] (9/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:23:37,516 DEBUG generators.py generate l.358] (9/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:23:43,207 DEBUG generators.py generate l.370] (9/10) Post-process Answer
[2024-03-03 19:23:43,207 INFO generators.py gen_for_qa l.548] (9/10) * Start with LLM "claude-2.1"
[2024-03-03 19:23:43,207 DEBUG generators.py generate l.349] (9/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:23:43,207 DEBUG generators.py generate l.358] (9/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:23:44,555 DEBUG generators.py generate l.370] (9/10) Post-process Answer
[2024-03-03 19:23:44,555 INFO generators.py gen_for_qa l.548] (9/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-03 19:23:44,555 DEBUG generators.py generate l.349] (9/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:23:44,555 DEBUG generators.py generate l.358] (9/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:23:45,634 DEBUG generators.py generate l.370] (9/10) Post-process Answer
[2024-03-03 19:23:45,636 INFO generators.py gen_for_qa l.548] (9/10) * Start with LLM "mistral/mistral-small"
[2024-03-03 19:23:45,640 DEBUG generators.py generate l.349] (9/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:23:45,641 DEBUG generators.py generate l.358] (9/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:23:47,132 DEBUG generators.py generate l.370] (9/10) Post-process Answer
[2024-03-03 19:23:47,135 INFO generators.py generate l.477] (9/10) End question "Who achieved the first controlled flight of an airplane? A) Les frères Wright B) Clément Ader C) Alberto Santos-Dumont D) Louis Blériot"
[2024-03-03 19:23:47,136 INFO generators.py generate l.475] (10/10) *** AnsGenerator for question "The invention of the incandescent lamp is often attributed to: A) Thomas Edison B) Joseph Swan C) Humphry Davy"
[2024-03-03 19:23:47,136 INFO generators.py gen_for_qa l.548] (10/10) * Start with LLM "gpt-4"
[2024-03-03 19:23:47,139 DEBUG generators.py generate l.349] (10/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:23:47,141 DEBUG generators.py generate l.358] (10/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:23:50,354 DEBUG generators.py generate l.370] (10/10) Post-process Answer
[2024-03-03 19:23:50,357 INFO generators.py gen_for_qa l.548] (10/10) * Start with LLM "gpt-3.5-turbo"
[2024-03-03 19:23:50,360 DEBUG generators.py generate l.349] (10/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:23:50,362 DEBUG generators.py generate l.358] (10/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:23:54,771 DEBUG generators.py generate l.370] (10/10) Post-process Answer
[2024-03-03 19:23:54,771 INFO generators.py gen_for_qa l.548] (10/10) * Start with LLM "gemini-pro"
[2024-03-03 19:23:54,782 DEBUG generators.py generate l.349] (10/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:23:54,785 DEBUG generators.py generate l.358] (10/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:24:00,245 DEBUG generators.py generate l.370] (10/10) Post-process Answer
[2024-03-03 19:24:00,251 INFO generators.py gen_for_qa l.548] (10/10) * Start with LLM "claude-2.1"
[2024-03-03 19:24:00,253 DEBUG generators.py generate l.349] (10/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:24:00,255 DEBUG generators.py generate l.358] (10/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:24:01,485 DEBUG generators.py generate l.370] (10/10) Post-process Answer
[2024-03-03 19:24:01,489 INFO generators.py gen_for_qa l.548] (10/10) * Start with LLM "mistral/mistral-large-latest"
[2024-03-03 19:24:01,491 DEBUG generators.py generate l.349] (10/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:24:01,495 DEBUG generators.py generate l.358] (10/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:24:02,618 DEBUG generators.py generate l.370] (10/10) Post-process Answer
[2024-03-03 19:24:02,622 INFO generators.py gen_for_qa l.548] (10/10) * Start with LLM "mistral/mistral-small"
[2024-03-03 19:24:02,626 DEBUG generators.py generate l.349] (10/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-03 19:24:02,634 DEBUG generators.py generate l.358] (10/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-03 19:24:05,021 DEBUG generators.py generate l.370] (10/10) Post-process Answer
[2024-03-03 19:24:05,027 INFO generators.py generate l.477] (10/10) End question "The invention of the incandescent lamp is often attributed to: A) Thomas Edison B) Joseph Swan C) Humphry Davy"
[2024-03-03 19:24:05,037 INFO expe.py save_to_json l.283] (10/10) Expe saved as JSON to expe\Answers\culture_en--10Q_0C_0F_6M_60A_0HE_0AE_2024-03-03_19,24,05.json
[2024-03-03 19:24:05,037 INFO main.py <module> l.99] (10/10) MAIN ENDS
[2024-03-03 19:30:05,370 INFO main.py <module> l.87] MAIN STARTS
[2024-03-03 19:30:05,470 INFO expe.py save_to_html l.296] Expe saved as HTML to expe\Answers\culture_en--10Q_0C_0F_6M_60A_0HE_0AE_2024-03-03_19,30,05.html
[2024-03-03 19:30:05,470 INFO main.py <module> l.99] MAIN ENDS
[2024-03-04 10:44:14,248 INFO main.py <module> l.87] MAIN STARTS
[2024-03-04 10:44:14,270 INFO generators.py generate l.475] (1/93) *** AnsGenerator for question "Qui a inventé le télégraphe électrique ?  A)  Samuel Morse B)  Charles Wheatstone C)  William Fothergill Cooke "
[2024-03-04 10:44:14,274 INFO generators.py gen_for_qa l.548] (1/93) * Start with LLM "gpt-4"
[2024-03-04 10:44:14,275 DEBUG generators.py generate l.349] (1/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:44:14,276 DEBUG generators.py generate l.358] (1/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:44:15,105 DEBUG generators.py generate l.370] (1/93) Post-process Answer
[2024-03-04 10:44:15,107 INFO generators.py gen_for_qa l.548] (1/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:44:15,111 DEBUG generators.py generate l.349] (1/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:44:15,113 DEBUG generators.py generate l.358] (1/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:44:15,667 DEBUG generators.py generate l.370] (1/93) Post-process Answer
[2024-03-04 10:44:15,667 INFO generators.py gen_for_qa l.548] (1/93) * Start with LLM "gemini-pro"
[2024-03-04 10:44:15,673 DEBUG generators.py generate l.349] (1/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:44:15,676 DEBUG generators.py generate l.358] (1/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:44:30,516 DEBUG generators.py generate l.370] (1/93) Post-process Answer
[2024-03-04 10:44:30,520 INFO generators.py gen_for_qa l.548] (1/93) * Start with LLM "claude-2.1"
[2024-03-04 10:44:30,524 DEBUG generators.py generate l.349] (1/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:44:30,525 DEBUG generators.py generate l.358] (1/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:44:33,856 DEBUG generators.py generate l.370] (1/93) Post-process Answer
[2024-03-04 10:44:33,858 INFO generators.py gen_for_qa l.548] (1/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:44:33,860 DEBUG generators.py generate l.349] (1/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:44:33,861 DEBUG generators.py generate l.358] (1/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:44:34,211 DEBUG generators.py generate l.370] (1/93) Post-process Answer
[2024-03-04 10:44:34,212 INFO generators.py gen_for_qa l.548] (1/93) * Start with LLM "command-nightly"
[2024-03-04 10:44:34,215 DEBUG generators.py generate l.349] (1/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:44:34,217 DEBUG generators.py generate l.358] (1/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:44:34,533 DEBUG generators.py generate l.370] (1/93) Post-process Answer
[2024-03-04 10:44:34,533 INFO generators.py gen_for_qa l.548] (1/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:44:34,541 DEBUG generators.py generate l.349] (1/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:44:34,542 DEBUG generators.py generate l.358] (1/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:44:34,954 ERROR generators.py complete l.400] (1/93) The following exception occurred with prompt meta={} user="Qui a inventé le télégraphe électrique ?  A)  Samuel Morse B)  Charles Wheatstone C)  William Fothergill Cooke .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:44:35,051 DEBUG generators.py generate l.373] (1/93) Reuse post-processing
[2024-03-04 10:44:35,052 INFO generators.py generate l.477] (1/93) End question "Qui a inventé le télégraphe électrique ?  A)  Samuel Morse B)  Charles Wheatstone C)  William Fothergill Cooke "
[2024-03-04 10:44:35,053 INFO generators.py generate l.475] (2/93) *** AnsGenerator for question "Qui a inventé la photographie ?  A)  Louis Daguerre B)  William Henry Fox Talbot "
[2024-03-04 10:44:35,054 INFO generators.py gen_for_qa l.548] (2/93) * Start with LLM "gpt-4"
[2024-03-04 10:44:35,055 DEBUG generators.py generate l.349] (2/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:44:35,056 DEBUG generators.py generate l.358] (2/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:44:35,802 DEBUG generators.py generate l.370] (2/93) Post-process Answer
[2024-03-04 10:44:35,804 INFO generators.py gen_for_qa l.548] (2/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:44:35,806 DEBUG generators.py generate l.349] (2/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:44:35,807 DEBUG generators.py generate l.358] (2/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:44:36,800 DEBUG generators.py generate l.370] (2/93) Post-process Answer
[2024-03-04 10:44:36,803 INFO generators.py gen_for_qa l.548] (2/93) * Start with LLM "gemini-pro"
[2024-03-04 10:44:36,804 DEBUG generators.py generate l.349] (2/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:44:36,805 DEBUG generators.py generate l.358] (2/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:44:39,017 DEBUG generators.py generate l.370] (2/93) Post-process Answer
[2024-03-04 10:44:39,019 INFO generators.py gen_for_qa l.548] (2/93) * Start with LLM "claude-2.1"
[2024-03-04 10:44:39,020 DEBUG generators.py generate l.349] (2/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:44:39,021 DEBUG generators.py generate l.358] (2/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:44:42,239 DEBUG generators.py generate l.370] (2/93) Post-process Answer
[2024-03-04 10:44:42,241 INFO generators.py gen_for_qa l.548] (2/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:44:42,244 DEBUG generators.py generate l.349] (2/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:44:42,246 DEBUG generators.py generate l.358] (2/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:44:42,595 DEBUG generators.py generate l.370] (2/93) Post-process Answer
[2024-03-04 10:44:42,597 INFO generators.py gen_for_qa l.548] (2/93) * Start with LLM "command-nightly"
[2024-03-04 10:44:42,599 DEBUG generators.py generate l.349] (2/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:44:42,600 DEBUG generators.py generate l.358] (2/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:44:42,911 DEBUG generators.py generate l.370] (2/93) Post-process Answer
[2024-03-04 10:44:42,913 INFO generators.py gen_for_qa l.548] (2/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:44:42,915 DEBUG generators.py generate l.349] (2/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:44:42,916 DEBUG generators.py generate l.358] (2/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:44:43,303 ERROR generators.py complete l.400] (2/93) The following exception occurred with prompt meta={} user="Qui a inventé la photographie ?  A)  Louis Daguerre B)  William Henry Fox Talbot .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:44:43,313 DEBUG generators.py generate l.373] (2/93) Reuse post-processing
[2024-03-04 10:44:43,314 INFO generators.py generate l.477] (2/93) End question "Qui a inventé la photographie ?  A)  Louis Daguerre B)  William Henry Fox Talbot "
[2024-03-04 10:44:43,315 INFO generators.py generate l.475] (3/93) *** AnsGenerator for question "Qui a inventé le moteur à vapeur ?  A)  Thomas Newcomen B)  Denis Papin "
[2024-03-04 10:44:43,316 INFO generators.py gen_for_qa l.548] (3/93) * Start with LLM "gpt-4"
[2024-03-04 10:44:43,318 DEBUG generators.py generate l.349] (3/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:44:43,320 DEBUG generators.py generate l.358] (3/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:44:44,407 DEBUG generators.py generate l.370] (3/93) Post-process Answer
[2024-03-04 10:44:44,408 INFO generators.py gen_for_qa l.548] (3/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:44:44,410 DEBUG generators.py generate l.349] (3/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:44:44,411 DEBUG generators.py generate l.358] (3/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:44:45,465 DEBUG generators.py generate l.370] (3/93) Post-process Answer
[2024-03-04 10:44:45,468 INFO generators.py gen_for_qa l.548] (3/93) * Start with LLM "gemini-pro"
[2024-03-04 10:44:45,468 DEBUG generators.py generate l.349] (3/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:44:45,472 DEBUG generators.py generate l.358] (3/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:44:47,930 DEBUG generators.py generate l.370] (3/93) Post-process Answer
[2024-03-04 10:44:47,933 INFO generators.py gen_for_qa l.548] (3/93) * Start with LLM "claude-2.1"
[2024-03-04 10:44:47,934 DEBUG generators.py generate l.349] (3/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:44:47,935 DEBUG generators.py generate l.358] (3/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:44:52,203 DEBUG generators.py generate l.370] (3/93) Post-process Answer
[2024-03-04 10:44:52,207 INFO generators.py gen_for_qa l.548] (3/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:44:52,210 DEBUG generators.py generate l.349] (3/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:44:52,213 DEBUG generators.py generate l.358] (3/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:44:52,858 DEBUG generators.py generate l.370] (3/93) Post-process Answer
[2024-03-04 10:44:52,869 INFO generators.py gen_for_qa l.548] (3/93) * Start with LLM "command-nightly"
[2024-03-04 10:44:52,875 DEBUG generators.py generate l.349] (3/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:44:52,879 DEBUG generators.py generate l.358] (3/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:44:53,195 DEBUG generators.py generate l.370] (3/93) Post-process Answer
[2024-03-04 10:44:53,198 INFO generators.py gen_for_qa l.548] (3/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:44:53,200 DEBUG generators.py generate l.349] (3/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:44:53,201 DEBUG generators.py generate l.358] (3/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:44:53,641 ERROR generators.py complete l.400] (3/93) The following exception occurred with prompt meta={} user="Qui a inventé le moteur à vapeur ?  A)  Thomas Newcomen B)  Denis Papin .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:44:53,647 DEBUG generators.py generate l.373] (3/93) Reuse post-processing
[2024-03-04 10:44:53,647 INFO generators.py generate l.477] (3/93) End question "Qui a inventé le moteur à vapeur ?  A)  Thomas Newcomen B)  Denis Papin "
[2024-03-04 10:44:53,647 INFO generators.py generate l.475] (4/93) *** AnsGenerator for question "Qui a inventé le bateau à vapeur ?  A)  James Watt B)  Claude de Jouffroy d'Abbans "
[2024-03-04 10:44:53,647 INFO generators.py gen_for_qa l.548] (4/93) * Start with LLM "gpt-4"
[2024-03-04 10:44:53,661 DEBUG generators.py generate l.349] (4/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:44:53,663 DEBUG generators.py generate l.358] (4/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:44:54,673 DEBUG generators.py generate l.370] (4/93) Post-process Answer
[2024-03-04 10:44:54,673 INFO generators.py gen_for_qa l.548] (4/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:44:54,673 DEBUG generators.py generate l.349] (4/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:44:54,689 DEBUG generators.py generate l.358] (4/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:44:55,724 DEBUG generators.py generate l.370] (4/93) Post-process Answer
[2024-03-04 10:44:55,740 INFO generators.py gen_for_qa l.548] (4/93) * Start with LLM "gemini-pro"
[2024-03-04 10:44:55,740 DEBUG generators.py generate l.349] (4/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:44:55,740 DEBUG generators.py generate l.358] (4/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:44:58,269 DEBUG generators.py generate l.370] (4/93) Post-process Answer
[2024-03-04 10:44:58,275 INFO generators.py gen_for_qa l.548] (4/93) * Start with LLM "claude-2.1"
[2024-03-04 10:44:58,275 DEBUG generators.py generate l.349] (4/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:44:58,285 DEBUG generators.py generate l.358] (4/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:45:02,853 DEBUG generators.py generate l.370] (4/93) Post-process Answer
[2024-03-04 10:45:02,857 INFO generators.py gen_for_qa l.548] (4/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:45:02,859 DEBUG generators.py generate l.349] (4/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:45:02,859 DEBUG generators.py generate l.358] (4/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:45:03,805 DEBUG generators.py generate l.370] (4/93) Post-process Answer
[2024-03-04 10:45:03,809 INFO generators.py gen_for_qa l.548] (4/93) * Start with LLM "command-nightly"
[2024-03-04 10:45:03,813 DEBUG generators.py generate l.349] (4/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:45:03,813 DEBUG generators.py generate l.358] (4/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:45:05,023 DEBUG generators.py generate l.370] (4/93) Post-process Answer
[2024-03-04 10:45:05,038 INFO generators.py gen_for_qa l.548] (4/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:45:05,038 DEBUG generators.py generate l.349] (4/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:45:05,038 DEBUG generators.py generate l.358] (4/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:45:05,634 ERROR generators.py complete l.400] (4/93) The following exception occurred with prompt meta={} user="Qui a inventé le bateau à vapeur ?  A)  James Watt B)  Claude de Jouffroy d'Abbans .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:45:05,650 DEBUG generators.py generate l.373] (4/93) Reuse post-processing
[2024-03-04 10:45:05,650 INFO generators.py generate l.477] (4/93) End question "Qui a inventé le bateau à vapeur ?  A)  James Watt B)  Claude de Jouffroy d'Abbans "
[2024-03-04 10:45:05,665 INFO generators.py generate l.475] (5/93) *** AnsGenerator for question "Qui a inventé le parachute ?  A)  Louis-Sébastien Lenormand B)  Sir George Cayley "
[2024-03-04 10:45:05,665 INFO generators.py gen_for_qa l.548] (5/93) * Start with LLM "gpt-4"
[2024-03-04 10:45:05,665 DEBUG generators.py generate l.349] (5/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:45:05,665 DEBUG generators.py generate l.358] (5/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:45:06,527 DEBUG generators.py generate l.370] (5/93) Post-process Answer
[2024-03-04 10:45:06,531 INFO generators.py gen_for_qa l.548] (5/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:45:06,535 DEBUG generators.py generate l.349] (5/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:45:06,538 DEBUG generators.py generate l.358] (5/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:45:07,245 DEBUG generators.py generate l.370] (5/93) Post-process Answer
[2024-03-04 10:45:07,251 INFO generators.py gen_for_qa l.548] (5/93) * Start with LLM "gemini-pro"
[2024-03-04 10:45:07,251 DEBUG generators.py generate l.349] (5/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:45:07,251 DEBUG generators.py generate l.358] (5/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:45:09,621 DEBUG generators.py generate l.370] (5/93) Post-process Answer
[2024-03-04 10:45:09,626 INFO generators.py gen_for_qa l.548] (5/93) * Start with LLM "claude-2.1"
[2024-03-04 10:45:09,626 DEBUG generators.py generate l.349] (5/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:45:09,634 DEBUG generators.py generate l.358] (5/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:45:13,248 DEBUG generators.py generate l.370] (5/93) Post-process Answer
[2024-03-04 10:45:13,248 INFO generators.py gen_for_qa l.548] (5/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:45:13,248 DEBUG generators.py generate l.349] (5/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:45:13,253 DEBUG generators.py generate l.358] (5/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:45:13,684 DEBUG generators.py generate l.370] (5/93) Post-process Answer
[2024-03-04 10:45:13,686 INFO generators.py gen_for_qa l.548] (5/93) * Start with LLM "command-nightly"
[2024-03-04 10:45:13,687 DEBUG generators.py generate l.349] (5/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:45:13,688 DEBUG generators.py generate l.358] (5/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:45:13,988 DEBUG generators.py generate l.370] (5/93) Post-process Answer
[2024-03-04 10:45:13,989 INFO generators.py gen_for_qa l.548] (5/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:45:13,990 DEBUG generators.py generate l.349] (5/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:45:13,992 DEBUG generators.py generate l.358] (5/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:45:14,498 ERROR generators.py complete l.400] (5/93) The following exception occurred with prompt meta={} user="Qui a inventé le parachute ?  A)  Louis-Sébastien Lenormand B)  Sir George Cayley .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:45:14,507 DEBUG generators.py generate l.373] (5/93) Reuse post-processing
[2024-03-04 10:45:14,516 INFO generators.py generate l.477] (5/93) End question "Qui a inventé le parachute ?  A)  Louis-Sébastien Lenormand B)  Sir George Cayley "
[2024-03-04 10:45:14,519 INFO generators.py generate l.475] (6/93) *** AnsGenerator for question "Qui a inventé le vélocipède ?  A)  Pierre Michaux B)  Kirkpatrick Macmillan "
[2024-03-04 10:45:14,519 INFO generators.py gen_for_qa l.548] (6/93) * Start with LLM "gpt-4"
[2024-03-04 10:45:14,519 DEBUG generators.py generate l.349] (6/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:45:14,535 DEBUG generators.py generate l.358] (6/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:45:15,657 DEBUG generators.py generate l.370] (6/93) Post-process Answer
[2024-03-04 10:45:15,657 INFO generators.py gen_for_qa l.548] (6/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:45:15,657 DEBUG generators.py generate l.349] (6/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:45:15,657 DEBUG generators.py generate l.358] (6/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:45:16,385 DEBUG generators.py generate l.370] (6/93) Post-process Answer
[2024-03-04 10:45:16,388 INFO generators.py gen_for_qa l.548] (6/93) * Start with LLM "gemini-pro"
[2024-03-04 10:45:16,389 DEBUG generators.py generate l.349] (6/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:45:16,390 DEBUG generators.py generate l.358] (6/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:45:19,491 DEBUG generators.py generate l.370] (6/93) Post-process Answer
[2024-03-04 10:45:19,491 INFO generators.py gen_for_qa l.548] (6/93) * Start with LLM "claude-2.1"
[2024-03-04 10:45:19,491 DEBUG generators.py generate l.349] (6/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:45:19,491 DEBUG generators.py generate l.358] (6/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:45:20,537 DEBUG generators.py generate l.370] (6/93) Post-process Answer
[2024-03-04 10:45:20,552 INFO generators.py gen_for_qa l.548] (6/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:45:20,553 DEBUG generators.py generate l.349] (6/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:45:20,556 DEBUG generators.py generate l.358] (6/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:45:20,939 DEBUG generators.py generate l.370] (6/93) Post-process Answer
[2024-03-04 10:45:20,939 INFO generators.py gen_for_qa l.548] (6/93) * Start with LLM "command-nightly"
[2024-03-04 10:45:20,939 DEBUG generators.py generate l.349] (6/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:45:20,955 DEBUG generators.py generate l.358] (6/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:45:21,216 DEBUG generators.py generate l.370] (6/93) Post-process Answer
[2024-03-04 10:45:21,226 INFO generators.py gen_for_qa l.548] (6/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:45:21,226 DEBUG generators.py generate l.349] (6/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:45:21,232 DEBUG generators.py generate l.358] (6/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:45:21,899 ERROR generators.py complete l.400] (6/93) The following exception occurred with prompt meta={} user="Qui a inventé le vélocipède ?  A)  Pierre Michaux B)  Kirkpatrick Macmillan .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:45:21,910 DEBUG generators.py generate l.373] (6/93) Reuse post-processing
[2024-03-04 10:45:21,916 INFO generators.py generate l.477] (6/93) End question "Qui a inventé le vélocipède ?  A)  Pierre Michaux B)  Kirkpatrick Macmillan "
[2024-03-04 10:45:21,916 INFO generators.py generate l.475] (7/93) *** AnsGenerator for question "Qui a inventé le cinématographe ?  A)  Auguste et Louis Lumière B)  William Friese-Greene "
[2024-03-04 10:45:21,916 INFO generators.py gen_for_qa l.548] (7/93) * Start with LLM "gpt-4"
[2024-03-04 10:45:21,916 DEBUG generators.py generate l.349] (7/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:45:21,916 DEBUG generators.py generate l.358] (7/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:45:22,619 DEBUG generators.py generate l.370] (7/93) Post-process Answer
[2024-03-04 10:45:22,633 INFO generators.py gen_for_qa l.548] (7/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:45:22,633 DEBUG generators.py generate l.349] (7/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:45:22,633 DEBUG generators.py generate l.358] (7/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:45:23,487 DEBUG generators.py generate l.370] (7/93) Post-process Answer
[2024-03-04 10:45:23,504 INFO generators.py gen_for_qa l.548] (7/93) * Start with LLM "gemini-pro"
[2024-03-04 10:45:23,504 DEBUG generators.py generate l.349] (7/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:45:23,504 DEBUG generators.py generate l.358] (7/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:45:26,218 DEBUG generators.py generate l.370] (7/93) Post-process Answer
[2024-03-04 10:45:26,218 INFO generators.py gen_for_qa l.548] (7/93) * Start with LLM "claude-2.1"
[2024-03-04 10:45:26,218 DEBUG generators.py generate l.349] (7/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:45:26,218 DEBUG generators.py generate l.358] (7/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:45:30,486 DEBUG generators.py generate l.370] (7/93) Post-process Answer
[2024-03-04 10:45:30,486 INFO generators.py gen_for_qa l.548] (7/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:45:30,486 DEBUG generators.py generate l.349] (7/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:45:30,498 DEBUG generators.py generate l.358] (7/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:45:31,235 DEBUG generators.py generate l.370] (7/93) Post-process Answer
[2024-03-04 10:45:31,235 INFO generators.py gen_for_qa l.548] (7/93) * Start with LLM "command-nightly"
[2024-03-04 10:45:31,250 DEBUG generators.py generate l.349] (7/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:45:31,250 DEBUG generators.py generate l.358] (7/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:45:31,557 DEBUG generators.py generate l.370] (7/93) Post-process Answer
[2024-03-04 10:45:31,557 INFO generators.py gen_for_qa l.548] (7/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:45:31,563 DEBUG generators.py generate l.349] (7/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:45:31,563 DEBUG generators.py generate l.358] (7/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:45:32,407 ERROR generators.py complete l.400] (7/93) The following exception occurred with prompt meta={} user="Qui a inventé le cinématographe ?  A)  Auguste et Louis Lumière B)  William Friese-Greene .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:45:32,407 DEBUG generators.py generate l.373] (7/93) Reuse post-processing
[2024-03-04 10:45:32,407 INFO generators.py generate l.477] (7/93) End question "Qui a inventé le cinématographe ?  A)  Auguste et Louis Lumière B)  William Friese-Greene "
[2024-03-04 10:45:32,407 INFO generators.py generate l.475] (8/93) *** AnsGenerator for question "Qui a inventé la machine à coudre ?  A)  Barthélemy Thimonnier B)  Thomas Saint "
[2024-03-04 10:45:32,407 INFO generators.py gen_for_qa l.548] (8/93) * Start with LLM "gpt-4"
[2024-03-04 10:45:32,407 DEBUG generators.py generate l.349] (8/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:45:32,422 DEBUG generators.py generate l.358] (8/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:45:33,587 DEBUG generators.py generate l.370] (8/93) Post-process Answer
[2024-03-04 10:45:33,592 INFO generators.py gen_for_qa l.548] (8/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:45:33,596 DEBUG generators.py generate l.349] (8/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:45:33,599 DEBUG generators.py generate l.358] (8/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:45:34,776 DEBUG generators.py generate l.370] (8/93) Post-process Answer
[2024-03-04 10:45:34,776 INFO generators.py gen_for_qa l.548] (8/93) * Start with LLM "gemini-pro"
[2024-03-04 10:45:34,776 DEBUG generators.py generate l.349] (8/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:45:34,791 DEBUG generators.py generate l.358] (8/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:45:37,098 DEBUG generators.py generate l.370] (8/93) Post-process Answer
[2024-03-04 10:45:37,114 INFO generators.py gen_for_qa l.548] (8/93) * Start with LLM "claude-2.1"
[2024-03-04 10:45:37,114 DEBUG generators.py generate l.349] (8/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:45:37,114 DEBUG generators.py generate l.358] (8/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:45:41,181 DEBUG generators.py generate l.370] (8/93) Post-process Answer
[2024-03-04 10:45:41,181 INFO generators.py gen_for_qa l.548] (8/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:45:41,181 DEBUG generators.py generate l.349] (8/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:45:41,181 DEBUG generators.py generate l.358] (8/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:45:42,069 DEBUG generators.py generate l.370] (8/93) Post-process Answer
[2024-03-04 10:45:42,069 INFO generators.py gen_for_qa l.548] (8/93) * Start with LLM "command-nightly"
[2024-03-04 10:45:42,069 DEBUG generators.py generate l.349] (8/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:45:42,079 DEBUG generators.py generate l.358] (8/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:45:42,362 DEBUG generators.py generate l.370] (8/93) Post-process Answer
[2024-03-04 10:45:42,364 INFO generators.py gen_for_qa l.548] (8/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:45:42,366 DEBUG generators.py generate l.349] (8/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:45:42,367 DEBUG generators.py generate l.358] (8/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:45:42,810 ERROR generators.py complete l.400] (8/93) The following exception occurred with prompt meta={} user="Qui a inventé la machine à coudre ?  A)  Barthélemy Thimonnier B)  Thomas Saint .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:45:42,810 DEBUG generators.py generate l.373] (8/93) Reuse post-processing
[2024-03-04 10:45:42,826 INFO generators.py generate l.477] (8/93) End question "Qui a inventé la machine à coudre ?  A)  Barthélemy Thimonnier B)  Thomas Saint "
[2024-03-04 10:45:42,826 INFO generators.py generate l.475] (9/93) *** AnsGenerator for question "Qui a inventé l'ampoule électrique ?  A)  Joseph Swan B)  Thomas Edison C)  Hiram Maxim "
[2024-03-04 10:45:42,826 INFO generators.py gen_for_qa l.548] (9/93) * Start with LLM "gpt-4"
[2024-03-04 10:45:42,826 DEBUG generators.py generate l.349] (9/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:45:42,842 DEBUG generators.py generate l.358] (9/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:45:43,776 DEBUG generators.py generate l.370] (9/93) Post-process Answer
[2024-03-04 10:45:43,776 INFO generators.py gen_for_qa l.548] (9/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:45:43,776 DEBUG generators.py generate l.349] (9/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:45:43,795 DEBUG generators.py generate l.358] (9/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:45:44,525 DEBUG generators.py generate l.370] (9/93) Post-process Answer
[2024-03-04 10:45:44,525 INFO generators.py gen_for_qa l.548] (9/93) * Start with LLM "gemini-pro"
[2024-03-04 10:45:44,536 DEBUG generators.py generate l.349] (9/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:45:44,538 DEBUG generators.py generate l.358] (9/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:45:47,056 DEBUG generators.py generate l.370] (9/93) Post-process Answer
[2024-03-04 10:45:47,056 INFO generators.py gen_for_qa l.548] (9/93) * Start with LLM "claude-2.1"
[2024-03-04 10:45:47,074 DEBUG generators.py generate l.349] (9/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:45:47,074 DEBUG generators.py generate l.358] (9/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:45:48,179 DEBUG generators.py generate l.370] (9/93) Post-process Answer
[2024-03-04 10:45:48,179 INFO generators.py gen_for_qa l.548] (9/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:45:48,179 DEBUG generators.py generate l.349] (9/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:45:48,179 DEBUG generators.py generate l.358] (9/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:45:48,831 DEBUG generators.py generate l.370] (9/93) Post-process Answer
[2024-03-04 10:45:48,833 INFO generators.py gen_for_qa l.548] (9/93) * Start with LLM "command-nightly"
[2024-03-04 10:45:48,836 DEBUG generators.py generate l.349] (9/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:45:48,837 DEBUG generators.py generate l.358] (9/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:45:49,154 DEBUG generators.py generate l.370] (9/93) Post-process Answer
[2024-03-04 10:45:49,156 INFO generators.py gen_for_qa l.548] (9/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:45:49,156 DEBUG generators.py generate l.349] (9/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:45:49,158 DEBUG generators.py generate l.358] (9/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:45:49,569 ERROR generators.py complete l.400] (9/93) The following exception occurred with prompt meta={} user="Qui a inventé l'ampoule électrique ?  A)  Joseph Swan B)  Thomas Edison C)  Hiram Maxim .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:45:49,578 DEBUG generators.py generate l.373] (9/93) Reuse post-processing
[2024-03-04 10:45:49,580 INFO generators.py generate l.477] (9/93) End question "Qui a inventé l'ampoule électrique ?  A)  Joseph Swan B)  Thomas Edison C)  Hiram Maxim "
[2024-03-04 10:45:49,581 INFO generators.py generate l.475] (10/93) *** AnsGenerator for question "Qui a inventé le téléphone ?  A)  Alexander Graham Bell B)  Elisha Gray C)  Antonio Meucci "
[2024-03-04 10:45:49,584 INFO generators.py gen_for_qa l.548] (10/93) * Start with LLM "gpt-4"
[2024-03-04 10:45:49,586 DEBUG generators.py generate l.349] (10/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:45:49,587 DEBUG generators.py generate l.358] (10/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:45:50,332 DEBUG generators.py generate l.370] (10/93) Post-process Answer
[2024-03-04 10:45:50,334 INFO generators.py gen_for_qa l.548] (10/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:45:50,336 DEBUG generators.py generate l.349] (10/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:45:50,337 DEBUG generators.py generate l.358] (10/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:45:51,485 DEBUG generators.py generate l.370] (10/93) Post-process Answer
[2024-03-04 10:45:51,501 INFO generators.py gen_for_qa l.548] (10/93) * Start with LLM "gemini-pro"
[2024-03-04 10:45:51,504 DEBUG generators.py generate l.349] (10/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:45:51,504 DEBUG generators.py generate l.358] (10/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:45:53,835 DEBUG generators.py generate l.370] (10/93) Post-process Answer
[2024-03-04 10:45:53,850 INFO generators.py gen_for_qa l.548] (10/93) * Start with LLM "claude-2.1"
[2024-03-04 10:45:53,850 DEBUG generators.py generate l.349] (10/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:45:53,850 DEBUG generators.py generate l.358] (10/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:45:57,511 DEBUG generators.py generate l.370] (10/93) Post-process Answer
[2024-03-04 10:45:57,511 INFO generators.py gen_for_qa l.548] (10/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:45:57,511 DEBUG generators.py generate l.349] (10/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:45:57,511 DEBUG generators.py generate l.358] (10/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:45:57,951 DEBUG generators.py generate l.370] (10/93) Post-process Answer
[2024-03-04 10:45:57,966 INFO generators.py gen_for_qa l.548] (10/93) * Start with LLM "command-nightly"
[2024-03-04 10:45:57,966 DEBUG generators.py generate l.349] (10/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:45:57,966 DEBUG generators.py generate l.358] (10/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:45:58,350 DEBUG generators.py generate l.370] (10/93) Post-process Answer
[2024-03-04 10:45:58,350 INFO generators.py gen_for_qa l.548] (10/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:45:58,350 DEBUG generators.py generate l.349] (10/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:45:58,364 DEBUG generators.py generate l.358] (10/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:45:59,054 ERROR generators.py complete l.400] (10/93) The following exception occurred with prompt meta={} user="Qui a inventé le téléphone ?  A)  Alexander Graham Bell B)  Elisha Gray C)  Antonio Meucci .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:45:59,070 DEBUG generators.py generate l.373] (10/93) Reuse post-processing
[2024-03-04 10:45:59,070 INFO generators.py generate l.477] (10/93) End question "Qui a inventé le téléphone ?  A)  Alexander Graham Bell B)  Elisha Gray C)  Antonio Meucci "
[2024-03-04 10:45:59,070 INFO generators.py generate l.475] (11/93) *** AnsGenerator for question "Qui a inventé la montgolfière ?  A)  Les frères Montgolfier B)  Richard Crosbie "
[2024-03-04 10:45:59,092 INFO generators.py gen_for_qa l.548] (11/93) * Start with LLM "gpt-4"
[2024-03-04 10:45:59,097 DEBUG generators.py generate l.349] (11/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:45:59,099 DEBUG generators.py generate l.358] (11/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:00,146 DEBUG generators.py generate l.370] (11/93) Post-process Answer
[2024-03-04 10:46:00,146 INFO generators.py gen_for_qa l.548] (11/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:46:00,146 DEBUG generators.py generate l.349] (11/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:00,146 DEBUG generators.py generate l.358] (11/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:01,082 DEBUG generators.py generate l.370] (11/93) Post-process Answer
[2024-03-04 10:46:01,082 INFO generators.py gen_for_qa l.548] (11/93) * Start with LLM "gemini-pro"
[2024-03-04 10:46:01,082 DEBUG generators.py generate l.349] (11/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:01,082 DEBUG generators.py generate l.358] (11/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:03,609 DEBUG generators.py generate l.370] (11/93) Post-process Answer
[2024-03-04 10:46:03,615 INFO generators.py gen_for_qa l.548] (11/93) * Start with LLM "claude-2.1"
[2024-03-04 10:46:03,618 DEBUG generators.py generate l.349] (11/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:03,619 DEBUG generators.py generate l.358] (11/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:06,113 DEBUG generators.py generate l.370] (11/93) Post-process Answer
[2024-03-04 10:46:06,113 INFO generators.py gen_for_qa l.548] (11/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:46:06,129 DEBUG generators.py generate l.349] (11/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:06,129 DEBUG generators.py generate l.358] (11/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:06,964 DEBUG generators.py generate l.370] (11/93) Post-process Answer
[2024-03-04 10:46:06,964 INFO generators.py gen_for_qa l.548] (11/93) * Start with LLM "command-nightly"
[2024-03-04 10:46:06,964 DEBUG generators.py generate l.349] (11/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:06,978 DEBUG generators.py generate l.358] (11/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:07,339 DEBUG generators.py generate l.370] (11/93) Post-process Answer
[2024-03-04 10:46:07,357 INFO generators.py gen_for_qa l.548] (11/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:46:07,358 DEBUG generators.py generate l.349] (11/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:07,358 DEBUG generators.py generate l.358] (11/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:07,830 ERROR generators.py complete l.400] (11/93) The following exception occurred with prompt meta={} user="Qui a inventé la montgolfière ?  A)  Les frères Montgolfier B)  Richard Crosbie .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:46:07,844 DEBUG generators.py generate l.373] (11/93) Reuse post-processing
[2024-03-04 10:46:07,852 INFO generators.py generate l.477] (11/93) End question "Qui a inventé la montgolfière ?  A)  Les frères Montgolfier B)  Richard Crosbie "
[2024-03-04 10:46:07,855 INFO generators.py generate l.475] (12/93) *** AnsGenerator for question "Qui a inventé le dirigeable ?  A)  Henri Giffard B)  George Cayley "
[2024-03-04 10:46:07,860 INFO generators.py gen_for_qa l.548] (12/93) * Start with LLM "gpt-4"
[2024-03-04 10:46:07,860 DEBUG generators.py generate l.349] (12/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:07,860 DEBUG generators.py generate l.358] (12/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:08,551 DEBUG generators.py generate l.370] (12/93) Post-process Answer
[2024-03-04 10:46:08,551 INFO generators.py gen_for_qa l.548] (12/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:46:08,551 DEBUG generators.py generate l.349] (12/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:08,551 DEBUG generators.py generate l.358] (12/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:09,551 DEBUG generators.py generate l.370] (12/93) Post-process Answer
[2024-03-04 10:46:09,567 INFO generators.py gen_for_qa l.548] (12/93) * Start with LLM "gemini-pro"
[2024-03-04 10:46:09,567 DEBUG generators.py generate l.349] (12/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:09,567 DEBUG generators.py generate l.358] (12/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:11,764 DEBUG generators.py generate l.370] (12/93) Post-process Answer
[2024-03-04 10:46:11,764 INFO generators.py gen_for_qa l.548] (12/93) * Start with LLM "claude-2.1"
[2024-03-04 10:46:11,773 DEBUG generators.py generate l.349] (12/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:11,773 DEBUG generators.py generate l.358] (12/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:15,153 DEBUG generators.py generate l.370] (12/93) Post-process Answer
[2024-03-04 10:46:15,153 INFO generators.py gen_for_qa l.548] (12/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:46:15,153 DEBUG generators.py generate l.349] (12/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:15,168 DEBUG generators.py generate l.358] (12/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:15,656 DEBUG generators.py generate l.370] (12/93) Post-process Answer
[2024-03-04 10:46:15,656 INFO generators.py gen_for_qa l.548] (12/93) * Start with LLM "command-nightly"
[2024-03-04 10:46:15,671 DEBUG generators.py generate l.349] (12/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:15,671 DEBUG generators.py generate l.358] (12/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:15,973 DEBUG generators.py generate l.370] (12/93) Post-process Answer
[2024-03-04 10:46:15,973 INFO generators.py gen_for_qa l.548] (12/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:46:15,989 DEBUG generators.py generate l.349] (12/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:15,992 DEBUG generators.py generate l.358] (12/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:16,554 ERROR generators.py complete l.400] (12/93) The following exception occurred with prompt meta={} user="Qui a inventé le dirigeable ?  A)  Henri Giffard B)  George Cayley .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:46:16,569 DEBUG generators.py generate l.373] (12/93) Reuse post-processing
[2024-03-04 10:46:16,569 INFO generators.py generate l.477] (12/93) End question "Qui a inventé le dirigeable ?  A)  Henri Giffard B)  George Cayley "
[2024-03-04 10:46:16,569 INFO generators.py generate l.475] (13/93) *** AnsGenerator for question "Qui a inventé la pile électrique ?  A)  Alessandro Volta B)  Luigi Galvani C)  Charles-François de Cisternay du Fay "
[2024-03-04 10:46:16,569 INFO generators.py gen_for_qa l.548] (13/93) * Start with LLM "gpt-4"
[2024-03-04 10:46:16,569 DEBUG generators.py generate l.349] (13/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:16,569 DEBUG generators.py generate l.358] (13/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:17,135 DEBUG generators.py generate l.370] (13/93) Post-process Answer
[2024-03-04 10:46:17,135 INFO generators.py gen_for_qa l.548] (13/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:46:17,151 DEBUG generators.py generate l.349] (13/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:17,151 DEBUG generators.py generate l.358] (13/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:17,780 DEBUG generators.py generate l.370] (13/93) Post-process Answer
[2024-03-04 10:46:17,780 INFO generators.py gen_for_qa l.548] (13/93) * Start with LLM "gemini-pro"
[2024-03-04 10:46:17,780 DEBUG generators.py generate l.349] (13/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:17,796 DEBUG generators.py generate l.358] (13/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:20,526 DEBUG generators.py generate l.370] (13/93) Post-process Answer
[2024-03-04 10:46:20,542 INFO generators.py gen_for_qa l.548] (13/93) * Start with LLM "claude-2.1"
[2024-03-04 10:46:20,542 DEBUG generators.py generate l.349] (13/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:20,542 DEBUG generators.py generate l.358] (13/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:25,543 DEBUG generators.py generate l.370] (13/93) Post-process Answer
[2024-03-04 10:46:25,555 INFO generators.py gen_for_qa l.548] (13/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:46:25,557 DEBUG generators.py generate l.349] (13/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:25,557 DEBUG generators.py generate l.358] (13/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:25,997 DEBUG generators.py generate l.370] (13/93) Post-process Answer
[2024-03-04 10:46:25,997 INFO generators.py gen_for_qa l.548] (13/93) * Start with LLM "command-nightly"
[2024-03-04 10:46:26,013 DEBUG generators.py generate l.349] (13/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:26,013 DEBUG generators.py generate l.358] (13/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:26,295 DEBUG generators.py generate l.370] (13/93) Post-process Answer
[2024-03-04 10:46:26,311 INFO generators.py gen_for_qa l.548] (13/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:46:26,311 DEBUG generators.py generate l.349] (13/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:26,311 DEBUG generators.py generate l.358] (13/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:26,881 ERROR generators.py complete l.400] (13/93) The following exception occurred with prompt meta={} user="Qui a inventé la pile électrique ?  A)  Alessandro Volta B)  Luigi Galvani C)  Charles-François de Cisternay du Fay .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:46:26,881 DEBUG generators.py generate l.373] (13/93) Reuse post-processing
[2024-03-04 10:46:26,896 INFO generators.py generate l.477] (13/93) End question "Qui a inventé la pile électrique ?  A)  Alessandro Volta B)  Luigi Galvani C)  Charles-François de Cisternay du Fay "
[2024-03-04 10:46:26,896 INFO generators.py generate l.475] (14/93) *** AnsGenerator for question "Qui a inventé le stéthoscope ?  A)  René Laennec B)  David Littmann "
[2024-03-04 10:46:26,896 INFO generators.py gen_for_qa l.548] (14/93) * Start with LLM "gpt-4"
[2024-03-04 10:46:26,896 DEBUG generators.py generate l.349] (14/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:26,896 DEBUG generators.py generate l.358] (14/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:28,788 DEBUG generators.py generate l.370] (14/93) Post-process Answer
[2024-03-04 10:46:28,807 INFO generators.py gen_for_qa l.548] (14/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:46:28,810 DEBUG generators.py generate l.349] (14/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:28,816 DEBUG generators.py generate l.358] (14/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:29,452 DEBUG generators.py generate l.370] (14/93) Post-process Answer
[2024-03-04 10:46:29,452 INFO generators.py gen_for_qa l.548] (14/93) * Start with LLM "gemini-pro"
[2024-03-04 10:46:29,452 DEBUG generators.py generate l.349] (14/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:29,452 DEBUG generators.py generate l.358] (14/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:31,595 DEBUG generators.py generate l.370] (14/93) Post-process Answer
[2024-03-04 10:46:31,610 INFO generators.py gen_for_qa l.548] (14/93) * Start with LLM "claude-2.1"
[2024-03-04 10:46:31,610 DEBUG generators.py generate l.349] (14/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:31,610 DEBUG generators.py generate l.358] (14/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:32,554 DEBUG generators.py generate l.370] (14/93) Post-process Answer
[2024-03-04 10:46:32,570 INFO generators.py gen_for_qa l.548] (14/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:46:32,570 DEBUG generators.py generate l.349] (14/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:32,570 DEBUG generators.py generate l.358] (14/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:33,435 DEBUG generators.py generate l.370] (14/93) Post-process Answer
[2024-03-04 10:46:33,435 INFO generators.py gen_for_qa l.548] (14/93) * Start with LLM "command-nightly"
[2024-03-04 10:46:33,435 DEBUG generators.py generate l.349] (14/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:33,435 DEBUG generators.py generate l.358] (14/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:33,781 DEBUG generators.py generate l.370] (14/93) Post-process Answer
[2024-03-04 10:46:33,781 INFO generators.py gen_for_qa l.548] (14/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:46:33,781 DEBUG generators.py generate l.349] (14/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:33,798 DEBUG generators.py generate l.358] (14/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:34,507 ERROR generators.py complete l.400] (14/93) The following exception occurred with prompt meta={} user="Qui a inventé le stéthoscope ?  A)  René Laennec B)  David Littmann .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:46:34,507 DEBUG generators.py generate l.373] (14/93) Reuse post-processing
[2024-03-04 10:46:34,522 INFO generators.py generate l.477] (14/93) End question "Qui a inventé le stéthoscope ?  A)  René Laennec B)  David Littmann "
[2024-03-04 10:46:34,522 INFO generators.py generate l.475] (15/93) *** AnsGenerator for question "Qui a inventé le microscope ?  A)  Zacharias Janssen B)  Galileo Galilei C)  Robert Hooke "
[2024-03-04 10:46:34,522 INFO generators.py gen_for_qa l.548] (15/93) * Start with LLM "gpt-4"
[2024-03-04 10:46:34,522 DEBUG generators.py generate l.349] (15/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:34,522 DEBUG generators.py generate l.358] (15/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:35,844 DEBUG generators.py generate l.370] (15/93) Post-process Answer
[2024-03-04 10:46:35,844 INFO generators.py gen_for_qa l.548] (15/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:46:35,844 DEBUG generators.py generate l.349] (15/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:35,859 DEBUG generators.py generate l.358] (15/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:36,838 DEBUG generators.py generate l.370] (15/93) Post-process Answer
[2024-03-04 10:46:36,838 INFO generators.py gen_for_qa l.548] (15/93) * Start with LLM "gemini-pro"
[2024-03-04 10:46:36,838 DEBUG generators.py generate l.349] (15/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:36,838 DEBUG generators.py generate l.358] (15/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:39,509 DEBUG generators.py generate l.370] (15/93) Post-process Answer
[2024-03-04 10:46:39,509 INFO generators.py gen_for_qa l.548] (15/93) * Start with LLM "claude-2.1"
[2024-03-04 10:46:39,525 DEBUG generators.py generate l.349] (15/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:39,525 DEBUG generators.py generate l.358] (15/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:40,489 DEBUG generators.py generate l.370] (15/93) Post-process Answer
[2024-03-04 10:46:40,489 INFO generators.py gen_for_qa l.548] (15/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:46:40,505 DEBUG generators.py generate l.349] (15/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:40,505 DEBUG generators.py generate l.358] (15/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:41,137 DEBUG generators.py generate l.370] (15/93) Post-process Answer
[2024-03-04 10:46:41,142 INFO generators.py gen_for_qa l.548] (15/93) * Start with LLM "command-nightly"
[2024-03-04 10:46:41,142 DEBUG generators.py generate l.349] (15/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:41,142 DEBUG generators.py generate l.358] (15/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:41,644 DEBUG generators.py generate l.370] (15/93) Post-process Answer
[2024-03-04 10:46:41,660 INFO generators.py gen_for_qa l.548] (15/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:46:41,660 DEBUG generators.py generate l.349] (15/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:41,660 DEBUG generators.py generate l.358] (15/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:42,137 ERROR generators.py complete l.400] (15/93) The following exception occurred with prompt meta={} user="Qui a inventé le microscope ?  A)  Zacharias Janssen B)  Galileo Galilei C)  Robert Hooke .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:46:42,141 DEBUG generators.py generate l.373] (15/93) Reuse post-processing
[2024-03-04 10:46:42,141 INFO generators.py generate l.477] (15/93) End question "Qui a inventé le microscope ?  A)  Zacharias Janssen B)  Galileo Galilei C)  Robert Hooke "
[2024-03-04 10:46:42,151 INFO generators.py generate l.475] (16/93) *** AnsGenerator for question "Qui a inventé le thermomètre ?  A)  Galileo Galilei B)  Daniel Gabriel Fahrenheit C)  Cornelis Drebbel "
[2024-03-04 10:46:42,153 INFO generators.py gen_for_qa l.548] (16/93) * Start with LLM "gpt-4"
[2024-03-04 10:46:42,155 DEBUG generators.py generate l.349] (16/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:42,158 DEBUG generators.py generate l.358] (16/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:43,055 DEBUG generators.py generate l.370] (16/93) Post-process Answer
[2024-03-04 10:46:43,062 INFO generators.py gen_for_qa l.548] (16/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:46:43,063 DEBUG generators.py generate l.349] (16/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:43,064 DEBUG generators.py generate l.358] (16/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:43,818 DEBUG generators.py generate l.370] (16/93) Post-process Answer
[2024-03-04 10:46:43,818 INFO generators.py gen_for_qa l.548] (16/93) * Start with LLM "gemini-pro"
[2024-03-04 10:46:43,818 DEBUG generators.py generate l.349] (16/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:43,818 DEBUG generators.py generate l.358] (16/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:46,324 DEBUG generators.py generate l.370] (16/93) Post-process Answer
[2024-03-04 10:46:46,326 INFO generators.py gen_for_qa l.548] (16/93) * Start with LLM "claude-2.1"
[2024-03-04 10:46:46,328 DEBUG generators.py generate l.349] (16/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:46,329 DEBUG generators.py generate l.358] (16/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:48,029 DEBUG generators.py generate l.370] (16/93) Post-process Answer
[2024-03-04 10:46:48,030 INFO generators.py gen_for_qa l.548] (16/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:46:48,036 DEBUG generators.py generate l.349] (16/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:48,040 DEBUG generators.py generate l.358] (16/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:48,620 DEBUG generators.py generate l.370] (16/93) Post-process Answer
[2024-03-04 10:46:48,624 INFO generators.py gen_for_qa l.548] (16/93) * Start with LLM "command-nightly"
[2024-03-04 10:46:48,625 DEBUG generators.py generate l.349] (16/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:48,626 DEBUG generators.py generate l.358] (16/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:48,908 DEBUG generators.py generate l.370] (16/93) Post-process Answer
[2024-03-04 10:46:48,908 INFO generators.py gen_for_qa l.548] (16/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:46:48,908 DEBUG generators.py generate l.349] (16/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:48,914 DEBUG generators.py generate l.358] (16/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:49,336 ERROR generators.py complete l.400] (16/93) The following exception occurred with prompt meta={} user="Qui a inventé le thermomètre ?  A)  Galileo Galilei B)  Daniel Gabriel Fahrenheit C)  Cornelis Drebbel .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:46:49,349 DEBUG generators.py generate l.373] (16/93) Reuse post-processing
[2024-03-04 10:46:49,353 INFO generators.py generate l.477] (16/93) End question "Qui a inventé le thermomètre ?  A)  Galileo Galilei B)  Daniel Gabriel Fahrenheit C)  Cornelis Drebbel "
[2024-03-04 10:46:49,353 INFO generators.py generate l.475] (17/93) *** AnsGenerator for question "Qui a inventé le baromètre ?  A)  Evangelista Torricelli B)  Blaise Pascal "
[2024-03-04 10:46:49,353 INFO generators.py gen_for_qa l.548] (17/93) * Start with LLM "gpt-4"
[2024-03-04 10:46:49,353 DEBUG generators.py generate l.349] (17/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:49,362 DEBUG generators.py generate l.358] (17/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:50,118 DEBUG generators.py generate l.370] (17/93) Post-process Answer
[2024-03-04 10:46:50,122 INFO generators.py gen_for_qa l.548] (17/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:46:50,123 DEBUG generators.py generate l.349] (17/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:50,123 DEBUG generators.py generate l.358] (17/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:50,722 DEBUG generators.py generate l.370] (17/93) Post-process Answer
[2024-03-04 10:46:50,725 INFO generators.py gen_for_qa l.548] (17/93) * Start with LLM "gemini-pro"
[2024-03-04 10:46:50,732 DEBUG generators.py generate l.349] (17/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:50,732 DEBUG generators.py generate l.358] (17/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:53,078 DEBUG generators.py generate l.370] (17/93) Post-process Answer
[2024-03-04 10:46:53,083 INFO generators.py gen_for_qa l.548] (17/93) * Start with LLM "claude-2.1"
[2024-03-04 10:46:53,087 DEBUG generators.py generate l.349] (17/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:53,087 DEBUG generators.py generate l.358] (17/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:54,272 DEBUG generators.py generate l.370] (17/93) Post-process Answer
[2024-03-04 10:46:54,272 INFO generators.py gen_for_qa l.548] (17/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:46:54,277 DEBUG generators.py generate l.349] (17/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:54,277 DEBUG generators.py generate l.358] (17/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:54,836 DEBUG generators.py generate l.370] (17/93) Post-process Answer
[2024-03-04 10:46:54,852 INFO generators.py gen_for_qa l.548] (17/93) * Start with LLM "command-nightly"
[2024-03-04 10:46:54,855 DEBUG generators.py generate l.349] (17/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:54,855 DEBUG generators.py generate l.358] (17/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:55,153 DEBUG generators.py generate l.370] (17/93) Post-process Answer
[2024-03-04 10:46:55,168 INFO generators.py gen_for_qa l.548] (17/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:46:55,170 DEBUG generators.py generate l.349] (17/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:55,170 DEBUG generators.py generate l.358] (17/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:55,705 ERROR generators.py complete l.400] (17/93) The following exception occurred with prompt meta={} user="Qui a inventé le baromètre ?  A)  Evangelista Torricelli B)  Blaise Pascal .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:46:55,705 DEBUG generators.py generate l.373] (17/93) Reuse post-processing
[2024-03-04 10:46:55,705 INFO generators.py generate l.477] (17/93) End question "Qui a inventé le baromètre ?  A)  Evangelista Torricelli B)  Blaise Pascal "
[2024-03-04 10:46:55,705 INFO generators.py generate l.475] (18/93) *** AnsGenerator for question "Qui a inventé la machine à vapeur à piston ?  A)  Thomas Savery B)  Denis Papin "
[2024-03-04 10:46:55,705 INFO generators.py gen_for_qa l.548] (18/93) * Start with LLM "gpt-4"
[2024-03-04 10:46:55,719 DEBUG generators.py generate l.349] (18/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:55,721 DEBUG generators.py generate l.358] (18/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:57,111 DEBUG generators.py generate l.370] (18/93) Post-process Answer
[2024-03-04 10:46:57,114 INFO generators.py gen_for_qa l.548] (18/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:46:57,114 DEBUG generators.py generate l.349] (18/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:57,114 DEBUG generators.py generate l.358] (18/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:57,747 DEBUG generators.py generate l.370] (18/93) Post-process Answer
[2024-03-04 10:46:57,754 INFO generators.py gen_for_qa l.548] (18/93) * Start with LLM "gemini-pro"
[2024-03-04 10:46:57,754 DEBUG generators.py generate l.349] (18/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:57,754 DEBUG generators.py generate l.358] (18/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:46:59,943 DEBUG generators.py generate l.370] (18/93) Post-process Answer
[2024-03-04 10:46:59,943 INFO generators.py gen_for_qa l.548] (18/93) * Start with LLM "claude-2.1"
[2024-03-04 10:46:59,943 DEBUG generators.py generate l.349] (18/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:46:59,943 DEBUG generators.py generate l.358] (18/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:01,229 DEBUG generators.py generate l.370] (18/93) Post-process Answer
[2024-03-04 10:47:01,229 INFO generators.py gen_for_qa l.548] (18/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:47:01,229 DEBUG generators.py generate l.349] (18/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:01,229 DEBUG generators.py generate l.358] (18/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:01,578 DEBUG generators.py generate l.370] (18/93) Post-process Answer
[2024-03-04 10:47:01,578 INFO generators.py gen_for_qa l.548] (18/93) * Start with LLM "command-nightly"
[2024-03-04 10:47:01,578 DEBUG generators.py generate l.349] (18/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:01,578 DEBUG generators.py generate l.358] (18/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:01,811 DEBUG generators.py generate l.370] (18/93) Post-process Answer
[2024-03-04 10:47:01,817 INFO generators.py gen_for_qa l.548] (18/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:47:01,817 DEBUG generators.py generate l.349] (18/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:01,820 DEBUG generators.py generate l.358] (18/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:02,230 ERROR generators.py complete l.400] (18/93) The following exception occurred with prompt meta={} user="Qui a inventé la machine à vapeur à piston ?  A)  Thomas Savery B)  Denis Papin .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:47:02,232 DEBUG generators.py generate l.373] (18/93) Reuse post-processing
[2024-03-04 10:47:02,232 INFO generators.py generate l.477] (18/93) End question "Qui a inventé la machine à vapeur à piston ?  A)  Thomas Savery B)  Denis Papin "
[2024-03-04 10:47:02,237 INFO generators.py generate l.475] (19/93) *** AnsGenerator for question "Qui a inventé la locomotive à vapeur ?  A)  Richard Trevithick B)  George Stephenson C)  Nicolas-Joseph Cugnot "
[2024-03-04 10:47:02,237 INFO generators.py gen_for_qa l.548] (19/93) * Start with LLM "gpt-4"
[2024-03-04 10:47:02,237 DEBUG generators.py generate l.349] (19/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:02,242 DEBUG generators.py generate l.358] (19/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:03,007 DEBUG generators.py generate l.370] (19/93) Post-process Answer
[2024-03-04 10:47:03,007 INFO generators.py gen_for_qa l.548] (19/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:47:03,016 DEBUG generators.py generate l.349] (19/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:03,017 DEBUG generators.py generate l.358] (19/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:03,572 DEBUG generators.py generate l.370] (19/93) Post-process Answer
[2024-03-04 10:47:03,574 INFO generators.py gen_for_qa l.548] (19/93) * Start with LLM "gemini-pro"
[2024-03-04 10:47:03,577 DEBUG generators.py generate l.349] (19/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:03,577 DEBUG generators.py generate l.358] (19/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:05,911 DEBUG generators.py generate l.370] (19/93) Post-process Answer
[2024-03-04 10:47:05,927 INFO generators.py gen_for_qa l.548] (19/93) * Start with LLM "claude-2.1"
[2024-03-04 10:47:05,927 DEBUG generators.py generate l.349] (19/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:05,927 DEBUG generators.py generate l.358] (19/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:07,039 DEBUG generators.py generate l.370] (19/93) Post-process Answer
[2024-03-04 10:47:07,045 INFO generators.py gen_for_qa l.548] (19/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:47:07,045 DEBUG generators.py generate l.349] (19/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:07,045 DEBUG generators.py generate l.358] (19/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:07,669 DEBUG generators.py generate l.370] (19/93) Post-process Answer
[2024-03-04 10:47:07,671 INFO generators.py gen_for_qa l.548] (19/93) * Start with LLM "command-nightly"
[2024-03-04 10:47:07,676 DEBUG generators.py generate l.349] (19/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:07,678 DEBUG generators.py generate l.358] (19/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:07,931 DEBUG generators.py generate l.370] (19/93) Post-process Answer
[2024-03-04 10:47:07,935 INFO generators.py gen_for_qa l.548] (19/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:47:07,935 DEBUG generators.py generate l.349] (19/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:07,940 DEBUG generators.py generate l.358] (19/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:08,456 ERROR generators.py complete l.400] (19/93) The following exception occurred with prompt meta={} user="Qui a inventé la locomotive à vapeur ?  A)  Richard Trevithick B)  George Stephenson C)  Nicolas-Joseph Cugnot .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:47:08,466 DEBUG generators.py generate l.373] (19/93) Reuse post-processing
[2024-03-04 10:47:08,467 INFO generators.py generate l.477] (19/93) End question "Qui a inventé la locomotive à vapeur ?  A)  Richard Trevithick B)  George Stephenson C)  Nicolas-Joseph Cugnot "
[2024-03-04 10:47:08,468 INFO generators.py generate l.475] (20/93) *** AnsGenerator for question "Qui a inventé le moteur à combustion interne ?  A)  Étienne Lenoir B)  Nikolaus Otto "
[2024-03-04 10:47:08,470 INFO generators.py gen_for_qa l.548] (20/93) * Start with LLM "gpt-4"
[2024-03-04 10:47:08,470 DEBUG generators.py generate l.349] (20/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:08,472 DEBUG generators.py generate l.358] (20/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:09,234 DEBUG generators.py generate l.370] (20/93) Post-process Answer
[2024-03-04 10:47:09,234 INFO generators.py gen_for_qa l.548] (20/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:47:09,234 DEBUG generators.py generate l.349] (20/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:09,249 DEBUG generators.py generate l.358] (20/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:09,782 DEBUG generators.py generate l.370] (20/93) Post-process Answer
[2024-03-04 10:47:09,782 INFO generators.py gen_for_qa l.548] (20/93) * Start with LLM "gemini-pro"
[2024-03-04 10:47:09,788 DEBUG generators.py generate l.349] (20/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:09,794 DEBUG generators.py generate l.358] (20/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:12,155 DEBUG generators.py generate l.370] (20/93) Post-process Answer
[2024-03-04 10:47:12,163 INFO generators.py gen_for_qa l.548] (20/93) * Start with LLM "claude-2.1"
[2024-03-04 10:47:12,163 DEBUG generators.py generate l.349] (20/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:12,163 DEBUG generators.py generate l.358] (20/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:13,043 DEBUG generators.py generate l.370] (20/93) Post-process Answer
[2024-03-04 10:47:13,045 INFO generators.py gen_for_qa l.548] (20/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:47:13,045 DEBUG generators.py generate l.349] (20/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:13,047 DEBUG generators.py generate l.358] (20/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:13,578 DEBUG generators.py generate l.370] (20/93) Post-process Answer
[2024-03-04 10:47:13,585 INFO generators.py gen_for_qa l.548] (20/93) * Start with LLM "command-nightly"
[2024-03-04 10:47:13,588 DEBUG generators.py generate l.349] (20/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:13,588 DEBUG generators.py generate l.358] (20/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:13,885 DEBUG generators.py generate l.370] (20/93) Post-process Answer
[2024-03-04 10:47:13,903 INFO generators.py gen_for_qa l.548] (20/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:47:13,907 DEBUG generators.py generate l.349] (20/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:13,907 DEBUG generators.py generate l.358] (20/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:14,439 ERROR generators.py complete l.400] (20/93) The following exception occurred with prompt meta={} user="Qui a inventé le moteur à combustion interne ?  A)  Étienne Lenoir B)  Nikolaus Otto .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:47:14,461 DEBUG generators.py generate l.373] (20/93) Reuse post-processing
[2024-03-04 10:47:14,461 INFO generators.py generate l.477] (20/93) End question "Qui a inventé le moteur à combustion interne ?  A)  Étienne Lenoir B)  Nikolaus Otto "
[2024-03-04 10:47:14,461 INFO generators.py generate l.475] (21/93) *** AnsGenerator for question "Qui a inventé le réfrigérateur ?  A)  John Gorrie B)  Carl von Linde C)  James Harrison "
[2024-03-04 10:47:14,472 INFO generators.py gen_for_qa l.548] (21/93) * Start with LLM "gpt-4"
[2024-03-04 10:47:14,476 DEBUG generators.py generate l.349] (21/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:14,480 DEBUG generators.py generate l.358] (21/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:15,487 DEBUG generators.py generate l.370] (21/93) Post-process Answer
[2024-03-04 10:47:15,489 INFO generators.py gen_for_qa l.548] (21/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:47:15,490 DEBUG generators.py generate l.349] (21/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:15,490 DEBUG generators.py generate l.358] (21/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:16,549 DEBUG generators.py generate l.370] (21/93) Post-process Answer
[2024-03-04 10:47:16,565 INFO generators.py gen_for_qa l.548] (21/93) * Start with LLM "gemini-pro"
[2024-03-04 10:47:16,565 DEBUG generators.py generate l.349] (21/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:16,565 DEBUG generators.py generate l.358] (21/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:18,887 DEBUG generators.py generate l.370] (21/93) Post-process Answer
[2024-03-04 10:47:18,903 INFO generators.py gen_for_qa l.548] (21/93) * Start with LLM "claude-2.1"
[2024-03-04 10:47:18,903 DEBUG generators.py generate l.349] (21/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:18,903 DEBUG generators.py generate l.358] (21/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:20,196 DEBUG generators.py generate l.370] (21/93) Post-process Answer
[2024-03-04 10:47:20,196 INFO generators.py gen_for_qa l.548] (21/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:47:20,206 DEBUG generators.py generate l.349] (21/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:20,206 DEBUG generators.py generate l.358] (21/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:20,758 DEBUG generators.py generate l.370] (21/93) Post-process Answer
[2024-03-04 10:47:20,758 INFO generators.py gen_for_qa l.548] (21/93) * Start with LLM "command-nightly"
[2024-03-04 10:47:20,758 DEBUG generators.py generate l.349] (21/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:20,770 DEBUG generators.py generate l.358] (21/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:20,996 DEBUG generators.py generate l.370] (21/93) Post-process Answer
[2024-03-04 10:47:20,996 INFO generators.py gen_for_qa l.548] (21/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:47:20,996 DEBUG generators.py generate l.349] (21/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:20,996 DEBUG generators.py generate l.358] (21/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:21,403 ERROR generators.py complete l.400] (21/93) The following exception occurred with prompt meta={} user="Qui a inventé le réfrigérateur ?  A)  John Gorrie B)  Carl von Linde C)  James Harrison .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:47:21,403 DEBUG generators.py generate l.373] (21/93) Reuse post-processing
[2024-03-04 10:47:21,403 INFO generators.py generate l.477] (21/93) End question "Qui a inventé le réfrigérateur ?  A)  John Gorrie B)  Carl von Linde C)  James Harrison "
[2024-03-04 10:47:21,418 INFO generators.py generate l.475] (22/93) *** AnsGenerator for question "Qui a inventé le transformateur électrique ?  A)  Lucien Gaulard et John Dixon Gibbs B)  Ottó Bláthy, Miksa Déri et Károly Zipernowsky "
[2024-03-04 10:47:21,418 INFO generators.py gen_for_qa l.548] (22/93) * Start with LLM "gpt-4"
[2024-03-04 10:47:21,418 DEBUG generators.py generate l.349] (22/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:21,418 DEBUG generators.py generate l.358] (22/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:22,111 DEBUG generators.py generate l.370] (22/93) Post-process Answer
[2024-03-04 10:47:22,111 INFO generators.py gen_for_qa l.548] (22/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:47:22,111 DEBUG generators.py generate l.349] (22/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:22,111 DEBUG generators.py generate l.358] (22/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:23,392 DEBUG generators.py generate l.370] (22/93) Post-process Answer
[2024-03-04 10:47:23,394 INFO generators.py gen_for_qa l.548] (22/93) * Start with LLM "gemini-pro"
[2024-03-04 10:47:23,394 DEBUG generators.py generate l.349] (22/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:23,394 DEBUG generators.py generate l.358] (22/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:25,612 DEBUG generators.py generate l.370] (22/93) Post-process Answer
[2024-03-04 10:47:25,612 INFO generators.py gen_for_qa l.548] (22/93) * Start with LLM "claude-2.1"
[2024-03-04 10:47:25,612 DEBUG generators.py generate l.349] (22/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:25,612 DEBUG generators.py generate l.358] (22/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:26,847 DEBUG generators.py generate l.370] (22/93) Post-process Answer
[2024-03-04 10:47:26,847 INFO generators.py gen_for_qa l.548] (22/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:47:26,847 DEBUG generators.py generate l.349] (22/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:26,847 DEBUG generators.py generate l.358] (22/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:27,791 DEBUG generators.py generate l.370] (22/93) Post-process Answer
[2024-03-04 10:47:27,791 INFO generators.py gen_for_qa l.548] (22/93) * Start with LLM "command-nightly"
[2024-03-04 10:47:27,791 DEBUG generators.py generate l.349] (22/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:27,791 DEBUG generators.py generate l.358] (22/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:28,120 DEBUG generators.py generate l.370] (22/93) Post-process Answer
[2024-03-04 10:47:28,120 INFO generators.py gen_for_qa l.548] (22/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:47:28,120 DEBUG generators.py generate l.349] (22/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:28,120 DEBUG generators.py generate l.358] (22/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:28,544 ERROR generators.py complete l.400] (22/93) The following exception occurred with prompt meta={} user="Qui a inventé le transformateur électrique ?  A)  Lucien Gaulard et John Dixon Gibbs B)  Ottó Bláthy, Miksa Déri et Károly Zipernowsky .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:47:28,560 DEBUG generators.py generate l.373] (22/93) Reuse post-processing
[2024-03-04 10:47:28,560 INFO generators.py generate l.477] (22/93) End question "Qui a inventé le transformateur électrique ?  A)  Lucien Gaulard et John Dixon Gibbs B)  Ottó Bláthy, Miksa Déri et Károly Zipernowsky "
[2024-03-04 10:47:28,560 INFO generators.py generate l.475] (23/93) *** AnsGenerator for question "Qui a inventé le phonographe ?  A)  Thomas Edison B)  Charles Cros "
[2024-03-04 10:47:28,575 INFO generators.py gen_for_qa l.548] (23/93) * Start with LLM "gpt-4"
[2024-03-04 10:47:28,578 DEBUG generators.py generate l.349] (23/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:28,578 DEBUG generators.py generate l.358] (23/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:29,238 DEBUG generators.py generate l.370] (23/93) Post-process Answer
[2024-03-04 10:47:29,254 INFO generators.py gen_for_qa l.548] (23/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:47:29,254 DEBUG generators.py generate l.349] (23/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:29,254 DEBUG generators.py generate l.358] (23/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:29,801 DEBUG generators.py generate l.370] (23/93) Post-process Answer
[2024-03-04 10:47:29,801 INFO generators.py gen_for_qa l.548] (23/93) * Start with LLM "gemini-pro"
[2024-03-04 10:47:29,816 DEBUG generators.py generate l.349] (23/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:29,816 DEBUG generators.py generate l.358] (23/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:31,957 DEBUG generators.py generate l.370] (23/93) Post-process Answer
[2024-03-04 10:47:31,957 INFO generators.py gen_for_qa l.548] (23/93) * Start with LLM "claude-2.1"
[2024-03-04 10:47:31,968 DEBUG generators.py generate l.349] (23/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:31,969 DEBUG generators.py generate l.358] (23/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:33,076 DEBUG generators.py generate l.370] (23/93) Post-process Answer
[2024-03-04 10:47:33,085 INFO generators.py gen_for_qa l.548] (23/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:47:33,085 DEBUG generators.py generate l.349] (23/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:33,092 DEBUG generators.py generate l.358] (23/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:33,517 DEBUG generators.py generate l.370] (23/93) Post-process Answer
[2024-03-04 10:47:33,517 INFO generators.py gen_for_qa l.548] (23/93) * Start with LLM "command-nightly"
[2024-03-04 10:47:33,517 DEBUG generators.py generate l.349] (23/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:33,532 DEBUG generators.py generate l.358] (23/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:33,814 DEBUG generators.py generate l.370] (23/93) Post-process Answer
[2024-03-04 10:47:33,814 INFO generators.py gen_for_qa l.548] (23/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:47:33,814 DEBUG generators.py generate l.349] (23/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:33,814 DEBUG generators.py generate l.358] (23/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:34,289 ERROR generators.py complete l.400] (23/93) The following exception occurred with prompt meta={} user="Qui a inventé le phonographe ?  A)  Thomas Edison B)  Charles Cros .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:47:34,289 DEBUG generators.py generate l.373] (23/93) Reuse post-processing
[2024-03-04 10:47:34,289 INFO generators.py generate l.477] (23/93) End question "Qui a inventé le phonographe ?  A)  Thomas Edison B)  Charles Cros "
[2024-03-04 10:47:34,289 INFO generators.py generate l.475] (24/93) *** AnsGenerator for question " Qui a découvert l'Amérique ?  A)  Christophe Colomb B)  Leif Erikson "
[2024-03-04 10:47:34,304 INFO generators.py gen_for_qa l.548] (24/93) * Start with LLM "gpt-4"
[2024-03-04 10:47:34,304 DEBUG generators.py generate l.349] (24/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:34,304 DEBUG generators.py generate l.358] (24/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:35,028 DEBUG generators.py generate l.370] (24/93) Post-process Answer
[2024-03-04 10:47:35,028 INFO generators.py gen_for_qa l.548] (24/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:47:35,041 DEBUG generators.py generate l.349] (24/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:35,041 DEBUG generators.py generate l.358] (24/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:36,012 DEBUG generators.py generate l.370] (24/93) Post-process Answer
[2024-03-04 10:47:36,012 INFO generators.py gen_for_qa l.548] (24/93) * Start with LLM "gemini-pro"
[2024-03-04 10:47:36,012 DEBUG generators.py generate l.349] (24/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:36,028 DEBUG generators.py generate l.358] (24/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:38,264 DEBUG generators.py generate l.370] (24/93) Post-process Answer
[2024-03-04 10:47:38,264 INFO generators.py gen_for_qa l.548] (24/93) * Start with LLM "claude-2.1"
[2024-03-04 10:47:38,264 DEBUG generators.py generate l.349] (24/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:38,264 DEBUG generators.py generate l.358] (24/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:39,277 DEBUG generators.py generate l.370] (24/93) Post-process Answer
[2024-03-04 10:47:39,277 INFO generators.py gen_for_qa l.548] (24/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:47:39,277 DEBUG generators.py generate l.349] (24/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:39,290 DEBUG generators.py generate l.358] (24/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:42,657 DEBUG generators.py generate l.370] (24/93) Post-process Answer
[2024-03-04 10:47:42,657 INFO generators.py gen_for_qa l.548] (24/93) * Start with LLM "command-nightly"
[2024-03-04 10:47:42,657 DEBUG generators.py generate l.349] (24/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:42,657 DEBUG generators.py generate l.358] (24/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:42,959 DEBUG generators.py generate l.370] (24/93) Post-process Answer
[2024-03-04 10:47:42,959 INFO generators.py gen_for_qa l.548] (24/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:47:42,959 DEBUG generators.py generate l.349] (24/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:42,976 DEBUG generators.py generate l.358] (24/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:43,528 ERROR generators.py complete l.400] (24/93) The following exception occurred with prompt meta={} user=" Qui a découvert l'Amérique ?  A)  Christophe Colomb B)  Leif Erikson .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:47:43,543 DEBUG generators.py generate l.373] (24/93) Reuse post-processing
[2024-03-04 10:47:43,543 INFO generators.py generate l.477] (24/93) End question " Qui a découvert l'Amérique ?  A)  Christophe Colomb B)  Leif Erikson "
[2024-03-04 10:47:43,543 INFO generators.py generate l.475] (25/93) *** AnsGenerator for question " Qui a inventé l'avion ?  A)  Orville et Wilbur Wright B)  Clément Ader C)  Gustave Whitehead "
[2024-03-04 10:47:43,543 INFO generators.py gen_for_qa l.548] (25/93) * Start with LLM "gpt-4"
[2024-03-04 10:47:43,543 DEBUG generators.py generate l.349] (25/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:43,543 DEBUG generators.py generate l.358] (25/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:44,486 DEBUG generators.py generate l.370] (25/93) Post-process Answer
[2024-03-04 10:47:44,486 INFO generators.py gen_for_qa l.548] (25/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:47:44,501 DEBUG generators.py generate l.349] (25/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:44,501 DEBUG generators.py generate l.358] (25/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:45,693 DEBUG generators.py generate l.370] (25/93) Post-process Answer
[2024-03-04 10:47:45,693 INFO generators.py gen_for_qa l.548] (25/93) * Start with LLM "gemini-pro"
[2024-03-04 10:47:45,693 DEBUG generators.py generate l.349] (25/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:45,693 DEBUG generators.py generate l.358] (25/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:48,101 DEBUG generators.py generate l.370] (25/93) Post-process Answer
[2024-03-04 10:47:48,101 INFO generators.py gen_for_qa l.548] (25/93) * Start with LLM "claude-2.1"
[2024-03-04 10:47:48,101 DEBUG generators.py generate l.349] (25/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:48,114 DEBUG generators.py generate l.358] (25/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:49,564 DEBUG generators.py generate l.370] (25/93) Post-process Answer
[2024-03-04 10:47:49,580 INFO generators.py gen_for_qa l.548] (25/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:47:49,580 DEBUG generators.py generate l.349] (25/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:49,580 DEBUG generators.py generate l.358] (25/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:50,099 DEBUG generators.py generate l.370] (25/93) Post-process Answer
[2024-03-04 10:47:50,099 INFO generators.py gen_for_qa l.548] (25/93) * Start with LLM "command-nightly"
[2024-03-04 10:47:50,099 DEBUG generators.py generate l.349] (25/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:50,115 DEBUG generators.py generate l.358] (25/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:50,384 DEBUG generators.py generate l.370] (25/93) Post-process Answer
[2024-03-04 10:47:50,399 INFO generators.py gen_for_qa l.548] (25/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:47:50,399 DEBUG generators.py generate l.349] (25/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:50,399 DEBUG generators.py generate l.358] (25/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:50,838 ERROR generators.py complete l.400] (25/93) The following exception occurred with prompt meta={} user=" Qui a inventé l'avion ?  A)  Orville et Wilbur Wright B)  Clément Ader C)  Gustave Whitehead .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:47:50,853 DEBUG generators.py generate l.373] (25/93) Reuse post-processing
[2024-03-04 10:47:50,853 INFO generators.py generate l.477] (25/93) End question " Qui a inventé l'avion ?  A)  Orville et Wilbur Wright B)  Clément Ader C)  Gustave Whitehead "
[2024-03-04 10:47:50,853 INFO generators.py generate l.475] (26/93) *** AnsGenerator for question " Qui a inventé la radio ?  A)  Guglielmo Marconi B)  Nikola Tesla C)  Oliver Lodge "
[2024-03-04 10:47:50,869 INFO generators.py gen_for_qa l.548] (26/93) * Start with LLM "gpt-4"
[2024-03-04 10:47:50,869 DEBUG generators.py generate l.349] (26/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:50,869 DEBUG generators.py generate l.358] (26/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:51,704 DEBUG generators.py generate l.370] (26/93) Post-process Answer
[2024-03-04 10:47:51,704 INFO generators.py gen_for_qa l.548] (26/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:47:51,720 DEBUG generators.py generate l.349] (26/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:51,720 DEBUG generators.py generate l.358] (26/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:52,846 DEBUG generators.py generate l.370] (26/93) Post-process Answer
[2024-03-04 10:47:52,846 INFO generators.py gen_for_qa l.548] (26/93) * Start with LLM "gemini-pro"
[2024-03-04 10:47:52,846 DEBUG generators.py generate l.349] (26/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:52,846 DEBUG generators.py generate l.358] (26/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:55,123 DEBUG generators.py generate l.370] (26/93) Post-process Answer
[2024-03-04 10:47:55,123 INFO generators.py gen_for_qa l.548] (26/93) * Start with LLM "claude-2.1"
[2024-03-04 10:47:55,123 DEBUG generators.py generate l.349] (26/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:55,138 DEBUG generators.py generate l.358] (26/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:56,615 DEBUG generators.py generate l.370] (26/93) Post-process Answer
[2024-03-04 10:47:56,615 INFO generators.py gen_for_qa l.548] (26/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:47:56,615 DEBUG generators.py generate l.349] (26/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:56,615 DEBUG generators.py generate l.358] (26/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:57,539 DEBUG generators.py generate l.370] (26/93) Post-process Answer
[2024-03-04 10:47:57,539 INFO generators.py gen_for_qa l.548] (26/93) * Start with LLM "command-nightly"
[2024-03-04 10:47:57,554 DEBUG generators.py generate l.349] (26/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:57,554 DEBUG generators.py generate l.358] (26/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:57,884 DEBUG generators.py generate l.370] (26/93) Post-process Answer
[2024-03-04 10:47:57,887 INFO generators.py gen_for_qa l.548] (26/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:47:57,887 DEBUG generators.py generate l.349] (26/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:57,902 DEBUG generators.py generate l.358] (26/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:58,629 ERROR generators.py complete l.400] (26/93) The following exception occurred with prompt meta={} user=" Qui a inventé la radio ?  A)  Guglielmo Marconi B)  Nikola Tesla C)  Oliver Lodge .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:47:58,644 DEBUG generators.py generate l.373] (26/93) Reuse post-processing
[2024-03-04 10:47:58,644 INFO generators.py generate l.477] (26/93) End question " Qui a inventé la radio ?  A)  Guglielmo Marconi B)  Nikola Tesla C)  Oliver Lodge "
[2024-03-04 10:47:58,644 INFO generators.py generate l.475] (27/93) *** AnsGenerator for question " Qui a inventé le télescope ?  A)  Hans Lippershey B)  Zacharias Janssen C)  Galileo Galilei "
[2024-03-04 10:47:58,644 INFO generators.py gen_for_qa l.548] (27/93) * Start with LLM "gpt-4"
[2024-03-04 10:47:58,644 DEBUG generators.py generate l.349] (27/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:58,660 DEBUG generators.py generate l.358] (27/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:47:59,906 DEBUG generators.py generate l.370] (27/93) Post-process Answer
[2024-03-04 10:47:59,907 INFO generators.py gen_for_qa l.548] (27/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:47:59,909 DEBUG generators.py generate l.349] (27/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:47:59,909 DEBUG generators.py generate l.358] (27/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:00,610 DEBUG generators.py generate l.370] (27/93) Post-process Answer
[2024-03-04 10:48:00,625 INFO generators.py gen_for_qa l.548] (27/93) * Start with LLM "gemini-pro"
[2024-03-04 10:48:00,625 DEBUG generators.py generate l.349] (27/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:00,625 DEBUG generators.py generate l.358] (27/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:02,755 DEBUG generators.py generate l.370] (27/93) Post-process Answer
[2024-03-04 10:48:02,759 INFO generators.py gen_for_qa l.548] (27/93) * Start with LLM "claude-2.1"
[2024-03-04 10:48:02,759 DEBUG generators.py generate l.349] (27/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:02,764 DEBUG generators.py generate l.358] (27/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:03,652 DEBUG generators.py generate l.370] (27/93) Post-process Answer
[2024-03-04 10:48:03,654 INFO generators.py gen_for_qa l.548] (27/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:48:03,655 DEBUG generators.py generate l.349] (27/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:03,656 DEBUG generators.py generate l.358] (27/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:05,862 DEBUG generators.py generate l.370] (27/93) Post-process Answer
[2024-03-04 10:48:05,862 INFO generators.py gen_for_qa l.548] (27/93) * Start with LLM "command-nightly"
[2024-03-04 10:48:05,878 DEBUG generators.py generate l.349] (27/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:05,878 DEBUG generators.py generate l.358] (27/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:06,146 DEBUG generators.py generate l.370] (27/93) Post-process Answer
[2024-03-04 10:48:06,146 INFO generators.py gen_for_qa l.548] (27/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:48:06,146 DEBUG generators.py generate l.349] (27/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:06,162 DEBUG generators.py generate l.358] (27/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:06,654 ERROR generators.py complete l.400] (27/93) The following exception occurred with prompt meta={} user=" Qui a inventé le télescope ?  A)  Hans Lippershey B)  Zacharias Janssen C)  Galileo Galilei .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:48:06,670 DEBUG generators.py generate l.373] (27/93) Reuse post-processing
[2024-03-04 10:48:06,670 INFO generators.py generate l.477] (27/93) End question " Qui a inventé le télescope ?  A)  Hans Lippershey B)  Zacharias Janssen C)  Galileo Galilei "
[2024-03-04 10:48:06,670 INFO generators.py generate l.475] (28/93) *** AnsGenerator for question " Qui a découvert l'oxygène ?  A)  Joseph Priestley B)  Carl Wilhelm Scheele C)  Antoine Lavoisier "
[2024-03-04 10:48:06,670 INFO generators.py gen_for_qa l.548] (28/93) * Start with LLM "gpt-4"
[2024-03-04 10:48:06,685 DEBUG generators.py generate l.349] (28/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:06,685 DEBUG generators.py generate l.358] (28/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:07,645 DEBUG generators.py generate l.370] (28/93) Post-process Answer
[2024-03-04 10:48:07,645 INFO generators.py gen_for_qa l.548] (28/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:48:07,661 DEBUG generators.py generate l.349] (28/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:07,661 DEBUG generators.py generate l.358] (28/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:08,676 DEBUG generators.py generate l.370] (28/93) Post-process Answer
[2024-03-04 10:48:08,689 INFO generators.py gen_for_qa l.548] (28/93) * Start with LLM "gemini-pro"
[2024-03-04 10:48:08,690 DEBUG generators.py generate l.349] (28/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:08,690 DEBUG generators.py generate l.358] (28/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:11,118 DEBUG generators.py generate l.370] (28/93) Post-process Answer
[2024-03-04 10:48:11,118 INFO generators.py gen_for_qa l.548] (28/93) * Start with LLM "claude-2.1"
[2024-03-04 10:48:11,118 DEBUG generators.py generate l.349] (28/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:11,118 DEBUG generators.py generate l.358] (28/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:12,155 DEBUG generators.py generate l.370] (28/93) Post-process Answer
[2024-03-04 10:48:12,155 INFO generators.py gen_for_qa l.548] (28/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:48:12,155 DEBUG generators.py generate l.349] (28/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:12,162 DEBUG generators.py generate l.358] (28/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:12,571 DEBUG generators.py generate l.370] (28/93) Post-process Answer
[2024-03-04 10:48:12,571 INFO generators.py gen_for_qa l.548] (28/93) * Start with LLM "command-nightly"
[2024-03-04 10:48:12,579 DEBUG generators.py generate l.349] (28/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:12,581 DEBUG generators.py generate l.358] (28/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:12,975 DEBUG generators.py generate l.370] (28/93) Post-process Answer
[2024-03-04 10:48:12,975 INFO generators.py gen_for_qa l.548] (28/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:48:12,991 DEBUG generators.py generate l.349] (28/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:12,991 DEBUG generators.py generate l.358] (28/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:13,680 ERROR generators.py complete l.400] (28/93) The following exception occurred with prompt meta={} user=" Qui a découvert l'oxygène ?  A)  Joseph Priestley B)  Carl Wilhelm Scheele C)  Antoine Lavoisier .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:48:13,699 DEBUG generators.py generate l.373] (28/93) Reuse post-processing
[2024-03-04 10:48:13,699 INFO generators.py generate l.477] (28/93) End question " Qui a découvert l'oxygène ?  A)  Joseph Priestley B)  Carl Wilhelm Scheele C)  Antoine Lavoisier "
[2024-03-04 10:48:13,699 INFO generators.py generate l.475] (29/93) *** AnsGenerator for question " Qui a inventé le moteur à réaction ?  A)  Frank Whittle B)  Hans von Ohain "
[2024-03-04 10:48:13,699 INFO generators.py gen_for_qa l.548] (29/93) * Start with LLM "gpt-4"
[2024-03-04 10:48:13,712 DEBUG generators.py generate l.349] (29/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:13,712 DEBUG generators.py generate l.358] (29/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:15,070 DEBUG generators.py generate l.370] (29/93) Post-process Answer
[2024-03-04 10:48:15,085 INFO generators.py gen_for_qa l.548] (29/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:48:15,085 DEBUG generators.py generate l.349] (29/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:15,085 DEBUG generators.py generate l.358] (29/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:16,111 DEBUG generators.py generate l.370] (29/93) Post-process Answer
[2024-03-04 10:48:16,127 INFO generators.py gen_for_qa l.548] (29/93) * Start with LLM "gemini-pro"
[2024-03-04 10:48:16,127 DEBUG generators.py generate l.349] (29/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:16,127 DEBUG generators.py generate l.358] (29/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:18,791 DEBUG generators.py generate l.370] (29/93) Post-process Answer
[2024-03-04 10:48:18,793 INFO generators.py gen_for_qa l.548] (29/93) * Start with LLM "claude-2.1"
[2024-03-04 10:48:18,793 DEBUG generators.py generate l.349] (29/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:18,805 DEBUG generators.py generate l.358] (29/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:19,949 DEBUG generators.py generate l.370] (29/93) Post-process Answer
[2024-03-04 10:48:19,969 INFO generators.py gen_for_qa l.548] (29/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:48:19,969 DEBUG generators.py generate l.349] (29/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:19,971 DEBUG generators.py generate l.358] (29/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:20,641 DEBUG generators.py generate l.370] (29/93) Post-process Answer
[2024-03-04 10:48:20,641 INFO generators.py gen_for_qa l.548] (29/93) * Start with LLM "command-nightly"
[2024-03-04 10:48:20,641 DEBUG generators.py generate l.349] (29/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:20,641 DEBUG generators.py generate l.358] (29/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:21,143 DEBUG generators.py generate l.370] (29/93) Post-process Answer
[2024-03-04 10:48:21,143 INFO generators.py gen_for_qa l.548] (29/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:48:21,143 DEBUG generators.py generate l.349] (29/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:21,143 DEBUG generators.py generate l.358] (29/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:21,601 ERROR generators.py complete l.400] (29/93) The following exception occurred with prompt meta={} user=" Qui a inventé le moteur à réaction ?  A)  Frank Whittle B)  Hans von Ohain .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:48:21,616 DEBUG generators.py generate l.373] (29/93) Reuse post-processing
[2024-03-04 10:48:21,616 INFO generators.py generate l.477] (29/93) End question " Qui a inventé le moteur à réaction ?  A)  Frank Whittle B)  Hans von Ohain "
[2024-03-04 10:48:21,616 INFO generators.py generate l.475] (30/93) *** AnsGenerator for question " Qui a inventé le radar ?  A)  Robert Watson-Watt B)  Christian Hülsmeyer C)  Guglielmo Marconi "
[2024-03-04 10:48:21,632 INFO generators.py gen_for_qa l.548] (30/93) * Start with LLM "gpt-4"
[2024-03-04 10:48:21,632 DEBUG generators.py generate l.349] (30/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:21,632 DEBUG generators.py generate l.358] (30/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:22,714 DEBUG generators.py generate l.370] (30/93) Post-process Answer
[2024-03-04 10:48:22,714 INFO generators.py gen_for_qa l.548] (30/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:48:22,730 DEBUG generators.py generate l.349] (30/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:22,730 DEBUG generators.py generate l.358] (30/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:23,438 DEBUG generators.py generate l.370] (30/93) Post-process Answer
[2024-03-04 10:48:23,454 INFO generators.py gen_for_qa l.548] (30/93) * Start with LLM "gemini-pro"
[2024-03-04 10:48:23,454 DEBUG generators.py generate l.349] (30/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:23,454 DEBUG generators.py generate l.358] (30/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:25,776 DEBUG generators.py generate l.370] (30/93) Post-process Answer
[2024-03-04 10:48:25,779 INFO generators.py gen_for_qa l.548] (30/93) * Start with LLM "claude-2.1"
[2024-03-04 10:48:25,779 DEBUG generators.py generate l.349] (30/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:25,788 DEBUG generators.py generate l.358] (30/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:26,986 DEBUG generators.py generate l.370] (30/93) Post-process Answer
[2024-03-04 10:48:26,986 INFO generators.py gen_for_qa l.548] (30/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:48:26,986 DEBUG generators.py generate l.349] (30/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:26,986 DEBUG generators.py generate l.358] (30/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:27,533 DEBUG generators.py generate l.370] (30/93) Post-process Answer
[2024-03-04 10:48:27,538 INFO generators.py gen_for_qa l.548] (30/93) * Start with LLM "command-nightly"
[2024-03-04 10:48:27,540 DEBUG generators.py generate l.349] (30/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:27,542 DEBUG generators.py generate l.358] (30/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:27,835 DEBUG generators.py generate l.370] (30/93) Post-process Answer
[2024-03-04 10:48:27,841 INFO generators.py gen_for_qa l.548] (30/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:48:27,841 DEBUG generators.py generate l.349] (30/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:27,841 DEBUG generators.py generate l.358] (30/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:29,492 ERROR generators.py complete l.400] (30/93) The following exception occurred with prompt meta={} user=" Qui a inventé le radar ?  A)  Robert Watson-Watt B)  Christian Hülsmeyer C)  Guglielmo Marconi .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:48:29,508 DEBUG generators.py generate l.373] (30/93) Reuse post-processing
[2024-03-04 10:48:29,508 INFO generators.py generate l.477] (30/93) End question " Qui a inventé le radar ?  A)  Robert Watson-Watt B)  Christian Hülsmeyer C)  Guglielmo Marconi "
[2024-03-04 10:48:29,508 INFO generators.py generate l.475] (31/93) *** AnsGenerator for question " Qui a découvert la pénicilline ?  A)  Alexander Fleming B)  Ernest Duchesne "
[2024-03-04 10:48:29,508 INFO generators.py gen_for_qa l.548] (31/93) * Start with LLM "gpt-4"
[2024-03-04 10:48:29,508 DEBUG generators.py generate l.349] (31/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:29,524 DEBUG generators.py generate l.358] (31/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:30,156 DEBUG generators.py generate l.370] (31/93) Post-process Answer
[2024-03-04 10:48:30,171 INFO generators.py gen_for_qa l.548] (31/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:48:30,171 DEBUG generators.py generate l.349] (31/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:30,171 DEBUG generators.py generate l.358] (31/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:30,877 DEBUG generators.py generate l.370] (31/93) Post-process Answer
[2024-03-04 10:48:30,893 INFO generators.py gen_for_qa l.548] (31/93) * Start with LLM "gemini-pro"
[2024-03-04 10:48:30,893 DEBUG generators.py generate l.349] (31/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:30,893 DEBUG generators.py generate l.358] (31/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:33,370 DEBUG generators.py generate l.370] (31/93) Post-process Answer
[2024-03-04 10:48:33,376 INFO generators.py gen_for_qa l.548] (31/93) * Start with LLM "claude-2.1"
[2024-03-04 10:48:33,376 DEBUG generators.py generate l.349] (31/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:33,376 DEBUG generators.py generate l.358] (31/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:34,416 DEBUG generators.py generate l.370] (31/93) Post-process Answer
[2024-03-04 10:48:34,416 INFO generators.py gen_for_qa l.548] (31/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:48:34,416 DEBUG generators.py generate l.349] (31/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:34,416 DEBUG generators.py generate l.358] (31/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:35,226 DEBUG generators.py generate l.370] (31/93) Post-process Answer
[2024-03-04 10:48:35,226 INFO generators.py gen_for_qa l.548] (31/93) * Start with LLM "command-nightly"
[2024-03-04 10:48:35,226 DEBUG generators.py generate l.349] (31/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:35,226 DEBUG generators.py generate l.358] (31/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:35,539 DEBUG generators.py generate l.370] (31/93) Post-process Answer
[2024-03-04 10:48:35,539 INFO generators.py gen_for_qa l.548] (31/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:48:35,539 DEBUG generators.py generate l.349] (31/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:35,539 DEBUG generators.py generate l.358] (31/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:36,154 ERROR generators.py complete l.400] (31/93) The following exception occurred with prompt meta={} user=" Qui a découvert la pénicilline ?  A)  Alexander Fleming B)  Ernest Duchesne .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:48:36,154 DEBUG generators.py generate l.373] (31/93) Reuse post-processing
[2024-03-04 10:48:36,154 INFO generators.py generate l.477] (31/93) End question " Qui a découvert la pénicilline ?  A)  Alexander Fleming B)  Ernest Duchesne "
[2024-03-04 10:48:36,169 INFO generators.py generate l.475] (32/93) *** AnsGenerator for question " Qui a inventé la télévision ?  A)  John Logie Baird B)  Philo Farnsworth C)  Vladimir Zworykin "
[2024-03-04 10:48:36,169 INFO generators.py gen_for_qa l.548] (32/93) * Start with LLM "gpt-4"
[2024-03-04 10:48:36,169 DEBUG generators.py generate l.349] (32/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:36,169 DEBUG generators.py generate l.358] (32/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:37,048 DEBUG generators.py generate l.370] (32/93) Post-process Answer
[2024-03-04 10:48:37,063 INFO generators.py gen_for_qa l.548] (32/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:48:37,063 DEBUG generators.py generate l.349] (32/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:37,071 DEBUG generators.py generate l.358] (32/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:37,738 DEBUG generators.py generate l.370] (32/93) Post-process Answer
[2024-03-04 10:48:37,741 INFO generators.py gen_for_qa l.548] (32/93) * Start with LLM "gemini-pro"
[2024-03-04 10:48:37,745 DEBUG generators.py generate l.349] (32/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:37,745 DEBUG generators.py generate l.358] (32/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:40,234 DEBUG generators.py generate l.370] (32/93) Post-process Answer
[2024-03-04 10:48:40,234 INFO generators.py gen_for_qa l.548] (32/93) * Start with LLM "claude-2.1"
[2024-03-04 10:48:40,234 DEBUG generators.py generate l.349] (32/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:40,250 DEBUG generators.py generate l.358] (32/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:41,689 DEBUG generators.py generate l.370] (32/93) Post-process Answer
[2024-03-04 10:48:41,689 INFO generators.py gen_for_qa l.548] (32/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:48:41,689 DEBUG generators.py generate l.349] (32/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:41,689 DEBUG generators.py generate l.358] (32/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:42,319 DEBUG generators.py generate l.370] (32/93) Post-process Answer
[2024-03-04 10:48:42,319 INFO generators.py gen_for_qa l.548] (32/93) * Start with LLM "command-nightly"
[2024-03-04 10:48:42,319 DEBUG generators.py generate l.349] (32/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:42,319 DEBUG generators.py generate l.358] (32/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:42,652 DEBUG generators.py generate l.370] (32/93) Post-process Answer
[2024-03-04 10:48:42,652 INFO generators.py gen_for_qa l.548] (32/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:48:42,652 DEBUG generators.py generate l.349] (32/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:42,652 DEBUG generators.py generate l.358] (32/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:43,307 ERROR generators.py complete l.400] (32/93) The following exception occurred with prompt meta={} user=" Qui a inventé la télévision ?  A)  John Logie Baird B)  Philo Farnsworth C)  Vladimir Zworykin .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:48:43,322 DEBUG generators.py generate l.373] (32/93) Reuse post-processing
[2024-03-04 10:48:43,322 INFO generators.py generate l.477] (32/93) End question " Qui a inventé la télévision ?  A)  John Logie Baird B)  Philo Farnsworth C)  Vladimir Zworykin "
[2024-03-04 10:48:43,338 INFO generators.py generate l.475] (33/93) *** AnsGenerator for question " Qui a découvert l'ADN ?  A)  James Watson et Francis Crick B)  Rosalind Franklin C)  Maurice Wilkins "
[2024-03-04 10:48:43,338 INFO generators.py gen_for_qa l.548] (33/93) * Start with LLM "gpt-4"
[2024-03-04 10:48:43,338 DEBUG generators.py generate l.349] (33/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:43,338 DEBUG generators.py generate l.358] (33/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:44,206 DEBUG generators.py generate l.370] (33/93) Post-process Answer
[2024-03-04 10:48:44,206 INFO generators.py gen_for_qa l.548] (33/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:48:44,206 DEBUG generators.py generate l.349] (33/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:44,206 DEBUG generators.py generate l.358] (33/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:45,184 DEBUG generators.py generate l.370] (33/93) Post-process Answer
[2024-03-04 10:48:45,201 INFO generators.py gen_for_qa l.548] (33/93) * Start with LLM "gemini-pro"
[2024-03-04 10:48:45,201 DEBUG generators.py generate l.349] (33/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:45,201 DEBUG generators.py generate l.358] (33/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:47,508 DEBUG generators.py generate l.370] (33/93) Post-process Answer
[2024-03-04 10:48:47,526 INFO generators.py gen_for_qa l.548] (33/93) * Start with LLM "claude-2.1"
[2024-03-04 10:48:47,526 DEBUG generators.py generate l.349] (33/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:47,526 DEBUG generators.py generate l.358] (33/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:48,809 DEBUG generators.py generate l.370] (33/93) Post-process Answer
[2024-03-04 10:48:48,809 INFO generators.py gen_for_qa l.548] (33/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:48:48,809 DEBUG generators.py generate l.349] (33/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:48,819 DEBUG generators.py generate l.358] (33/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:49,397 DEBUG generators.py generate l.370] (33/93) Post-process Answer
[2024-03-04 10:48:49,397 INFO generators.py gen_for_qa l.548] (33/93) * Start with LLM "command-nightly"
[2024-03-04 10:48:49,397 DEBUG generators.py generate l.349] (33/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:49,397 DEBUG generators.py generate l.358] (33/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:49,712 DEBUG generators.py generate l.370] (33/93) Post-process Answer
[2024-03-04 10:48:49,712 INFO generators.py gen_for_qa l.548] (33/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:48:49,712 DEBUG generators.py generate l.349] (33/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:49,730 DEBUG generators.py generate l.358] (33/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:50,193 ERROR generators.py complete l.400] (33/93) The following exception occurred with prompt meta={} user=" Qui a découvert l'ADN ?  A)  James Watson et Francis Crick B)  Rosalind Franklin C)  Maurice Wilkins .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:48:50,199 DEBUG generators.py generate l.373] (33/93) Reuse post-processing
[2024-03-04 10:48:50,199 INFO generators.py generate l.477] (33/93) End question " Qui a découvert l'ADN ?  A)  James Watson et Francis Crick B)  Rosalind Franklin C)  Maurice Wilkins "
[2024-03-04 10:48:50,199 INFO generators.py generate l.475] (34/93) *** AnsGenerator for question " Qui a inventé le laser ?  A)  Gordon Gould B)  Charles Hard Townes C)  Arthur Leonard Schawlow "
[2024-03-04 10:48:50,199 INFO generators.py gen_for_qa l.548] (34/93) * Start with LLM "gpt-4"
[2024-03-04 10:48:50,199 DEBUG generators.py generate l.349] (34/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:50,199 DEBUG generators.py generate l.358] (34/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:50,942 DEBUG generators.py generate l.370] (34/93) Post-process Answer
[2024-03-04 10:48:50,942 INFO generators.py gen_for_qa l.548] (34/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:48:50,942 DEBUG generators.py generate l.349] (34/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:50,954 DEBUG generators.py generate l.358] (34/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:52,198 DEBUG generators.py generate l.370] (34/93) Post-process Answer
[2024-03-04 10:48:52,198 INFO generators.py gen_for_qa l.548] (34/93) * Start with LLM "gemini-pro"
[2024-03-04 10:48:52,198 DEBUG generators.py generate l.349] (34/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:52,198 DEBUG generators.py generate l.358] (34/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:54,579 DEBUG generators.py generate l.370] (34/93) Post-process Answer
[2024-03-04 10:48:54,581 INFO generators.py gen_for_qa l.548] (34/93) * Start with LLM "claude-2.1"
[2024-03-04 10:48:54,583 DEBUG generators.py generate l.349] (34/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:54,585 DEBUG generators.py generate l.358] (34/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:55,533 DEBUG generators.py generate l.370] (34/93) Post-process Answer
[2024-03-04 10:48:55,533 INFO generators.py gen_for_qa l.548] (34/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:48:55,533 DEBUG generators.py generate l.349] (34/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:55,546 DEBUG generators.py generate l.358] (34/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:58,207 DEBUG generators.py generate l.370] (34/93) Post-process Answer
[2024-03-04 10:48:58,215 INFO generators.py gen_for_qa l.548] (34/93) * Start with LLM "command-nightly"
[2024-03-04 10:48:58,217 DEBUG generators.py generate l.349] (34/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:58,220 DEBUG generators.py generate l.358] (34/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:48:58,581 DEBUG generators.py generate l.370] (34/93) Post-process Answer
[2024-03-04 10:48:58,581 INFO generators.py gen_for_qa l.548] (34/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:48:58,592 DEBUG generators.py generate l.349] (34/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:48:58,592 DEBUG generators.py generate l.358] (34/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:00,494 ERROR generators.py complete l.400] (34/93) The following exception occurred with prompt meta={} user=" Qui a inventé le laser ?  A)  Gordon Gould B)  Charles Hard Townes C)  Arthur Leonard Schawlow .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:49:00,510 DEBUG generators.py generate l.373] (34/93) Reuse post-processing
[2024-03-04 10:49:00,525 INFO generators.py generate l.477] (34/93) End question " Qui a inventé le laser ?  A)  Gordon Gould B)  Charles Hard Townes C)  Arthur Leonard Schawlow "
[2024-03-04 10:49:00,525 INFO generators.py generate l.475] (35/93) *** AnsGenerator for question " Qui a inventé le transistor ?  A)  John Bardeen, Walter Brattain et William Shockley B)  Julius Edgar Lilienfeld C)  Oskar Heil "
[2024-03-04 10:49:00,525 INFO generators.py gen_for_qa l.548] (35/93) * Start with LLM "gpt-4"
[2024-03-04 10:49:00,525 DEBUG generators.py generate l.349] (35/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:00,541 DEBUG generators.py generate l.358] (35/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:01,313 DEBUG generators.py generate l.370] (35/93) Post-process Answer
[2024-03-04 10:49:01,328 INFO generators.py gen_for_qa l.548] (35/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:49:01,328 DEBUG generators.py generate l.349] (35/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:01,328 DEBUG generators.py generate l.358] (35/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:01,971 DEBUG generators.py generate l.370] (35/93) Post-process Answer
[2024-03-04 10:49:01,971 INFO generators.py gen_for_qa l.548] (35/93) * Start with LLM "gemini-pro"
[2024-03-04 10:49:01,971 DEBUG generators.py generate l.349] (35/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:01,971 DEBUG generators.py generate l.358] (35/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:04,085 DEBUG generators.py generate l.370] (35/93) Post-process Answer
[2024-03-04 10:49:04,088 INFO generators.py gen_for_qa l.548] (35/93) * Start with LLM "claude-2.1"
[2024-03-04 10:49:04,088 DEBUG generators.py generate l.349] (35/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:04,092 DEBUG generators.py generate l.358] (35/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:05,275 DEBUG generators.py generate l.370] (35/93) Post-process Answer
[2024-03-04 10:49:05,275 INFO generators.py gen_for_qa l.548] (35/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:49:05,275 DEBUG generators.py generate l.349] (35/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:05,275 DEBUG generators.py generate l.358] (35/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:05,987 DEBUG generators.py generate l.370] (35/93) Post-process Answer
[2024-03-04 10:49:05,987 INFO generators.py gen_for_qa l.548] (35/93) * Start with LLM "command-nightly"
[2024-03-04 10:49:06,003 DEBUG generators.py generate l.349] (35/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:06,003 DEBUG generators.py generate l.358] (35/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:06,322 DEBUG generators.py generate l.370] (35/93) Post-process Answer
[2024-03-04 10:49:06,322 INFO generators.py gen_for_qa l.548] (35/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:49:06,334 DEBUG generators.py generate l.349] (35/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:06,334 DEBUG generators.py generate l.358] (35/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:06,764 ERROR generators.py complete l.400] (35/93) The following exception occurred with prompt meta={} user=" Qui a inventé le transistor ?  A)  John Bardeen, Walter Brattain et William Shockley B)  Julius Edgar Lilienfeld C)  Oskar Heil .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:49:06,784 DEBUG generators.py generate l.373] (35/93) Reuse post-processing
[2024-03-04 10:49:06,784 INFO generators.py generate l.477] (35/93) End question " Qui a inventé le transistor ?  A)  John Bardeen, Walter Brattain et William Shockley B)  Julius Edgar Lilienfeld C)  Oskar Heil "
[2024-03-04 10:49:06,789 INFO generators.py generate l.475] (36/93) *** AnsGenerator for question " Qui a inventé l'ordinateur ?  A)  Charles Babbage B)  Alan Turing C)  John Atanasoff et Clifford Berry "
[2024-03-04 10:49:06,792 INFO generators.py gen_for_qa l.548] (36/93) * Start with LLM "gpt-4"
[2024-03-04 10:49:06,794 DEBUG generators.py generate l.349] (36/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:06,797 DEBUG generators.py generate l.358] (36/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:07,512 DEBUG generators.py generate l.370] (36/93) Post-process Answer
[2024-03-04 10:49:07,512 INFO generators.py gen_for_qa l.548] (36/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:49:07,512 DEBUG generators.py generate l.349] (36/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:07,520 DEBUG generators.py generate l.358] (36/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:08,572 DEBUG generators.py generate l.370] (36/93) Post-process Answer
[2024-03-04 10:49:08,575 INFO generators.py gen_for_qa l.548] (36/93) * Start with LLM "gemini-pro"
[2024-03-04 10:49:08,576 DEBUG generators.py generate l.349] (36/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:08,577 DEBUG generators.py generate l.358] (36/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:10,775 DEBUG generators.py generate l.370] (36/93) Post-process Answer
[2024-03-04 10:49:10,777 INFO generators.py gen_for_qa l.548] (36/93) * Start with LLM "claude-2.1"
[2024-03-04 10:49:10,778 DEBUG generators.py generate l.349] (36/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:10,780 DEBUG generators.py generate l.358] (36/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:12,155 DEBUG generators.py generate l.370] (36/93) Post-process Answer
[2024-03-04 10:49:12,155 INFO generators.py gen_for_qa l.548] (36/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:49:12,170 DEBUG generators.py generate l.349] (36/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:12,170 DEBUG generators.py generate l.358] (36/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:12,781 DEBUG generators.py generate l.370] (36/93) Post-process Answer
[2024-03-04 10:49:12,781 INFO generators.py gen_for_qa l.548] (36/93) * Start with LLM "command-nightly"
[2024-03-04 10:49:12,799 DEBUG generators.py generate l.349] (36/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:12,801 DEBUG generators.py generate l.358] (36/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:13,095 DEBUG generators.py generate l.370] (36/93) Post-process Answer
[2024-03-04 10:49:13,098 INFO generators.py gen_for_qa l.548] (36/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:49:13,108 DEBUG generators.py generate l.349] (36/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:13,110 DEBUG generators.py generate l.358] (36/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:13,613 ERROR generators.py complete l.400] (36/93) The following exception occurred with prompt meta={} user=" Qui a inventé l'ordinateur ?  A)  Charles Babbage B)  Alan Turing C)  John Atanasoff et Clifford Berry .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:49:13,613 DEBUG generators.py generate l.373] (36/93) Reuse post-processing
[2024-03-04 10:49:13,631 INFO generators.py generate l.477] (36/93) End question " Qui a inventé l'ordinateur ?  A)  Charles Babbage B)  Alan Turing C)  John Atanasoff et Clifford Berry "
[2024-03-04 10:49:13,633 INFO generators.py generate l.475] (37/93) *** AnsGenerator for question " Qui a découvert le boson de Higgs ?  A)  Peter Higgs B)  François Englert C)  Robert Brout "
[2024-03-04 10:49:13,633 INFO generators.py gen_for_qa l.548] (37/93) * Start with LLM "gpt-4"
[2024-03-04 10:49:13,633 DEBUG generators.py generate l.349] (37/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:13,645 DEBUG generators.py generate l.358] (37/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:14,759 DEBUG generators.py generate l.370] (37/93) Post-process Answer
[2024-03-04 10:49:14,761 INFO generators.py gen_for_qa l.548] (37/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:49:14,764 DEBUG generators.py generate l.349] (37/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:14,767 DEBUG generators.py generate l.358] (37/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:15,675 DEBUG generators.py generate l.370] (37/93) Post-process Answer
[2024-03-04 10:49:15,677 INFO generators.py gen_for_qa l.548] (37/93) * Start with LLM "gemini-pro"
[2024-03-04 10:49:15,678 DEBUG generators.py generate l.349] (37/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:15,679 DEBUG generators.py generate l.358] (37/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:18,090 DEBUG generators.py generate l.370] (37/93) Post-process Answer
[2024-03-04 10:49:18,093 INFO generators.py gen_for_qa l.548] (37/93) * Start with LLM "claude-2.1"
[2024-03-04 10:49:18,093 DEBUG generators.py generate l.349] (37/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:18,096 DEBUG generators.py generate l.358] (37/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:19,055 DEBUG generators.py generate l.370] (37/93) Post-process Answer
[2024-03-04 10:49:19,059 INFO generators.py gen_for_qa l.548] (37/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:49:19,059 DEBUG generators.py generate l.349] (37/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:19,059 DEBUG generators.py generate l.358] (37/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:19,504 DEBUG generators.py generate l.370] (37/93) Post-process Answer
[2024-03-04 10:49:19,504 INFO generators.py gen_for_qa l.548] (37/93) * Start with LLM "command-nightly"
[2024-03-04 10:49:19,504 DEBUG generators.py generate l.349] (37/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:19,504 DEBUG generators.py generate l.358] (37/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:19,780 DEBUG generators.py generate l.370] (37/93) Post-process Answer
[2024-03-04 10:49:19,780 INFO generators.py gen_for_qa l.548] (37/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:49:19,780 DEBUG generators.py generate l.349] (37/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:19,789 DEBUG generators.py generate l.358] (37/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:20,175 ERROR generators.py complete l.400] (37/93) The following exception occurred with prompt meta={} user=" Qui a découvert le boson de Higgs ?  A)  Peter Higgs B)  François Englert C)  Robert Brout .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:49:20,187 DEBUG generators.py generate l.373] (37/93) Reuse post-processing
[2024-03-04 10:49:20,189 INFO generators.py generate l.477] (37/93) End question " Qui a découvert le boson de Higgs ?  A)  Peter Higgs B)  François Englert C)  Robert Brout "
[2024-03-04 10:49:20,191 INFO generators.py generate l.475] (38/93) *** AnsGenerator for question " Qui a inventé le World Wide Web ?  A)  Tim Berners-Lee "
[2024-03-04 10:49:20,193 INFO generators.py gen_for_qa l.548] (38/93) * Start with LLM "gpt-4"
[2024-03-04 10:49:20,193 DEBUG generators.py generate l.349] (38/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:20,193 DEBUG generators.py generate l.358] (38/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:21,026 DEBUG generators.py generate l.370] (38/93) Post-process Answer
[2024-03-04 10:49:21,028 INFO generators.py gen_for_qa l.548] (38/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:49:21,029 DEBUG generators.py generate l.349] (38/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:21,030 DEBUG generators.py generate l.358] (38/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:21,621 DEBUG generators.py generate l.370] (38/93) Post-process Answer
[2024-03-04 10:49:21,621 INFO generators.py gen_for_qa l.548] (38/93) * Start with LLM "gemini-pro"
[2024-03-04 10:49:21,621 DEBUG generators.py generate l.349] (38/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:21,621 DEBUG generators.py generate l.358] (38/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:24,007 DEBUG generators.py generate l.370] (38/93) Post-process Answer
[2024-03-04 10:49:24,007 INFO generators.py gen_for_qa l.548] (38/93) * Start with LLM "claude-2.1"
[2024-03-04 10:49:24,023 DEBUG generators.py generate l.349] (38/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:24,023 DEBUG generators.py generate l.358] (38/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:25,037 DEBUG generators.py generate l.370] (38/93) Post-process Answer
[2024-03-04 10:49:25,042 INFO generators.py gen_for_qa l.548] (38/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:49:25,046 DEBUG generators.py generate l.349] (38/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:25,049 DEBUG generators.py generate l.358] (38/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:25,577 DEBUG generators.py generate l.370] (38/93) Post-process Answer
[2024-03-04 10:49:25,577 INFO generators.py gen_for_qa l.548] (38/93) * Start with LLM "command-nightly"
[2024-03-04 10:49:25,577 DEBUG generators.py generate l.349] (38/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:25,587 DEBUG generators.py generate l.358] (38/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:25,831 DEBUG generators.py generate l.370] (38/93) Post-process Answer
[2024-03-04 10:49:25,831 INFO generators.py gen_for_qa l.548] (38/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:49:25,831 DEBUG generators.py generate l.349] (38/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:25,835 DEBUG generators.py generate l.358] (38/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:26,228 ERROR generators.py complete l.400] (38/93) The following exception occurred with prompt meta={} user=" Qui a inventé le World Wide Web ?  A)  Tim Berners-Lee .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:49:26,238 DEBUG generators.py generate l.373] (38/93) Reuse post-processing
[2024-03-04 10:49:26,240 INFO generators.py generate l.477] (38/93) End question " Qui a inventé le World Wide Web ?  A)  Tim Berners-Lee "
[2024-03-04 10:49:26,243 INFO generators.py generate l.475] (39/93) *** AnsGenerator for question " Qui a découvert le vaccin contre la variole ?  A)  Edward Jenner B)  Benjamin Jesty "
[2024-03-04 10:49:26,243 INFO generators.py gen_for_qa l.548] (39/93) * Start with LLM "gpt-4"
[2024-03-04 10:49:26,243 DEBUG generators.py generate l.349] (39/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:26,246 DEBUG generators.py generate l.358] (39/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:26,890 DEBUG generators.py generate l.370] (39/93) Post-process Answer
[2024-03-04 10:49:26,890 INFO generators.py gen_for_qa l.548] (39/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:49:26,903 DEBUG generators.py generate l.349] (39/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:26,905 DEBUG generators.py generate l.358] (39/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:27,616 DEBUG generators.py generate l.370] (39/93) Post-process Answer
[2024-03-04 10:49:27,616 INFO generators.py gen_for_qa l.548] (39/93) * Start with LLM "gemini-pro"
[2024-03-04 10:49:27,616 DEBUG generators.py generate l.349] (39/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:27,616 DEBUG generators.py generate l.358] (39/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:29,806 DEBUG generators.py generate l.370] (39/93) Post-process Answer
[2024-03-04 10:49:29,817 INFO generators.py gen_for_qa l.548] (39/93) * Start with LLM "claude-2.1"
[2024-03-04 10:49:29,817 DEBUG generators.py generate l.349] (39/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:29,817 DEBUG generators.py generate l.358] (39/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:30,967 DEBUG generators.py generate l.370] (39/93) Post-process Answer
[2024-03-04 10:49:30,967 INFO generators.py gen_for_qa l.548] (39/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:49:30,967 DEBUG generators.py generate l.349] (39/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:30,967 DEBUG generators.py generate l.358] (39/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:31,318 DEBUG generators.py generate l.370] (39/93) Post-process Answer
[2024-03-04 10:49:31,318 INFO generators.py gen_for_qa l.548] (39/93) * Start with LLM "command-nightly"
[2024-03-04 10:49:31,318 DEBUG generators.py generate l.349] (39/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:31,330 DEBUG generators.py generate l.358] (39/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:31,660 DEBUG generators.py generate l.370] (39/93) Post-process Answer
[2024-03-04 10:49:31,660 INFO generators.py gen_for_qa l.548] (39/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:49:31,660 DEBUG generators.py generate l.349] (39/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:31,660 DEBUG generators.py generate l.358] (39/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:32,135 ERROR generators.py complete l.400] (39/93) The following exception occurred with prompt meta={} user=" Qui a découvert le vaccin contre la variole ?  A)  Edward Jenner B)  Benjamin Jesty .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:49:32,149 DEBUG generators.py generate l.373] (39/93) Reuse post-processing
[2024-03-04 10:49:32,149 INFO generators.py generate l.477] (39/93) End question " Qui a découvert le vaccin contre la variole ?  A)  Edward Jenner B)  Benjamin Jesty "
[2024-03-04 10:49:32,149 INFO generators.py generate l.475] (40/93) *** AnsGenerator for question " Qui a inventé le sous-marin ?  A)  Cornelis Drebbel B)  David Bushnell C)  Robert Fulton "
[2024-03-04 10:49:32,149 INFO generators.py gen_for_qa l.548] (40/93) * Start with LLM "gpt-4"
[2024-03-04 10:49:32,167 DEBUG generators.py generate l.349] (40/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:32,171 DEBUG generators.py generate l.358] (40/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:33,380 DEBUG generators.py generate l.370] (40/93) Post-process Answer
[2024-03-04 10:49:33,395 INFO generators.py gen_for_qa l.548] (40/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:49:33,395 DEBUG generators.py generate l.349] (40/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:33,402 DEBUG generators.py generate l.358] (40/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:33,982 DEBUG generators.py generate l.370] (40/93) Post-process Answer
[2024-03-04 10:49:33,982 INFO generators.py gen_for_qa l.548] (40/93) * Start with LLM "gemini-pro"
[2024-03-04 10:49:33,982 DEBUG generators.py generate l.349] (40/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:33,992 DEBUG generators.py generate l.358] (40/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:36,065 DEBUG generators.py generate l.370] (40/93) Post-process Answer
[2024-03-04 10:49:36,081 INFO generators.py gen_for_qa l.548] (40/93) * Start with LLM "claude-2.1"
[2024-03-04 10:49:36,081 DEBUG generators.py generate l.349] (40/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:36,081 DEBUG generators.py generate l.358] (40/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:37,300 DEBUG generators.py generate l.370] (40/93) Post-process Answer
[2024-03-04 10:49:37,308 INFO generators.py gen_for_qa l.548] (40/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:49:37,311 DEBUG generators.py generate l.349] (40/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:37,311 DEBUG generators.py generate l.358] (40/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:37,781 DEBUG generators.py generate l.370] (40/93) Post-process Answer
[2024-03-04 10:49:37,781 INFO generators.py gen_for_qa l.548] (40/93) * Start with LLM "command-nightly"
[2024-03-04 10:49:37,781 DEBUG generators.py generate l.349] (40/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:37,781 DEBUG generators.py generate l.358] (40/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:38,066 DEBUG generators.py generate l.370] (40/93) Post-process Answer
[2024-03-04 10:49:38,066 INFO generators.py gen_for_qa l.548] (40/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:49:38,082 DEBUG generators.py generate l.349] (40/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:38,082 DEBUG generators.py generate l.358] (40/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:38,507 ERROR generators.py complete l.400] (40/93) The following exception occurred with prompt meta={} user=" Qui a inventé le sous-marin ?  A)  Cornelis Drebbel B)  David Bushnell C)  Robert Fulton .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:49:38,527 DEBUG generators.py generate l.373] (40/93) Reuse post-processing
[2024-03-04 10:49:38,527 INFO generators.py generate l.477] (40/93) End question " Qui a inventé le sous-marin ?  A)  Cornelis Drebbel B)  David Bushnell C)  Robert Fulton "
[2024-03-04 10:49:38,537 INFO generators.py generate l.475] (41/93) *** AnsGenerator for question " Qui a découvert la loi de la gravitation universelle ?  A)  Isaac Newton B)  Robert Hooke "
[2024-03-04 10:49:38,537 INFO generators.py gen_for_qa l.548] (41/93) * Start with LLM "gpt-4"
[2024-03-04 10:49:38,546 DEBUG generators.py generate l.349] (41/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:38,546 DEBUG generators.py generate l.358] (41/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:39,967 DEBUG generators.py generate l.370] (41/93) Post-process Answer
[2024-03-04 10:49:39,967 INFO generators.py gen_for_qa l.548] (41/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:49:39,983 DEBUG generators.py generate l.349] (41/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:39,983 DEBUG generators.py generate l.358] (41/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:40,672 DEBUG generators.py generate l.370] (41/93) Post-process Answer
[2024-03-04 10:49:40,673 INFO generators.py gen_for_qa l.548] (41/93) * Start with LLM "gemini-pro"
[2024-03-04 10:49:40,674 DEBUG generators.py generate l.349] (41/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:40,674 DEBUG generators.py generate l.358] (41/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:42,816 DEBUG generators.py generate l.370] (41/93) Post-process Answer
[2024-03-04 10:49:42,819 INFO generators.py gen_for_qa l.548] (41/93) * Start with LLM "claude-2.1"
[2024-03-04 10:49:42,821 DEBUG generators.py generate l.349] (41/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:42,823 DEBUG generators.py generate l.358] (41/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:43,893 DEBUG generators.py generate l.370] (41/93) Post-process Answer
[2024-03-04 10:49:43,893 INFO generators.py gen_for_qa l.548] (41/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:49:43,897 DEBUG generators.py generate l.349] (41/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:43,899 DEBUG generators.py generate l.358] (41/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:44,254 DEBUG generators.py generate l.370] (41/93) Post-process Answer
[2024-03-04 10:49:44,258 INFO generators.py gen_for_qa l.548] (41/93) * Start with LLM "command-nightly"
[2024-03-04 10:49:44,259 DEBUG generators.py generate l.349] (41/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:44,261 DEBUG generators.py generate l.358] (41/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:44,510 DEBUG generators.py generate l.370] (41/93) Post-process Answer
[2024-03-04 10:49:44,519 INFO generators.py gen_for_qa l.548] (41/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:49:44,529 DEBUG generators.py generate l.349] (41/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:44,535 DEBUG generators.py generate l.358] (41/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:45,091 ERROR generators.py complete l.400] (41/93) The following exception occurred with prompt meta={} user=" Qui a découvert la loi de la gravitation universelle ?  A)  Isaac Newton B)  Robert Hooke .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:49:45,105 DEBUG generators.py generate l.373] (41/93) Reuse post-processing
[2024-03-04 10:49:45,105 INFO generators.py generate l.477] (41/93) End question " Qui a découvert la loi de la gravitation universelle ?  A)  Isaac Newton B)  Robert Hooke "
[2024-03-04 10:49:45,105 INFO generators.py generate l.475] (42/93) *** AnsGenerator for question " Qui a inventé le microscope électronique ?  A)  Ernst Ruska et Max Knoll "
[2024-03-04 10:49:45,108 INFO generators.py gen_for_qa l.548] (42/93) * Start with LLM "gpt-4"
[2024-03-04 10:49:45,110 DEBUG generators.py generate l.349] (42/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:45,110 DEBUG generators.py generate l.358] (42/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:45,703 DEBUG generators.py generate l.370] (42/93) Post-process Answer
[2024-03-04 10:49:45,703 INFO generators.py gen_for_qa l.548] (42/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:49:45,703 DEBUG generators.py generate l.349] (42/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:45,703 DEBUG generators.py generate l.358] (42/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:46,850 DEBUG generators.py generate l.370] (42/93) Post-process Answer
[2024-03-04 10:49:46,866 INFO generators.py gen_for_qa l.548] (42/93) * Start with LLM "gemini-pro"
[2024-03-04 10:49:46,866 DEBUG generators.py generate l.349] (42/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:46,873 DEBUG generators.py generate l.358] (42/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:49,038 DEBUG generators.py generate l.370] (42/93) Post-process Answer
[2024-03-04 10:49:49,055 INFO generators.py gen_for_qa l.548] (42/93) * Start with LLM "claude-2.1"
[2024-03-04 10:49:49,058 DEBUG generators.py generate l.349] (42/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:49,058 DEBUG generators.py generate l.358] (42/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:51,049 DEBUG generators.py generate l.370] (42/93) Post-process Answer
[2024-03-04 10:49:51,055 INFO generators.py gen_for_qa l.548] (42/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:49:51,056 DEBUG generators.py generate l.349] (42/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:51,057 DEBUG generators.py generate l.358] (42/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:51,448 DEBUG generators.py generate l.370] (42/93) Post-process Answer
[2024-03-04 10:49:51,448 INFO generators.py gen_for_qa l.548] (42/93) * Start with LLM "command-nightly"
[2024-03-04 10:49:51,452 DEBUG generators.py generate l.349] (42/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:51,453 DEBUG generators.py generate l.358] (42/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:51,694 DEBUG generators.py generate l.370] (42/93) Post-process Answer
[2024-03-04 10:49:51,699 INFO generators.py gen_for_qa l.548] (42/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:49:51,710 DEBUG generators.py generate l.349] (42/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:51,712 DEBUG generators.py generate l.358] (42/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:52,091 ERROR generators.py complete l.400] (42/93) The following exception occurred with prompt meta={} user=" Qui a inventé le microscope électronique ?  A)  Ernst Ruska et Max Knoll .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:49:52,104 DEBUG generators.py generate l.373] (42/93) Reuse post-processing
[2024-03-04 10:49:52,105 INFO generators.py generate l.477] (42/93) End question " Qui a inventé le microscope électronique ?  A)  Ernst Ruska et Max Knoll "
[2024-03-04 10:49:52,106 INFO generators.py generate l.475] (43/93) *** AnsGenerator for question " Qui a découvert la structure de la molécule de benzène ?  A)  August Kekulé B)  Archibald Scott Couper "
[2024-03-04 10:49:52,108 INFO generators.py gen_for_qa l.548] (43/93) * Start with LLM "gpt-4"
[2024-03-04 10:49:52,109 DEBUG generators.py generate l.349] (43/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:52,111 DEBUG generators.py generate l.358] (43/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:53,543 DEBUG generators.py generate l.370] (43/93) Post-process Answer
[2024-03-04 10:49:53,547 INFO generators.py gen_for_qa l.548] (43/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:49:53,550 DEBUG generators.py generate l.349] (43/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:53,551 DEBUG generators.py generate l.358] (43/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:54,359 DEBUG generators.py generate l.370] (43/93) Post-process Answer
[2024-03-04 10:49:54,363 INFO generators.py gen_for_qa l.548] (43/93) * Start with LLM "gemini-pro"
[2024-03-04 10:49:54,367 DEBUG generators.py generate l.349] (43/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:54,370 DEBUG generators.py generate l.358] (43/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:56,574 DEBUG generators.py generate l.370] (43/93) Post-process Answer
[2024-03-04 10:49:56,575 INFO generators.py gen_for_qa l.548] (43/93) * Start with LLM "claude-2.1"
[2024-03-04 10:49:56,582 DEBUG generators.py generate l.349] (43/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:56,584 DEBUG generators.py generate l.358] (43/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:57,462 DEBUG generators.py generate l.370] (43/93) Post-process Answer
[2024-03-04 10:49:57,462 INFO generators.py gen_for_qa l.548] (43/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:49:57,471 DEBUG generators.py generate l.349] (43/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:57,472 DEBUG generators.py generate l.358] (43/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:57,856 DEBUG generators.py generate l.370] (43/93) Post-process Answer
[2024-03-04 10:49:57,859 INFO generators.py gen_for_qa l.548] (43/93) * Start with LLM "command-nightly"
[2024-03-04 10:49:57,862 DEBUG generators.py generate l.349] (43/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:57,864 DEBUG generators.py generate l.358] (43/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:58,124 DEBUG generators.py generate l.370] (43/93) Post-process Answer
[2024-03-04 10:49:58,128 INFO generators.py gen_for_qa l.548] (43/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:49:58,132 DEBUG generators.py generate l.349] (43/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:58,135 DEBUG generators.py generate l.358] (43/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:58,637 ERROR generators.py complete l.400] (43/93) The following exception occurred with prompt meta={} user=" Qui a découvert la structure de la molécule de benzène ?  A)  August Kekulé B)  Archibald Scott Couper .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:49:58,646 DEBUG generators.py generate l.373] (43/93) Reuse post-processing
[2024-03-04 10:49:58,648 INFO generators.py generate l.477] (43/93) End question " Qui a découvert la structure de la molécule de benzène ?  A)  August Kekulé B)  Archibald Scott Couper "
[2024-03-04 10:49:58,648 INFO generators.py generate l.475] (44/93) *** AnsGenerator for question " Qui a inventé la dynamite ?  A)  Alfred Nobel B)  Ascanio Sobrero "
[2024-03-04 10:49:58,650 INFO generators.py gen_for_qa l.548] (44/93) * Start with LLM "gpt-4"
[2024-03-04 10:49:58,652 DEBUG generators.py generate l.349] (44/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:58,652 DEBUG generators.py generate l.358] (44/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:59,340 DEBUG generators.py generate l.370] (44/93) Post-process Answer
[2024-03-04 10:49:59,343 INFO generators.py gen_for_qa l.548] (44/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:49:59,346 DEBUG generators.py generate l.349] (44/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:59,347 DEBUG generators.py generate l.358] (44/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:49:59,920 DEBUG generators.py generate l.370] (44/93) Post-process Answer
[2024-03-04 10:49:59,931 INFO generators.py gen_for_qa l.548] (44/93) * Start with LLM "gemini-pro"
[2024-03-04 10:49:59,933 DEBUG generators.py generate l.349] (44/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:49:59,934 DEBUG generators.py generate l.358] (44/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:02,194 DEBUG generators.py generate l.370] (44/93) Post-process Answer
[2024-03-04 10:50:02,197 INFO generators.py gen_for_qa l.548] (44/93) * Start with LLM "claude-2.1"
[2024-03-04 10:50:02,200 DEBUG generators.py generate l.349] (44/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:02,203 DEBUG generators.py generate l.358] (44/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:03,035 DEBUG generators.py generate l.370] (44/93) Post-process Answer
[2024-03-04 10:50:03,039 INFO generators.py gen_for_qa l.548] (44/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:50:03,041 DEBUG generators.py generate l.349] (44/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:03,043 DEBUG generators.py generate l.358] (44/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:03,431 DEBUG generators.py generate l.370] (44/93) Post-process Answer
[2024-03-04 10:50:03,440 INFO generators.py gen_for_qa l.548] (44/93) * Start with LLM "command-nightly"
[2024-03-04 10:50:03,442 DEBUG generators.py generate l.349] (44/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:03,445 DEBUG generators.py generate l.358] (44/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:03,695 DEBUG generators.py generate l.370] (44/93) Post-process Answer
[2024-03-04 10:50:03,695 INFO generators.py gen_for_qa l.548] (44/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:50:03,704 DEBUG generators.py generate l.349] (44/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:03,706 DEBUG generators.py generate l.358] (44/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:04,250 ERROR generators.py complete l.400] (44/93) The following exception occurred with prompt meta={} user=" Qui a inventé la dynamite ?  A)  Alfred Nobel B)  Ascanio Sobrero .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:50:04,260 DEBUG generators.py generate l.373] (44/93) Reuse post-processing
[2024-03-04 10:50:04,263 INFO generators.py generate l.477] (44/93) End question " Qui a inventé la dynamite ?  A)  Alfred Nobel B)  Ascanio Sobrero "
[2024-03-04 10:50:04,263 INFO generators.py generate l.475] (45/93) *** AnsGenerator for question " Qui a découvert les rayons X ?  A)  Wilhelm Röntgen B)  Thomas Edison C)  Nikola Tesla "
[2024-03-04 10:50:04,268 INFO generators.py gen_for_qa l.548] (45/93) * Start with LLM "gpt-4"
[2024-03-04 10:50:04,269 DEBUG generators.py generate l.349] (45/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:04,270 DEBUG generators.py generate l.358] (45/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:05,073 DEBUG generators.py generate l.370] (45/93) Post-process Answer
[2024-03-04 10:50:05,075 INFO generators.py gen_for_qa l.548] (45/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:50:05,076 DEBUG generators.py generate l.349] (45/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:05,077 DEBUG generators.py generate l.358] (45/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:05,757 DEBUG generators.py generate l.370] (45/93) Post-process Answer
[2024-03-04 10:50:05,759 INFO generators.py gen_for_qa l.548] (45/93) * Start with LLM "gemini-pro"
[2024-03-04 10:50:05,761 DEBUG generators.py generate l.349] (45/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:05,762 DEBUG generators.py generate l.358] (45/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:07,870 DEBUG generators.py generate l.370] (45/93) Post-process Answer
[2024-03-04 10:50:07,871 INFO generators.py gen_for_qa l.548] (45/93) * Start with LLM "claude-2.1"
[2024-03-04 10:50:07,872 DEBUG generators.py generate l.349] (45/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:07,874 DEBUG generators.py generate l.358] (45/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:08,873 DEBUG generators.py generate l.370] (45/93) Post-process Answer
[2024-03-04 10:50:08,876 INFO generators.py gen_for_qa l.548] (45/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:50:08,876 DEBUG generators.py generate l.349] (45/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:08,884 DEBUG generators.py generate l.358] (45/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:09,212 DEBUG generators.py generate l.370] (45/93) Post-process Answer
[2024-03-04 10:50:09,212 INFO generators.py gen_for_qa l.548] (45/93) * Start with LLM "command-nightly"
[2024-03-04 10:50:09,218 DEBUG generators.py generate l.349] (45/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:09,220 DEBUG generators.py generate l.358] (45/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:09,450 DEBUG generators.py generate l.370] (45/93) Post-process Answer
[2024-03-04 10:50:09,452 INFO generators.py gen_for_qa l.548] (45/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:50:09,452 DEBUG generators.py generate l.349] (45/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:09,453 DEBUG generators.py generate l.358] (45/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:09,843 ERROR generators.py complete l.400] (45/93) The following exception occurred with prompt meta={} user=" Qui a découvert les rayons X ?  A)  Wilhelm Röntgen B)  Thomas Edison C)  Nikola Tesla .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:50:09,872 DEBUG generators.py generate l.373] (45/93) Reuse post-processing
[2024-03-04 10:50:09,875 INFO generators.py generate l.477] (45/93) End question " Qui a découvert les rayons X ?  A)  Wilhelm Röntgen B)  Thomas Edison C)  Nikola Tesla "
[2024-03-04 10:50:09,878 INFO generators.py generate l.475] (46/93) *** AnsGenerator for question " Qui a inventé le gramophone ?  A)  Emile Berliner B)  Thomas Edison C)  Charles Cros "
[2024-03-04 10:50:09,882 INFO generators.py gen_for_qa l.548] (46/93) * Start with LLM "gpt-4"
[2024-03-04 10:50:09,885 DEBUG generators.py generate l.349] (46/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:09,889 DEBUG generators.py generate l.358] (46/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:10,892 DEBUG generators.py generate l.370] (46/93) Post-process Answer
[2024-03-04 10:50:10,892 INFO generators.py gen_for_qa l.548] (46/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:50:10,896 DEBUG generators.py generate l.349] (46/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:10,898 DEBUG generators.py generate l.358] (46/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:11,633 DEBUG generators.py generate l.370] (46/93) Post-process Answer
[2024-03-04 10:50:11,634 INFO generators.py gen_for_qa l.548] (46/93) * Start with LLM "gemini-pro"
[2024-03-04 10:50:11,634 DEBUG generators.py generate l.349] (46/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:11,639 DEBUG generators.py generate l.358] (46/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:13,961 DEBUG generators.py generate l.370] (46/93) Post-process Answer
[2024-03-04 10:50:13,963 INFO generators.py gen_for_qa l.548] (46/93) * Start with LLM "claude-2.1"
[2024-03-04 10:50:13,963 DEBUG generators.py generate l.349] (46/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:13,965 DEBUG generators.py generate l.358] (46/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:16,572 DEBUG generators.py generate l.370] (46/93) Post-process Answer
[2024-03-04 10:50:16,580 INFO generators.py gen_for_qa l.548] (46/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:50:16,582 DEBUG generators.py generate l.349] (46/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:16,583 DEBUG generators.py generate l.358] (46/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:17,020 DEBUG generators.py generate l.370] (46/93) Post-process Answer
[2024-03-04 10:50:17,022 INFO generators.py gen_for_qa l.548] (46/93) * Start with LLM "command-nightly"
[2024-03-04 10:50:17,024 DEBUG generators.py generate l.349] (46/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:17,025 DEBUG generators.py generate l.358] (46/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:17,284 DEBUG generators.py generate l.370] (46/93) Post-process Answer
[2024-03-04 10:50:17,288 INFO generators.py gen_for_qa l.548] (46/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:50:17,292 DEBUG generators.py generate l.349] (46/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:17,295 DEBUG generators.py generate l.358] (46/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:17,694 ERROR generators.py complete l.400] (46/93) The following exception occurred with prompt meta={} user=" Qui a inventé le gramophone ?  A)  Emile Berliner B)  Thomas Edison C)  Charles Cros .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:50:17,706 DEBUG generators.py generate l.373] (46/93) Reuse post-processing
[2024-03-04 10:50:17,708 INFO generators.py generate l.477] (46/93) End question " Qui a inventé le gramophone ?  A)  Emile Berliner B)  Thomas Edison C)  Charles Cros "
[2024-03-04 10:50:17,710 INFO generators.py generate l.475] (47/93) *** AnsGenerator for question " Qui a découvert les rayons cosmiques ?  A)  Victor Hess B)  Robert Millikan "
[2024-03-04 10:50:17,712 INFO generators.py gen_for_qa l.548] (47/93) * Start with LLM "gpt-4"
[2024-03-04 10:50:17,714 DEBUG generators.py generate l.349] (47/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:17,716 DEBUG generators.py generate l.358] (47/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:18,503 DEBUG generators.py generate l.370] (47/93) Post-process Answer
[2024-03-04 10:50:18,504 INFO generators.py gen_for_qa l.548] (47/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:50:18,505 DEBUG generators.py generate l.349] (47/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:18,506 DEBUG generators.py generate l.358] (47/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:19,162 DEBUG generators.py generate l.370] (47/93) Post-process Answer
[2024-03-04 10:50:19,162 INFO generators.py gen_for_qa l.548] (47/93) * Start with LLM "gemini-pro"
[2024-03-04 10:50:19,173 DEBUG generators.py generate l.349] (47/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:19,174 DEBUG generators.py generate l.358] (47/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:21,420 DEBUG generators.py generate l.370] (47/93) Post-process Answer
[2024-03-04 10:50:21,424 INFO generators.py gen_for_qa l.548] (47/93) * Start with LLM "claude-2.1"
[2024-03-04 10:50:21,427 DEBUG generators.py generate l.349] (47/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:21,429 DEBUG generators.py generate l.358] (47/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:22,762 DEBUG generators.py generate l.370] (47/93) Post-process Answer
[2024-03-04 10:50:22,763 INFO generators.py gen_for_qa l.548] (47/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:50:22,766 DEBUG generators.py generate l.349] (47/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:22,767 DEBUG generators.py generate l.358] (47/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:23,182 DEBUG generators.py generate l.370] (47/93) Post-process Answer
[2024-03-04 10:50:23,185 INFO generators.py gen_for_qa l.548] (47/93) * Start with LLM "command-nightly"
[2024-03-04 10:50:23,188 DEBUG generators.py generate l.349] (47/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:23,190 DEBUG generators.py generate l.358] (47/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:23,440 DEBUG generators.py generate l.370] (47/93) Post-process Answer
[2024-03-04 10:50:23,452 INFO generators.py gen_for_qa l.548] (47/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:50:23,456 DEBUG generators.py generate l.349] (47/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:23,458 DEBUG generators.py generate l.358] (47/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:24,413 ERROR generators.py complete l.400] (47/93) The following exception occurred with prompt meta={} user=" Qui a découvert les rayons cosmiques ?  A)  Victor Hess B)  Robert Millikan .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:50:24,425 DEBUG generators.py generate l.373] (47/93) Reuse post-processing
[2024-03-04 10:50:24,425 INFO generators.py generate l.477] (47/93) End question " Qui a découvert les rayons cosmiques ?  A)  Victor Hess B)  Robert Millikan "
[2024-03-04 10:50:24,430 INFO generators.py generate l.475] (48/93) *** AnsGenerator for question " Qui a inventé le premier satellite artificiel ?  A)  Sergueï Korolev B)  Wernher von Braun "
[2024-03-04 10:50:24,434 INFO generators.py gen_for_qa l.548] (48/93) * Start with LLM "gpt-4"
[2024-03-04 10:50:24,435 DEBUG generators.py generate l.349] (48/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:24,437 DEBUG generators.py generate l.358] (48/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:25,049 DEBUG generators.py generate l.370] (48/93) Post-process Answer
[2024-03-04 10:50:25,065 INFO generators.py gen_for_qa l.548] (48/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:50:25,067 DEBUG generators.py generate l.349] (48/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:25,068 DEBUG generators.py generate l.358] (48/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:25,766 DEBUG generators.py generate l.370] (48/93) Post-process Answer
[2024-03-04 10:50:25,768 INFO generators.py gen_for_qa l.548] (48/93) * Start with LLM "gemini-pro"
[2024-03-04 10:50:25,770 DEBUG generators.py generate l.349] (48/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:25,771 DEBUG generators.py generate l.358] (48/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:27,899 DEBUG generators.py generate l.370] (48/93) Post-process Answer
[2024-03-04 10:50:27,904 INFO generators.py gen_for_qa l.548] (48/93) * Start with LLM "claude-2.1"
[2024-03-04 10:50:27,908 DEBUG generators.py generate l.349] (48/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:27,911 DEBUG generators.py generate l.358] (48/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:29,065 DEBUG generators.py generate l.370] (48/93) Post-process Answer
[2024-03-04 10:50:29,069 INFO generators.py gen_for_qa l.548] (48/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:50:29,074 DEBUG generators.py generate l.349] (48/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:29,076 DEBUG generators.py generate l.358] (48/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:29,559 DEBUG generators.py generate l.370] (48/93) Post-process Answer
[2024-03-04 10:50:29,560 INFO generators.py gen_for_qa l.548] (48/93) * Start with LLM "command-nightly"
[2024-03-04 10:50:29,564 DEBUG generators.py generate l.349] (48/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:29,565 DEBUG generators.py generate l.358] (48/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:29,802 DEBUG generators.py generate l.370] (48/93) Post-process Answer
[2024-03-04 10:50:29,805 INFO generators.py gen_for_qa l.548] (48/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:50:29,805 DEBUG generators.py generate l.349] (48/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:29,805 DEBUG generators.py generate l.358] (48/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:30,217 ERROR generators.py complete l.400] (48/93) The following exception occurred with prompt meta={} user=" Qui a inventé le premier satellite artificiel ?  A)  Sergueï Korolev B)  Wernher von Braun .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:50:30,238 DEBUG generators.py generate l.373] (48/93) Reuse post-processing
[2024-03-04 10:50:30,238 INFO generators.py generate l.477] (48/93) End question " Qui a inventé le premier satellite artificiel ?  A)  Sergueï Korolev B)  Wernher von Braun "
[2024-03-04 10:50:30,238 INFO generators.py generate l.475] (49/93) *** AnsGenerator for question " Qui a découvert la loi de la conservation de l'énergie ?  A)  James Joule B)  Julius Robert Mayer C)  Hermann von Helmholtz "
[2024-03-04 10:50:30,244 INFO generators.py gen_for_qa l.548] (49/93) * Start with LLM "gpt-4"
[2024-03-04 10:50:30,245 DEBUG generators.py generate l.349] (49/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:30,246 DEBUG generators.py generate l.358] (49/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:30,851 DEBUG generators.py generate l.370] (49/93) Post-process Answer
[2024-03-04 10:50:30,852 INFO generators.py gen_for_qa l.548] (49/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:50:30,852 DEBUG generators.py generate l.349] (49/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:30,855 DEBUG generators.py generate l.358] (49/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:31,264 DEBUG generators.py generate l.370] (49/93) Post-process Answer
[2024-03-04 10:50:31,266 INFO generators.py gen_for_qa l.548] (49/93) * Start with LLM "gemini-pro"
[2024-03-04 10:50:31,267 DEBUG generators.py generate l.349] (49/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:31,268 DEBUG generators.py generate l.358] (49/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:33,977 DEBUG generators.py generate l.370] (49/93) Post-process Answer
[2024-03-04 10:50:33,983 INFO generators.py gen_for_qa l.548] (49/93) * Start with LLM "claude-2.1"
[2024-03-04 10:50:33,987 DEBUG generators.py generate l.349] (49/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:33,989 DEBUG generators.py generate l.358] (49/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:35,675 DEBUG generators.py generate l.370] (49/93) Post-process Answer
[2024-03-04 10:50:35,680 INFO generators.py gen_for_qa l.548] (49/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:50:35,685 DEBUG generators.py generate l.349] (49/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:35,689 DEBUG generators.py generate l.358] (49/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:36,185 DEBUG generators.py generate l.370] (49/93) Post-process Answer
[2024-03-04 10:50:36,185 INFO generators.py gen_for_qa l.548] (49/93) * Start with LLM "command-nightly"
[2024-03-04 10:50:36,185 DEBUG generators.py generate l.349] (49/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:36,196 DEBUG generators.py generate l.358] (49/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:36,563 DEBUG generators.py generate l.370] (49/93) Post-process Answer
[2024-03-04 10:50:36,563 INFO generators.py gen_for_qa l.548] (49/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:50:36,578 DEBUG generators.py generate l.349] (49/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:36,580 DEBUG generators.py generate l.358] (49/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:37,131 ERROR generators.py complete l.400] (49/93) The following exception occurred with prompt meta={} user=" Qui a découvert la loi de la conservation de l'énergie ?  A)  James Joule B)  Julius Robert Mayer C)  Hermann von Helmholtz .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:50:37,146 DEBUG generators.py generate l.373] (49/93) Reuse post-processing
[2024-03-04 10:50:37,146 INFO generators.py generate l.477] (49/93) End question " Qui a découvert la loi de la conservation de l'énergie ?  A)  James Joule B)  Julius Robert Mayer C)  Hermann von Helmholtz "
[2024-03-04 10:50:37,151 INFO generators.py generate l.475] (50/93) *** AnsGenerator for question " Qui a inventé la machine à calculer ?  A)  Blaise Pascal B)  Gottfried Leibniz C)  Charles Babbage "
[2024-03-04 10:50:37,153 INFO generators.py gen_for_qa l.548] (50/93) * Start with LLM "gpt-4"
[2024-03-04 10:50:37,154 DEBUG generators.py generate l.349] (50/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:37,154 DEBUG generators.py generate l.358] (50/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:38,105 DEBUG generators.py generate l.370] (50/93) Post-process Answer
[2024-03-04 10:50:38,105 INFO generators.py gen_for_qa l.548] (50/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:50:38,105 DEBUG generators.py generate l.349] (50/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:38,105 DEBUG generators.py generate l.358] (50/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:39,339 DEBUG generators.py generate l.370] (50/93) Post-process Answer
[2024-03-04 10:50:39,355 INFO generators.py gen_for_qa l.548] (50/93) * Start with LLM "gemini-pro"
[2024-03-04 10:50:39,355 DEBUG generators.py generate l.349] (50/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:39,362 DEBUG generators.py generate l.358] (50/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:41,764 DEBUG generators.py generate l.370] (50/93) Post-process Answer
[2024-03-04 10:50:41,764 INFO generators.py gen_for_qa l.548] (50/93) * Start with LLM "claude-2.1"
[2024-03-04 10:50:41,764 DEBUG generators.py generate l.349] (50/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:41,779 DEBUG generators.py generate l.358] (50/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:43,033 DEBUG generators.py generate l.370] (50/93) Post-process Answer
[2024-03-04 10:50:43,033 INFO generators.py gen_for_qa l.548] (50/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:50:43,033 DEBUG generators.py generate l.349] (50/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:43,033 DEBUG generators.py generate l.358] (50/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:43,563 DEBUG generators.py generate l.370] (50/93) Post-process Answer
[2024-03-04 10:50:43,565 INFO generators.py gen_for_qa l.548] (50/93) * Start with LLM "command-nightly"
[2024-03-04 10:50:43,566 DEBUG generators.py generate l.349] (50/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:43,567 DEBUG generators.py generate l.358] (50/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:44,030 DEBUG generators.py generate l.370] (50/93) Post-process Answer
[2024-03-04 10:50:44,046 INFO generators.py gen_for_qa l.548] (50/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:50:44,046 DEBUG generators.py generate l.349] (50/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:44,046 DEBUG generators.py generate l.358] (50/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:44,740 ERROR generators.py complete l.400] (50/93) The following exception occurred with prompt meta={} user=" Qui a inventé la machine à calculer ?  A)  Blaise Pascal B)  Gottfried Leibniz C)  Charles Babbage .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:50:44,755 DEBUG generators.py generate l.373] (50/93) Reuse post-processing
[2024-03-04 10:50:44,755 INFO generators.py generate l.477] (50/93) End question " Qui a inventé la machine à calculer ?  A)  Blaise Pascal B)  Gottfried Leibniz C)  Charles Babbage "
[2024-03-04 10:50:44,755 INFO generators.py generate l.475] (51/93) *** AnsGenerator for question " Qui a découvert la loi de la conservation de la masse ?  A)  Antoine Lavoisier B)  Mikhail Lomonosov "
[2024-03-04 10:50:44,755 INFO generators.py gen_for_qa l.548] (51/93) * Start with LLM "gpt-4"
[2024-03-04 10:50:44,771 DEBUG generators.py generate l.349] (51/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:44,771 DEBUG generators.py generate l.358] (51/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:45,953 DEBUG generators.py generate l.370] (51/93) Post-process Answer
[2024-03-04 10:50:45,953 INFO generators.py gen_for_qa l.548] (51/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:50:45,968 DEBUG generators.py generate l.349] (51/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:45,968 DEBUG generators.py generate l.358] (51/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:47,422 DEBUG generators.py generate l.370] (51/93) Post-process Answer
[2024-03-04 10:50:47,438 INFO generators.py gen_for_qa l.548] (51/93) * Start with LLM "gemini-pro"
[2024-03-04 10:50:47,438 DEBUG generators.py generate l.349] (51/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:47,438 DEBUG generators.py generate l.358] (51/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:49,979 DEBUG generators.py generate l.370] (51/93) Post-process Answer
[2024-03-04 10:50:49,984 INFO generators.py gen_for_qa l.548] (51/93) * Start with LLM "claude-2.1"
[2024-03-04 10:50:49,986 DEBUG generators.py generate l.349] (51/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:49,986 DEBUG generators.py generate l.358] (51/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:51,548 DEBUG generators.py generate l.370] (51/93) Post-process Answer
[2024-03-04 10:50:51,549 INFO generators.py gen_for_qa l.548] (51/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:50:51,550 DEBUG generators.py generate l.349] (51/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:51,553 DEBUG generators.py generate l.358] (51/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:52,029 DEBUG generators.py generate l.370] (51/93) Post-process Answer
[2024-03-04 10:50:52,044 INFO generators.py gen_for_qa l.548] (51/93) * Start with LLM "command-nightly"
[2024-03-04 10:50:52,044 DEBUG generators.py generate l.349] (51/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:52,044 DEBUG generators.py generate l.358] (51/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:52,461 DEBUG generators.py generate l.370] (51/93) Post-process Answer
[2024-03-04 10:50:52,463 INFO generators.py gen_for_qa l.548] (51/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:50:52,464 DEBUG generators.py generate l.349] (51/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:52,465 DEBUG generators.py generate l.358] (51/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:53,270 ERROR generators.py complete l.400] (51/93) The following exception occurred with prompt meta={} user=" Qui a découvert la loi de la conservation de la masse ?  A)  Antoine Lavoisier B)  Mikhail Lomonosov .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:50:53,284 DEBUG generators.py generate l.373] (51/93) Reuse post-processing
[2024-03-04 10:50:53,287 INFO generators.py generate l.477] (51/93) End question " Qui a découvert la loi de la conservation de la masse ?  A)  Antoine Lavoisier B)  Mikhail Lomonosov "
[2024-03-04 10:50:53,291 INFO generators.py generate l.475] (52/93) *** AnsGenerator for question " Qui a inventé le premier ordinateur programmable ?  A)  Konrad Zuse B)  John Atanasoff et Clifford Berry "
[2024-03-04 10:50:53,295 INFO generators.py gen_for_qa l.548] (52/93) * Start with LLM "gpt-4"
[2024-03-04 10:50:53,299 DEBUG generators.py generate l.349] (52/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:53,304 DEBUG generators.py generate l.358] (52/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:54,391 DEBUG generators.py generate l.370] (52/93) Post-process Answer
[2024-03-04 10:50:54,393 INFO generators.py gen_for_qa l.548] (52/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:50:54,395 DEBUG generators.py generate l.349] (52/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:54,396 DEBUG generators.py generate l.358] (52/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:55,227 DEBUG generators.py generate l.370] (52/93) Post-process Answer
[2024-03-04 10:50:55,233 INFO generators.py gen_for_qa l.548] (52/93) * Start with LLM "gemini-pro"
[2024-03-04 10:50:55,237 DEBUG generators.py generate l.349] (52/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:55,241 DEBUG generators.py generate l.358] (52/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:57,536 DEBUG generators.py generate l.370] (52/93) Post-process Answer
[2024-03-04 10:50:57,540 INFO generators.py gen_for_qa l.548] (52/93) * Start with LLM "claude-2.1"
[2024-03-04 10:50:57,543 DEBUG generators.py generate l.349] (52/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:57,545 DEBUG generators.py generate l.358] (52/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:59,264 DEBUG generators.py generate l.370] (52/93) Post-process Answer
[2024-03-04 10:50:59,264 INFO generators.py gen_for_qa l.548] (52/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:50:59,264 DEBUG generators.py generate l.349] (52/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:59,264 DEBUG generators.py generate l.358] (52/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:50:59,735 DEBUG generators.py generate l.370] (52/93) Post-process Answer
[2024-03-04 10:50:59,735 INFO generators.py gen_for_qa l.548] (52/93) * Start with LLM "command-nightly"
[2024-03-04 10:50:59,752 DEBUG generators.py generate l.349] (52/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:50:59,752 DEBUG generators.py generate l.358] (52/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:00,065 DEBUG generators.py generate l.370] (52/93) Post-process Answer
[2024-03-04 10:51:00,069 INFO generators.py gen_for_qa l.548] (52/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:51:00,069 DEBUG generators.py generate l.349] (52/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:00,069 DEBUG generators.py generate l.358] (52/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:00,676 ERROR generators.py complete l.400] (52/93) The following exception occurred with prompt meta={} user=" Qui a inventé le premier ordinateur programmable ?  A)  Konrad Zuse B)  John Atanasoff et Clifford Berry .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:51:00,697 DEBUG generators.py generate l.373] (52/93) Reuse post-processing
[2024-03-04 10:51:00,697 INFO generators.py generate l.477] (52/93) End question " Qui a inventé le premier ordinateur programmable ?  A)  Konrad Zuse B)  John Atanasoff et Clifford Berry "
[2024-03-04 10:51:00,703 INFO generators.py generate l.475] (53/93) *** AnsGenerator for question " Qui a découvert la structure de l'ADN ?  A)  James Watson et Francis Crick B)  Rosalind Franklin C)  Maurice Wilkins "
[2024-03-04 10:51:00,706 INFO generators.py gen_for_qa l.548] (53/93) * Start with LLM "gpt-4"
[2024-03-04 10:51:00,708 DEBUG generators.py generate l.349] (53/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:00,710 DEBUG generators.py generate l.358] (53/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:01,533 DEBUG generators.py generate l.370] (53/93) Post-process Answer
[2024-03-04 10:51:01,533 INFO generators.py gen_for_qa l.548] (53/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:51:01,546 DEBUG generators.py generate l.349] (53/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:01,546 DEBUG generators.py generate l.358] (53/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:02,525 DEBUG generators.py generate l.370] (53/93) Post-process Answer
[2024-03-04 10:51:02,528 INFO generators.py gen_for_qa l.548] (53/93) * Start with LLM "gemini-pro"
[2024-03-04 10:51:02,531 DEBUG generators.py generate l.349] (53/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:02,532 DEBUG generators.py generate l.358] (53/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:05,102 DEBUG generators.py generate l.370] (53/93) Post-process Answer
[2024-03-04 10:51:05,104 INFO generators.py gen_for_qa l.548] (53/93) * Start with LLM "claude-2.1"
[2024-03-04 10:51:05,106 DEBUG generators.py generate l.349] (53/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:05,108 DEBUG generators.py generate l.358] (53/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:06,362 DEBUG generators.py generate l.370] (53/93) Post-process Answer
[2024-03-04 10:51:06,362 INFO generators.py gen_for_qa l.548] (53/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:51:06,362 DEBUG generators.py generate l.349] (53/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:06,377 DEBUG generators.py generate l.358] (53/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:06,836 DEBUG generators.py generate l.370] (53/93) Post-process Answer
[2024-03-04 10:51:06,838 INFO generators.py gen_for_qa l.548] (53/93) * Start with LLM "command-nightly"
[2024-03-04 10:51:06,840 DEBUG generators.py generate l.349] (53/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:06,840 DEBUG generators.py generate l.358] (53/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:07,068 DEBUG generators.py generate l.370] (53/93) Post-process Answer
[2024-03-04 10:51:07,068 INFO generators.py gen_for_qa l.548] (53/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:51:07,068 DEBUG generators.py generate l.349] (53/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:07,081 DEBUG generators.py generate l.358] (53/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:07,499 ERROR generators.py complete l.400] (53/93) The following exception occurred with prompt meta={} user=" Qui a découvert la structure de l'ADN ?  A)  James Watson et Francis Crick B)  Rosalind Franklin C)  Maurice Wilkins .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:51:07,499 DEBUG generators.py generate l.373] (53/93) Reuse post-processing
[2024-03-04 10:51:07,499 INFO generators.py generate l.477] (53/93) End question " Qui a découvert la structure de l'ADN ?  A)  James Watson et Francis Crick B)  Rosalind Franklin C)  Maurice Wilkins "
[2024-03-04 10:51:07,499 INFO generators.py generate l.475] (54/93) *** AnsGenerator for question " Qui a inventé le premier laser fonctionnel ?  A)  Theodore Maiman B)  Gordon Gould C)  Charles Hard Townes "
[2024-03-04 10:51:07,499 INFO generators.py gen_for_qa l.548] (54/93) * Start with LLM "gpt-4"
[2024-03-04 10:51:07,499 DEBUG generators.py generate l.349] (54/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:07,516 DEBUG generators.py generate l.358] (54/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:08,757 DEBUG generators.py generate l.370] (54/93) Post-process Answer
[2024-03-04 10:51:08,757 INFO generators.py gen_for_qa l.548] (54/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:51:08,757 DEBUG generators.py generate l.349] (54/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:08,757 DEBUG generators.py generate l.358] (54/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:09,429 DEBUG generators.py generate l.370] (54/93) Post-process Answer
[2024-03-04 10:51:09,430 INFO generators.py gen_for_qa l.548] (54/93) * Start with LLM "gemini-pro"
[2024-03-04 10:51:09,431 DEBUG generators.py generate l.349] (54/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:09,433 DEBUG generators.py generate l.358] (54/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:11,935 DEBUG generators.py generate l.370] (54/93) Post-process Answer
[2024-03-04 10:51:11,951 INFO generators.py gen_for_qa l.548] (54/93) * Start with LLM "claude-2.1"
[2024-03-04 10:51:11,953 DEBUG generators.py generate l.349] (54/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:11,953 DEBUG generators.py generate l.358] (54/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:13,269 DEBUG generators.py generate l.370] (54/93) Post-process Answer
[2024-03-04 10:51:13,269 INFO generators.py gen_for_qa l.548] (54/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:51:13,269 DEBUG generators.py generate l.349] (54/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:13,269 DEBUG generators.py generate l.358] (54/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:13,692 DEBUG generators.py generate l.370] (54/93) Post-process Answer
[2024-03-04 10:51:13,697 INFO generators.py gen_for_qa l.548] (54/93) * Start with LLM "command-nightly"
[2024-03-04 10:51:13,697 DEBUG generators.py generate l.349] (54/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:13,699 DEBUG generators.py generate l.358] (54/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:13,948 DEBUG generators.py generate l.370] (54/93) Post-process Answer
[2024-03-04 10:51:13,950 INFO generators.py gen_for_qa l.548] (54/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:51:13,951 DEBUG generators.py generate l.349] (54/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:13,952 DEBUG generators.py generate l.358] (54/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:14,437 ERROR generators.py complete l.400] (54/93) The following exception occurred with prompt meta={} user=" Qui a inventé le premier laser fonctionnel ?  A)  Theodore Maiman B)  Gordon Gould C)  Charles Hard Townes .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:51:14,437 DEBUG generators.py generate l.373] (54/93) Reuse post-processing
[2024-03-04 10:51:14,437 INFO generators.py generate l.477] (54/93) End question " Qui a inventé le premier laser fonctionnel ?  A)  Theodore Maiman B)  Gordon Gould C)  Charles Hard Townes "
[2024-03-04 10:51:14,437 INFO generators.py generate l.475] (55/93) *** AnsGenerator for question " Qui a découvert la loi de la conservation de la quantité de mouvement ?  A)  Isaac Newton B)  René Descartes C)  Christiaan Huygens "
[2024-03-04 10:51:14,437 INFO generators.py gen_for_qa l.548] (55/93) * Start with LLM "gpt-4"
[2024-03-04 10:51:14,437 DEBUG generators.py generate l.349] (55/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:14,450 DEBUG generators.py generate l.358] (55/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:15,081 DEBUG generators.py generate l.370] (55/93) Post-process Answer
[2024-03-04 10:51:15,081 INFO generators.py gen_for_qa l.548] (55/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:51:15,081 DEBUG generators.py generate l.349] (55/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:15,081 DEBUG generators.py generate l.358] (55/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:15,644 DEBUG generators.py generate l.370] (55/93) Post-process Answer
[2024-03-04 10:51:15,648 INFO generators.py gen_for_qa l.548] (55/93) * Start with LLM "gemini-pro"
[2024-03-04 10:51:15,649 DEBUG generators.py generate l.349] (55/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:15,650 DEBUG generators.py generate l.358] (55/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:18,567 DEBUG generators.py generate l.370] (55/93) Post-process Answer
[2024-03-04 10:51:18,570 INFO generators.py gen_for_qa l.548] (55/93) * Start with LLM "claude-2.1"
[2024-03-04 10:51:18,573 DEBUG generators.py generate l.349] (55/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:18,573 DEBUG generators.py generate l.358] (55/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:19,774 DEBUG generators.py generate l.370] (55/93) Post-process Answer
[2024-03-04 10:51:19,774 INFO generators.py gen_for_qa l.548] (55/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:51:19,778 DEBUG generators.py generate l.349] (55/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:19,779 DEBUG generators.py generate l.358] (55/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:20,183 DEBUG generators.py generate l.370] (55/93) Post-process Answer
[2024-03-04 10:51:20,185 INFO generators.py gen_for_qa l.548] (55/93) * Start with LLM "command-nightly"
[2024-03-04 10:51:20,186 DEBUG generators.py generate l.349] (55/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:20,187 DEBUG generators.py generate l.358] (55/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:20,449 DEBUG generators.py generate l.370] (55/93) Post-process Answer
[2024-03-04 10:51:20,449 INFO generators.py gen_for_qa l.548] (55/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:51:20,449 DEBUG generators.py generate l.349] (55/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:20,449 DEBUG generators.py generate l.358] (55/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:20,842 ERROR generators.py complete l.400] (55/93) The following exception occurred with prompt meta={} user=" Qui a découvert la loi de la conservation de la quantité de mouvement ?  A)  Isaac Newton B)  René Descartes C)  Christiaan Huygens .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:51:20,858 DEBUG generators.py generate l.373] (55/93) Reuse post-processing
[2024-03-04 10:51:20,858 INFO generators.py generate l.477] (55/93) End question " Qui a découvert la loi de la conservation de la quantité de mouvement ?  A)  Isaac Newton B)  René Descartes C)  Christiaan Huygens "
[2024-03-04 10:51:20,858 INFO generators.py generate l.475] (56/93) *** AnsGenerator for question " Qui a inventé le premier microprocesseur ?  A)  Federico Faggin, Ted Hoff et Stanley Mazor B)  Marcian Hoff "
[2024-03-04 10:51:20,858 INFO generators.py gen_for_qa l.548] (56/93) * Start with LLM "gpt-4"
[2024-03-04 10:51:20,858 DEBUG generators.py generate l.349] (56/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:20,858 DEBUG generators.py generate l.358] (56/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:21,723 DEBUG generators.py generate l.370] (56/93) Post-process Answer
[2024-03-04 10:51:21,724 INFO generators.py gen_for_qa l.548] (56/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:51:21,725 DEBUG generators.py generate l.349] (56/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:21,726 DEBUG generators.py generate l.358] (56/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:22,735 DEBUG generators.py generate l.370] (56/93) Post-process Answer
[2024-03-04 10:51:22,739 INFO generators.py gen_for_qa l.548] (56/93) * Start with LLM "gemini-pro"
[2024-03-04 10:51:22,741 DEBUG generators.py generate l.349] (56/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:22,741 DEBUG generators.py generate l.358] (56/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:25,487 DEBUG generators.py generate l.370] (56/93) Post-process Answer
[2024-03-04 10:51:25,487 INFO generators.py gen_for_qa l.548] (56/93) * Start with LLM "claude-2.1"
[2024-03-04 10:51:25,495 DEBUG generators.py generate l.349] (56/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:25,497 DEBUG generators.py generate l.358] (56/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:26,548 DEBUG generators.py generate l.370] (56/93) Post-process Answer
[2024-03-04 10:51:26,550 INFO generators.py gen_for_qa l.548] (56/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:51:26,552 DEBUG generators.py generate l.349] (56/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:26,554 DEBUG generators.py generate l.358] (56/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:27,101 DEBUG generators.py generate l.370] (56/93) Post-process Answer
[2024-03-04 10:51:27,103 INFO generators.py gen_for_qa l.548] (56/93) * Start with LLM "command-nightly"
[2024-03-04 10:51:27,105 DEBUG generators.py generate l.349] (56/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:27,106 DEBUG generators.py generate l.358] (56/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:27,362 DEBUG generators.py generate l.370] (56/93) Post-process Answer
[2024-03-04 10:51:27,364 INFO generators.py gen_for_qa l.548] (56/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:51:27,364 DEBUG generators.py generate l.349] (56/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:27,364 DEBUG generators.py generate l.358] (56/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:27,824 ERROR generators.py complete l.400] (56/93) The following exception occurred with prompt meta={} user=" Qui a inventé le premier microprocesseur ?  A)  Federico Faggin, Ted Hoff et Stanley Mazor B)  Marcian Hoff .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:51:27,827 DEBUG generators.py generate l.373] (56/93) Reuse post-processing
[2024-03-04 10:51:27,829 INFO generators.py generate l.477] (56/93) End question " Qui a inventé le premier microprocesseur ?  A)  Federico Faggin, Ted Hoff et Stanley Mazor B)  Marcian Hoff "
[2024-03-04 10:51:27,830 INFO generators.py generate l.475] (57/93) *** AnsGenerator for question " Qui a découvert la loi de la conservation de la charge électrique ?  A)  Michael Faraday B)  Charles-Augustin de Coulomb C)  Benjamin Franklin "
[2024-03-04 10:51:27,832 INFO generators.py gen_for_qa l.548] (57/93) * Start with LLM "gpt-4"
[2024-03-04 10:51:27,833 DEBUG generators.py generate l.349] (57/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:27,836 DEBUG generators.py generate l.358] (57/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:29,181 DEBUG generators.py generate l.370] (57/93) Post-process Answer
[2024-03-04 10:51:29,181 INFO generators.py gen_for_qa l.548] (57/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:51:29,181 DEBUG generators.py generate l.349] (57/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:29,181 DEBUG generators.py generate l.358] (57/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:29,842 DEBUG generators.py generate l.370] (57/93) Post-process Answer
[2024-03-04 10:51:29,846 INFO generators.py gen_for_qa l.548] (57/93) * Start with LLM "gemini-pro"
[2024-03-04 10:51:29,849 DEBUG generators.py generate l.349] (57/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:29,852 DEBUG generators.py generate l.358] (57/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:32,333 DEBUG generators.py generate l.370] (57/93) Post-process Answer
[2024-03-04 10:51:32,341 INFO generators.py gen_for_qa l.548] (57/93) * Start with LLM "claude-2.1"
[2024-03-04 10:51:32,341 DEBUG generators.py generate l.349] (57/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:32,343 DEBUG generators.py generate l.358] (57/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:33,672 DEBUG generators.py generate l.370] (57/93) Post-process Answer
[2024-03-04 10:51:33,673 INFO generators.py gen_for_qa l.548] (57/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:51:33,675 DEBUG generators.py generate l.349] (57/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:33,677 DEBUG generators.py generate l.358] (57/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:36,144 DEBUG generators.py generate l.370] (57/93) Post-process Answer
[2024-03-04 10:51:36,146 INFO generators.py gen_for_qa l.548] (57/93) * Start with LLM "command-nightly"
[2024-03-04 10:51:36,147 DEBUG generators.py generate l.349] (57/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:36,149 DEBUG generators.py generate l.358] (57/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:36,402 DEBUG generators.py generate l.370] (57/93) Post-process Answer
[2024-03-04 10:51:36,405 INFO generators.py gen_for_qa l.548] (57/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:51:36,406 DEBUG generators.py generate l.349] (57/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:36,407 DEBUG generators.py generate l.358] (57/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:36,801 ERROR generators.py complete l.400] (57/93) The following exception occurred with prompt meta={} user=" Qui a découvert la loi de la conservation de la charge électrique ?  A)  Michael Faraday B)  Charles-Augustin de Coulomb C)  Benjamin Franklin .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:51:36,816 DEBUG generators.py generate l.373] (57/93) Reuse post-processing
[2024-03-04 10:51:36,828 INFO generators.py generate l.477] (57/93) End question " Qui a découvert la loi de la conservation de la charge électrique ?  A)  Michael Faraday B)  Charles-Augustin de Coulomb C)  Benjamin Franklin "
[2024-03-04 10:51:36,832 INFO generators.py generate l.475] (58/93) *** AnsGenerator for question " Qui a inventé le premier téléphone portable ?  A)  Martin Cooper B)  Rudy Krolopp "
[2024-03-04 10:51:36,832 INFO generators.py gen_for_qa l.548] (58/93) * Start with LLM "gpt-4"
[2024-03-04 10:51:36,832 DEBUG generators.py generate l.349] (58/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:36,844 DEBUG generators.py generate l.358] (58/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:37,757 DEBUG generators.py generate l.370] (58/93) Post-process Answer
[2024-03-04 10:51:37,773 INFO generators.py gen_for_qa l.548] (58/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:51:37,773 DEBUG generators.py generate l.349] (58/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:37,773 DEBUG generators.py generate l.358] (58/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:38,621 DEBUG generators.py generate l.370] (58/93) Post-process Answer
[2024-03-04 10:51:38,621 INFO generators.py gen_for_qa l.548] (58/93) * Start with LLM "gemini-pro"
[2024-03-04 10:51:38,621 DEBUG generators.py generate l.349] (58/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:38,621 DEBUG generators.py generate l.358] (58/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:41,185 DEBUG generators.py generate l.370] (58/93) Post-process Answer
[2024-03-04 10:51:41,187 INFO generators.py gen_for_qa l.548] (58/93) * Start with LLM "claude-2.1"
[2024-03-04 10:51:41,188 DEBUG generators.py generate l.349] (58/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:41,189 DEBUG generators.py generate l.358] (58/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:42,981 DEBUG generators.py generate l.370] (58/93) Post-process Answer
[2024-03-04 10:51:42,981 INFO generators.py gen_for_qa l.548] (58/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:51:42,981 DEBUG generators.py generate l.349] (58/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:42,997 DEBUG generators.py generate l.358] (58/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:43,393 DEBUG generators.py generate l.370] (58/93) Post-process Answer
[2024-03-04 10:51:43,395 INFO generators.py gen_for_qa l.548] (58/93) * Start with LLM "command-nightly"
[2024-03-04 10:51:43,396 DEBUG generators.py generate l.349] (58/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:43,397 DEBUG generators.py generate l.358] (58/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:43,641 DEBUG generators.py generate l.370] (58/93) Post-process Answer
[2024-03-04 10:51:43,642 INFO generators.py gen_for_qa l.548] (58/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:51:43,644 DEBUG generators.py generate l.349] (58/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:43,648 DEBUG generators.py generate l.358] (58/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:44,085 ERROR generators.py complete l.400] (58/93) The following exception occurred with prompt meta={} user=" Qui a inventé le premier téléphone portable ?  A)  Martin Cooper B)  Rudy Krolopp .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:51:44,089 DEBUG generators.py generate l.373] (58/93) Reuse post-processing
[2024-03-04 10:51:44,090 INFO generators.py generate l.477] (58/93) End question " Qui a inventé le premier téléphone portable ?  A)  Martin Cooper B)  Rudy Krolopp "
[2024-03-04 10:51:44,091 INFO generators.py generate l.475] (59/93) *** AnsGenerator for question " Où a été inventé le premier avion ?  A)  Kitty Hawk B)  Le Bourget C)  Bridgeport "
[2024-03-04 10:51:44,093 INFO generators.py gen_for_qa l.548] (59/93) * Start with LLM "gpt-4"
[2024-03-04 10:51:44,095 DEBUG generators.py generate l.349] (59/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:44,095 DEBUG generators.py generate l.358] (59/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:44,774 DEBUG generators.py generate l.370] (59/93) Post-process Answer
[2024-03-04 10:51:44,774 INFO generators.py gen_for_qa l.548] (59/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:51:44,774 DEBUG generators.py generate l.349] (59/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:44,774 DEBUG generators.py generate l.358] (59/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:45,418 DEBUG generators.py generate l.370] (59/93) Post-process Answer
[2024-03-04 10:51:45,419 INFO generators.py gen_for_qa l.548] (59/93) * Start with LLM "gemini-pro"
[2024-03-04 10:51:45,421 DEBUG generators.py generate l.349] (59/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:45,422 DEBUG generators.py generate l.358] (59/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:47,730 DEBUG generators.py generate l.370] (59/93) Post-process Answer
[2024-03-04 10:51:47,734 INFO generators.py gen_for_qa l.548] (59/93) * Start with LLM "claude-2.1"
[2024-03-04 10:51:47,737 DEBUG generators.py generate l.349] (59/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:47,740 DEBUG generators.py generate l.358] (59/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:48,758 DEBUG generators.py generate l.370] (59/93) Post-process Answer
[2024-03-04 10:51:48,759 INFO generators.py gen_for_qa l.548] (59/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:51:48,760 DEBUG generators.py generate l.349] (59/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:48,762 DEBUG generators.py generate l.358] (59/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:49,132 DEBUG generators.py generate l.370] (59/93) Post-process Answer
[2024-03-04 10:51:49,134 INFO generators.py gen_for_qa l.548] (59/93) * Start with LLM "command-nightly"
[2024-03-04 10:51:49,135 DEBUG generators.py generate l.349] (59/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:49,137 DEBUG generators.py generate l.358] (59/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:49,379 DEBUG generators.py generate l.370] (59/93) Post-process Answer
[2024-03-04 10:51:49,381 INFO generators.py gen_for_qa l.548] (59/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:51:49,382 DEBUG generators.py generate l.349] (59/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:49,382 DEBUG generators.py generate l.358] (59/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:49,903 ERROR generators.py complete l.400] (59/93) The following exception occurred with prompt meta={} user=" Où a été inventé le premier avion ?  A)  Kitty Hawk B)  Le Bourget C)  Bridgeport .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:51:49,905 DEBUG generators.py generate l.373] (59/93) Reuse post-processing
[2024-03-04 10:51:49,905 INFO generators.py generate l.477] (59/93) End question " Où a été inventé le premier avion ?  A)  Kitty Hawk B)  Le Bourget C)  Bridgeport "
[2024-03-04 10:51:49,905 INFO generators.py generate l.475] (60/93) *** AnsGenerator for question " Où a été découvert l'Amérique par Christophe Colomb ?  A)  San Salvador B)  Pointe Isabela C)  Guanahani "
[2024-03-04 10:51:49,914 INFO generators.py gen_for_qa l.548] (60/93) * Start with LLM "gpt-4"
[2024-03-04 10:51:49,914 DEBUG generators.py generate l.349] (60/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:49,918 DEBUG generators.py generate l.358] (60/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:50,493 DEBUG generators.py generate l.370] (60/93) Post-process Answer
[2024-03-04 10:51:50,495 INFO generators.py gen_for_qa l.548] (60/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:51:50,496 DEBUG generators.py generate l.349] (60/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:50,498 DEBUG generators.py generate l.358] (60/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:51,622 DEBUG generators.py generate l.370] (60/93) Post-process Answer
[2024-03-04 10:51:51,623 INFO generators.py gen_for_qa l.548] (60/93) * Start with LLM "gemini-pro"
[2024-03-04 10:51:51,624 DEBUG generators.py generate l.349] (60/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:51,626 DEBUG generators.py generate l.358] (60/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:54,254 DEBUG generators.py generate l.370] (60/93) Post-process Answer
[2024-03-04 10:51:54,254 INFO generators.py gen_for_qa l.548] (60/93) * Start with LLM "claude-2.1"
[2024-03-04 10:51:54,254 DEBUG generators.py generate l.349] (60/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:54,270 DEBUG generators.py generate l.358] (60/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:55,361 DEBUG generators.py generate l.370] (60/93) Post-process Answer
[2024-03-04 10:51:55,364 INFO generators.py gen_for_qa l.548] (60/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:51:55,366 DEBUG generators.py generate l.349] (60/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:55,370 DEBUG generators.py generate l.358] (60/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:55,849 DEBUG generators.py generate l.370] (60/93) Post-process Answer
[2024-03-04 10:51:55,851 INFO generators.py gen_for_qa l.548] (60/93) * Start with LLM "command-nightly"
[2024-03-04 10:51:55,851 DEBUG generators.py generate l.349] (60/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:55,852 DEBUG generators.py generate l.358] (60/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:56,319 DEBUG generators.py generate l.370] (60/93) Post-process Answer
[2024-03-04 10:51:56,334 INFO generators.py gen_for_qa l.548] (60/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:51:56,336 DEBUG generators.py generate l.349] (60/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:56,336 DEBUG generators.py generate l.358] (60/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:56,835 ERROR generators.py complete l.400] (60/93) The following exception occurred with prompt meta={} user=" Où a été découvert l'Amérique par Christophe Colomb ?  A)  San Salvador B)  Pointe Isabela C)  Guanahani .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:51:56,836 DEBUG generators.py generate l.373] (60/93) Reuse post-processing
[2024-03-04 10:51:56,836 INFO generators.py generate l.477] (60/93) End question " Où a été découvert l'Amérique par Christophe Colomb ?  A)  San Salvador B)  Pointe Isabela C)  Guanahani "
[2024-03-04 10:51:56,836 INFO generators.py generate l.475] (61/93) *** AnsGenerator for question " Où a été inventé le premier ordinateur programmable ?  A)  Berlin B)  Iowa C)  Bletchley Park "
[2024-03-04 10:51:56,851 INFO generators.py gen_for_qa l.548] (61/93) * Start with LLM "gpt-4"
[2024-03-04 10:51:56,853 DEBUG generators.py generate l.349] (61/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:56,855 DEBUG generators.py generate l.358] (61/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:58,249 DEBUG generators.py generate l.370] (61/93) Post-process Answer
[2024-03-04 10:51:58,251 INFO generators.py gen_for_qa l.548] (61/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:51:58,252 DEBUG generators.py generate l.349] (61/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:58,253 DEBUG generators.py generate l.358] (61/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:51:58,937 DEBUG generators.py generate l.370] (61/93) Post-process Answer
[2024-03-04 10:51:58,937 INFO generators.py gen_for_qa l.548] (61/93) * Start with LLM "gemini-pro"
[2024-03-04 10:51:58,952 DEBUG generators.py generate l.349] (61/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:51:58,952 DEBUG generators.py generate l.358] (61/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:01,459 DEBUG generators.py generate l.370] (61/93) Post-process Answer
[2024-03-04 10:52:01,462 INFO generators.py gen_for_qa l.548] (61/93) * Start with LLM "claude-2.1"
[2024-03-04 10:52:01,465 DEBUG generators.py generate l.349] (61/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:01,467 DEBUG generators.py generate l.358] (61/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:03,317 DEBUG generators.py generate l.370] (61/93) Post-process Answer
[2024-03-04 10:52:03,318 INFO generators.py gen_for_qa l.548] (61/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:52:03,319 DEBUG generators.py generate l.349] (61/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:03,320 DEBUG generators.py generate l.358] (61/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:03,771 DEBUG generators.py generate l.370] (61/93) Post-process Answer
[2024-03-04 10:52:03,773 INFO generators.py gen_for_qa l.548] (61/93) * Start with LLM "command-nightly"
[2024-03-04 10:52:03,776 DEBUG generators.py generate l.349] (61/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:03,777 DEBUG generators.py generate l.358] (61/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:04,006 DEBUG generators.py generate l.370] (61/93) Post-process Answer
[2024-03-04 10:52:04,007 INFO generators.py gen_for_qa l.548] (61/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:52:04,008 DEBUG generators.py generate l.349] (61/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:04,009 DEBUG generators.py generate l.358] (61/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:04,502 ERROR generators.py complete l.400] (61/93) The following exception occurred with prompt meta={} user=" Où a été inventé le premier ordinateur programmable ?  A)  Berlin B)  Iowa C)  Bletchley Park .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:52:04,509 DEBUG generators.py generate l.373] (61/93) Reuse post-processing
[2024-03-04 10:52:04,511 INFO generators.py generate l.477] (61/93) End question " Où a été inventé le premier ordinateur programmable ?  A)  Berlin B)  Iowa C)  Bletchley Park "
[2024-03-04 10:52:04,511 INFO generators.py generate l.475] (62/93) *** AnsGenerator for question " Où a été découverte la pénicilline ?  A)  Londres B)  Paris "
[2024-03-04 10:52:04,515 INFO generators.py gen_for_qa l.548] (62/93) * Start with LLM "gpt-4"
[2024-03-04 10:52:04,516 DEBUG generators.py generate l.349] (62/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:04,518 DEBUG generators.py generate l.358] (62/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:05,219 DEBUG generators.py generate l.370] (62/93) Post-process Answer
[2024-03-04 10:52:05,219 INFO generators.py gen_for_qa l.548] (62/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:52:05,233 DEBUG generators.py generate l.349] (62/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:05,237 DEBUG generators.py generate l.358] (62/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:06,449 DEBUG generators.py generate l.370] (62/93) Post-process Answer
[2024-03-04 10:52:06,449 INFO generators.py gen_for_qa l.548] (62/93) * Start with LLM "gemini-pro"
[2024-03-04 10:52:06,449 DEBUG generators.py generate l.349] (62/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:06,449 DEBUG generators.py generate l.358] (62/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:08,748 DEBUG generators.py generate l.370] (62/93) Post-process Answer
[2024-03-04 10:52:08,764 INFO generators.py gen_for_qa l.548] (62/93) * Start with LLM "claude-2.1"
[2024-03-04 10:52:08,765 DEBUG generators.py generate l.349] (62/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:08,765 DEBUG generators.py generate l.358] (62/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:09,815 DEBUG generators.py generate l.370] (62/93) Post-process Answer
[2024-03-04 10:52:09,815 INFO generators.py gen_for_qa l.548] (62/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:52:09,815 DEBUG generators.py generate l.349] (62/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:09,815 DEBUG generators.py generate l.358] (62/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:10,415 DEBUG generators.py generate l.370] (62/93) Post-process Answer
[2024-03-04 10:52:10,415 INFO generators.py gen_for_qa l.548] (62/93) * Start with LLM "command-nightly"
[2024-03-04 10:52:10,415 DEBUG generators.py generate l.349] (62/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:10,415 DEBUG generators.py generate l.358] (62/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:10,776 DEBUG generators.py generate l.370] (62/93) Post-process Answer
[2024-03-04 10:52:10,779 INFO generators.py gen_for_qa l.548] (62/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:52:10,781 DEBUG generators.py generate l.349] (62/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:10,783 DEBUG generators.py generate l.358] (62/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:11,232 ERROR generators.py complete l.400] (62/93) The following exception occurred with prompt meta={} user=" Où a été découverte la pénicilline ?  A)  Londres B)  Paris .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:52:11,248 DEBUG generators.py generate l.373] (62/93) Reuse post-processing
[2024-03-04 10:52:11,248 INFO generators.py generate l.477] (62/93) End question " Où a été découverte la pénicilline ?  A)  Londres B)  Paris "
[2024-03-04 10:52:11,248 INFO generators.py generate l.475] (63/93) *** AnsGenerator for question " Où a été inventé le premier satellite artificiel ?  A)  Moscou B)  Cape Canaveral C)  Baïkonour "
[2024-03-04 10:52:11,248 INFO generators.py gen_for_qa l.548] (63/93) * Start with LLM "gpt-4"
[2024-03-04 10:52:11,265 DEBUG generators.py generate l.349] (63/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:11,269 DEBUG generators.py generate l.358] (63/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:11,897 DEBUG generators.py generate l.370] (63/93) Post-process Answer
[2024-03-04 10:52:11,897 INFO generators.py gen_for_qa l.548] (63/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:52:11,897 DEBUG generators.py generate l.349] (63/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:11,897 DEBUG generators.py generate l.358] (63/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:13,046 DEBUG generators.py generate l.370] (63/93) Post-process Answer
[2024-03-04 10:52:13,047 INFO generators.py gen_for_qa l.548] (63/93) * Start with LLM "gemini-pro"
[2024-03-04 10:52:13,047 DEBUG generators.py generate l.349] (63/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:13,047 DEBUG generators.py generate l.358] (63/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:15,497 DEBUG generators.py generate l.370] (63/93) Post-process Answer
[2024-03-04 10:52:15,498 INFO generators.py gen_for_qa l.548] (63/93) * Start with LLM "claude-2.1"
[2024-03-04 10:52:15,498 DEBUG generators.py generate l.349] (63/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:15,498 DEBUG generators.py generate l.358] (63/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:16,579 DEBUG generators.py generate l.370] (63/93) Post-process Answer
[2024-03-04 10:52:16,596 INFO generators.py gen_for_qa l.548] (63/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:52:16,597 DEBUG generators.py generate l.349] (63/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:16,597 DEBUG generators.py generate l.358] (63/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:18,915 DEBUG generators.py generate l.370] (63/93) Post-process Answer
[2024-03-04 10:52:18,917 INFO generators.py gen_for_qa l.548] (63/93) * Start with LLM "command-nightly"
[2024-03-04 10:52:18,919 DEBUG generators.py generate l.349] (63/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:18,921 DEBUG generators.py generate l.358] (63/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:19,178 DEBUG generators.py generate l.370] (63/93) Post-process Answer
[2024-03-04 10:52:19,188 INFO generators.py gen_for_qa l.548] (63/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:52:19,190 DEBUG generators.py generate l.349] (63/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:19,190 DEBUG generators.py generate l.358] (63/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:19,777 ERROR generators.py complete l.400] (63/93) The following exception occurred with prompt meta={} user=" Où a été inventé le premier satellite artificiel ?  A)  Moscou B)  Cape Canaveral C)  Baïkonour .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:52:19,805 DEBUG generators.py generate l.373] (63/93) Reuse post-processing
[2024-03-04 10:52:19,811 INFO generators.py generate l.477] (63/93) End question " Où a été inventé le premier satellite artificiel ?  A)  Moscou B)  Cape Canaveral C)  Baïkonour "
[2024-03-04 10:52:19,815 INFO generators.py generate l.475] (64/93) *** AnsGenerator for question " Où a été découvert le feu par l'Homme ?  A)  Vallée du Rift B)  Grotte de Zhoukoudian C)  Grotte de Wonderwerk "
[2024-03-04 10:52:19,817 INFO generators.py gen_for_qa l.548] (64/93) * Start with LLM "gpt-4"
[2024-03-04 10:52:19,819 DEBUG generators.py generate l.349] (64/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:19,822 DEBUG generators.py generate l.358] (64/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:20,843 DEBUG generators.py generate l.370] (64/93) Post-process Answer
[2024-03-04 10:52:20,843 INFO generators.py gen_for_qa l.548] (64/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:52:20,843 DEBUG generators.py generate l.349] (64/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:20,843 DEBUG generators.py generate l.358] (64/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:22,161 DEBUG generators.py generate l.370] (64/93) Post-process Answer
[2024-03-04 10:52:22,161 INFO generators.py gen_for_qa l.548] (64/93) * Start with LLM "gemini-pro"
[2024-03-04 10:52:22,176 DEBUG generators.py generate l.349] (64/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:22,179 DEBUG generators.py generate l.358] (64/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:24,728 DEBUG generators.py generate l.370] (64/93) Post-process Answer
[2024-03-04 10:52:24,730 INFO generators.py gen_for_qa l.548] (64/93) * Start with LLM "claude-2.1"
[2024-03-04 10:52:24,730 DEBUG generators.py generate l.349] (64/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:24,733 DEBUG generators.py generate l.358] (64/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:25,791 DEBUG generators.py generate l.370] (64/93) Post-process Answer
[2024-03-04 10:52:25,793 INFO generators.py gen_for_qa l.548] (64/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:52:25,795 DEBUG generators.py generate l.349] (64/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:25,797 DEBUG generators.py generate l.358] (64/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:26,727 DEBUG generators.py generate l.370] (64/93) Post-process Answer
[2024-03-04 10:52:26,738 INFO generators.py gen_for_qa l.548] (64/93) * Start with LLM "command-nightly"
[2024-03-04 10:52:26,743 DEBUG generators.py generate l.349] (64/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:26,747 DEBUG generators.py generate l.358] (64/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:27,029 DEBUG generators.py generate l.370] (64/93) Post-process Answer
[2024-03-04 10:52:27,038 INFO generators.py gen_for_qa l.548] (64/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:52:27,042 DEBUG generators.py generate l.349] (64/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:27,043 DEBUG generators.py generate l.358] (64/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:27,525 ERROR generators.py complete l.400] (64/93) The following exception occurred with prompt meta={} user=" Où a été découvert le feu par l'Homme ?  A)  Vallée du Rift B)  Grotte de Zhoukoudian C)  Grotte de Wonderwerk .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:52:27,543 DEBUG generators.py generate l.373] (64/93) Reuse post-processing
[2024-03-04 10:52:27,543 INFO generators.py generate l.477] (64/93) End question " Où a été découvert le feu par l'Homme ?  A)  Vallée du Rift B)  Grotte de Zhoukoudian C)  Grotte de Wonderwerk "
[2024-03-04 10:52:27,543 INFO generators.py generate l.475] (65/93) *** AnsGenerator for question " Où a été inventé le premier microprocesseur ?  A)  Silicon Valley B)  Palo Alto C)  Santa Clara "
[2024-03-04 10:52:27,557 INFO generators.py gen_for_qa l.548] (65/93) * Start with LLM "gpt-4"
[2024-03-04 10:52:27,557 DEBUG generators.py generate l.349] (65/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:27,557 DEBUG generators.py generate l.358] (65/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:28,425 DEBUG generators.py generate l.370] (65/93) Post-process Answer
[2024-03-04 10:52:28,440 INFO generators.py gen_for_qa l.548] (65/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:52:28,442 DEBUG generators.py generate l.349] (65/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:28,443 DEBUG generators.py generate l.358] (65/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:29,042 DEBUG generators.py generate l.370] (65/93) Post-process Answer
[2024-03-04 10:52:29,042 INFO generators.py gen_for_qa l.548] (65/93) * Start with LLM "gemini-pro"
[2024-03-04 10:52:29,057 DEBUG generators.py generate l.349] (65/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:29,058 DEBUG generators.py generate l.358] (65/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:31,342 DEBUG generators.py generate l.370] (65/93) Post-process Answer
[2024-03-04 10:52:31,342 INFO generators.py gen_for_qa l.548] (65/93) * Start with LLM "claude-2.1"
[2024-03-04 10:52:31,342 DEBUG generators.py generate l.349] (65/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:31,342 DEBUG generators.py generate l.358] (65/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:32,290 DEBUG generators.py generate l.370] (65/93) Post-process Answer
[2024-03-04 10:52:32,290 INFO generators.py gen_for_qa l.548] (65/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:52:32,297 DEBUG generators.py generate l.349] (65/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:32,299 DEBUG generators.py generate l.358] (65/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:34,068 DEBUG generators.py generate l.370] (65/93) Post-process Answer
[2024-03-04 10:52:34,070 INFO generators.py gen_for_qa l.548] (65/93) * Start with LLM "command-nightly"
[2024-03-04 10:52:34,071 DEBUG generators.py generate l.349] (65/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:34,073 DEBUG generators.py generate l.358] (65/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:34,363 DEBUG generators.py generate l.370] (65/93) Post-process Answer
[2024-03-04 10:52:34,366 INFO generators.py gen_for_qa l.548] (65/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:52:34,367 DEBUG generators.py generate l.349] (65/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:34,367 DEBUG generators.py generate l.358] (65/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:34,835 ERROR generators.py complete l.400] (65/93) The following exception occurred with prompt meta={} user=" Où a été inventé le premier microprocesseur ?  A)  Silicon Valley B)  Palo Alto C)  Santa Clara .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:52:34,835 DEBUG generators.py generate l.373] (65/93) Reuse post-processing
[2024-03-04 10:52:34,851 INFO generators.py generate l.477] (65/93) End question " Où a été inventé le premier microprocesseur ?  A)  Silicon Valley B)  Palo Alto C)  Santa Clara "
[2024-03-04 10:52:34,851 INFO generators.py generate l.475] (66/93) *** AnsGenerator for question " Où a été découvert le premier dinosaure ?  A)  Angleterre B)  États-Unis C)  France "
[2024-03-04 10:52:34,851 INFO generators.py gen_for_qa l.548] (66/93) * Start with LLM "gpt-4"
[2024-03-04 10:52:34,851 DEBUG generators.py generate l.349] (66/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:34,867 DEBUG generators.py generate l.358] (66/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:35,473 DEBUG generators.py generate l.370] (66/93) Post-process Answer
[2024-03-04 10:52:35,490 INFO generators.py gen_for_qa l.548] (66/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:52:35,490 DEBUG generators.py generate l.349] (66/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:35,490 DEBUG generators.py generate l.358] (66/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:36,175 DEBUG generators.py generate l.370] (66/93) Post-process Answer
[2024-03-04 10:52:36,191 INFO generators.py gen_for_qa l.548] (66/93) * Start with LLM "gemini-pro"
[2024-03-04 10:52:36,192 DEBUG generators.py generate l.349] (66/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:36,194 DEBUG generators.py generate l.358] (66/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:38,711 DEBUG generators.py generate l.370] (66/93) Post-process Answer
[2024-03-04 10:52:38,713 INFO generators.py gen_for_qa l.548] (66/93) * Start with LLM "claude-2.1"
[2024-03-04 10:52:38,716 DEBUG generators.py generate l.349] (66/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:38,718 DEBUG generators.py generate l.358] (66/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:40,124 DEBUG generators.py generate l.370] (66/93) Post-process Answer
[2024-03-04 10:52:40,126 INFO generators.py gen_for_qa l.548] (66/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:52:40,127 DEBUG generators.py generate l.349] (66/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:40,129 DEBUG generators.py generate l.358] (66/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:40,487 DEBUG generators.py generate l.370] (66/93) Post-process Answer
[2024-03-04 10:52:40,491 INFO generators.py gen_for_qa l.548] (66/93) * Start with LLM "command-nightly"
[2024-03-04 10:52:40,491 DEBUG generators.py generate l.349] (66/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:40,493 DEBUG generators.py generate l.358] (66/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:40,746 DEBUG generators.py generate l.370] (66/93) Post-process Answer
[2024-03-04 10:52:40,747 INFO generators.py gen_for_qa l.548] (66/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:52:40,749 DEBUG generators.py generate l.349] (66/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:40,750 DEBUG generators.py generate l.358] (66/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:41,180 ERROR generators.py complete l.400] (66/93) The following exception occurred with prompt meta={} user=" Où a été découvert le premier dinosaure ?  A)  Angleterre B)  États-Unis C)  France .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:52:41,184 DEBUG generators.py generate l.373] (66/93) Reuse post-processing
[2024-03-04 10:52:41,186 INFO generators.py generate l.477] (66/93) End question " Où a été découvert le premier dinosaure ?  A)  Angleterre B)  États-Unis C)  France "
[2024-03-04 10:52:41,188 INFO generators.py generate l.475] (67/93) *** AnsGenerator for question " Où a été inventé le premier téléphone ?  A)  Boston B)  Philadelphie C)  Washington D.C. "
[2024-03-04 10:52:41,190 INFO generators.py gen_for_qa l.548] (67/93) * Start with LLM "gpt-4"
[2024-03-04 10:52:41,193 DEBUG generators.py generate l.349] (67/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:41,194 DEBUG generators.py generate l.358] (67/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:41,930 DEBUG generators.py generate l.370] (67/93) Post-process Answer
[2024-03-04 10:52:41,936 INFO generators.py gen_for_qa l.548] (67/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:52:41,939 DEBUG generators.py generate l.349] (67/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:41,942 DEBUG generators.py generate l.358] (67/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:42,642 DEBUG generators.py generate l.370] (67/93) Post-process Answer
[2024-03-04 10:52:42,646 INFO generators.py gen_for_qa l.548] (67/93) * Start with LLM "gemini-pro"
[2024-03-04 10:52:42,649 DEBUG generators.py generate l.349] (67/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:42,652 DEBUG generators.py generate l.358] (67/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:45,123 DEBUG generators.py generate l.370] (67/93) Post-process Answer
[2024-03-04 10:52:45,130 INFO generators.py gen_for_qa l.548] (67/93) * Start with LLM "claude-2.1"
[2024-03-04 10:52:45,137 DEBUG generators.py generate l.349] (67/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:45,139 DEBUG generators.py generate l.358] (67/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:46,206 DEBUG generators.py generate l.370] (67/93) Post-process Answer
[2024-03-04 10:52:46,206 INFO generators.py gen_for_qa l.548] (67/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:52:46,206 DEBUG generators.py generate l.349] (67/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:46,206 DEBUG generators.py generate l.358] (67/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:47,055 DEBUG generators.py generate l.370] (67/93) Post-process Answer
[2024-03-04 10:52:47,055 INFO generators.py gen_for_qa l.548] (67/93) * Start with LLM "command-nightly"
[2024-03-04 10:52:47,055 DEBUG generators.py generate l.349] (67/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:47,061 DEBUG generators.py generate l.358] (67/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:47,374 DEBUG generators.py generate l.370] (67/93) Post-process Answer
[2024-03-04 10:52:47,380 INFO generators.py gen_for_qa l.548] (67/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:52:47,387 DEBUG generators.py generate l.349] (67/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:47,390 DEBUG generators.py generate l.358] (67/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:47,905 ERROR generators.py complete l.400] (67/93) The following exception occurred with prompt meta={} user=" Où a été inventé le premier téléphone ?  A)  Boston B)  Philadelphie C)  Washington D.C. .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:52:47,924 DEBUG generators.py generate l.373] (67/93) Reuse post-processing
[2024-03-04 10:52:47,926 INFO generators.py generate l.477] (67/93) End question " Où a été inventé le premier téléphone ?  A)  Boston B)  Philadelphie C)  Washington D.C. "
[2024-03-04 10:52:47,926 INFO generators.py generate l.475] (68/93) *** AnsGenerator for question " Où a été découvert le premier gisement de pétrole ?  A)  Titusville B)  Bakou C)  Tulsa "
[2024-03-04 10:52:47,937 INFO generators.py gen_for_qa l.548] (68/93) * Start with LLM "gpt-4"
[2024-03-04 10:52:47,940 DEBUG generators.py generate l.349] (68/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:47,943 DEBUG generators.py generate l.358] (68/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:48,555 DEBUG generators.py generate l.370] (68/93) Post-process Answer
[2024-03-04 10:52:48,558 INFO generators.py gen_for_qa l.548] (68/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:52:48,560 DEBUG generators.py generate l.349] (68/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:48,564 DEBUG generators.py generate l.358] (68/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:49,061 DEBUG generators.py generate l.370] (68/93) Post-process Answer
[2024-03-04 10:52:49,064 INFO generators.py gen_for_qa l.548] (68/93) * Start with LLM "gemini-pro"
[2024-03-04 10:52:49,068 DEBUG generators.py generate l.349] (68/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:49,069 DEBUG generators.py generate l.358] (68/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:51,587 DEBUG generators.py generate l.370] (68/93) Post-process Answer
[2024-03-04 10:52:51,593 INFO generators.py gen_for_qa l.548] (68/93) * Start with LLM "claude-2.1"
[2024-03-04 10:52:51,600 DEBUG generators.py generate l.349] (68/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:51,602 DEBUG generators.py generate l.358] (68/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:52,593 DEBUG generators.py generate l.370] (68/93) Post-process Answer
[2024-03-04 10:52:52,599 INFO generators.py gen_for_qa l.548] (68/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:52:52,603 DEBUG generators.py generate l.349] (68/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:52,603 DEBUG generators.py generate l.358] (68/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:53,011 DEBUG generators.py generate l.370] (68/93) Post-process Answer
[2024-03-04 10:52:53,016 INFO generators.py gen_for_qa l.548] (68/93) * Start with LLM "command-nightly"
[2024-03-04 10:52:53,021 DEBUG generators.py generate l.349] (68/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:53,021 DEBUG generators.py generate l.358] (68/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:53,342 DEBUG generators.py generate l.370] (68/93) Post-process Answer
[2024-03-04 10:52:53,347 INFO generators.py gen_for_qa l.548] (68/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:52:53,352 DEBUG generators.py generate l.349] (68/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:53,355 DEBUG generators.py generate l.358] (68/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:53,843 ERROR generators.py complete l.400] (68/93) The following exception occurred with prompt meta={} user=" Où a été découvert le premier gisement de pétrole ?  A)  Titusville B)  Bakou C)  Tulsa .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:52:53,854 DEBUG generators.py generate l.373] (68/93) Reuse post-processing
[2024-03-04 10:52:53,860 INFO generators.py generate l.477] (68/93) End question " Où a été découvert le premier gisement de pétrole ?  A)  Titusville B)  Bakou C)  Tulsa "
[2024-03-04 10:52:53,860 INFO generators.py generate l.475] (69/93) *** AnsGenerator for question " Où a été inventé le premier sous-marin ?  A)  Londres B)  La Haye C)  New York "
[2024-03-04 10:52:53,860 INFO generators.py gen_for_qa l.548] (69/93) * Start with LLM "gpt-4"
[2024-03-04 10:52:53,866 DEBUG generators.py generate l.349] (69/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:53,868 DEBUG generators.py generate l.358] (69/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:54,806 DEBUG generators.py generate l.370] (69/93) Post-process Answer
[2024-03-04 10:52:54,811 INFO generators.py gen_for_qa l.548] (69/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:52:54,816 DEBUG generators.py generate l.349] (69/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:54,819 DEBUG generators.py generate l.358] (69/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:55,618 DEBUG generators.py generate l.370] (69/93) Post-process Answer
[2024-03-04 10:52:55,618 INFO generators.py gen_for_qa l.548] (69/93) * Start with LLM "gemini-pro"
[2024-03-04 10:52:55,628 DEBUG generators.py generate l.349] (69/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:55,630 DEBUG generators.py generate l.358] (69/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:57,870 DEBUG generators.py generate l.370] (69/93) Post-process Answer
[2024-03-04 10:52:57,874 INFO generators.py gen_for_qa l.548] (69/93) * Start with LLM "claude-2.1"
[2024-03-04 10:52:57,878 DEBUG generators.py generate l.349] (69/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:57,884 DEBUG generators.py generate l.358] (69/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:52:58,852 DEBUG generators.py generate l.370] (69/93) Post-process Answer
[2024-03-04 10:52:58,857 INFO generators.py gen_for_qa l.548] (69/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:52:58,861 DEBUG generators.py generate l.349] (69/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:52:58,865 DEBUG generators.py generate l.358] (69/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:00,234 DEBUG generators.py generate l.370] (69/93) Post-process Answer
[2024-03-04 10:53:00,234 INFO generators.py gen_for_qa l.548] (69/93) * Start with LLM "command-nightly"
[2024-03-04 10:53:00,234 DEBUG generators.py generate l.349] (69/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:00,243 DEBUG generators.py generate l.358] (69/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:00,557 DEBUG generators.py generate l.370] (69/93) Post-process Answer
[2024-03-04 10:53:00,562 INFO generators.py gen_for_qa l.548] (69/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:53:00,564 DEBUG generators.py generate l.349] (69/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:00,566 DEBUG generators.py generate l.358] (69/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:01,064 ERROR generators.py complete l.400] (69/93) The following exception occurred with prompt meta={} user=" Où a été inventé le premier sous-marin ?  A)  Londres B)  La Haye C)  New York .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:53:01,068 DEBUG generators.py generate l.373] (69/93) Reuse post-processing
[2024-03-04 10:53:01,068 INFO generators.py generate l.477] (69/93) End question " Où a été inventé le premier sous-marin ?  A)  Londres B)  La Haye C)  New York "
[2024-03-04 10:53:01,068 INFO generators.py generate l.475] (70/93) *** AnsGenerator for question " Où a été découvert le premier vaccin ?  A)  Gloucestershire B)  Dorset C)  Somerset "
[2024-03-04 10:53:01,068 INFO generators.py gen_for_qa l.548] (70/93) * Start with LLM "gpt-4"
[2024-03-04 10:53:01,068 DEBUG generators.py generate l.349] (70/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:01,068 DEBUG generators.py generate l.358] (70/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:01,681 DEBUG generators.py generate l.370] (70/93) Post-process Answer
[2024-03-04 10:53:01,683 INFO generators.py gen_for_qa l.548] (70/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:53:01,685 DEBUG generators.py generate l.349] (70/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:01,685 DEBUG generators.py generate l.358] (70/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:02,334 DEBUG generators.py generate l.370] (70/93) Post-process Answer
[2024-03-04 10:53:02,345 INFO generators.py gen_for_qa l.548] (70/93) * Start with LLM "gemini-pro"
[2024-03-04 10:53:02,346 DEBUG generators.py generate l.349] (70/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:02,346 DEBUG generators.py generate l.358] (70/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:04,720 DEBUG generators.py generate l.370] (70/93) Post-process Answer
[2024-03-04 10:53:04,720 INFO generators.py gen_for_qa l.548] (70/93) * Start with LLM "claude-2.1"
[2024-03-04 10:53:04,720 DEBUG generators.py generate l.349] (70/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:04,720 DEBUG generators.py generate l.358] (70/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:06,662 DEBUG generators.py generate l.370] (70/93) Post-process Answer
[2024-03-04 10:53:06,662 INFO generators.py gen_for_qa l.548] (70/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:53:06,662 DEBUG generators.py generate l.349] (70/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:06,674 DEBUG generators.py generate l.358] (70/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:10,400 DEBUG generators.py generate l.370] (70/93) Post-process Answer
[2024-03-04 10:53:10,402 INFO generators.py gen_for_qa l.548] (70/93) * Start with LLM "command-nightly"
[2024-03-04 10:53:10,402 DEBUG generators.py generate l.349] (70/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:10,405 DEBUG generators.py generate l.358] (70/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:10,879 DEBUG generators.py generate l.370] (70/93) Post-process Answer
[2024-03-04 10:53:10,879 INFO generators.py gen_for_qa l.548] (70/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:53:10,894 DEBUG generators.py generate l.349] (70/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:10,896 DEBUG generators.py generate l.358] (70/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:11,331 ERROR generators.py complete l.400] (70/93) The following exception occurred with prompt meta={} user=" Où a été découvert le premier vaccin ?  A)  Gloucestershire B)  Dorset C)  Somerset .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:53:11,345 DEBUG generators.py generate l.373] (70/93) Reuse post-processing
[2024-03-04 10:53:11,349 INFO generators.py generate l.477] (70/93) End question " Où a été découvert le premier vaccin ?  A)  Gloucestershire B)  Dorset C)  Somerset "
[2024-03-04 10:53:11,353 INFO generators.py generate l.475] (71/93) *** AnsGenerator for question " Où a été inventé le premier moteur à combustion interne ?  A)  Paris B)  Francfort C)  Londres "
[2024-03-04 10:53:11,356 INFO generators.py gen_for_qa l.548] (71/93) * Start with LLM "gpt-4"
[2024-03-04 10:53:11,361 DEBUG generators.py generate l.349] (71/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:11,364 DEBUG generators.py generate l.358] (71/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:12,176 DEBUG generators.py generate l.370] (71/93) Post-process Answer
[2024-03-04 10:53:12,178 INFO generators.py gen_for_qa l.548] (71/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:53:12,182 DEBUG generators.py generate l.349] (71/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:12,186 DEBUG generators.py generate l.358] (71/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:12,884 DEBUG generators.py generate l.370] (71/93) Post-process Answer
[2024-03-04 10:53:12,891 INFO generators.py gen_for_qa l.548] (71/93) * Start with LLM "gemini-pro"
[2024-03-04 10:53:12,894 DEBUG generators.py generate l.349] (71/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:12,894 DEBUG generators.py generate l.358] (71/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:15,212 DEBUG generators.py generate l.370] (71/93) Post-process Answer
[2024-03-04 10:53:15,212 INFO generators.py gen_for_qa l.548] (71/93) * Start with LLM "claude-2.1"
[2024-03-04 10:53:15,224 DEBUG generators.py generate l.349] (71/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:15,224 DEBUG generators.py generate l.358] (71/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:16,507 DEBUG generators.py generate l.370] (71/93) Post-process Answer
[2024-03-04 10:53:16,512 INFO generators.py gen_for_qa l.548] (71/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:53:16,515 DEBUG generators.py generate l.349] (71/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:16,517 DEBUG generators.py generate l.358] (71/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:16,890 DEBUG generators.py generate l.370] (71/93) Post-process Answer
[2024-03-04 10:53:16,890 INFO generators.py gen_for_qa l.548] (71/93) * Start with LLM "command-nightly"
[2024-03-04 10:53:16,890 DEBUG generators.py generate l.349] (71/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:16,890 DEBUG generators.py generate l.358] (71/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:17,277 DEBUG generators.py generate l.370] (71/93) Post-process Answer
[2024-03-04 10:53:17,277 INFO generators.py gen_for_qa l.548] (71/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:53:17,277 DEBUG generators.py generate l.349] (71/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:17,277 DEBUG generators.py generate l.358] (71/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:17,698 ERROR generators.py complete l.400] (71/93) The following exception occurred with prompt meta={} user=" Où a été inventé le premier moteur à combustion interne ?  A)  Paris B)  Francfort C)  Londres .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:53:17,708 DEBUG generators.py generate l.373] (71/93) Reuse post-processing
[2024-03-04 10:53:17,710 INFO generators.py generate l.477] (71/93) End question " Où a été inventé le premier moteur à combustion interne ?  A)  Paris B)  Francfort C)  Londres "
[2024-03-04 10:53:17,712 INFO generators.py generate l.475] (72/93) *** AnsGenerator for question " Où a été découvert le premier code de lois écrites ?  A)  Ur B)  Babylone C)  Ebla "
[2024-03-04 10:53:17,712 INFO generators.py gen_for_qa l.548] (72/93) * Start with LLM "gpt-4"
[2024-03-04 10:53:17,715 DEBUG generators.py generate l.349] (72/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:17,715 DEBUG generators.py generate l.358] (72/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:18,567 DEBUG generators.py generate l.370] (72/93) Post-process Answer
[2024-03-04 10:53:18,571 INFO generators.py gen_for_qa l.548] (72/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:53:18,571 DEBUG generators.py generate l.349] (72/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:18,571 DEBUG generators.py generate l.358] (72/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:19,298 DEBUG generators.py generate l.370] (72/93) Post-process Answer
[2024-03-04 10:53:19,301 INFO generators.py gen_for_qa l.548] (72/93) * Start with LLM "gemini-pro"
[2024-03-04 10:53:19,303 DEBUG generators.py generate l.349] (72/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:19,306 DEBUG generators.py generate l.358] (72/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:21,790 DEBUG generators.py generate l.370] (72/93) Post-process Answer
[2024-03-04 10:53:21,790 INFO generators.py gen_for_qa l.548] (72/93) * Start with LLM "claude-2.1"
[2024-03-04 10:53:21,808 DEBUG generators.py generate l.349] (72/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:21,811 DEBUG generators.py generate l.358] (72/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:23,082 DEBUG generators.py generate l.370] (72/93) Post-process Answer
[2024-03-04 10:53:23,083 INFO generators.py gen_for_qa l.548] (72/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:53:23,084 DEBUG generators.py generate l.349] (72/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:23,086 DEBUG generators.py generate l.358] (72/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:23,585 DEBUG generators.py generate l.370] (72/93) Post-process Answer
[2024-03-04 10:53:23,591 INFO generators.py gen_for_qa l.548] (72/93) * Start with LLM "command-nightly"
[2024-03-04 10:53:23,591 DEBUG generators.py generate l.349] (72/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:23,591 DEBUG generators.py generate l.358] (72/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:23,890 DEBUG generators.py generate l.370] (72/93) Post-process Answer
[2024-03-04 10:53:23,905 INFO generators.py gen_for_qa l.548] (72/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:53:23,905 DEBUG generators.py generate l.349] (72/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:23,905 DEBUG generators.py generate l.358] (72/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:24,364 ERROR generators.py complete l.400] (72/93) The following exception occurred with prompt meta={} user=" Où a été découvert le premier code de lois écrites ?  A)  Ur B)  Babylone C)  Ebla .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:53:24,372 DEBUG generators.py generate l.373] (72/93) Reuse post-processing
[2024-03-04 10:53:24,373 INFO generators.py generate l.477] (72/93) End question " Où a été découvert le premier code de lois écrites ?  A)  Ur B)  Babylone C)  Ebla "
[2024-03-04 10:53:24,374 INFO generators.py generate l.475] (73/93) *** AnsGenerator for question " Où a été inventé le premier télescope ?  A)  Middelburg B)  Gdansk C)  Florence "
[2024-03-04 10:53:24,376 INFO generators.py gen_for_qa l.548] (73/93) * Start with LLM "gpt-4"
[2024-03-04 10:53:24,377 DEBUG generators.py generate l.349] (73/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:24,379 DEBUG generators.py generate l.358] (73/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:25,373 DEBUG generators.py generate l.370] (73/93) Post-process Answer
[2024-03-04 10:53:25,376 INFO generators.py gen_for_qa l.548] (73/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:53:25,377 DEBUG generators.py generate l.349] (73/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:25,377 DEBUG generators.py generate l.358] (73/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:25,975 DEBUG generators.py generate l.370] (73/93) Post-process Answer
[2024-03-04 10:53:25,975 INFO generators.py gen_for_qa l.548] (73/93) * Start with LLM "gemini-pro"
[2024-03-04 10:53:25,975 DEBUG generators.py generate l.349] (73/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:25,975 DEBUG generators.py generate l.358] (73/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:28,362 DEBUG generators.py generate l.370] (73/93) Post-process Answer
[2024-03-04 10:53:28,376 INFO generators.py gen_for_qa l.548] (73/93) * Start with LLM "claude-2.1"
[2024-03-04 10:53:28,376 DEBUG generators.py generate l.349] (73/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:28,376 DEBUG generators.py generate l.358] (73/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:29,399 DEBUG generators.py generate l.370] (73/93) Post-process Answer
[2024-03-04 10:53:29,408 INFO generators.py gen_for_qa l.548] (73/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:53:29,408 DEBUG generators.py generate l.349] (73/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:29,414 DEBUG generators.py generate l.358] (73/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:29,805 DEBUG generators.py generate l.370] (73/93) Post-process Answer
[2024-03-04 10:53:29,805 INFO generators.py gen_for_qa l.548] (73/93) * Start with LLM "command-nightly"
[2024-03-04 10:53:29,805 DEBUG generators.py generate l.349] (73/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:29,821 DEBUG generators.py generate l.358] (73/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:30,154 DEBUG generators.py generate l.370] (73/93) Post-process Answer
[2024-03-04 10:53:30,158 INFO generators.py gen_for_qa l.548] (73/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:53:30,158 DEBUG generators.py generate l.349] (73/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:30,158 DEBUG generators.py generate l.358] (73/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:30,703 ERROR generators.py complete l.400] (73/93) The following exception occurred with prompt meta={} user=" Où a été inventé le premier télescope ?  A)  Middelburg B)  Gdansk C)  Florence .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:53:30,703 DEBUG generators.py generate l.373] (73/93) Reuse post-processing
[2024-03-04 10:53:30,703 INFO generators.py generate l.477] (73/93) End question " Où a été inventé le premier télescope ?  A)  Middelburg B)  Gdansk C)  Florence "
[2024-03-04 10:53:30,718 INFO generators.py generate l.475] (74/93) *** AnsGenerator for question " Où a été découvert le premier site préhistorique ?  A)  Vallée de la Vézère B)  Vallée du Nil C)  Vallée de l'Omo "
[2024-03-04 10:53:30,720 INFO generators.py gen_for_qa l.548] (74/93) * Start with LLM "gpt-4"
[2024-03-04 10:53:30,720 DEBUG generators.py generate l.349] (74/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:30,724 DEBUG generators.py generate l.358] (74/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:31,378 DEBUG generators.py generate l.370] (74/93) Post-process Answer
[2024-03-04 10:53:31,382 INFO generators.py gen_for_qa l.548] (74/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:53:31,384 DEBUG generators.py generate l.349] (74/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:31,385 DEBUG generators.py generate l.358] (74/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:32,382 DEBUG generators.py generate l.370] (74/93) Post-process Answer
[2024-03-04 10:53:32,384 INFO generators.py gen_for_qa l.548] (74/93) * Start with LLM "gemini-pro"
[2024-03-04 10:53:32,387 DEBUG generators.py generate l.349] (74/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:32,388 DEBUG generators.py generate l.358] (74/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:34,633 DEBUG generators.py generate l.370] (74/93) Post-process Answer
[2024-03-04 10:53:34,633 INFO generators.py gen_for_qa l.548] (74/93) * Start with LLM "claude-2.1"
[2024-03-04 10:53:34,633 DEBUG generators.py generate l.349] (74/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:34,633 DEBUG generators.py generate l.358] (74/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:36,153 DEBUG generators.py generate l.370] (74/93) Post-process Answer
[2024-03-04 10:53:36,154 INFO generators.py gen_for_qa l.548] (74/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:53:36,155 DEBUG generators.py generate l.349] (74/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:36,155 DEBUG generators.py generate l.358] (74/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:36,656 DEBUG generators.py generate l.370] (74/93) Post-process Answer
[2024-03-04 10:53:36,662 INFO generators.py gen_for_qa l.548] (74/93) * Start with LLM "command-nightly"
[2024-03-04 10:53:36,667 DEBUG generators.py generate l.349] (74/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:36,667 DEBUG generators.py generate l.358] (74/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:36,982 DEBUG generators.py generate l.370] (74/93) Post-process Answer
[2024-03-04 10:53:36,983 INFO generators.py gen_for_qa l.548] (74/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:53:36,985 DEBUG generators.py generate l.349] (74/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:36,987 DEBUG generators.py generate l.358] (74/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:37,505 ERROR generators.py complete l.400] (74/93) The following exception occurred with prompt meta={} user=" Où a été découvert le premier site préhistorique ?  A)  Vallée de la Vézère B)  Vallée du Nil C)  Vallée de l'Omo .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:53:37,509 DEBUG generators.py generate l.373] (74/93) Reuse post-processing
[2024-03-04 10:53:37,510 INFO generators.py generate l.477] (74/93) End question " Où a été découvert le premier site préhistorique ?  A)  Vallée de la Vézère B)  Vallée du Nil C)  Vallée de l'Omo "
[2024-03-04 10:53:37,511 INFO generators.py generate l.475] (75/93) *** AnsGenerator for question " Où a été inventé le premier appareil photographique ?  A)  Paris B)  Lacock C)  Nice "
[2024-03-04 10:53:37,514 INFO generators.py gen_for_qa l.548] (75/93) * Start with LLM "gpt-4"
[2024-03-04 10:53:37,514 DEBUG generators.py generate l.349] (75/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:37,517 DEBUG generators.py generate l.358] (75/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:38,322 DEBUG generators.py generate l.370] (75/93) Post-process Answer
[2024-03-04 10:53:38,324 INFO generators.py gen_for_qa l.548] (75/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:53:38,325 DEBUG generators.py generate l.349] (75/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:38,327 DEBUG generators.py generate l.358] (75/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:38,780 DEBUG generators.py generate l.370] (75/93) Post-process Answer
[2024-03-04 10:53:38,781 INFO generators.py gen_for_qa l.548] (75/93) * Start with LLM "gemini-pro"
[2024-03-04 10:53:38,782 DEBUG generators.py generate l.349] (75/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:38,784 DEBUG generators.py generate l.358] (75/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:41,604 DEBUG generators.py generate l.370] (75/93) Post-process Answer
[2024-03-04 10:53:41,605 INFO generators.py gen_for_qa l.548] (75/93) * Start with LLM "claude-2.1"
[2024-03-04 10:53:41,607 DEBUG generators.py generate l.349] (75/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:41,609 DEBUG generators.py generate l.358] (75/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:43,322 DEBUG generators.py generate l.370] (75/93) Post-process Answer
[2024-03-04 10:53:43,327 INFO generators.py gen_for_qa l.548] (75/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:53:43,334 DEBUG generators.py generate l.349] (75/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:43,338 DEBUG generators.py generate l.358] (75/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:45,700 DEBUG generators.py generate l.370] (75/93) Post-process Answer
[2024-03-04 10:53:45,705 INFO generators.py gen_for_qa l.548] (75/93) * Start with LLM "command-nightly"
[2024-03-04 10:53:45,709 DEBUG generators.py generate l.349] (75/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:45,713 DEBUG generators.py generate l.358] (75/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:46,018 DEBUG generators.py generate l.370] (75/93) Post-process Answer
[2024-03-04 10:53:46,018 INFO generators.py gen_for_qa l.548] (75/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:53:46,031 DEBUG generators.py generate l.349] (75/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:46,034 DEBUG generators.py generate l.358] (75/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:46,558 ERROR generators.py complete l.400] (75/93) The following exception occurred with prompt meta={} user=" Où a été inventé le premier appareil photographique ?  A)  Paris B)  Lacock C)  Nice .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:53:46,573 DEBUG generators.py generate l.373] (75/93) Reuse post-processing
[2024-03-04 10:53:46,573 INFO generators.py generate l.477] (75/93) End question " Où a été inventé le premier appareil photographique ?  A)  Paris B)  Lacock C)  Nice "
[2024-03-04 10:53:46,573 INFO generators.py generate l.475] (76/93) *** AnsGenerator for question " Où a été découvert le premier gisement de charbon ?  A)  Newcastle upon Tyne B)  Liège C)  Essen "
[2024-03-04 10:53:46,583 INFO generators.py gen_for_qa l.548] (76/93) * Start with LLM "gpt-4"
[2024-03-04 10:53:46,583 DEBUG generators.py generate l.349] (76/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:46,583 DEBUG generators.py generate l.358] (76/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:47,558 DEBUG generators.py generate l.370] (76/93) Post-process Answer
[2024-03-04 10:53:47,563 INFO generators.py gen_for_qa l.548] (76/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:53:47,566 DEBUG generators.py generate l.349] (76/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:47,569 DEBUG generators.py generate l.358] (76/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:48,276 DEBUG generators.py generate l.370] (76/93) Post-process Answer
[2024-03-04 10:53:48,276 INFO generators.py gen_for_qa l.548] (76/93) * Start with LLM "gemini-pro"
[2024-03-04 10:53:48,276 DEBUG generators.py generate l.349] (76/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:48,289 DEBUG generators.py generate l.358] (76/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:50,367 DEBUG generators.py generate l.370] (76/93) Post-process Answer
[2024-03-04 10:53:50,382 INFO generators.py gen_for_qa l.548] (76/93) * Start with LLM "claude-2.1"
[2024-03-04 10:53:50,382 DEBUG generators.py generate l.349] (76/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:50,382 DEBUG generators.py generate l.358] (76/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:51,758 DEBUG generators.py generate l.370] (76/93) Post-process Answer
[2024-03-04 10:53:51,764 INFO generators.py gen_for_qa l.548] (76/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:53:51,769 DEBUG generators.py generate l.349] (76/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:51,773 DEBUG generators.py generate l.358] (76/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:52,289 DEBUG generators.py generate l.370] (76/93) Post-process Answer
[2024-03-04 10:53:52,289 INFO generators.py gen_for_qa l.548] (76/93) * Start with LLM "command-nightly"
[2024-03-04 10:53:52,289 DEBUG generators.py generate l.349] (76/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:52,289 DEBUG generators.py generate l.358] (76/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:52,565 DEBUG generators.py generate l.370] (76/93) Post-process Answer
[2024-03-04 10:53:52,565 INFO generators.py gen_for_qa l.548] (76/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:53:52,565 DEBUG generators.py generate l.349] (76/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:52,565 DEBUG generators.py generate l.358] (76/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:53,008 ERROR generators.py complete l.400] (76/93) The following exception occurred with prompt meta={} user=" Où a été découvert le premier gisement de charbon ?  A)  Newcastle upon Tyne B)  Liège C)  Essen .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:53:53,039 DEBUG generators.py generate l.373] (76/93) Reuse post-processing
[2024-03-04 10:53:53,041 INFO generators.py generate l.477] (76/93) End question " Où a été découvert le premier gisement de charbon ?  A)  Newcastle upon Tyne B)  Liège C)  Essen "
[2024-03-04 10:53:53,041 INFO generators.py generate l.475] (77/93) *** AnsGenerator for question " Où a été inventé le premier bateau à vapeur ?  A)  Paris B)  Glasgow C)  Philadelphie "
[2024-03-04 10:53:53,041 INFO generators.py gen_for_qa l.548] (77/93) * Start with LLM "gpt-4"
[2024-03-04 10:53:53,055 DEBUG generators.py generate l.349] (77/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:53,055 DEBUG generators.py generate l.358] (77/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:53,971 DEBUG generators.py generate l.370] (77/93) Post-process Answer
[2024-03-04 10:53:53,977 INFO generators.py gen_for_qa l.548] (77/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:53:53,982 DEBUG generators.py generate l.349] (77/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:53,986 DEBUG generators.py generate l.358] (77/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:55,047 DEBUG generators.py generate l.370] (77/93) Post-process Answer
[2024-03-04 10:53:55,049 INFO generators.py gen_for_qa l.548] (77/93) * Start with LLM "gemini-pro"
[2024-03-04 10:53:55,052 DEBUG generators.py generate l.349] (77/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:55,055 DEBUG generators.py generate l.358] (77/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:57,514 DEBUG generators.py generate l.370] (77/93) Post-process Answer
[2024-03-04 10:53:57,532 INFO generators.py gen_for_qa l.548] (77/93) * Start with LLM "claude-2.1"
[2024-03-04 10:53:57,532 DEBUG generators.py generate l.349] (77/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:57,532 DEBUG generators.py generate l.358] (77/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:58,643 DEBUG generators.py generate l.370] (77/93) Post-process Answer
[2024-03-04 10:53:58,644 INFO generators.py gen_for_qa l.548] (77/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:53:58,644 DEBUG generators.py generate l.349] (77/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:58,644 DEBUG generators.py generate l.358] (77/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:59,047 DEBUG generators.py generate l.370] (77/93) Post-process Answer
[2024-03-04 10:53:59,047 INFO generators.py gen_for_qa l.548] (77/93) * Start with LLM "command-nightly"
[2024-03-04 10:53:59,061 DEBUG generators.py generate l.349] (77/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:59,064 DEBUG generators.py generate l.358] (77/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:53:59,622 DEBUG generators.py generate l.370] (77/93) Post-process Answer
[2024-03-04 10:53:59,624 INFO generators.py gen_for_qa l.548] (77/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:53:59,626 DEBUG generators.py generate l.349] (77/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:53:59,629 DEBUG generators.py generate l.358] (77/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:00,115 ERROR generators.py complete l.400] (77/93) The following exception occurred with prompt meta={} user=" Où a été inventé le premier bateau à vapeur ?  A)  Paris B)  Glasgow C)  Philadelphie .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:54:00,117 DEBUG generators.py generate l.373] (77/93) Reuse post-processing
[2024-03-04 10:54:00,117 INFO generators.py generate l.477] (77/93) End question " Où a été inventé le premier bateau à vapeur ?  A)  Paris B)  Glasgow C)  Philadelphie "
[2024-03-04 10:54:00,117 INFO generators.py generate l.475] (78/93) *** AnsGenerator for question " Où a été découvert le premier système d'écriture ?  A)  Uruk B)  Sumer C)  Susa "
[2024-03-04 10:54:00,117 INFO generators.py gen_for_qa l.548] (78/93) * Start with LLM "gpt-4"
[2024-03-04 10:54:00,123 DEBUG generators.py generate l.349] (78/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:00,126 DEBUG generators.py generate l.358] (78/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:01,009 DEBUG generators.py generate l.370] (78/93) Post-process Answer
[2024-03-04 10:54:01,011 INFO generators.py gen_for_qa l.548] (78/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:54:01,012 DEBUG generators.py generate l.349] (78/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:01,013 DEBUG generators.py generate l.358] (78/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:01,993 DEBUG generators.py generate l.370] (78/93) Post-process Answer
[2024-03-04 10:54:01,993 INFO generators.py gen_for_qa l.548] (78/93) * Start with LLM "gemini-pro"
[2024-03-04 10:54:02,007 DEBUG generators.py generate l.349] (78/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:02,007 DEBUG generators.py generate l.358] (78/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:04,303 DEBUG generators.py generate l.370] (78/93) Post-process Answer
[2024-03-04 10:54:04,311 INFO generators.py gen_for_qa l.548] (78/93) * Start with LLM "claude-2.1"
[2024-03-04 10:54:04,315 DEBUG generators.py generate l.349] (78/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:04,319 DEBUG generators.py generate l.358] (78/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:05,539 DEBUG generators.py generate l.370] (78/93) Post-process Answer
[2024-03-04 10:54:05,539 INFO generators.py gen_for_qa l.548] (78/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:54:05,539 DEBUG generators.py generate l.349] (78/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:05,555 DEBUG generators.py generate l.358] (78/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:06,087 DEBUG generators.py generate l.370] (78/93) Post-process Answer
[2024-03-04 10:54:06,102 INFO generators.py gen_for_qa l.548] (78/93) * Start with LLM "command-nightly"
[2024-03-04 10:54:06,108 DEBUG generators.py generate l.349] (78/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:06,108 DEBUG generators.py generate l.358] (78/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:06,493 DEBUG generators.py generate l.370] (78/93) Post-process Answer
[2024-03-04 10:54:06,493 INFO generators.py gen_for_qa l.548] (78/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:54:06,511 DEBUG generators.py generate l.349] (78/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:06,516 DEBUG generators.py generate l.358] (78/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:06,976 ERROR generators.py complete l.400] (78/93) The following exception occurred with prompt meta={} user=" Où a été découvert le premier système d'écriture ?  A)  Uruk B)  Sumer C)  Susa .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:54:06,992 DEBUG generators.py generate l.373] (78/93) Reuse post-processing
[2024-03-04 10:54:06,992 INFO generators.py generate l.477] (78/93) End question " Où a été découvert le premier système d'écriture ?  A)  Uruk B)  Sumer C)  Susa "
[2024-03-04 10:54:06,992 INFO generators.py generate l.475] (79/93) *** AnsGenerator for question " Où a été inventé le premier réseau ferroviaire ?  A)  Stockton et Darlington B)  Liverpool et Manchester C)  Baltimore et Ohio "
[2024-03-04 10:54:06,992 INFO generators.py gen_for_qa l.548] (79/93) * Start with LLM "gpt-4"
[2024-03-04 10:54:07,007 DEBUG generators.py generate l.349] (79/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:07,007 DEBUG generators.py generate l.358] (79/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:08,003 DEBUG generators.py generate l.370] (79/93) Post-process Answer
[2024-03-04 10:54:08,007 INFO generators.py gen_for_qa l.548] (79/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:54:08,010 DEBUG generators.py generate l.349] (79/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:08,010 DEBUG generators.py generate l.358] (79/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:09,366 DEBUG generators.py generate l.370] (79/93) Post-process Answer
[2024-03-04 10:54:09,366 INFO generators.py gen_for_qa l.548] (79/93) * Start with LLM "gemini-pro"
[2024-03-04 10:54:09,366 DEBUG generators.py generate l.349] (79/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:09,384 DEBUG generators.py generate l.358] (79/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:11,986 DEBUG generators.py generate l.370] (79/93) Post-process Answer
[2024-03-04 10:54:11,991 INFO generators.py gen_for_qa l.548] (79/93) * Start with LLM "claude-2.1"
[2024-03-04 10:54:11,993 DEBUG generators.py generate l.349] (79/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:11,993 DEBUG generators.py generate l.358] (79/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:13,320 DEBUG generators.py generate l.370] (79/93) Post-process Answer
[2024-03-04 10:54:13,336 INFO generators.py gen_for_qa l.548] (79/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:54:13,336 DEBUG generators.py generate l.349] (79/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:13,336 DEBUG generators.py generate l.358] (79/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:14,195 DEBUG generators.py generate l.370] (79/93) Post-process Answer
[2024-03-04 10:54:14,211 INFO generators.py gen_for_qa l.548] (79/93) * Start with LLM "command-nightly"
[2024-03-04 10:54:14,211 DEBUG generators.py generate l.349] (79/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:14,211 DEBUG generators.py generate l.358] (79/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:14,573 DEBUG generators.py generate l.370] (79/93) Post-process Answer
[2024-03-04 10:54:14,573 INFO generators.py gen_for_qa l.548] (79/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:54:14,573 DEBUG generators.py generate l.349] (79/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:14,587 DEBUG generators.py generate l.358] (79/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:15,195 ERROR generators.py complete l.400] (79/93) The following exception occurred with prompt meta={} user=" Où a été inventé le premier réseau ferroviaire ?  A)  Stockton et Darlington B)  Liverpool et Manchester C)  Baltimore et Ohio .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:54:15,219 DEBUG generators.py generate l.373] (79/93) Reuse post-processing
[2024-03-04 10:54:15,223 INFO generators.py generate l.477] (79/93) End question " Où a été inventé le premier réseau ferroviaire ?  A)  Stockton et Darlington B)  Liverpool et Manchester C)  Baltimore et Ohio "
[2024-03-04 10:54:15,227 INFO generators.py generate l.475] (80/93) *** AnsGenerator for question " Qui a découvert l'Amérique (pour les Européens) ?  A)  Christophe Colomb B)  Leif Erikson C)  Amerigo Vespucci "
[2024-03-04 10:54:15,228 INFO generators.py gen_for_qa l.548] (80/93) * Start with LLM "gpt-4"
[2024-03-04 10:54:15,228 DEBUG generators.py generate l.349] (80/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:15,228 DEBUG generators.py generate l.358] (80/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:15,875 DEBUG generators.py generate l.370] (80/93) Post-process Answer
[2024-03-04 10:54:15,878 INFO generators.py gen_for_qa l.548] (80/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:54:15,880 DEBUG generators.py generate l.349] (80/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:15,881 DEBUG generators.py generate l.358] (80/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:16,645 DEBUG generators.py generate l.370] (80/93) Post-process Answer
[2024-03-04 10:54:16,649 INFO generators.py gen_for_qa l.548] (80/93) * Start with LLM "gemini-pro"
[2024-03-04 10:54:16,649 DEBUG generators.py generate l.349] (80/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:16,649 DEBUG generators.py generate l.358] (80/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:19,180 DEBUG generators.py generate l.370] (80/93) Post-process Answer
[2024-03-04 10:54:19,197 INFO generators.py gen_for_qa l.548] (80/93) * Start with LLM "claude-2.1"
[2024-03-04 10:54:19,197 DEBUG generators.py generate l.349] (80/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:19,197 DEBUG generators.py generate l.358] (80/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:20,726 DEBUG generators.py generate l.370] (80/93) Post-process Answer
[2024-03-04 10:54:20,726 INFO generators.py gen_for_qa l.548] (80/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:54:20,726 DEBUG generators.py generate l.349] (80/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:20,742 DEBUG generators.py generate l.358] (80/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:21,108 DEBUG generators.py generate l.370] (80/93) Post-process Answer
[2024-03-04 10:54:21,108 INFO generators.py gen_for_qa l.548] (80/93) * Start with LLM "command-nightly"
[2024-03-04 10:54:21,118 DEBUG generators.py generate l.349] (80/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:21,120 DEBUG generators.py generate l.358] (80/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:21,399 DEBUG generators.py generate l.370] (80/93) Post-process Answer
[2024-03-04 10:54:21,399 INFO generators.py gen_for_qa l.548] (80/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:54:21,399 DEBUG generators.py generate l.349] (80/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:21,399 DEBUG generators.py generate l.358] (80/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:22,044 ERROR generators.py complete l.400] (80/93) The following exception occurred with prompt meta={} user=" Qui a découvert l'Amérique (pour les Européens) ?  A)  Christophe Colomb B)  Leif Erikson C)  Amerigo Vespucci .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:54:22,044 DEBUG generators.py generate l.373] (80/93) Reuse post-processing
[2024-03-04 10:54:22,060 INFO generators.py generate l.477] (80/93) End question " Qui a découvert l'Amérique (pour les Européens) ?  A)  Christophe Colomb B)  Leif Erikson C)  Amerigo Vespucci "
[2024-03-04 10:54:22,060 INFO generators.py generate l.475] (81/93) *** AnsGenerator for question " Qui a découvert l'Australie (pour les Européens) ?  A)  James Cook B)  Willem Janszoon C)  Abel Tasman "
[2024-03-04 10:54:22,060 INFO generators.py gen_for_qa l.548] (81/93) * Start with LLM "gpt-4"
[2024-03-04 10:54:22,060 DEBUG generators.py generate l.349] (81/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:22,076 DEBUG generators.py generate l.358] (81/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:22,872 DEBUG generators.py generate l.370] (81/93) Post-process Answer
[2024-03-04 10:54:22,875 INFO generators.py gen_for_qa l.548] (81/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:54:22,879 DEBUG generators.py generate l.349] (81/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:22,881 DEBUG generators.py generate l.358] (81/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:23,680 DEBUG generators.py generate l.370] (81/93) Post-process Answer
[2024-03-04 10:54:23,697 INFO generators.py gen_for_qa l.548] (81/93) * Start with LLM "gemini-pro"
[2024-03-04 10:54:23,697 DEBUG generators.py generate l.349] (81/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:23,697 DEBUG generators.py generate l.358] (81/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:25,829 DEBUG generators.py generate l.370] (81/93) Post-process Answer
[2024-03-04 10:54:25,832 INFO generators.py gen_for_qa l.548] (81/93) * Start with LLM "claude-2.1"
[2024-03-04 10:54:25,834 DEBUG generators.py generate l.349] (81/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:25,837 DEBUG generators.py generate l.358] (81/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:27,021 DEBUG generators.py generate l.370] (81/93) Post-process Answer
[2024-03-04 10:54:27,027 INFO generators.py gen_for_qa l.548] (81/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:54:27,029 DEBUG generators.py generate l.349] (81/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:27,031 DEBUG generators.py generate l.358] (81/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:27,595 DEBUG generators.py generate l.370] (81/93) Post-process Answer
[2024-03-04 10:54:27,599 INFO generators.py gen_for_qa l.548] (81/93) * Start with LLM "command-nightly"
[2024-03-04 10:54:27,602 DEBUG generators.py generate l.349] (81/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:27,606 DEBUG generators.py generate l.358] (81/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:28,058 DEBUG generators.py generate l.370] (81/93) Post-process Answer
[2024-03-04 10:54:28,058 INFO generators.py gen_for_qa l.548] (81/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:54:28,058 DEBUG generators.py generate l.349] (81/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:28,058 DEBUG generators.py generate l.358] (81/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:28,570 ERROR generators.py complete l.400] (81/93) The following exception occurred with prompt meta={} user=" Qui a découvert l'Australie (pour les Européens) ?  A)  James Cook B)  Willem Janszoon C)  Abel Tasman .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:54:28,586 DEBUG generators.py generate l.373] (81/93) Reuse post-processing
[2024-03-04 10:54:28,595 INFO generators.py generate l.477] (81/93) End question " Qui a découvert l'Australie (pour les Européens) ?  A)  James Cook B)  Willem Janszoon C)  Abel Tasman "
[2024-03-04 10:54:28,597 INFO generators.py generate l.475] (82/93) *** AnsGenerator for question " Qui a découvert l'Antarctique ?  A)  Fabian von Bellingshausen B)  Edward Bransfield C)  Nathaniel Palmer "
[2024-03-04 10:54:28,597 INFO generators.py gen_for_qa l.548] (82/93) * Start with LLM "gpt-4"
[2024-03-04 10:54:28,601 DEBUG generators.py generate l.349] (82/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:28,601 DEBUG generators.py generate l.358] (82/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:29,773 DEBUG generators.py generate l.370] (82/93) Post-process Answer
[2024-03-04 10:54:29,777 INFO generators.py gen_for_qa l.548] (82/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:54:29,777 DEBUG generators.py generate l.349] (82/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:29,777 DEBUG generators.py generate l.358] (82/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:30,789 DEBUG generators.py generate l.370] (82/93) Post-process Answer
[2024-03-04 10:54:30,789 INFO generators.py gen_for_qa l.548] (82/93) * Start with LLM "gemini-pro"
[2024-03-04 10:54:30,789 DEBUG generators.py generate l.349] (82/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:30,789 DEBUG generators.py generate l.358] (82/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:34,692 DEBUG generators.py generate l.370] (82/93) Post-process Answer
[2024-03-04 10:54:34,695 INFO generators.py gen_for_qa l.548] (82/93) * Start with LLM "claude-2.1"
[2024-03-04 10:54:34,699 DEBUG generators.py generate l.349] (82/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:34,700 DEBUG generators.py generate l.358] (82/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:35,898 DEBUG generators.py generate l.370] (82/93) Post-process Answer
[2024-03-04 10:54:35,917 INFO generators.py gen_for_qa l.548] (82/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:54:35,917 DEBUG generators.py generate l.349] (82/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:35,917 DEBUG generators.py generate l.358] (82/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:38,932 DEBUG generators.py generate l.370] (82/93) Post-process Answer
[2024-03-04 10:54:38,936 INFO generators.py gen_for_qa l.548] (82/93) * Start with LLM "command-nightly"
[2024-03-04 10:54:38,940 DEBUG generators.py generate l.349] (82/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:38,943 DEBUG generators.py generate l.358] (82/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:39,437 DEBUG generators.py generate l.370] (82/93) Post-process Answer
[2024-03-04 10:54:39,449 INFO generators.py gen_for_qa l.548] (82/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:54:39,453 DEBUG generators.py generate l.349] (82/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:39,455 DEBUG generators.py generate l.358] (82/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:40,020 ERROR generators.py complete l.400] (82/93) The following exception occurred with prompt meta={} user=" Qui a découvert l'Antarctique ?  A)  Fabian von Bellingshausen B)  Edward Bransfield C)  Nathaniel Palmer .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:54:40,023 DEBUG generators.py generate l.373] (82/93) Reuse post-processing
[2024-03-04 10:54:40,023 INFO generators.py generate l.477] (82/93) End question " Qui a découvert l'Antarctique ?  A)  Fabian von Bellingshausen B)  Edward Bransfield C)  Nathaniel Palmer "
[2024-03-04 10:54:40,023 INFO generators.py generate l.475] (83/93) *** AnsGenerator for question " Qui a découvert le Canada (pour les Européens) ?  A)  John Cabot B)  Jacques Cartier C)  Giovanni Verrazzano "
[2024-03-04 10:54:40,023 INFO generators.py gen_for_qa l.548] (83/93) * Start with LLM "gpt-4"
[2024-03-04 10:54:40,039 DEBUG generators.py generate l.349] (83/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:40,039 DEBUG generators.py generate l.358] (83/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:40,878 DEBUG generators.py generate l.370] (83/93) Post-process Answer
[2024-03-04 10:54:40,881 INFO generators.py gen_for_qa l.548] (83/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:54:40,883 DEBUG generators.py generate l.349] (83/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:40,885 DEBUG generators.py generate l.358] (83/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:42,203 DEBUG generators.py generate l.370] (83/93) Post-process Answer
[2024-03-04 10:54:42,208 INFO generators.py gen_for_qa l.548] (83/93) * Start with LLM "gemini-pro"
[2024-03-04 10:54:42,211 DEBUG generators.py generate l.349] (83/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:42,211 DEBUG generators.py generate l.358] (83/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:46,524 DEBUG generators.py generate l.370] (83/93) Post-process Answer
[2024-03-04 10:54:46,540 INFO generators.py gen_for_qa l.548] (83/93) * Start with LLM "claude-2.1"
[2024-03-04 10:54:46,548 DEBUG generators.py generate l.349] (83/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:46,548 DEBUG generators.py generate l.358] (83/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:47,887 DEBUG generators.py generate l.370] (83/93) Post-process Answer
[2024-03-04 10:54:47,890 INFO generators.py gen_for_qa l.548] (83/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:54:47,890 DEBUG generators.py generate l.349] (83/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:47,890 DEBUG generators.py generate l.358] (83/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:48,398 DEBUG generators.py generate l.370] (83/93) Post-process Answer
[2024-03-04 10:54:48,398 INFO generators.py gen_for_qa l.548] (83/93) * Start with LLM "command-nightly"
[2024-03-04 10:54:48,398 DEBUG generators.py generate l.349] (83/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:48,398 DEBUG generators.py generate l.358] (83/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:48,771 DEBUG generators.py generate l.370] (83/93) Post-process Answer
[2024-03-04 10:54:48,775 INFO generators.py gen_for_qa l.548] (83/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:54:48,778 DEBUG generators.py generate l.349] (83/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:48,790 DEBUG generators.py generate l.358] (83/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:49,304 ERROR generators.py complete l.400] (83/93) The following exception occurred with prompt meta={} user=" Qui a découvert le Canada (pour les Européens) ?  A)  John Cabot B)  Jacques Cartier C)  Giovanni Verrazzano .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:54:49,318 DEBUG generators.py generate l.373] (83/93) Reuse post-processing
[2024-03-04 10:54:49,322 INFO generators.py generate l.477] (83/93) End question " Qui a découvert le Canada (pour les Européens) ?  A)  John Cabot B)  Jacques Cartier C)  Giovanni Verrazzano "
[2024-03-04 10:54:49,324 INFO generators.py generate l.475] (84/93) *** AnsGenerator for question " Qui a découvert les îles Galápagos ?  A)  Tomás de Berlanga B)  Francis Drake "
[2024-03-04 10:54:49,328 INFO generators.py gen_for_qa l.548] (84/93) * Start with LLM "gpt-4"
[2024-03-04 10:54:49,331 DEBUG generators.py generate l.349] (84/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:49,336 DEBUG generators.py generate l.358] (84/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:49,946 DEBUG generators.py generate l.370] (84/93) Post-process Answer
[2024-03-04 10:54:49,950 INFO generators.py gen_for_qa l.548] (84/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:54:49,951 DEBUG generators.py generate l.349] (84/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:49,951 DEBUG generators.py generate l.358] (84/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:50,539 DEBUG generators.py generate l.370] (84/93) Post-process Answer
[2024-03-04 10:54:50,539 INFO generators.py gen_for_qa l.548] (84/93) * Start with LLM "gemini-pro"
[2024-03-04 10:54:50,555 DEBUG generators.py generate l.349] (84/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:50,555 DEBUG generators.py generate l.358] (84/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:54,182 DEBUG generators.py generate l.370] (84/93) Post-process Answer
[2024-03-04 10:54:54,197 INFO generators.py gen_for_qa l.548] (84/93) * Start with LLM "claude-2.1"
[2024-03-04 10:54:54,197 DEBUG generators.py generate l.349] (84/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:54,197 DEBUG generators.py generate l.358] (84/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:55,570 DEBUG generators.py generate l.370] (84/93) Post-process Answer
[2024-03-04 10:54:55,586 INFO generators.py gen_for_qa l.548] (84/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:54:55,586 DEBUG generators.py generate l.349] (84/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:55,586 DEBUG generators.py generate l.358] (84/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:56,055 DEBUG generators.py generate l.370] (84/93) Post-process Answer
[2024-03-04 10:54:56,055 INFO generators.py gen_for_qa l.548] (84/93) * Start with LLM "command-nightly"
[2024-03-04 10:54:56,055 DEBUG generators.py generate l.349] (84/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:56,055 DEBUG generators.py generate l.358] (84/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:56,418 DEBUG generators.py generate l.370] (84/93) Post-process Answer
[2024-03-04 10:54:56,422 INFO generators.py gen_for_qa l.548] (84/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:54:56,425 DEBUG generators.py generate l.349] (84/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:56,428 DEBUG generators.py generate l.358] (84/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:57,418 ERROR generators.py complete l.400] (84/93) The following exception occurred with prompt meta={} user=" Qui a découvert les îles Galápagos ?  A)  Tomás de Berlanga B)  Francis Drake .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:54:57,428 DEBUG generators.py generate l.373] (84/93) Reuse post-processing
[2024-03-04 10:54:57,431 INFO generators.py generate l.477] (84/93) End question " Qui a découvert les îles Galápagos ?  A)  Tomás de Berlanga B)  Francis Drake "
[2024-03-04 10:54:57,432 INFO generators.py generate l.475] (85/93) *** AnsGenerator for question " Qui a découvert l'Amérique centrale (pour les Européens) ?  A)  Christophe Colomb B)  Vasco Núñez de Balboa "
[2024-03-04 10:54:57,434 INFO generators.py gen_for_qa l.548] (85/93) * Start with LLM "gpt-4"
[2024-03-04 10:54:57,435 DEBUG generators.py generate l.349] (85/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:57,435 DEBUG generators.py generate l.358] (85/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:58,133 DEBUG generators.py generate l.370] (85/93) Post-process Answer
[2024-03-04 10:54:58,133 INFO generators.py gen_for_qa l.548] (85/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:54:58,149 DEBUG generators.py generate l.349] (85/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:58,149 DEBUG generators.py generate l.358] (85/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:54:58,724 DEBUG generators.py generate l.370] (85/93) Post-process Answer
[2024-03-04 10:54:58,726 INFO generators.py gen_for_qa l.548] (85/93) * Start with LLM "gemini-pro"
[2024-03-04 10:54:58,726 DEBUG generators.py generate l.349] (85/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:54:58,726 DEBUG generators.py generate l.358] (85/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:55:02,480 DEBUG generators.py generate l.370] (85/93) Post-process Answer
[2024-03-04 10:55:02,493 INFO generators.py gen_for_qa l.548] (85/93) * Start with LLM "claude-2.1"
[2024-03-04 10:55:02,501 DEBUG generators.py generate l.349] (85/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:55:02,502 DEBUG generators.py generate l.358] (85/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:55:03,774 DEBUG generators.py generate l.370] (85/93) Post-process Answer
[2024-03-04 10:55:03,789 INFO generators.py gen_for_qa l.548] (85/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:55:03,789 DEBUG generators.py generate l.349] (85/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:55:03,789 DEBUG generators.py generate l.358] (85/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:55:06,883 DEBUG generators.py generate l.370] (85/93) Post-process Answer
[2024-03-04 10:55:06,884 INFO generators.py gen_for_qa l.548] (85/93) * Start with LLM "command-nightly"
[2024-03-04 10:55:06,884 DEBUG generators.py generate l.349] (85/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:55:06,884 DEBUG generators.py generate l.358] (85/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:55:07,432 DEBUG generators.py generate l.370] (85/93) Post-process Answer
[2024-03-04 10:55:07,433 INFO generators.py gen_for_qa l.548] (85/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:55:07,436 DEBUG generators.py generate l.349] (85/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:55:07,437 DEBUG generators.py generate l.358] (85/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:55:08,032 ERROR generators.py complete l.400] (85/93) The following exception occurred with prompt meta={} user=" Qui a découvert l'Amérique centrale (pour les Européens) ?  A)  Christophe Colomb B)  Vasco Núñez de Balboa .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:55:08,044 DEBUG generators.py generate l.373] (85/93) Reuse post-processing
[2024-03-04 10:55:08,047 INFO generators.py generate l.477] (85/93) End question " Qui a découvert l'Amérique centrale (pour les Européens) ?  A)  Christophe Colomb B)  Vasco Núñez de Balboa "
[2024-03-04 10:55:08,050 INFO generators.py generate l.475] (86/93) *** AnsGenerator for question " Qui a découvert la Nouvelle-Zélande (pour les Européens) ?  A)  Abel Tasman B)  James Cook C)  Jean-François-Marie de Surville "
[2024-03-04 10:55:08,053 INFO generators.py gen_for_qa l.548] (86/93) * Start with LLM "gpt-4"
[2024-03-04 10:55:08,057 DEBUG generators.py generate l.349] (86/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:55:08,057 DEBUG generators.py generate l.358] (86/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:55:08,805 DEBUG generators.py generate l.370] (86/93) Post-process Answer
[2024-03-04 10:55:08,805 INFO generators.py gen_for_qa l.548] (86/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:55:08,805 DEBUG generators.py generate l.349] (86/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:55:08,805 DEBUG generators.py generate l.358] (86/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:55:09,870 DEBUG generators.py generate l.370] (86/93) Post-process Answer
[2024-03-04 10:55:09,883 INFO generators.py gen_for_qa l.548] (86/93) * Start with LLM "gemini-pro"
[2024-03-04 10:55:09,883 DEBUG generators.py generate l.349] (86/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:55:09,883 DEBUG generators.py generate l.358] (86/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:55:13,508 DEBUG generators.py generate l.370] (86/93) Post-process Answer
[2024-03-04 10:55:13,508 INFO generators.py gen_for_qa l.548] (86/93) * Start with LLM "claude-2.1"
[2024-03-04 10:55:13,508 DEBUG generators.py generate l.349] (86/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:55:13,508 DEBUG generators.py generate l.358] (86/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:55:14,524 DEBUG generators.py generate l.370] (86/93) Post-process Answer
[2024-03-04 10:55:14,538 INFO generators.py gen_for_qa l.548] (86/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:55:14,540 DEBUG generators.py generate l.349] (86/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:55:14,540 DEBUG generators.py generate l.358] (86/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:55:15,024 DEBUG generators.py generate l.370] (86/93) Post-process Answer
[2024-03-04 10:55:15,034 INFO generators.py gen_for_qa l.548] (86/93) * Start with LLM "command-nightly"
[2024-03-04 10:55:15,034 DEBUG generators.py generate l.349] (86/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:55:15,040 DEBUG generators.py generate l.358] (86/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:55:15,334 DEBUG generators.py generate l.370] (86/93) Post-process Answer
[2024-03-04 10:55:15,338 INFO generators.py gen_for_qa l.548] (86/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:55:15,338 DEBUG generators.py generate l.349] (86/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:55:15,338 DEBUG generators.py generate l.358] (86/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:55:15,773 ERROR generators.py complete l.400] (86/93) The following exception occurred with prompt meta={} user=" Qui a découvert la Nouvelle-Zélande (pour les Européens) ?  A)  Abel Tasman B)  James Cook C)  Jean-François-Marie de Surville .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:55:15,787 DEBUG generators.py generate l.373] (86/93) Reuse post-processing
[2024-03-04 10:55:15,787 INFO generators.py generate l.477] (86/93) End question " Qui a découvert la Nouvelle-Zélande (pour les Européens) ?  A)  Abel Tasman B)  James Cook C)  Jean-François-Marie de Surville "
[2024-03-04 10:55:15,787 INFO generators.py generate l.475] (87/93) *** AnsGenerator for question " Qui a découvert le fleuve Amazone ?  A)  Francisco de Orellana B)  Vicente Yáñez Pinzón C)  Diego de Ordaz "
[2024-03-04 10:55:15,793 INFO generators.py gen_for_qa l.548] (87/93) * Start with LLM "gpt-4"
[2024-03-04 10:55:15,793 DEBUG generators.py generate l.349] (87/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:55:15,793 DEBUG generators.py generate l.358] (87/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:55:16,929 DEBUG generators.py generate l.370] (87/93) Post-process Answer
[2024-03-04 10:55:16,933 INFO generators.py gen_for_qa l.548] (87/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:55:16,935 DEBUG generators.py generate l.349] (87/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:55:16,938 DEBUG generators.py generate l.358] (87/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:55:18,028 DEBUG generators.py generate l.370] (87/93) Post-process Answer
[2024-03-04 10:55:18,031 INFO generators.py gen_for_qa l.548] (87/93) * Start with LLM "gemini-pro"
[2024-03-04 10:55:18,033 DEBUG generators.py generate l.349] (87/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:55:18,036 DEBUG generators.py generate l.358] (87/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:55:21,665 DEBUG generators.py generate l.370] (87/93) Post-process Answer
[2024-03-04 10:55:21,671 INFO generators.py gen_for_qa l.548] (87/93) * Start with LLM "claude-2.1"
[2024-03-04 10:55:21,675 DEBUG generators.py generate l.349] (87/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:55:21,677 DEBUG generators.py generate l.358] (87/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:55:22,775 DEBUG generators.py generate l.370] (87/93) Post-process Answer
[2024-03-04 10:55:22,777 INFO generators.py gen_for_qa l.548] (87/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:55:22,778 DEBUG generators.py generate l.349] (87/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:55:22,779 DEBUG generators.py generate l.358] (87/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:55:23,169 DEBUG generators.py generate l.370] (87/93) Post-process Answer
[2024-03-04 10:55:23,171 INFO generators.py gen_for_qa l.548] (87/93) * Start with LLM "command-nightly"
[2024-03-04 10:55:23,172 DEBUG generators.py generate l.349] (87/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:55:23,173 DEBUG generators.py generate l.358] (87/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:55:23,418 DEBUG generators.py generate l.370] (87/93) Post-process Answer
[2024-03-04 10:55:23,420 INFO generators.py gen_for_qa l.548] (87/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:55:23,420 DEBUG generators.py generate l.349] (87/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:55:23,420 DEBUG generators.py generate l.358] (87/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:55:23,882 ERROR generators.py complete l.400] (87/93) The following exception occurred with prompt meta={} user=" Qui a découvert le fleuve Amazone ?  A)  Francisco de Orellana B)  Vicente Yáñez Pinzón C)  Diego de Ordaz .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:55:23,885 DEBUG generators.py generate l.373] (87/93) Reuse post-processing
[2024-03-04 10:55:23,885 INFO generators.py generate l.477] (87/93) End question " Qui a découvert le fleuve Amazone ?  A)  Francisco de Orellana B)  Vicente Yáñez Pinzón C)  Diego de Ordaz "
[2024-03-04 10:55:23,889 INFO generators.py generate l.475] (88/93) *** AnsGenerator for question " Qui a découvert le détroit de Magellan ?  A)  Fernand de Magellan B)  Francisco de Almeida C)  Estêvão Gomes "
[2024-03-04 10:55:23,889 INFO generators.py gen_for_qa l.548] (88/93) * Start with LLM "gpt-4"
[2024-03-04 10:55:23,889 DEBUG generators.py generate l.349] (88/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:55:23,896 DEBUG generators.py generate l.358] (88/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:55:24,746 DEBUG generators.py generate l.370] (88/93) Post-process Answer
[2024-03-04 10:55:24,749 INFO generators.py gen_for_qa l.548] (88/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:55:24,749 DEBUG generators.py generate l.349] (88/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:55:24,749 DEBUG generators.py generate l.358] (88/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:55:25,850 DEBUG generators.py generate l.370] (88/93) Post-process Answer
[2024-03-04 10:55:25,850 INFO generators.py gen_for_qa l.548] (88/93) * Start with LLM "gemini-pro"
[2024-03-04 10:55:25,850 DEBUG generators.py generate l.349] (88/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:55:25,850 DEBUG generators.py generate l.358] (88/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:55:29,870 DEBUG generators.py generate l.370] (88/93) Post-process Answer
[2024-03-04 10:55:29,873 INFO generators.py gen_for_qa l.548] (88/93) * Start with LLM "claude-2.1"
[2024-03-04 10:55:29,873 DEBUG generators.py generate l.349] (88/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:55:29,873 DEBUG generators.py generate l.358] (88/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:55:30,941 DEBUG generators.py generate l.370] (88/93) Post-process Answer
[2024-03-04 10:55:30,942 INFO generators.py gen_for_qa l.548] (88/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:55:30,945 DEBUG generators.py generate l.349] (88/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:55:30,946 DEBUG generators.py generate l.358] (88/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:55:31,301 DEBUG generators.py generate l.370] (88/93) Post-process Answer
[2024-03-04 10:55:31,301 INFO generators.py gen_for_qa l.548] (88/93) * Start with LLM "command-nightly"
[2024-03-04 10:55:31,301 DEBUG generators.py generate l.349] (88/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:55:31,301 DEBUG generators.py generate l.358] (88/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:55:31,587 DEBUG generators.py generate l.370] (88/93) Post-process Answer
[2024-03-04 10:55:31,587 INFO generators.py gen_for_qa l.548] (88/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:55:31,602 DEBUG generators.py generate l.349] (88/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:55:31,602 DEBUG generators.py generate l.358] (88/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:55:58,554 ERROR generators.py complete l.400] (88/93) The following exception occurred with prompt meta={} user=" Qui a découvert le détroit de Magellan ?  A)  Fernand de Magellan B)  Francisco de Almeida C)  Estêvão Gomes .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:55:58,554 DEBUG generators.py generate l.373] (88/93) Reuse post-processing
[2024-03-04 10:55:58,562 INFO generators.py generate l.477] (88/93) End question " Qui a découvert le détroit de Magellan ?  A)  Fernand de Magellan B)  Francisco de Almeida C)  Estêvão Gomes "
[2024-03-04 10:55:58,562 INFO generators.py generate l.475] (89/93) *** AnsGenerator for question " Qui a découvert l'archipel d'Hawaï (pour les Européens) ?  A)  James Cook B)  Juan Gaetano "
[2024-03-04 10:55:58,562 INFO generators.py gen_for_qa l.548] (89/93) * Start with LLM "gpt-4"
[2024-03-04 10:55:58,562 DEBUG generators.py generate l.349] (89/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:55:58,562 DEBUG generators.py generate l.358] (89/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:55:59,564 DEBUG generators.py generate l.370] (89/93) Post-process Answer
[2024-03-04 10:55:59,565 INFO generators.py gen_for_qa l.548] (89/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:55:59,568 DEBUG generators.py generate l.349] (89/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:55:59,569 DEBUG generators.py generate l.358] (89/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:56:00,025 DEBUG generators.py generate l.370] (89/93) Post-process Answer
[2024-03-04 10:56:00,036 INFO generators.py gen_for_qa l.548] (89/93) * Start with LLM "gemini-pro"
[2024-03-04 10:56:00,040 DEBUG generators.py generate l.349] (89/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:56:00,040 DEBUG generators.py generate l.358] (89/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:56:04,050 DEBUG generators.py generate l.370] (89/93) Post-process Answer
[2024-03-04 10:56:04,068 INFO generators.py gen_for_qa l.548] (89/93) * Start with LLM "claude-2.1"
[2024-03-04 10:56:04,068 DEBUG generators.py generate l.349] (89/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:56:04,079 DEBUG generators.py generate l.358] (89/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:56:05,085 DEBUG generators.py generate l.370] (89/93) Post-process Answer
[2024-03-04 10:56:05,085 INFO generators.py gen_for_qa l.548] (89/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:56:05,085 DEBUG generators.py generate l.349] (89/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:56:05,101 DEBUG generators.py generate l.358] (89/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:56:05,414 DEBUG generators.py generate l.370] (89/93) Post-process Answer
[2024-03-04 10:56:05,417 INFO generators.py gen_for_qa l.548] (89/93) * Start with LLM "command-nightly"
[2024-03-04 10:56:05,417 DEBUG generators.py generate l.349] (89/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:56:05,417 DEBUG generators.py generate l.358] (89/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:56:05,742 DEBUG generators.py generate l.370] (89/93) Post-process Answer
[2024-03-04 10:56:05,748 INFO generators.py gen_for_qa l.548] (89/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:56:05,752 DEBUG generators.py generate l.349] (89/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:56:05,755 DEBUG generators.py generate l.358] (89/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:56:06,208 ERROR generators.py complete l.400] (89/93) The following exception occurred with prompt meta={} user=" Qui a découvert l'archipel d'Hawaï (pour les Européens) ?  A)  James Cook B)  Juan Gaetano .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:56:06,214 DEBUG generators.py generate l.373] (89/93) Reuse post-processing
[2024-03-04 10:56:06,214 INFO generators.py generate l.477] (89/93) End question " Qui a découvert l'archipel d'Hawaï (pour les Européens) ?  A)  James Cook B)  Juan Gaetano "
[2024-03-04 10:56:06,216 INFO generators.py generate l.475] (90/93) *** AnsGenerator for question " Qui a découvert le passage du Nord-Ouest ?  A)  Roald Amundsen B)  John Franklin C)  Robert McClure "
[2024-03-04 10:56:06,216 INFO generators.py gen_for_qa l.548] (90/93) * Start with LLM "gpt-4"
[2024-03-04 10:56:06,216 DEBUG generators.py generate l.349] (90/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:56:06,216 DEBUG generators.py generate l.358] (90/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:56:06,821 DEBUG generators.py generate l.370] (90/93) Post-process Answer
[2024-03-04 10:56:06,837 INFO generators.py gen_for_qa l.548] (90/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:56:06,837 DEBUG generators.py generate l.349] (90/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:56:06,837 DEBUG generators.py generate l.358] (90/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:56:07,875 DEBUG generators.py generate l.370] (90/93) Post-process Answer
[2024-03-04 10:56:07,878 INFO generators.py gen_for_qa l.548] (90/93) * Start with LLM "gemini-pro"
[2024-03-04 10:56:07,880 DEBUG generators.py generate l.349] (90/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:56:07,882 DEBUG generators.py generate l.358] (90/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:56:11,448 DEBUG generators.py generate l.370] (90/93) Post-process Answer
[2024-03-04 10:56:11,450 INFO generators.py gen_for_qa l.548] (90/93) * Start with LLM "claude-2.1"
[2024-03-04 10:56:11,451 DEBUG generators.py generate l.349] (90/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:56:11,452 DEBUG generators.py generate l.358] (90/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:56:12,828 DEBUG generators.py generate l.370] (90/93) Post-process Answer
[2024-03-04 10:56:12,833 INFO generators.py gen_for_qa l.548] (90/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:56:12,835 DEBUG generators.py generate l.349] (90/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:56:12,835 DEBUG generators.py generate l.358] (90/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:56:13,315 DEBUG generators.py generate l.370] (90/93) Post-process Answer
[2024-03-04 10:56:13,315 INFO generators.py gen_for_qa l.548] (90/93) * Start with LLM "command-nightly"
[2024-03-04 10:56:13,324 DEBUG generators.py generate l.349] (90/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:56:13,328 DEBUG generators.py generate l.358] (90/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:56:13,617 DEBUG generators.py generate l.370] (90/93) Post-process Answer
[2024-03-04 10:56:13,633 INFO generators.py gen_for_qa l.548] (90/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:56:13,633 DEBUG generators.py generate l.349] (90/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:56:13,633 DEBUG generators.py generate l.358] (90/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:56:14,117 ERROR generators.py complete l.400] (90/93) The following exception occurred with prompt meta={} user=" Qui a découvert le passage du Nord-Ouest ?  A)  Roald Amundsen B)  John Franklin C)  Robert McClure .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:56:14,117 DEBUG generators.py generate l.373] (90/93) Reuse post-processing
[2024-03-04 10:56:14,133 INFO generators.py generate l.477] (90/93) End question " Qui a découvert le passage du Nord-Ouest ?  A)  Roald Amundsen B)  John Franklin C)  Robert McClure "
[2024-03-04 10:56:14,133 INFO generators.py generate l.475] (91/93) *** AnsGenerator for question " Qui a découvert le cap de Bonne-Espérance ?  A)  Bartolomeu Dias B)  Vasco da Gama "
[2024-03-04 10:56:14,133 INFO generators.py gen_for_qa l.548] (91/93) * Start with LLM "gpt-4"
[2024-03-04 10:56:14,133 DEBUG generators.py generate l.349] (91/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:56:14,133 DEBUG generators.py generate l.358] (91/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:56:14,779 DEBUG generators.py generate l.370] (91/93) Post-process Answer
[2024-03-04 10:56:14,781 INFO generators.py gen_for_qa l.548] (91/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:56:14,783 DEBUG generators.py generate l.349] (91/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:56:14,786 DEBUG generators.py generate l.358] (91/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:56:15,415 DEBUG generators.py generate l.370] (91/93) Post-process Answer
[2024-03-04 10:56:15,415 INFO generators.py gen_for_qa l.548] (91/93) * Start with LLM "gemini-pro"
[2024-03-04 10:56:15,417 DEBUG generators.py generate l.349] (91/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:56:15,418 DEBUG generators.py generate l.358] (91/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:56:19,155 DEBUG generators.py generate l.370] (91/93) Post-process Answer
[2024-03-04 10:56:19,155 INFO generators.py gen_for_qa l.548] (91/93) * Start with LLM "claude-2.1"
[2024-03-04 10:56:19,155 DEBUG generators.py generate l.349] (91/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:56:19,155 DEBUG generators.py generate l.358] (91/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:56:20,649 DEBUG generators.py generate l.370] (91/93) Post-process Answer
[2024-03-04 10:56:20,664 INFO generators.py gen_for_qa l.548] (91/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:56:20,664 DEBUG generators.py generate l.349] (91/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:56:20,664 DEBUG generators.py generate l.358] (91/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:56:21,282 DEBUG generators.py generate l.370] (91/93) Post-process Answer
[2024-03-04 10:56:21,283 INFO generators.py gen_for_qa l.548] (91/93) * Start with LLM "command-nightly"
[2024-03-04 10:56:21,284 DEBUG generators.py generate l.349] (91/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:56:21,286 DEBUG generators.py generate l.358] (91/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:56:21,538 DEBUG generators.py generate l.370] (91/93) Post-process Answer
[2024-03-04 10:56:21,554 INFO generators.py gen_for_qa l.548] (91/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:56:21,554 DEBUG generators.py generate l.349] (91/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:56:21,554 DEBUG generators.py generate l.358] (91/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:56:45,024 ERROR generators.py complete l.400] (91/93) The following exception occurred with prompt meta={} user=" Qui a découvert le cap de Bonne-Espérance ?  A)  Bartolomeu Dias B)  Vasco da Gama .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:56:45,040 DEBUG generators.py generate l.373] (91/93) Reuse post-processing
[2024-03-04 10:56:45,040 INFO generators.py generate l.477] (91/93) End question " Qui a découvert le cap de Bonne-Espérance ?  A)  Bartolomeu Dias B)  Vasco da Gama "
[2024-03-04 10:56:45,049 INFO generators.py generate l.475] (92/93) *** AnsGenerator for question " Qui a découvert la source du Nil ?  A)  John Hanning Speke B)  Richard Francis Burton C)  Samuel Baker "
[2024-03-04 10:56:45,049 INFO generators.py gen_for_qa l.548] (92/93) * Start with LLM "gpt-4"
[2024-03-04 10:56:45,057 DEBUG generators.py generate l.349] (92/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:56:45,057 DEBUG generators.py generate l.358] (92/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:56:46,941 DEBUG generators.py generate l.370] (92/93) Post-process Answer
[2024-03-04 10:56:46,943 INFO generators.py gen_for_qa l.548] (92/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:56:46,945 DEBUG generators.py generate l.349] (92/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:56:46,945 DEBUG generators.py generate l.358] (92/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:56:48,087 DEBUG generators.py generate l.370] (92/93) Post-process Answer
[2024-03-04 10:56:48,087 INFO generators.py gen_for_qa l.548] (92/93) * Start with LLM "gemini-pro"
[2024-03-04 10:56:48,087 DEBUG generators.py generate l.349] (92/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:56:48,087 DEBUG generators.py generate l.358] (92/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:56:52,606 DEBUG generators.py generate l.370] (92/93) Post-process Answer
[2024-03-04 10:56:52,611 INFO generators.py gen_for_qa l.548] (92/93) * Start with LLM "claude-2.1"
[2024-03-04 10:56:52,615 DEBUG generators.py generate l.349] (92/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:56:52,618 DEBUG generators.py generate l.358] (92/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:56:54,148 DEBUG generators.py generate l.370] (92/93) Post-process Answer
[2024-03-04 10:56:54,153 INFO generators.py gen_for_qa l.548] (92/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:56:54,156 DEBUG generators.py generate l.349] (92/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:56:54,158 DEBUG generators.py generate l.358] (92/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:56:54,977 DEBUG generators.py generate l.370] (92/93) Post-process Answer
[2024-03-04 10:56:54,993 INFO generators.py gen_for_qa l.548] (92/93) * Start with LLM "command-nightly"
[2024-03-04 10:56:54,993 DEBUG generators.py generate l.349] (92/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:56:54,993 DEBUG generators.py generate l.358] (92/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:56:55,273 DEBUG generators.py generate l.370] (92/93) Post-process Answer
[2024-03-04 10:56:55,290 INFO generators.py gen_for_qa l.548] (92/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:56:55,290 DEBUG generators.py generate l.349] (92/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:56:55,290 DEBUG generators.py generate l.358] (92/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:56:55,869 ERROR generators.py complete l.400] (92/93) The following exception occurred with prompt meta={} user=" Qui a découvert la source du Nil ?  A)  John Hanning Speke B)  Richard Francis Burton C)  Samuel Baker .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:56:55,889 DEBUG generators.py generate l.373] (92/93) Reuse post-processing
[2024-03-04 10:56:55,893 INFO generators.py generate l.477] (92/93) End question " Qui a découvert la source du Nil ?  A)  John Hanning Speke B)  Richard Francis Burton C)  Samuel Baker "
[2024-03-04 10:56:55,896 INFO generators.py generate l.475] (93/93) *** AnsGenerator for question " Qui a découvert la Terre de Feu ?  A)  Ferdinand Magellan B)  Hernando de Magallanes "
[2024-03-04 10:56:55,898 INFO generators.py gen_for_qa l.548] (93/93) * Start with LLM "gpt-4"
[2024-03-04 10:56:55,903 DEBUG generators.py generate l.349] (93/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:56:55,903 DEBUG generators.py generate l.358] (93/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:56:56,791 DEBUG generators.py generate l.370] (93/93) Post-process Answer
[2024-03-04 10:56:56,791 INFO generators.py gen_for_qa l.548] (93/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 10:56:56,791 DEBUG generators.py generate l.349] (93/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:56:56,791 DEBUG generators.py generate l.358] (93/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:56:57,445 DEBUG generators.py generate l.370] (93/93) Post-process Answer
[2024-03-04 10:56:57,463 INFO generators.py gen_for_qa l.548] (93/93) * Start with LLM "gemini-pro"
[2024-03-04 10:56:57,463 DEBUG generators.py generate l.349] (93/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:56:57,463 DEBUG generators.py generate l.358] (93/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:57:01,087 DEBUG generators.py generate l.370] (93/93) Post-process Answer
[2024-03-04 10:57:01,087 INFO generators.py gen_for_qa l.548] (93/93) * Start with LLM "claude-2.1"
[2024-03-04 10:57:01,087 DEBUG generators.py generate l.349] (93/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:57:01,087 DEBUG generators.py generate l.358] (93/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:57:02,428 DEBUG generators.py generate l.370] (93/93) Post-process Answer
[2024-03-04 10:57:02,429 INFO generators.py gen_for_qa l.548] (93/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 10:57:02,429 DEBUG generators.py generate l.349] (93/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:57:02,440 DEBUG generators.py generate l.358] (93/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:57:03,009 DEBUG generators.py generate l.370] (93/93) Post-process Answer
[2024-03-04 10:57:03,009 INFO generators.py gen_for_qa l.548] (93/93) * Start with LLM "command-nightly"
[2024-03-04 10:57:03,009 DEBUG generators.py generate l.349] (93/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:57:03,024 DEBUG generators.py generate l.358] (93/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:57:03,352 DEBUG generators.py generate l.370] (93/93) Post-process Answer
[2024-03-04 10:57:03,368 INFO generators.py gen_for_qa l.548] (93/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 10:57:03,368 DEBUG generators.py generate l.349] (93/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 10:57:03,368 DEBUG generators.py generate l.358] (93/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 10:57:21,368 ERROR generators.py complete l.400] (93/93) The following exception occurred with prompt meta={} user=" Qui a découvert la Terre de Feu ?  A)  Ferdinand Magellan B)  Hernando de Magallanes .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7752, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7542, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 10:57:21,385 DEBUG generators.py generate l.373] (93/93) Reuse post-processing
[2024-03-04 10:57:21,385 INFO generators.py generate l.477] (93/93) End question " Qui a découvert la Terre de Feu ?  A)  Ferdinand Magellan B)  Hernando de Magallanes "
[2024-03-04 10:57:21,446 INFO expe.py save_to_json l.283] (93/93) Expe saved as JSON to expe\Answers\culture_fr_v1_gen_with_LeChat--93Q_0C_0F_7M_558A_0HE_0AE_2024-03-04_10,57,21.json
[2024-03-04 10:57:21,460 INFO main.py <module> l.99] (93/93) MAIN ENDS
[2024-03-04 11:02:20,132 INFO main.py <module> l.87] MAIN STARTS
[2024-03-04 11:02:20,306 INFO expe.py save_to_html l.296] Expe saved as HTML to expe\Answers\culture_fr_v1_gen_with_LeChat--93Q_0C_0F_7M_558A_0HE_0AE_2024-03-04_11,02,20.html
[2024-03-04 11:02:20,311 INFO main.py <module> l.99] MAIN ENDS
[2024-03-04 11:21:02,858 INFO main.py <module> l.87] MAIN STARTS
[2024-03-04 11:21:35,817 INFO main.py <module> l.87] MAIN STARTS
[2024-03-04 11:21:37,098 INFO expe.py save_to_spreadsheet l.376] Expe saved as Spreadsheet to expe\Answers\culture_fr_v1_gen_with_LeChat--93Q_0C_0F_7M_558A_0HE_0AE_2024-03-04_11,21,35.xlsx
[2024-03-04 11:21:37,107 INFO main.py <module> l.100] MAIN ENDS
[2024-03-04 11:41:28,796 INFO main.py <module> l.87] MAIN STARTS
[2024-03-04 11:41:28,812 INFO generators.py generate l.475] (1/93) *** AnsGenerator for question "Qui a inventé le télégraphe électrique ?  A)  Samuel Morse B)  Charles Wheatstone C)  William Fothergill Cooke "
[2024-03-04 11:41:28,813 INFO generators.py gen_for_qa l.548] (1/93) * Start with LLM "gpt-4"
[2024-03-04 11:41:28,815 DEBUG generators.py generate l.349] (1/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:41:28,815 DEBUG generators.py generate l.358] (1/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:41:29,462 DEBUG generators.py generate l.370] (1/93) Post-process Answer
[2024-03-04 11:41:29,464 INFO generators.py gen_for_qa l.548] (1/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:41:29,467 DEBUG generators.py generate l.349] (1/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:41:29,468 DEBUG generators.py generate l.358] (1/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:41:30,130 DEBUG generators.py generate l.370] (1/93) Post-process Answer
[2024-03-04 11:41:30,131 INFO generators.py gen_for_qa l.548] (1/93) * Start with LLM "gemini-pro"
[2024-03-04 11:41:30,135 DEBUG generators.py generate l.349] (1/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:41:30,137 DEBUG generators.py generate l.358] (1/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:41:41,614 DEBUG generators.py generate l.370] (1/93) Post-process Answer
[2024-03-04 11:41:41,619 INFO generators.py gen_for_qa l.548] (1/93) * Start with LLM "claude-2.1"
[2024-03-04 11:41:41,621 DEBUG generators.py generate l.349] (1/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:41:41,626 DEBUG generators.py generate l.358] (1/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:41:42,673 DEBUG generators.py generate l.370] (1/93) Post-process Answer
[2024-03-04 11:41:42,673 INFO generators.py gen_for_qa l.548] (1/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:41:42,675 DEBUG generators.py generate l.349] (1/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:41:42,676 DEBUG generators.py generate l.358] (1/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:41:43,001 DEBUG generators.py generate l.370] (1/93) Post-process Answer
[2024-03-04 11:41:43,004 INFO generators.py gen_for_qa l.548] (1/93) * Start with LLM "command-nightly"
[2024-03-04 11:41:43,005 DEBUG generators.py generate l.349] (1/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:41:43,008 DEBUG generators.py generate l.358] (1/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:41:43,305 DEBUG generators.py generate l.370] (1/93) Post-process Answer
[2024-03-04 11:41:43,310 INFO generators.py gen_for_qa l.548] (1/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:41:43,313 DEBUG generators.py generate l.349] (1/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:41:43,316 DEBUG generators.py generate l.358] (1/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:41:43,753 ERROR generators.py complete l.400] (1/93) The following exception occurred with prompt meta={} user="Qui a inventé le télégraphe électrique ?  A)  Samuel Morse B)  Charles Wheatstone C)  William Fothergill Cooke .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:41:43,773 DEBUG generators.py generate l.373] (1/93) Reuse post-processing
[2024-03-04 11:41:43,777 INFO generators.py generate l.477] (1/93) End question "Qui a inventé le télégraphe électrique ?  A)  Samuel Morse B)  Charles Wheatstone C)  William Fothergill Cooke "
[2024-03-04 11:41:43,778 INFO generators.py generate l.475] (2/93) *** AnsGenerator for question "Qui a inventé la photographie ?  A)  Louis Daguerre B)  William Henry Fox Talbot "
[2024-03-04 11:41:43,781 INFO generators.py gen_for_qa l.548] (2/93) * Start with LLM "gpt-4"
[2024-03-04 11:41:43,781 DEBUG generators.py generate l.349] (2/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:41:43,781 DEBUG generators.py generate l.358] (2/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:41:44,699 DEBUG generators.py generate l.370] (2/93) Post-process Answer
[2024-03-04 11:41:44,704 INFO generators.py gen_for_qa l.548] (2/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:41:44,704 DEBUG generators.py generate l.349] (2/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:41:44,705 DEBUG generators.py generate l.358] (2/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:41:45,321 DEBUG generators.py generate l.370] (2/93) Post-process Answer
[2024-03-04 11:41:45,322 INFO generators.py gen_for_qa l.548] (2/93) * Start with LLM "gemini-pro"
[2024-03-04 11:41:45,327 DEBUG generators.py generate l.349] (2/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:41:45,328 DEBUG generators.py generate l.358] (2/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:41:48,891 DEBUG generators.py generate l.370] (2/93) Post-process Answer
[2024-03-04 11:41:48,893 INFO generators.py gen_for_qa l.548] (2/93) * Start with LLM "claude-2.1"
[2024-03-04 11:41:48,894 DEBUG generators.py generate l.349] (2/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:41:48,895 DEBUG generators.py generate l.358] (2/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:41:49,991 DEBUG generators.py generate l.370] (2/93) Post-process Answer
[2024-03-04 11:41:49,991 INFO generators.py gen_for_qa l.548] (2/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:41:49,995 DEBUG generators.py generate l.349] (2/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:41:49,996 DEBUG generators.py generate l.358] (2/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:41:50,432 DEBUG generators.py generate l.370] (2/93) Post-process Answer
[2024-03-04 11:41:50,434 INFO generators.py gen_for_qa l.548] (2/93) * Start with LLM "command-nightly"
[2024-03-04 11:41:50,436 DEBUG generators.py generate l.349] (2/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:41:50,437 DEBUG generators.py generate l.358] (2/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:41:50,692 DEBUG generators.py generate l.370] (2/93) Post-process Answer
[2024-03-04 11:41:50,707 INFO generators.py gen_for_qa l.548] (2/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:41:50,711 DEBUG generators.py generate l.349] (2/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:41:50,713 DEBUG generators.py generate l.358] (2/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:41:51,122 ERROR generators.py complete l.400] (2/93) The following exception occurred with prompt meta={} user="Qui a inventé la photographie ?  A)  Louis Daguerre B)  William Henry Fox Talbot .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:41:51,136 DEBUG generators.py generate l.373] (2/93) Reuse post-processing
[2024-03-04 11:41:51,139 INFO generators.py generate l.477] (2/93) End question "Qui a inventé la photographie ?  A)  Louis Daguerre B)  William Henry Fox Talbot "
[2024-03-04 11:41:51,140 INFO generators.py generate l.475] (3/93) *** AnsGenerator for question "Qui a inventé le moteur à vapeur ?  A)  Thomas Newcomen B)  Denis Papin "
[2024-03-04 11:41:51,143 INFO generators.py gen_for_qa l.548] (3/93) * Start with LLM "gpt-4"
[2024-03-04 11:41:51,144 DEBUG generators.py generate l.349] (3/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:41:51,147 DEBUG generators.py generate l.358] (3/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:41:51,773 DEBUG generators.py generate l.370] (3/93) Post-process Answer
[2024-03-04 11:41:51,775 INFO generators.py gen_for_qa l.548] (3/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:41:51,777 DEBUG generators.py generate l.349] (3/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:41:51,779 DEBUG generators.py generate l.358] (3/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:41:52,305 DEBUG generators.py generate l.370] (3/93) Post-process Answer
[2024-03-04 11:41:52,308 INFO generators.py gen_for_qa l.548] (3/93) * Start with LLM "gemini-pro"
[2024-03-04 11:41:52,310 DEBUG generators.py generate l.349] (3/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:41:52,312 DEBUG generators.py generate l.358] (3/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:41:55,996 DEBUG generators.py generate l.370] (3/93) Post-process Answer
[2024-03-04 11:41:56,001 INFO generators.py gen_for_qa l.548] (3/93) * Start with LLM "claude-2.1"
[2024-03-04 11:41:56,005 DEBUG generators.py generate l.349] (3/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:41:56,006 DEBUG generators.py generate l.358] (3/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:41:57,152 DEBUG generators.py generate l.370] (3/93) Post-process Answer
[2024-03-04 11:41:57,155 INFO generators.py gen_for_qa l.548] (3/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:41:57,158 DEBUG generators.py generate l.349] (3/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:41:57,159 DEBUG generators.py generate l.358] (3/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:41:57,538 DEBUG generators.py generate l.370] (3/93) Post-process Answer
[2024-03-04 11:41:57,541 INFO generators.py gen_for_qa l.548] (3/93) * Start with LLM "command-nightly"
[2024-03-04 11:41:57,544 DEBUG generators.py generate l.349] (3/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:41:57,545 DEBUG generators.py generate l.358] (3/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:41:57,807 DEBUG generators.py generate l.370] (3/93) Post-process Answer
[2024-03-04 11:41:57,816 INFO generators.py gen_for_qa l.548] (3/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:41:57,817 DEBUG generators.py generate l.349] (3/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:41:57,819 DEBUG generators.py generate l.358] (3/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:41:58,224 ERROR generators.py complete l.400] (3/93) The following exception occurred with prompt meta={} user="Qui a inventé le moteur à vapeur ?  A)  Thomas Newcomen B)  Denis Papin .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:41:58,229 DEBUG generators.py generate l.373] (3/93) Reuse post-processing
[2024-03-04 11:41:58,229 INFO generators.py generate l.477] (3/93) End question "Qui a inventé le moteur à vapeur ?  A)  Thomas Newcomen B)  Denis Papin "
[2024-03-04 11:41:58,230 INFO generators.py generate l.475] (4/93) *** AnsGenerator for question "Qui a inventé le bateau à vapeur ?  A)  James Watt B)  Claude de Jouffroy d'Abbans "
[2024-03-04 11:41:58,231 INFO generators.py gen_for_qa l.548] (4/93) * Start with LLM "gpt-4"
[2024-03-04 11:41:58,232 DEBUG generators.py generate l.349] (4/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:41:58,232 DEBUG generators.py generate l.358] (4/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:41:59,266 DEBUG generators.py generate l.370] (4/93) Post-process Answer
[2024-03-04 11:41:59,270 INFO generators.py gen_for_qa l.548] (4/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:41:59,272 DEBUG generators.py generate l.349] (4/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:41:59,273 DEBUG generators.py generate l.358] (4/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:42:00,306 DEBUG generators.py generate l.370] (4/93) Post-process Answer
[2024-03-04 11:42:00,306 INFO generators.py gen_for_qa l.548] (4/93) * Start with LLM "gemini-pro"
[2024-03-04 11:42:00,310 DEBUG generators.py generate l.349] (4/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:42:00,311 DEBUG generators.py generate l.358] (4/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:42:14,782 INFO main.py <module> l.87] MAIN STARTS
[2024-03-04 11:42:42,021 INFO generators.py generate l.475] (1/93) *** AnsGenerator for question "Qui a inventé le télégraphe électrique ?  A)  Samuel Morse B)  Charles Wheatstone C)  William Fothergill Cooke "
[2024-03-04 11:42:47,580 INFO generators.py gen_for_qa l.548] (1/93) * Start with LLM "gpt-4"
[2024-03-04 11:43:41,247 INFO main.py <module> l.87] MAIN STARTS
[2024-03-04 11:44:19,446 INFO main.py <module> l.87] MAIN STARTS
[2024-03-04 11:44:19,449 INFO generators.py generate l.475] (1/93) *** AnsGenerator for question "Qui a inventé le télégraphe électrique ?  A)  Samuel Morse B)  Charles Wheatstone C)  William Fothergill Cooke "
[2024-03-04 11:44:19,465 INFO generators.py gen_for_qa l.548] (1/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:19,466 DEBUG generators.py gen_for_qa l.554] (1/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:19,467 DEBUG generators.py generate l.352] (1/93) Reuse existing Prompt
[2024-03-04 11:44:19,468 DEBUG generators.py generate l.365] (1/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:19,469 DEBUG generators.py generate l.373] (1/93) Reuse post-processing
[2024-03-04 11:44:19,470 INFO generators.py gen_for_qa l.548] (1/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:19,471 DEBUG generators.py gen_for_qa l.554] (1/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:19,472 DEBUG generators.py generate l.352] (1/93) Reuse existing Prompt
[2024-03-04 11:44:19,473 DEBUG generators.py generate l.365] (1/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:19,473 DEBUG generators.py generate l.373] (1/93) Reuse post-processing
[2024-03-04 11:44:19,475 INFO generators.py gen_for_qa l.548] (1/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:19,476 DEBUG generators.py gen_for_qa l.554] (1/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:19,477 DEBUG generators.py generate l.352] (1/93) Reuse existing Prompt
[2024-03-04 11:44:19,477 DEBUG generators.py generate l.365] (1/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:19,477 DEBUG generators.py generate l.373] (1/93) Reuse post-processing
[2024-03-04 11:44:19,478 INFO generators.py gen_for_qa l.548] (1/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:19,478 DEBUG generators.py gen_for_qa l.554] (1/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:19,478 DEBUG generators.py generate l.352] (1/93) Reuse existing Prompt
[2024-03-04 11:44:19,479 DEBUG generators.py generate l.365] (1/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:19,479 DEBUG generators.py generate l.373] (1/93) Reuse post-processing
[2024-03-04 11:44:19,479 INFO generators.py gen_for_qa l.548] (1/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:19,479 DEBUG generators.py gen_for_qa l.554] (1/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:19,479 DEBUG generators.py generate l.352] (1/93) Reuse existing Prompt
[2024-03-04 11:44:19,479 DEBUG generators.py generate l.365] (1/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:19,479 DEBUG generators.py generate l.373] (1/93) Reuse post-processing
[2024-03-04 11:44:19,479 INFO generators.py gen_for_qa l.548] (1/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:19,479 DEBUG generators.py gen_for_qa l.554] (1/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:19,485 DEBUG generators.py generate l.352] (1/93) Reuse existing Prompt
[2024-03-04 11:44:19,485 DEBUG generators.py generate l.365] (1/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:19,486 DEBUG generators.py generate l.373] (1/93) Reuse post-processing
[2024-03-04 11:44:19,486 INFO generators.py gen_for_qa l.548] (1/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:19,487 DEBUG generators.py generate l.349] (1/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:19,489 DEBUG generators.py generate l.358] (1/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:20,091 ERROR generators.py complete l.400] (1/93) The following exception occurred with prompt meta={} user="Qui a inventé le télégraphe électrique ?  A)  Samuel Morse B)  Charles Wheatstone C)  William Fothergill Cooke .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:20,109 DEBUG generators.py generate l.373] (1/93) Reuse post-processing
[2024-03-04 11:44:20,111 INFO generators.py generate l.477] (1/93) End question "Qui a inventé le télégraphe électrique ?  A)  Samuel Morse B)  Charles Wheatstone C)  William Fothergill Cooke "
[2024-03-04 11:44:20,113 INFO generators.py generate l.475] (2/93) *** AnsGenerator for question "Qui a inventé la photographie ?  A)  Louis Daguerre B)  William Henry Fox Talbot "
[2024-03-04 11:44:20,114 INFO generators.py gen_for_qa l.548] (2/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:20,117 DEBUG generators.py gen_for_qa l.554] (2/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:20,118 DEBUG generators.py generate l.352] (2/93) Reuse existing Prompt
[2024-03-04 11:44:20,119 DEBUG generators.py generate l.365] (2/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:20,121 DEBUG generators.py generate l.373] (2/93) Reuse post-processing
[2024-03-04 11:44:20,122 INFO generators.py gen_for_qa l.548] (2/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:20,124 DEBUG generators.py gen_for_qa l.554] (2/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:20,126 DEBUG generators.py generate l.352] (2/93) Reuse existing Prompt
[2024-03-04 11:44:20,127 DEBUG generators.py generate l.365] (2/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:20,128 DEBUG generators.py generate l.373] (2/93) Reuse post-processing
[2024-03-04 11:44:20,128 INFO generators.py gen_for_qa l.548] (2/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:20,130 DEBUG generators.py gen_for_qa l.554] (2/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:20,131 DEBUG generators.py generate l.352] (2/93) Reuse existing Prompt
[2024-03-04 11:44:20,132 DEBUG generators.py generate l.365] (2/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:20,132 DEBUG generators.py generate l.373] (2/93) Reuse post-processing
[2024-03-04 11:44:20,134 INFO generators.py gen_for_qa l.548] (2/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:20,135 DEBUG generators.py gen_for_qa l.554] (2/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:20,136 DEBUG generators.py generate l.352] (2/93) Reuse existing Prompt
[2024-03-04 11:44:20,137 DEBUG generators.py generate l.365] (2/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:20,140 DEBUG generators.py generate l.373] (2/93) Reuse post-processing
[2024-03-04 11:44:20,141 INFO generators.py gen_for_qa l.548] (2/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:20,143 DEBUG generators.py gen_for_qa l.554] (2/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:20,144 DEBUG generators.py generate l.352] (2/93) Reuse existing Prompt
[2024-03-04 11:44:20,144 DEBUG generators.py generate l.365] (2/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:20,144 DEBUG generators.py generate l.373] (2/93) Reuse post-processing
[2024-03-04 11:44:20,147 INFO generators.py gen_for_qa l.548] (2/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:20,148 DEBUG generators.py gen_for_qa l.554] (2/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:20,148 DEBUG generators.py generate l.352] (2/93) Reuse existing Prompt
[2024-03-04 11:44:20,149 DEBUG generators.py generate l.365] (2/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:20,151 DEBUG generators.py generate l.373] (2/93) Reuse post-processing
[2024-03-04 11:44:20,151 INFO generators.py gen_for_qa l.548] (2/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:20,153 DEBUG generators.py generate l.349] (2/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:20,154 DEBUG generators.py generate l.358] (2/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:20,553 ERROR generators.py complete l.400] (2/93) The following exception occurred with prompt meta={} user="Qui a inventé la photographie ?  A)  Louis Daguerre B)  William Henry Fox Talbot .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:20,560 DEBUG generators.py generate l.373] (2/93) Reuse post-processing
[2024-03-04 11:44:20,561 INFO generators.py generate l.477] (2/93) End question "Qui a inventé la photographie ?  A)  Louis Daguerre B)  William Henry Fox Talbot "
[2024-03-04 11:44:20,563 INFO generators.py generate l.475] (3/93) *** AnsGenerator for question "Qui a inventé le moteur à vapeur ?  A)  Thomas Newcomen B)  Denis Papin "
[2024-03-04 11:44:20,563 INFO generators.py gen_for_qa l.548] (3/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:20,563 DEBUG generators.py gen_for_qa l.554] (3/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:20,567 DEBUG generators.py generate l.352] (3/93) Reuse existing Prompt
[2024-03-04 11:44:20,567 DEBUG generators.py generate l.365] (3/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:20,567 DEBUG generators.py generate l.373] (3/93) Reuse post-processing
[2024-03-04 11:44:20,567 INFO generators.py gen_for_qa l.548] (3/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:20,570 DEBUG generators.py gen_for_qa l.554] (3/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:20,572 DEBUG generators.py generate l.352] (3/93) Reuse existing Prompt
[2024-03-04 11:44:20,573 DEBUG generators.py generate l.365] (3/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:20,575 DEBUG generators.py generate l.373] (3/93) Reuse post-processing
[2024-03-04 11:44:20,575 INFO generators.py gen_for_qa l.548] (3/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:20,575 DEBUG generators.py gen_for_qa l.554] (3/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:20,578 DEBUG generators.py generate l.352] (3/93) Reuse existing Prompt
[2024-03-04 11:44:20,578 DEBUG generators.py generate l.365] (3/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:20,580 DEBUG generators.py generate l.373] (3/93) Reuse post-processing
[2024-03-04 11:44:20,582 INFO generators.py gen_for_qa l.548] (3/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:20,583 DEBUG generators.py gen_for_qa l.554] (3/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:20,583 DEBUG generators.py generate l.352] (3/93) Reuse existing Prompt
[2024-03-04 11:44:20,585 DEBUG generators.py generate l.365] (3/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:20,585 DEBUG generators.py generate l.373] (3/93) Reuse post-processing
[2024-03-04 11:44:20,586 INFO generators.py gen_for_qa l.548] (3/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:20,587 DEBUG generators.py gen_for_qa l.554] (3/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:20,588 DEBUG generators.py generate l.352] (3/93) Reuse existing Prompt
[2024-03-04 11:44:20,591 DEBUG generators.py generate l.365] (3/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:20,592 DEBUG generators.py generate l.373] (3/93) Reuse post-processing
[2024-03-04 11:44:20,594 INFO generators.py gen_for_qa l.548] (3/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:20,594 DEBUG generators.py gen_for_qa l.554] (3/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:20,595 DEBUG generators.py generate l.352] (3/93) Reuse existing Prompt
[2024-03-04 11:44:20,596 DEBUG generators.py generate l.365] (3/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:20,596 DEBUG generators.py generate l.373] (3/93) Reuse post-processing
[2024-03-04 11:44:20,597 INFO generators.py gen_for_qa l.548] (3/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:20,597 DEBUG generators.py generate l.349] (3/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:20,597 DEBUG generators.py generate l.358] (3/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:21,005 ERROR generators.py complete l.400] (3/93) The following exception occurred with prompt meta={} user="Qui a inventé le moteur à vapeur ?  A)  Thomas Newcomen B)  Denis Papin .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:21,020 DEBUG generators.py generate l.373] (3/93) Reuse post-processing
[2024-03-04 11:44:21,023 INFO generators.py generate l.477] (3/93) End question "Qui a inventé le moteur à vapeur ?  A)  Thomas Newcomen B)  Denis Papin "
[2024-03-04 11:44:21,025 INFO generators.py generate l.475] (4/93) *** AnsGenerator for question "Qui a inventé le bateau à vapeur ?  A)  James Watt B)  Claude de Jouffroy d'Abbans "
[2024-03-04 11:44:21,027 INFO generators.py gen_for_qa l.548] (4/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:21,027 DEBUG generators.py gen_for_qa l.554] (4/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:21,029 DEBUG generators.py generate l.352] (4/93) Reuse existing Prompt
[2024-03-04 11:44:21,029 DEBUG generators.py generate l.365] (4/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:21,033 DEBUG generators.py generate l.373] (4/93) Reuse post-processing
[2024-03-04 11:44:21,034 INFO generators.py gen_for_qa l.548] (4/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:21,036 DEBUG generators.py gen_for_qa l.554] (4/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:21,039 DEBUG generators.py generate l.352] (4/93) Reuse existing Prompt
[2024-03-04 11:44:21,040 DEBUG generators.py generate l.365] (4/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:21,040 DEBUG generators.py generate l.373] (4/93) Reuse post-processing
[2024-03-04 11:44:21,044 INFO generators.py gen_for_qa l.548] (4/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:21,045 DEBUG generators.py gen_for_qa l.554] (4/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:21,046 DEBUG generators.py generate l.352] (4/93) Reuse existing Prompt
[2024-03-04 11:44:21,047 DEBUG generators.py generate l.365] (4/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:21,048 DEBUG generators.py generate l.373] (4/93) Reuse post-processing
[2024-03-04 11:44:21,050 INFO generators.py gen_for_qa l.548] (4/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:21,051 DEBUG generators.py gen_for_qa l.554] (4/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:21,052 DEBUG generators.py generate l.352] (4/93) Reuse existing Prompt
[2024-03-04 11:44:21,053 DEBUG generators.py generate l.365] (4/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:21,055 DEBUG generators.py generate l.373] (4/93) Reuse post-processing
[2024-03-04 11:44:21,057 INFO generators.py gen_for_qa l.548] (4/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:21,058 DEBUG generators.py gen_for_qa l.554] (4/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:21,060 DEBUG generators.py generate l.352] (4/93) Reuse existing Prompt
[2024-03-04 11:44:21,061 DEBUG generators.py generate l.365] (4/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:21,061 DEBUG generators.py generate l.373] (4/93) Reuse post-processing
[2024-03-04 11:44:21,061 INFO generators.py gen_for_qa l.548] (4/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:21,061 DEBUG generators.py gen_for_qa l.554] (4/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:21,065 DEBUG generators.py generate l.352] (4/93) Reuse existing Prompt
[2024-03-04 11:44:21,066 DEBUG generators.py generate l.365] (4/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:21,067 DEBUG generators.py generate l.373] (4/93) Reuse post-processing
[2024-03-04 11:44:21,067 INFO generators.py gen_for_qa l.548] (4/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:21,068 DEBUG generators.py generate l.349] (4/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:21,070 DEBUG generators.py generate l.358] (4/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:21,464 ERROR generators.py complete l.400] (4/93) The following exception occurred with prompt meta={} user="Qui a inventé le bateau à vapeur ?  A)  James Watt B)  Claude de Jouffroy d'Abbans .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:21,478 DEBUG generators.py generate l.373] (4/93) Reuse post-processing
[2024-03-04 11:44:21,480 INFO generators.py generate l.477] (4/93) End question "Qui a inventé le bateau à vapeur ?  A)  James Watt B)  Claude de Jouffroy d'Abbans "
[2024-03-04 11:44:21,484 INFO generators.py generate l.475] (5/93) *** AnsGenerator for question "Qui a inventé le parachute ?  A)  Louis-Sébastien Lenormand B)  Sir George Cayley "
[2024-03-04 11:44:21,487 INFO generators.py gen_for_qa l.548] (5/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:21,489 DEBUG generators.py gen_for_qa l.554] (5/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:21,492 DEBUG generators.py generate l.352] (5/93) Reuse existing Prompt
[2024-03-04 11:44:21,493 DEBUG generators.py generate l.365] (5/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:21,494 DEBUG generators.py generate l.373] (5/93) Reuse post-processing
[2024-03-04 11:44:21,495 INFO generators.py gen_for_qa l.548] (5/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:21,496 DEBUG generators.py gen_for_qa l.554] (5/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:21,497 DEBUG generators.py generate l.352] (5/93) Reuse existing Prompt
[2024-03-04 11:44:21,500 DEBUG generators.py generate l.365] (5/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:21,501 DEBUG generators.py generate l.373] (5/93) Reuse post-processing
[2024-03-04 11:44:21,502 INFO generators.py gen_for_qa l.548] (5/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:21,503 DEBUG generators.py gen_for_qa l.554] (5/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:21,505 DEBUG generators.py generate l.352] (5/93) Reuse existing Prompt
[2024-03-04 11:44:21,507 DEBUG generators.py generate l.365] (5/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:21,507 DEBUG generators.py generate l.373] (5/93) Reuse post-processing
[2024-03-04 11:44:21,510 INFO generators.py gen_for_qa l.548] (5/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:21,511 DEBUG generators.py gen_for_qa l.554] (5/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:21,512 DEBUG generators.py generate l.352] (5/93) Reuse existing Prompt
[2024-03-04 11:44:21,513 DEBUG generators.py generate l.365] (5/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:21,513 DEBUG generators.py generate l.373] (5/93) Reuse post-processing
[2024-03-04 11:44:21,515 INFO generators.py gen_for_qa l.548] (5/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:21,516 DEBUG generators.py gen_for_qa l.554] (5/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:21,516 DEBUG generators.py generate l.352] (5/93) Reuse existing Prompt
[2024-03-04 11:44:21,516 DEBUG generators.py generate l.365] (5/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:21,519 DEBUG generators.py generate l.373] (5/93) Reuse post-processing
[2024-03-04 11:44:21,522 INFO generators.py gen_for_qa l.548] (5/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:21,523 DEBUG generators.py gen_for_qa l.554] (5/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:21,525 DEBUG generators.py generate l.352] (5/93) Reuse existing Prompt
[2024-03-04 11:44:21,526 DEBUG generators.py generate l.365] (5/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:21,527 DEBUG generators.py generate l.373] (5/93) Reuse post-processing
[2024-03-04 11:44:21,529 INFO generators.py gen_for_qa l.548] (5/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:21,529 DEBUG generators.py generate l.349] (5/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:21,530 DEBUG generators.py generate l.358] (5/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:21,975 ERROR generators.py complete l.400] (5/93) The following exception occurred with prompt meta={} user="Qui a inventé le parachute ?  A)  Louis-Sébastien Lenormand B)  Sir George Cayley .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:21,992 DEBUG generators.py generate l.373] (5/93) Reuse post-processing
[2024-03-04 11:44:21,995 INFO generators.py generate l.477] (5/93) End question "Qui a inventé le parachute ?  A)  Louis-Sébastien Lenormand B)  Sir George Cayley "
[2024-03-04 11:44:21,998 INFO generators.py generate l.475] (6/93) *** AnsGenerator for question "Qui a inventé le vélocipède ?  A)  Pierre Michaux B)  Kirkpatrick Macmillan "
[2024-03-04 11:44:21,999 INFO generators.py gen_for_qa l.548] (6/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:22,003 DEBUG generators.py gen_for_qa l.554] (6/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:22,004 DEBUG generators.py generate l.352] (6/93) Reuse existing Prompt
[2024-03-04 11:44:22,006 DEBUG generators.py generate l.365] (6/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:22,008 DEBUG generators.py generate l.373] (6/93) Reuse post-processing
[2024-03-04 11:44:22,009 INFO generators.py gen_for_qa l.548] (6/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:22,011 DEBUG generators.py gen_for_qa l.554] (6/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:22,013 DEBUG generators.py generate l.352] (6/93) Reuse existing Prompt
[2024-03-04 11:44:22,013 DEBUG generators.py generate l.365] (6/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:22,015 DEBUG generators.py generate l.373] (6/93) Reuse post-processing
[2024-03-04 11:44:22,016 INFO generators.py gen_for_qa l.548] (6/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:22,017 DEBUG generators.py gen_for_qa l.554] (6/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:22,019 DEBUG generators.py generate l.352] (6/93) Reuse existing Prompt
[2024-03-04 11:44:22,020 DEBUG generators.py generate l.365] (6/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:22,022 DEBUG generators.py generate l.373] (6/93) Reuse post-processing
[2024-03-04 11:44:22,023 INFO generators.py gen_for_qa l.548] (6/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:22,023 DEBUG generators.py gen_for_qa l.554] (6/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:22,025 DEBUG generators.py generate l.352] (6/93) Reuse existing Prompt
[2024-03-04 11:44:22,027 DEBUG generators.py generate l.365] (6/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:22,027 DEBUG generators.py generate l.373] (6/93) Reuse post-processing
[2024-03-04 11:44:22,029 INFO generators.py gen_for_qa l.548] (6/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:22,029 DEBUG generators.py gen_for_qa l.554] (6/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:22,030 DEBUG generators.py generate l.352] (6/93) Reuse existing Prompt
[2024-03-04 11:44:22,031 DEBUG generators.py generate l.365] (6/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:22,032 DEBUG generators.py generate l.373] (6/93) Reuse post-processing
[2024-03-04 11:44:22,033 INFO generators.py gen_for_qa l.548] (6/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:22,033 DEBUG generators.py gen_for_qa l.554] (6/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:22,033 DEBUG generators.py generate l.352] (6/93) Reuse existing Prompt
[2024-03-04 11:44:22,036 DEBUG generators.py generate l.365] (6/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:22,038 DEBUG generators.py generate l.373] (6/93) Reuse post-processing
[2024-03-04 11:44:22,039 INFO generators.py gen_for_qa l.548] (6/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:22,041 DEBUG generators.py generate l.349] (6/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:22,042 DEBUG generators.py generate l.358] (6/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:22,433 ERROR generators.py complete l.400] (6/93) The following exception occurred with prompt meta={} user="Qui a inventé le vélocipède ?  A)  Pierre Michaux B)  Kirkpatrick Macmillan .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:22,448 DEBUG generators.py generate l.373] (6/93) Reuse post-processing
[2024-03-04 11:44:22,451 INFO generators.py generate l.477] (6/93) End question "Qui a inventé le vélocipède ?  A)  Pierre Michaux B)  Kirkpatrick Macmillan "
[2024-03-04 11:44:22,453 INFO generators.py generate l.475] (7/93) *** AnsGenerator for question "Qui a inventé le cinématographe ?  A)  Auguste et Louis Lumière B)  William Friese-Greene "
[2024-03-04 11:44:22,456 INFO generators.py gen_for_qa l.548] (7/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:22,456 DEBUG generators.py gen_for_qa l.554] (7/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:22,460 DEBUG generators.py generate l.352] (7/93) Reuse existing Prompt
[2024-03-04 11:44:22,461 DEBUG generators.py generate l.365] (7/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:22,463 DEBUG generators.py generate l.373] (7/93) Reuse post-processing
[2024-03-04 11:44:22,464 INFO generators.py gen_for_qa l.548] (7/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:22,464 DEBUG generators.py gen_for_qa l.554] (7/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:22,467 DEBUG generators.py generate l.352] (7/93) Reuse existing Prompt
[2024-03-04 11:44:22,467 DEBUG generators.py generate l.365] (7/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:22,469 DEBUG generators.py generate l.373] (7/93) Reuse post-processing
[2024-03-04 11:44:22,471 INFO generators.py gen_for_qa l.548] (7/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:22,472 DEBUG generators.py gen_for_qa l.554] (7/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:22,474 DEBUG generators.py generate l.352] (7/93) Reuse existing Prompt
[2024-03-04 11:44:22,475 DEBUG generators.py generate l.365] (7/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:22,477 DEBUG generators.py generate l.373] (7/93) Reuse post-processing
[2024-03-04 11:44:22,478 INFO generators.py gen_for_qa l.548] (7/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:22,480 DEBUG generators.py gen_for_qa l.554] (7/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:22,480 DEBUG generators.py generate l.352] (7/93) Reuse existing Prompt
[2024-03-04 11:44:22,480 DEBUG generators.py generate l.365] (7/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:22,483 DEBUG generators.py generate l.373] (7/93) Reuse post-processing
[2024-03-04 11:44:22,483 INFO generators.py gen_for_qa l.548] (7/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:22,485 DEBUG generators.py gen_for_qa l.554] (7/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:22,485 DEBUG generators.py generate l.352] (7/93) Reuse existing Prompt
[2024-03-04 11:44:22,486 DEBUG generators.py generate l.365] (7/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:22,489 DEBUG generators.py generate l.373] (7/93) Reuse post-processing
[2024-03-04 11:44:22,490 INFO generators.py gen_for_qa l.548] (7/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:22,490 DEBUG generators.py gen_for_qa l.554] (7/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:22,493 DEBUG generators.py generate l.352] (7/93) Reuse existing Prompt
[2024-03-04 11:44:22,494 DEBUG generators.py generate l.365] (7/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:22,494 DEBUG generators.py generate l.373] (7/93) Reuse post-processing
[2024-03-04 11:44:22,496 INFO generators.py gen_for_qa l.548] (7/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:22,496 DEBUG generators.py generate l.349] (7/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:22,497 DEBUG generators.py generate l.358] (7/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:22,897 ERROR generators.py complete l.400] (7/93) The following exception occurred with prompt meta={} user="Qui a inventé le cinématographe ?  A)  Auguste et Louis Lumière B)  William Friese-Greene .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:22,908 DEBUG generators.py generate l.373] (7/93) Reuse post-processing
[2024-03-04 11:44:22,912 INFO generators.py generate l.477] (7/93) End question "Qui a inventé le cinématographe ?  A)  Auguste et Louis Lumière B)  William Friese-Greene "
[2024-03-04 11:44:22,915 INFO generators.py generate l.475] (8/93) *** AnsGenerator for question "Qui a inventé la machine à coudre ?  A)  Barthélemy Thimonnier B)  Thomas Saint "
[2024-03-04 11:44:22,917 INFO generators.py gen_for_qa l.548] (8/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:22,918 DEBUG generators.py gen_for_qa l.554] (8/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:22,920 DEBUG generators.py generate l.352] (8/93) Reuse existing Prompt
[2024-03-04 11:44:22,922 DEBUG generators.py generate l.365] (8/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:22,923 DEBUG generators.py generate l.373] (8/93) Reuse post-processing
[2024-03-04 11:44:22,923 INFO generators.py gen_for_qa l.548] (8/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:22,927 DEBUG generators.py gen_for_qa l.554] (8/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:22,928 DEBUG generators.py generate l.352] (8/93) Reuse existing Prompt
[2024-03-04 11:44:22,929 DEBUG generators.py generate l.365] (8/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:22,930 DEBUG generators.py generate l.373] (8/93) Reuse post-processing
[2024-03-04 11:44:22,932 INFO generators.py gen_for_qa l.548] (8/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:22,932 DEBUG generators.py gen_for_qa l.554] (8/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:22,932 DEBUG generators.py generate l.352] (8/93) Reuse existing Prompt
[2024-03-04 11:44:22,935 DEBUG generators.py generate l.365] (8/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:22,937 DEBUG generators.py generate l.373] (8/93) Reuse post-processing
[2024-03-04 11:44:22,938 INFO generators.py gen_for_qa l.548] (8/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:22,939 DEBUG generators.py gen_for_qa l.554] (8/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:22,941 DEBUG generators.py generate l.352] (8/93) Reuse existing Prompt
[2024-03-04 11:44:22,942 DEBUG generators.py generate l.365] (8/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:22,943 DEBUG generators.py generate l.373] (8/93) Reuse post-processing
[2024-03-04 11:44:22,944 INFO generators.py gen_for_qa l.548] (8/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:22,945 DEBUG generators.py gen_for_qa l.554] (8/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:22,946 DEBUG generators.py generate l.352] (8/93) Reuse existing Prompt
[2024-03-04 11:44:22,946 DEBUG generators.py generate l.365] (8/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:22,947 DEBUG generators.py generate l.373] (8/93) Reuse post-processing
[2024-03-04 11:44:22,948 INFO generators.py gen_for_qa l.548] (8/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:22,948 DEBUG generators.py gen_for_qa l.554] (8/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:22,950 DEBUG generators.py generate l.352] (8/93) Reuse existing Prompt
[2024-03-04 11:44:22,950 DEBUG generators.py generate l.365] (8/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:22,952 DEBUG generators.py generate l.373] (8/93) Reuse post-processing
[2024-03-04 11:44:22,953 INFO generators.py gen_for_qa l.548] (8/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:22,955 DEBUG generators.py generate l.349] (8/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:22,957 DEBUG generators.py generate l.358] (8/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:23,338 ERROR generators.py complete l.400] (8/93) The following exception occurred with prompt meta={} user="Qui a inventé la machine à coudre ?  A)  Barthélemy Thimonnier B)  Thomas Saint .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:23,362 DEBUG generators.py generate l.373] (8/93) Reuse post-processing
[2024-03-04 11:44:23,364 INFO generators.py generate l.477] (8/93) End question "Qui a inventé la machine à coudre ?  A)  Barthélemy Thimonnier B)  Thomas Saint "
[2024-03-04 11:44:23,367 INFO generators.py generate l.475] (9/93) *** AnsGenerator for question "Qui a inventé l'ampoule électrique ?  A)  Joseph Swan B)  Thomas Edison C)  Hiram Maxim "
[2024-03-04 11:44:23,371 INFO generators.py gen_for_qa l.548] (9/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:23,373 DEBUG generators.py gen_for_qa l.554] (9/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:23,373 DEBUG generators.py generate l.352] (9/93) Reuse existing Prompt
[2024-03-04 11:44:23,378 DEBUG generators.py generate l.365] (9/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:23,380 DEBUG generators.py generate l.373] (9/93) Reuse post-processing
[2024-03-04 11:44:23,381 INFO generators.py gen_for_qa l.548] (9/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:23,382 DEBUG generators.py gen_for_qa l.554] (9/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:23,384 DEBUG generators.py generate l.352] (9/93) Reuse existing Prompt
[2024-03-04 11:44:23,385 DEBUG generators.py generate l.365] (9/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:23,386 DEBUG generators.py generate l.373] (9/93) Reuse post-processing
[2024-03-04 11:44:23,389 INFO generators.py gen_for_qa l.548] (9/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:23,390 DEBUG generators.py gen_for_qa l.554] (9/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:23,392 DEBUG generators.py generate l.352] (9/93) Reuse existing Prompt
[2024-03-04 11:44:23,393 DEBUG generators.py generate l.365] (9/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:23,394 DEBUG generators.py generate l.373] (9/93) Reuse post-processing
[2024-03-04 11:44:23,394 INFO generators.py gen_for_qa l.548] (9/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:23,397 DEBUG generators.py gen_for_qa l.554] (9/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:23,398 DEBUG generators.py generate l.352] (9/93) Reuse existing Prompt
[2024-03-04 11:44:23,399 DEBUG generators.py generate l.365] (9/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:23,399 DEBUG generators.py generate l.373] (9/93) Reuse post-processing
[2024-03-04 11:44:23,400 INFO generators.py gen_for_qa l.548] (9/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:23,401 DEBUG generators.py gen_for_qa l.554] (9/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:23,403 DEBUG generators.py generate l.352] (9/93) Reuse existing Prompt
[2024-03-04 11:44:23,404 DEBUG generators.py generate l.365] (9/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:23,406 DEBUG generators.py generate l.373] (9/93) Reuse post-processing
[2024-03-04 11:44:23,407 INFO generators.py gen_for_qa l.548] (9/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:23,409 DEBUG generators.py gen_for_qa l.554] (9/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:23,410 DEBUG generators.py generate l.352] (9/93) Reuse existing Prompt
[2024-03-04 11:44:23,410 DEBUG generators.py generate l.365] (9/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:23,411 DEBUG generators.py generate l.373] (9/93) Reuse post-processing
[2024-03-04 11:44:23,411 INFO generators.py gen_for_qa l.548] (9/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:23,411 DEBUG generators.py generate l.349] (9/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:23,414 DEBUG generators.py generate l.358] (9/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:23,807 ERROR generators.py complete l.400] (9/93) The following exception occurred with prompt meta={} user="Qui a inventé l'ampoule électrique ?  A)  Joseph Swan B)  Thomas Edison C)  Hiram Maxim .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:23,814 DEBUG generators.py generate l.373] (9/93) Reuse post-processing
[2024-03-04 11:44:23,817 INFO generators.py generate l.477] (9/93) End question "Qui a inventé l'ampoule électrique ?  A)  Joseph Swan B)  Thomas Edison C)  Hiram Maxim "
[2024-03-04 11:44:23,820 INFO generators.py generate l.475] (10/93) *** AnsGenerator for question "Qui a inventé le téléphone ?  A)  Alexander Graham Bell B)  Elisha Gray C)  Antonio Meucci "
[2024-03-04 11:44:23,823 INFO generators.py gen_for_qa l.548] (10/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:23,825 DEBUG generators.py gen_for_qa l.554] (10/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:23,826 DEBUG generators.py generate l.352] (10/93) Reuse existing Prompt
[2024-03-04 11:44:23,828 DEBUG generators.py generate l.365] (10/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:23,830 DEBUG generators.py generate l.373] (10/93) Reuse post-processing
[2024-03-04 11:44:23,831 INFO generators.py gen_for_qa l.548] (10/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:23,832 DEBUG generators.py gen_for_qa l.554] (10/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:23,833 DEBUG generators.py generate l.352] (10/93) Reuse existing Prompt
[2024-03-04 11:44:23,835 DEBUG generators.py generate l.365] (10/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:23,837 DEBUG generators.py generate l.373] (10/93) Reuse post-processing
[2024-03-04 11:44:23,839 INFO generators.py gen_for_qa l.548] (10/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:23,842 DEBUG generators.py gen_for_qa l.554] (10/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:23,843 DEBUG generators.py generate l.352] (10/93) Reuse existing Prompt
[2024-03-04 11:44:23,844 DEBUG generators.py generate l.365] (10/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:23,846 DEBUG generators.py generate l.373] (10/93) Reuse post-processing
[2024-03-04 11:44:23,848 INFO generators.py gen_for_qa l.548] (10/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:23,849 DEBUG generators.py gen_for_qa l.554] (10/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:23,850 DEBUG generators.py generate l.352] (10/93) Reuse existing Prompt
[2024-03-04 11:44:23,851 DEBUG generators.py generate l.365] (10/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:23,852 DEBUG generators.py generate l.373] (10/93) Reuse post-processing
[2024-03-04 11:44:23,853 INFO generators.py gen_for_qa l.548] (10/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:23,857 DEBUG generators.py gen_for_qa l.554] (10/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:23,858 DEBUG generators.py generate l.352] (10/93) Reuse existing Prompt
[2024-03-04 11:44:23,859 DEBUG generators.py generate l.365] (10/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:23,860 DEBUG generators.py generate l.373] (10/93) Reuse post-processing
[2024-03-04 11:44:23,861 INFO generators.py gen_for_qa l.548] (10/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:23,862 DEBUG generators.py gen_for_qa l.554] (10/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:23,863 DEBUG generators.py generate l.352] (10/93) Reuse existing Prompt
[2024-03-04 11:44:23,863 DEBUG generators.py generate l.365] (10/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:23,863 DEBUG generators.py generate l.373] (10/93) Reuse post-processing
[2024-03-04 11:44:23,866 INFO generators.py gen_for_qa l.548] (10/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:23,866 DEBUG generators.py generate l.349] (10/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:23,866 DEBUG generators.py generate l.358] (10/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:24,257 ERROR generators.py complete l.400] (10/93) The following exception occurred with prompt meta={} user="Qui a inventé le téléphone ?  A)  Alexander Graham Bell B)  Elisha Gray C)  Antonio Meucci .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:24,266 DEBUG generators.py generate l.373] (10/93) Reuse post-processing
[2024-03-04 11:44:24,268 INFO generators.py generate l.477] (10/93) End question "Qui a inventé le téléphone ?  A)  Alexander Graham Bell B)  Elisha Gray C)  Antonio Meucci "
[2024-03-04 11:44:24,271 INFO generators.py generate l.475] (11/93) *** AnsGenerator for question "Qui a inventé la montgolfière ?  A)  Les frères Montgolfier B)  Richard Crosbie "
[2024-03-04 11:44:24,273 INFO generators.py gen_for_qa l.548] (11/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:24,273 DEBUG generators.py gen_for_qa l.554] (11/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:24,277 DEBUG generators.py generate l.352] (11/93) Reuse existing Prompt
[2024-03-04 11:44:24,279 DEBUG generators.py generate l.365] (11/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:24,280 DEBUG generators.py generate l.373] (11/93) Reuse post-processing
[2024-03-04 11:44:24,281 INFO generators.py gen_for_qa l.548] (11/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:24,282 DEBUG generators.py gen_for_qa l.554] (11/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:24,283 DEBUG generators.py generate l.352] (11/93) Reuse existing Prompt
[2024-03-04 11:44:24,284 DEBUG generators.py generate l.365] (11/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:24,285 DEBUG generators.py generate l.373] (11/93) Reuse post-processing
[2024-03-04 11:44:24,287 INFO generators.py gen_for_qa l.548] (11/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:24,289 DEBUG generators.py gen_for_qa l.554] (11/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:24,290 DEBUG generators.py generate l.352] (11/93) Reuse existing Prompt
[2024-03-04 11:44:24,290 DEBUG generators.py generate l.365] (11/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:24,292 DEBUG generators.py generate l.373] (11/93) Reuse post-processing
[2024-03-04 11:44:24,293 INFO generators.py gen_for_qa l.548] (11/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:24,293 DEBUG generators.py gen_for_qa l.554] (11/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:24,295 DEBUG generators.py generate l.352] (11/93) Reuse existing Prompt
[2024-03-04 11:44:24,295 DEBUG generators.py generate l.365] (11/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:24,296 DEBUG generators.py generate l.373] (11/93) Reuse post-processing
[2024-03-04 11:44:24,297 INFO generators.py gen_for_qa l.548] (11/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:24,298 DEBUG generators.py gen_for_qa l.554] (11/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:24,299 DEBUG generators.py generate l.352] (11/93) Reuse existing Prompt
[2024-03-04 11:44:24,300 DEBUG generators.py generate l.365] (11/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:24,300 DEBUG generators.py generate l.373] (11/93) Reuse post-processing
[2024-03-04 11:44:24,302 INFO generators.py gen_for_qa l.548] (11/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:24,303 DEBUG generators.py gen_for_qa l.554] (11/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:24,305 DEBUG generators.py generate l.352] (11/93) Reuse existing Prompt
[2024-03-04 11:44:24,307 DEBUG generators.py generate l.365] (11/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:24,307 DEBUG generators.py generate l.373] (11/93) Reuse post-processing
[2024-03-04 11:44:24,308 INFO generators.py gen_for_qa l.548] (11/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:24,309 DEBUG generators.py generate l.349] (11/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:24,309 DEBUG generators.py generate l.358] (11/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:24,732 ERROR generators.py complete l.400] (11/93) The following exception occurred with prompt meta={} user="Qui a inventé la montgolfière ?  A)  Les frères Montgolfier B)  Richard Crosbie .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:24,747 DEBUG generators.py generate l.373] (11/93) Reuse post-processing
[2024-03-04 11:44:24,749 INFO generators.py generate l.477] (11/93) End question "Qui a inventé la montgolfière ?  A)  Les frères Montgolfier B)  Richard Crosbie "
[2024-03-04 11:44:24,752 INFO generators.py generate l.475] (12/93) *** AnsGenerator for question "Qui a inventé le dirigeable ?  A)  Henri Giffard B)  George Cayley "
[2024-03-04 11:44:24,754 INFO generators.py gen_for_qa l.548] (12/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:24,756 DEBUG generators.py gen_for_qa l.554] (12/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:24,760 DEBUG generators.py generate l.352] (12/93) Reuse existing Prompt
[2024-03-04 11:44:24,761 DEBUG generators.py generate l.365] (12/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:24,763 DEBUG generators.py generate l.373] (12/93) Reuse post-processing
[2024-03-04 11:44:24,763 INFO generators.py gen_for_qa l.548] (12/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:24,765 DEBUG generators.py gen_for_qa l.554] (12/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:24,767 DEBUG generators.py generate l.352] (12/93) Reuse existing Prompt
[2024-03-04 11:44:24,768 DEBUG generators.py generate l.365] (12/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:24,769 DEBUG generators.py generate l.373] (12/93) Reuse post-processing
[2024-03-04 11:44:24,771 INFO generators.py gen_for_qa l.548] (12/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:24,774 DEBUG generators.py gen_for_qa l.554] (12/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:24,775 DEBUG generators.py generate l.352] (12/93) Reuse existing Prompt
[2024-03-04 11:44:24,776 DEBUG generators.py generate l.365] (12/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:24,777 DEBUG generators.py generate l.373] (12/93) Reuse post-processing
[2024-03-04 11:44:24,778 INFO generators.py gen_for_qa l.548] (12/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:24,779 DEBUG generators.py gen_for_qa l.554] (12/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:24,779 DEBUG generators.py generate l.352] (12/93) Reuse existing Prompt
[2024-03-04 11:44:24,781 DEBUG generators.py generate l.365] (12/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:24,781 DEBUG generators.py generate l.373] (12/93) Reuse post-processing
[2024-03-04 11:44:24,783 INFO generators.py gen_for_qa l.548] (12/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:24,784 DEBUG generators.py gen_for_qa l.554] (12/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:24,784 DEBUG generators.py generate l.352] (12/93) Reuse existing Prompt
[2024-03-04 11:44:24,785 DEBUG generators.py generate l.365] (12/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:24,788 DEBUG generators.py generate l.373] (12/93) Reuse post-processing
[2024-03-04 11:44:24,789 INFO generators.py gen_for_qa l.548] (12/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:24,792 DEBUG generators.py gen_for_qa l.554] (12/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:24,792 DEBUG generators.py generate l.352] (12/93) Reuse existing Prompt
[2024-03-04 11:44:24,793 DEBUG generators.py generate l.365] (12/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:24,794 DEBUG generators.py generate l.373] (12/93) Reuse post-processing
[2024-03-04 11:44:24,796 INFO generators.py gen_for_qa l.548] (12/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:24,797 DEBUG generators.py generate l.349] (12/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:24,798 DEBUG generators.py generate l.358] (12/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:25,197 ERROR generators.py complete l.400] (12/93) The following exception occurred with prompt meta={} user="Qui a inventé le dirigeable ?  A)  Henri Giffard B)  George Cayley .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:25,212 DEBUG generators.py generate l.373] (12/93) Reuse post-processing
[2024-03-04 11:44:25,218 INFO generators.py generate l.477] (12/93) End question "Qui a inventé le dirigeable ?  A)  Henri Giffard B)  George Cayley "
[2024-03-04 11:44:25,222 INFO generators.py generate l.475] (13/93) *** AnsGenerator for question "Qui a inventé la pile électrique ?  A)  Alessandro Volta B)  Luigi Galvani C)  Charles-François de Cisternay du Fay "
[2024-03-04 11:44:25,226 INFO generators.py gen_for_qa l.548] (13/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:25,229 DEBUG generators.py gen_for_qa l.554] (13/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:25,230 DEBUG generators.py generate l.352] (13/93) Reuse existing Prompt
[2024-03-04 11:44:25,234 DEBUG generators.py generate l.365] (13/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:25,236 DEBUG generators.py generate l.373] (13/93) Reuse post-processing
[2024-03-04 11:44:25,237 INFO generators.py gen_for_qa l.548] (13/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:25,240 DEBUG generators.py gen_for_qa l.554] (13/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:25,241 DEBUG generators.py generate l.352] (13/93) Reuse existing Prompt
[2024-03-04 11:44:25,242 DEBUG generators.py generate l.365] (13/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:25,244 DEBUG generators.py generate l.373] (13/93) Reuse post-processing
[2024-03-04 11:44:25,245 INFO generators.py gen_for_qa l.548] (13/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:25,246 DEBUG generators.py gen_for_qa l.554] (13/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:25,247 DEBUG generators.py generate l.352] (13/93) Reuse existing Prompt
[2024-03-04 11:44:25,249 DEBUG generators.py generate l.365] (13/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:25,250 DEBUG generators.py generate l.373] (13/93) Reuse post-processing
[2024-03-04 11:44:25,251 INFO generators.py gen_for_qa l.548] (13/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:25,253 DEBUG generators.py gen_for_qa l.554] (13/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:25,255 DEBUG generators.py generate l.352] (13/93) Reuse existing Prompt
[2024-03-04 11:44:25,256 DEBUG generators.py generate l.365] (13/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:25,257 DEBUG generators.py generate l.373] (13/93) Reuse post-processing
[2024-03-04 11:44:25,259 INFO generators.py gen_for_qa l.548] (13/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:25,260 DEBUG generators.py gen_for_qa l.554] (13/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:25,260 DEBUG generators.py generate l.352] (13/93) Reuse existing Prompt
[2024-03-04 11:44:25,261 DEBUG generators.py generate l.365] (13/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:25,262 DEBUG generators.py generate l.373] (13/93) Reuse post-processing
[2024-03-04 11:44:25,263 INFO generators.py gen_for_qa l.548] (13/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:25,263 DEBUG generators.py gen_for_qa l.554] (13/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:25,265 DEBUG generators.py generate l.352] (13/93) Reuse existing Prompt
[2024-03-04 11:44:25,266 DEBUG generators.py generate l.365] (13/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:25,267 DEBUG generators.py generate l.373] (13/93) Reuse post-processing
[2024-03-04 11:44:25,267 INFO generators.py gen_for_qa l.548] (13/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:25,268 DEBUG generators.py generate l.349] (13/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:25,271 DEBUG generators.py generate l.358] (13/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:25,647 ERROR generators.py complete l.400] (13/93) The following exception occurred with prompt meta={} user="Qui a inventé la pile électrique ?  A)  Alessandro Volta B)  Luigi Galvani C)  Charles-François de Cisternay du Fay .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:25,668 DEBUG generators.py generate l.373] (13/93) Reuse post-processing
[2024-03-04 11:44:25,670 INFO generators.py generate l.477] (13/93) End question "Qui a inventé la pile électrique ?  A)  Alessandro Volta B)  Luigi Galvani C)  Charles-François de Cisternay du Fay "
[2024-03-04 11:44:25,673 INFO generators.py generate l.475] (14/93) *** AnsGenerator for question "Qui a inventé le stéthoscope ?  A)  René Laennec B)  David Littmann "
[2024-03-04 11:44:25,675 INFO generators.py gen_for_qa l.548] (14/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:25,677 DEBUG generators.py gen_for_qa l.554] (14/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:25,677 DEBUG generators.py generate l.352] (14/93) Reuse existing Prompt
[2024-03-04 11:44:25,679 DEBUG generators.py generate l.365] (14/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:25,681 DEBUG generators.py generate l.373] (14/93) Reuse post-processing
[2024-03-04 11:44:25,683 INFO generators.py gen_for_qa l.548] (14/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:25,685 DEBUG generators.py gen_for_qa l.554] (14/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:25,687 DEBUG generators.py generate l.352] (14/93) Reuse existing Prompt
[2024-03-04 11:44:25,688 DEBUG generators.py generate l.365] (14/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:25,690 DEBUG generators.py generate l.373] (14/93) Reuse post-processing
[2024-03-04 11:44:25,690 INFO generators.py gen_for_qa l.548] (14/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:25,694 DEBUG generators.py gen_for_qa l.554] (14/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:25,695 DEBUG generators.py generate l.352] (14/93) Reuse existing Prompt
[2024-03-04 11:44:25,698 DEBUG generators.py generate l.365] (14/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:25,699 DEBUG generators.py generate l.373] (14/93) Reuse post-processing
[2024-03-04 11:44:25,699 INFO generators.py gen_for_qa l.548] (14/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:25,704 DEBUG generators.py gen_for_qa l.554] (14/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:25,706 DEBUG generators.py generate l.352] (14/93) Reuse existing Prompt
[2024-03-04 11:44:25,708 DEBUG generators.py generate l.365] (14/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:25,709 DEBUG generators.py generate l.373] (14/93) Reuse post-processing
[2024-03-04 11:44:25,711 INFO generators.py gen_for_qa l.548] (14/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:25,712 DEBUG generators.py gen_for_qa l.554] (14/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:25,712 DEBUG generators.py generate l.352] (14/93) Reuse existing Prompt
[2024-03-04 11:44:25,716 DEBUG generators.py generate l.365] (14/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:25,718 DEBUG generators.py generate l.373] (14/93) Reuse post-processing
[2024-03-04 11:44:25,721 INFO generators.py gen_for_qa l.548] (14/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:25,723 DEBUG generators.py gen_for_qa l.554] (14/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:25,723 DEBUG generators.py generate l.352] (14/93) Reuse existing Prompt
[2024-03-04 11:44:25,727 DEBUG generators.py generate l.365] (14/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:25,729 DEBUG generators.py generate l.373] (14/93) Reuse post-processing
[2024-03-04 11:44:25,729 INFO generators.py gen_for_qa l.548] (14/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:25,731 DEBUG generators.py generate l.349] (14/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:25,739 DEBUG generators.py generate l.358] (14/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:26,163 ERROR generators.py complete l.400] (14/93) The following exception occurred with prompt meta={} user="Qui a inventé le stéthoscope ?  A)  René Laennec B)  David Littmann .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:26,182 DEBUG generators.py generate l.373] (14/93) Reuse post-processing
[2024-03-04 11:44:26,187 INFO generators.py generate l.477] (14/93) End question "Qui a inventé le stéthoscope ?  A)  René Laennec B)  David Littmann "
[2024-03-04 11:44:26,192 INFO generators.py generate l.475] (15/93) *** AnsGenerator for question "Qui a inventé le microscope ?  A)  Zacharias Janssen B)  Galileo Galilei C)  Robert Hooke "
[2024-03-04 11:44:26,196 INFO generators.py gen_for_qa l.548] (15/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:26,201 DEBUG generators.py gen_for_qa l.554] (15/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:26,207 DEBUG generators.py generate l.352] (15/93) Reuse existing Prompt
[2024-03-04 11:44:26,211 DEBUG generators.py generate l.365] (15/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:26,213 DEBUG generators.py generate l.373] (15/93) Reuse post-processing
[2024-03-04 11:44:26,216 INFO generators.py gen_for_qa l.548] (15/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:26,220 DEBUG generators.py gen_for_qa l.554] (15/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:26,225 DEBUG generators.py generate l.352] (15/93) Reuse existing Prompt
[2024-03-04 11:44:26,227 DEBUG generators.py generate l.365] (15/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:26,229 DEBUG generators.py generate l.373] (15/93) Reuse post-processing
[2024-03-04 11:44:26,229 INFO generators.py gen_for_qa l.548] (15/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:26,229 DEBUG generators.py gen_for_qa l.554] (15/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:26,235 DEBUG generators.py generate l.352] (15/93) Reuse existing Prompt
[2024-03-04 11:44:26,237 DEBUG generators.py generate l.365] (15/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:26,239 DEBUG generators.py generate l.373] (15/93) Reuse post-processing
[2024-03-04 11:44:26,241 INFO generators.py gen_for_qa l.548] (15/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:26,242 DEBUG generators.py gen_for_qa l.554] (15/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:26,244 DEBUG generators.py generate l.352] (15/93) Reuse existing Prompt
[2024-03-04 11:44:26,245 DEBUG generators.py generate l.365] (15/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:26,247 DEBUG generators.py generate l.373] (15/93) Reuse post-processing
[2024-03-04 11:44:26,248 INFO generators.py gen_for_qa l.548] (15/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:26,249 DEBUG generators.py gen_for_qa l.554] (15/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:26,251 DEBUG generators.py generate l.352] (15/93) Reuse existing Prompt
[2024-03-04 11:44:26,252 DEBUG generators.py generate l.365] (15/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:26,254 DEBUG generators.py generate l.373] (15/93) Reuse post-processing
[2024-03-04 11:44:26,256 INFO generators.py gen_for_qa l.548] (15/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:26,257 DEBUG generators.py gen_for_qa l.554] (15/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:26,259 DEBUG generators.py generate l.352] (15/93) Reuse existing Prompt
[2024-03-04 11:44:26,259 DEBUG generators.py generate l.365] (15/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:26,260 DEBUG generators.py generate l.373] (15/93) Reuse post-processing
[2024-03-04 11:44:26,261 INFO generators.py gen_for_qa l.548] (15/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:26,261 DEBUG generators.py generate l.349] (15/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:26,264 DEBUG generators.py generate l.358] (15/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:26,676 ERROR generators.py complete l.400] (15/93) The following exception occurred with prompt meta={} user="Qui a inventé le microscope ?  A)  Zacharias Janssen B)  Galileo Galilei C)  Robert Hooke .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:26,693 DEBUG generators.py generate l.373] (15/93) Reuse post-processing
[2024-03-04 11:44:26,699 INFO generators.py generate l.477] (15/93) End question "Qui a inventé le microscope ?  A)  Zacharias Janssen B)  Galileo Galilei C)  Robert Hooke "
[2024-03-04 11:44:26,701 INFO generators.py generate l.475] (16/93) *** AnsGenerator for question "Qui a inventé le thermomètre ?  A)  Galileo Galilei B)  Daniel Gabriel Fahrenheit C)  Cornelis Drebbel "
[2024-03-04 11:44:26,704 INFO generators.py gen_for_qa l.548] (16/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:26,706 DEBUG generators.py gen_for_qa l.554] (16/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:26,709 DEBUG generators.py generate l.352] (16/93) Reuse existing Prompt
[2024-03-04 11:44:26,710 DEBUG generators.py generate l.365] (16/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:26,712 DEBUG generators.py generate l.373] (16/93) Reuse post-processing
[2024-03-04 11:44:26,712 INFO generators.py gen_for_qa l.548] (16/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:26,712 DEBUG generators.py gen_for_qa l.554] (16/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:26,717 DEBUG generators.py generate l.352] (16/93) Reuse existing Prompt
[2024-03-04 11:44:26,718 DEBUG generators.py generate l.365] (16/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:26,720 DEBUG generators.py generate l.373] (16/93) Reuse post-processing
[2024-03-04 11:44:26,722 INFO generators.py gen_for_qa l.548] (16/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:26,723 DEBUG generators.py gen_for_qa l.554] (16/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:26,725 DEBUG generators.py generate l.352] (16/93) Reuse existing Prompt
[2024-03-04 11:44:26,726 DEBUG generators.py generate l.365] (16/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:26,727 DEBUG generators.py generate l.373] (16/93) Reuse post-processing
[2024-03-04 11:44:26,728 INFO generators.py gen_for_qa l.548] (16/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:26,728 DEBUG generators.py gen_for_qa l.554] (16/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:26,728 DEBUG generators.py generate l.352] (16/93) Reuse existing Prompt
[2024-03-04 11:44:26,728 DEBUG generators.py generate l.365] (16/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:26,732 DEBUG generators.py generate l.373] (16/93) Reuse post-processing
[2024-03-04 11:44:26,733 INFO generators.py gen_for_qa l.548] (16/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:26,734 DEBUG generators.py gen_for_qa l.554] (16/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:26,735 DEBUG generators.py generate l.352] (16/93) Reuse existing Prompt
[2024-03-04 11:44:26,737 DEBUG generators.py generate l.365] (16/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:26,739 DEBUG generators.py generate l.373] (16/93) Reuse post-processing
[2024-03-04 11:44:26,740 INFO generators.py gen_for_qa l.548] (16/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:26,741 DEBUG generators.py gen_for_qa l.554] (16/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:26,742 DEBUG generators.py generate l.352] (16/93) Reuse existing Prompt
[2024-03-04 11:44:26,744 DEBUG generators.py generate l.365] (16/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:26,744 DEBUG generators.py generate l.373] (16/93) Reuse post-processing
[2024-03-04 11:44:26,745 INFO generators.py gen_for_qa l.548] (16/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:26,746 DEBUG generators.py generate l.349] (16/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:26,747 DEBUG generators.py generate l.358] (16/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:27,208 ERROR generators.py complete l.400] (16/93) The following exception occurred with prompt meta={} user="Qui a inventé le thermomètre ?  A)  Galileo Galilei B)  Daniel Gabriel Fahrenheit C)  Cornelis Drebbel .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:27,226 DEBUG generators.py generate l.373] (16/93) Reuse post-processing
[2024-03-04 11:44:27,228 INFO generators.py generate l.477] (16/93) End question "Qui a inventé le thermomètre ?  A)  Galileo Galilei B)  Daniel Gabriel Fahrenheit C)  Cornelis Drebbel "
[2024-03-04 11:44:27,230 INFO generators.py generate l.475] (17/93) *** AnsGenerator for question "Qui a inventé le baromètre ?  A)  Evangelista Torricelli B)  Blaise Pascal "
[2024-03-04 11:44:27,231 INFO generators.py gen_for_qa l.548] (17/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:27,233 DEBUG generators.py gen_for_qa l.554] (17/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:27,235 DEBUG generators.py generate l.352] (17/93) Reuse existing Prompt
[2024-03-04 11:44:27,237 DEBUG generators.py generate l.365] (17/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:27,238 DEBUG generators.py generate l.373] (17/93) Reuse post-processing
[2024-03-04 11:44:27,242 INFO generators.py gen_for_qa l.548] (17/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:27,243 DEBUG generators.py gen_for_qa l.554] (17/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:27,244 DEBUG generators.py generate l.352] (17/93) Reuse existing Prompt
[2024-03-04 11:44:27,245 DEBUG generators.py generate l.365] (17/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:27,246 DEBUG generators.py generate l.373] (17/93) Reuse post-processing
[2024-03-04 11:44:27,248 INFO generators.py gen_for_qa l.548] (17/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:27,248 DEBUG generators.py gen_for_qa l.554] (17/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:27,248 DEBUG generators.py generate l.352] (17/93) Reuse existing Prompt
[2024-03-04 11:44:27,252 DEBUG generators.py generate l.365] (17/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:27,253 DEBUG generators.py generate l.373] (17/93) Reuse post-processing
[2024-03-04 11:44:27,255 INFO generators.py gen_for_qa l.548] (17/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:27,256 DEBUG generators.py gen_for_qa l.554] (17/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:27,258 DEBUG generators.py generate l.352] (17/93) Reuse existing Prompt
[2024-03-04 11:44:27,258 DEBUG generators.py generate l.365] (17/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:27,258 DEBUG generators.py generate l.373] (17/93) Reuse post-processing
[2024-03-04 11:44:27,262 INFO generators.py gen_for_qa l.548] (17/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:27,263 DEBUG generators.py gen_for_qa l.554] (17/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:27,264 DEBUG generators.py generate l.352] (17/93) Reuse existing Prompt
[2024-03-04 11:44:27,265 DEBUG generators.py generate l.365] (17/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:27,266 DEBUG generators.py generate l.373] (17/93) Reuse post-processing
[2024-03-04 11:44:27,267 INFO generators.py gen_for_qa l.548] (17/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:27,270 DEBUG generators.py gen_for_qa l.554] (17/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:27,272 DEBUG generators.py generate l.352] (17/93) Reuse existing Prompt
[2024-03-04 11:44:27,272 DEBUG generators.py generate l.365] (17/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:27,274 DEBUG generators.py generate l.373] (17/93) Reuse post-processing
[2024-03-04 11:44:27,275 INFO generators.py gen_for_qa l.548] (17/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:27,275 DEBUG generators.py generate l.349] (17/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:27,277 DEBUG generators.py generate l.358] (17/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:27,711 ERROR generators.py complete l.400] (17/93) The following exception occurred with prompt meta={} user="Qui a inventé le baromètre ?  A)  Evangelista Torricelli B)  Blaise Pascal .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:27,730 DEBUG generators.py generate l.373] (17/93) Reuse post-processing
[2024-03-04 11:44:27,733 INFO generators.py generate l.477] (17/93) End question "Qui a inventé le baromètre ?  A)  Evangelista Torricelli B)  Blaise Pascal "
[2024-03-04 11:44:27,734 INFO generators.py generate l.475] (18/93) *** AnsGenerator for question "Qui a inventé la machine à vapeur à piston ?  A)  Thomas Savery B)  Denis Papin "
[2024-03-04 11:44:27,738 INFO generators.py gen_for_qa l.548] (18/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:27,740 DEBUG generators.py gen_for_qa l.554] (18/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:27,742 DEBUG generators.py generate l.352] (18/93) Reuse existing Prompt
[2024-03-04 11:44:27,744 DEBUG generators.py generate l.365] (18/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:27,746 DEBUG generators.py generate l.373] (18/93) Reuse post-processing
[2024-03-04 11:44:27,747 INFO generators.py gen_for_qa l.548] (18/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:27,747 DEBUG generators.py gen_for_qa l.554] (18/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:27,749 DEBUG generators.py generate l.352] (18/93) Reuse existing Prompt
[2024-03-04 11:44:27,752 DEBUG generators.py generate l.365] (18/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:27,753 DEBUG generators.py generate l.373] (18/93) Reuse post-processing
[2024-03-04 11:44:27,755 INFO generators.py gen_for_qa l.548] (18/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:27,756 DEBUG generators.py gen_for_qa l.554] (18/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:27,758 DEBUG generators.py generate l.352] (18/93) Reuse existing Prompt
[2024-03-04 11:44:27,758 DEBUG generators.py generate l.365] (18/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:27,760 DEBUG generators.py generate l.373] (18/93) Reuse post-processing
[2024-03-04 11:44:27,760 INFO generators.py gen_for_qa l.548] (18/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:27,763 DEBUG generators.py gen_for_qa l.554] (18/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:27,763 DEBUG generators.py generate l.352] (18/93) Reuse existing Prompt
[2024-03-04 11:44:27,764 DEBUG generators.py generate l.365] (18/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:27,766 DEBUG generators.py generate l.373] (18/93) Reuse post-processing
[2024-03-04 11:44:27,767 INFO generators.py gen_for_qa l.548] (18/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:27,768 DEBUG generators.py gen_for_qa l.554] (18/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:27,769 DEBUG generators.py generate l.352] (18/93) Reuse existing Prompt
[2024-03-04 11:44:27,771 DEBUG generators.py generate l.365] (18/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:27,771 DEBUG generators.py generate l.373] (18/93) Reuse post-processing
[2024-03-04 11:44:27,774 INFO generators.py gen_for_qa l.548] (18/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:27,775 DEBUG generators.py gen_for_qa l.554] (18/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:27,776 DEBUG generators.py generate l.352] (18/93) Reuse existing Prompt
[2024-03-04 11:44:27,777 DEBUG generators.py generate l.365] (18/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:27,777 DEBUG generators.py generate l.373] (18/93) Reuse post-processing
[2024-03-04 11:44:27,778 INFO generators.py gen_for_qa l.548] (18/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:27,778 DEBUG generators.py generate l.349] (18/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:27,781 DEBUG generators.py generate l.358] (18/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:28,174 ERROR generators.py complete l.400] (18/93) The following exception occurred with prompt meta={} user="Qui a inventé la machine à vapeur à piston ?  A)  Thomas Savery B)  Denis Papin .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:28,179 DEBUG generators.py generate l.373] (18/93) Reuse post-processing
[2024-03-04 11:44:28,179 INFO generators.py generate l.477] (18/93) End question "Qui a inventé la machine à vapeur à piston ?  A)  Thomas Savery B)  Denis Papin "
[2024-03-04 11:44:28,181 INFO generators.py generate l.475] (19/93) *** AnsGenerator for question "Qui a inventé la locomotive à vapeur ?  A)  Richard Trevithick B)  George Stephenson C)  Nicolas-Joseph Cugnot "
[2024-03-04 11:44:28,182 INFO generators.py gen_for_qa l.548] (19/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:28,184 DEBUG generators.py gen_for_qa l.554] (19/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:28,186 DEBUG generators.py generate l.352] (19/93) Reuse existing Prompt
[2024-03-04 11:44:28,188 DEBUG generators.py generate l.365] (19/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:28,188 DEBUG generators.py generate l.373] (19/93) Reuse post-processing
[2024-03-04 11:44:28,190 INFO generators.py gen_for_qa l.548] (19/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:28,191 DEBUG generators.py gen_for_qa l.554] (19/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:28,192 DEBUG generators.py generate l.352] (19/93) Reuse existing Prompt
[2024-03-04 11:44:28,192 DEBUG generators.py generate l.365] (19/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:28,193 DEBUG generators.py generate l.373] (19/93) Reuse post-processing
[2024-03-04 11:44:28,193 INFO generators.py gen_for_qa l.548] (19/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:28,195 DEBUG generators.py gen_for_qa l.554] (19/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:28,196 DEBUG generators.py generate l.352] (19/93) Reuse existing Prompt
[2024-03-04 11:44:28,197 DEBUG generators.py generate l.365] (19/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:28,197 DEBUG generators.py generate l.373] (19/93) Reuse post-processing
[2024-03-04 11:44:28,198 INFO generators.py gen_for_qa l.548] (19/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:28,198 DEBUG generators.py gen_for_qa l.554] (19/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:28,199 DEBUG generators.py generate l.352] (19/93) Reuse existing Prompt
[2024-03-04 11:44:28,200 DEBUG generators.py generate l.365] (19/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:28,203 DEBUG generators.py generate l.373] (19/93) Reuse post-processing
[2024-03-04 11:44:28,204 INFO generators.py gen_for_qa l.548] (19/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:28,206 DEBUG generators.py gen_for_qa l.554] (19/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:28,207 DEBUG generators.py generate l.352] (19/93) Reuse existing Prompt
[2024-03-04 11:44:28,207 DEBUG generators.py generate l.365] (19/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:28,208 DEBUG generators.py generate l.373] (19/93) Reuse post-processing
[2024-03-04 11:44:28,209 INFO generators.py gen_for_qa l.548] (19/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:28,210 DEBUG generators.py gen_for_qa l.554] (19/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:28,211 DEBUG generators.py generate l.352] (19/93) Reuse existing Prompt
[2024-03-04 11:44:28,211 DEBUG generators.py generate l.365] (19/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:28,213 DEBUG generators.py generate l.373] (19/93) Reuse post-processing
[2024-03-04 11:44:28,213 INFO generators.py gen_for_qa l.548] (19/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:28,213 DEBUG generators.py generate l.349] (19/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:28,216 DEBUG generators.py generate l.358] (19/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:28,609 ERROR generators.py complete l.400] (19/93) The following exception occurred with prompt meta={} user="Qui a inventé la locomotive à vapeur ?  A)  Richard Trevithick B)  George Stephenson C)  Nicolas-Joseph Cugnot .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:28,622 DEBUG generators.py generate l.373] (19/93) Reuse post-processing
[2024-03-04 11:44:28,627 INFO generators.py generate l.477] (19/93) End question "Qui a inventé la locomotive à vapeur ?  A)  Richard Trevithick B)  George Stephenson C)  Nicolas-Joseph Cugnot "
[2024-03-04 11:44:28,629 INFO generators.py generate l.475] (20/93) *** AnsGenerator for question "Qui a inventé le moteur à combustion interne ?  A)  Étienne Lenoir B)  Nikolaus Otto "
[2024-03-04 11:44:28,631 INFO generators.py gen_for_qa l.548] (20/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:28,632 DEBUG generators.py gen_for_qa l.554] (20/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:28,635 DEBUG generators.py generate l.352] (20/93) Reuse existing Prompt
[2024-03-04 11:44:28,635 DEBUG generators.py generate l.365] (20/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:28,639 DEBUG generators.py generate l.373] (20/93) Reuse post-processing
[2024-03-04 11:44:28,640 INFO generators.py gen_for_qa l.548] (20/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:28,641 DEBUG generators.py gen_for_qa l.554] (20/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:28,643 DEBUG generators.py generate l.352] (20/93) Reuse existing Prompt
[2024-03-04 11:44:28,644 DEBUG generators.py generate l.365] (20/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:28,644 DEBUG generators.py generate l.373] (20/93) Reuse post-processing
[2024-03-04 11:44:28,644 INFO generators.py gen_for_qa l.548] (20/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:28,644 DEBUG generators.py gen_for_qa l.554] (20/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:28,649 DEBUG generators.py generate l.352] (20/93) Reuse existing Prompt
[2024-03-04 11:44:28,650 DEBUG generators.py generate l.365] (20/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:28,652 DEBUG generators.py generate l.373] (20/93) Reuse post-processing
[2024-03-04 11:44:28,654 INFO generators.py gen_for_qa l.548] (20/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:28,655 DEBUG generators.py gen_for_qa l.554] (20/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:28,656 DEBUG generators.py generate l.352] (20/93) Reuse existing Prompt
[2024-03-04 11:44:28,656 DEBUG generators.py generate l.365] (20/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:28,659 DEBUG generators.py generate l.373] (20/93) Reuse post-processing
[2024-03-04 11:44:28,660 INFO generators.py gen_for_qa l.548] (20/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:28,660 DEBUG generators.py gen_for_qa l.554] (20/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:28,661 DEBUG generators.py generate l.352] (20/93) Reuse existing Prompt
[2024-03-04 11:44:28,663 DEBUG generators.py generate l.365] (20/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:28,663 DEBUG generators.py generate l.373] (20/93) Reuse post-processing
[2024-03-04 11:44:28,663 INFO generators.py gen_for_qa l.548] (20/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:28,663 DEBUG generators.py gen_for_qa l.554] (20/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:28,663 DEBUG generators.py generate l.352] (20/93) Reuse existing Prompt
[2024-03-04 11:44:28,667 DEBUG generators.py generate l.365] (20/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:28,670 DEBUG generators.py generate l.373] (20/93) Reuse post-processing
[2024-03-04 11:44:28,671 INFO generators.py gen_for_qa l.548] (20/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:28,672 DEBUG generators.py generate l.349] (20/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:28,674 DEBUG generators.py generate l.358] (20/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:29,065 ERROR generators.py complete l.400] (20/93) The following exception occurred with prompt meta={} user="Qui a inventé le moteur à combustion interne ?  A)  Étienne Lenoir B)  Nikolaus Otto .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:29,072 DEBUG generators.py generate l.373] (20/93) Reuse post-processing
[2024-03-04 11:44:29,075 INFO generators.py generate l.477] (20/93) End question "Qui a inventé le moteur à combustion interne ?  A)  Étienne Lenoir B)  Nikolaus Otto "
[2024-03-04 11:44:29,077 INFO generators.py generate l.475] (21/93) *** AnsGenerator for question "Qui a inventé le réfrigérateur ?  A)  John Gorrie B)  Carl von Linde C)  James Harrison "
[2024-03-04 11:44:29,078 INFO generators.py gen_for_qa l.548] (21/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:29,080 DEBUG generators.py gen_for_qa l.554] (21/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:29,083 DEBUG generators.py generate l.352] (21/93) Reuse existing Prompt
[2024-03-04 11:44:29,085 DEBUG generators.py generate l.365] (21/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:29,087 DEBUG generators.py generate l.373] (21/93) Reuse post-processing
[2024-03-04 11:44:29,089 INFO generators.py gen_for_qa l.548] (21/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:29,091 DEBUG generators.py gen_for_qa l.554] (21/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:29,091 DEBUG generators.py generate l.352] (21/93) Reuse existing Prompt
[2024-03-04 11:44:29,095 DEBUG generators.py generate l.365] (21/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:29,096 DEBUG generators.py generate l.373] (21/93) Reuse post-processing
[2024-03-04 11:44:29,098 INFO generators.py gen_for_qa l.548] (21/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:29,099 DEBUG generators.py gen_for_qa l.554] (21/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:29,101 DEBUG generators.py generate l.352] (21/93) Reuse existing Prompt
[2024-03-04 11:44:29,103 DEBUG generators.py generate l.365] (21/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:29,106 DEBUG generators.py generate l.373] (21/93) Reuse post-processing
[2024-03-04 11:44:29,108 INFO generators.py gen_for_qa l.548] (21/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:29,110 DEBUG generators.py gen_for_qa l.554] (21/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:29,111 DEBUG generators.py generate l.352] (21/93) Reuse existing Prompt
[2024-03-04 11:44:29,113 DEBUG generators.py generate l.365] (21/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:29,115 DEBUG generators.py generate l.373] (21/93) Reuse post-processing
[2024-03-04 11:44:29,116 INFO generators.py gen_for_qa l.548] (21/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:29,117 DEBUG generators.py gen_for_qa l.554] (21/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:29,120 DEBUG generators.py generate l.352] (21/93) Reuse existing Prompt
[2024-03-04 11:44:29,122 DEBUG generators.py generate l.365] (21/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:29,122 DEBUG generators.py generate l.373] (21/93) Reuse post-processing
[2024-03-04 11:44:29,126 INFO generators.py gen_for_qa l.548] (21/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:29,127 DEBUG generators.py gen_for_qa l.554] (21/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:29,128 DEBUG generators.py generate l.352] (21/93) Reuse existing Prompt
[2024-03-04 11:44:29,130 DEBUG generators.py generate l.365] (21/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:29,131 DEBUG generators.py generate l.373] (21/93) Reuse post-processing
[2024-03-04 11:44:29,131 INFO generators.py gen_for_qa l.548] (21/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:29,134 DEBUG generators.py generate l.349] (21/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:29,135 DEBUG generators.py generate l.358] (21/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:29,533 ERROR generators.py complete l.400] (21/93) The following exception occurred with prompt meta={} user="Qui a inventé le réfrigérateur ?  A)  John Gorrie B)  Carl von Linde C)  James Harrison .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:29,541 DEBUG generators.py generate l.373] (21/93) Reuse post-processing
[2024-03-04 11:44:29,543 INFO generators.py generate l.477] (21/93) End question "Qui a inventé le réfrigérateur ?  A)  John Gorrie B)  Carl von Linde C)  James Harrison "
[2024-03-04 11:44:29,545 INFO generators.py generate l.475] (22/93) *** AnsGenerator for question "Qui a inventé le transformateur électrique ?  A)  Lucien Gaulard et John Dixon Gibbs B)  Ottó Bláthy, Miksa Déri et Károly Zipernowsky "
[2024-03-04 11:44:29,545 INFO generators.py gen_for_qa l.548] (22/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:29,545 DEBUG generators.py gen_for_qa l.554] (22/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:29,549 DEBUG generators.py generate l.352] (22/93) Reuse existing Prompt
[2024-03-04 11:44:29,550 DEBUG generators.py generate l.365] (22/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:29,552 DEBUG generators.py generate l.373] (22/93) Reuse post-processing
[2024-03-04 11:44:29,554 INFO generators.py gen_for_qa l.548] (22/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:29,556 DEBUG generators.py gen_for_qa l.554] (22/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:29,557 DEBUG generators.py generate l.352] (22/93) Reuse existing Prompt
[2024-03-04 11:44:29,558 DEBUG generators.py generate l.365] (22/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:29,560 DEBUG generators.py generate l.373] (22/93) Reuse post-processing
[2024-03-04 11:44:29,561 INFO generators.py gen_for_qa l.548] (22/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:29,562 DEBUG generators.py gen_for_qa l.554] (22/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:29,563 DEBUG generators.py generate l.352] (22/93) Reuse existing Prompt
[2024-03-04 11:44:29,564 DEBUG generators.py generate l.365] (22/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:29,565 DEBUG generators.py generate l.373] (22/93) Reuse post-processing
[2024-03-04 11:44:29,568 INFO generators.py gen_for_qa l.548] (22/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:29,569 DEBUG generators.py gen_for_qa l.554] (22/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:29,571 DEBUG generators.py generate l.352] (22/93) Reuse existing Prompt
[2024-03-04 11:44:29,573 DEBUG generators.py generate l.365] (22/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:29,574 DEBUG generators.py generate l.373] (22/93) Reuse post-processing
[2024-03-04 11:44:29,575 INFO generators.py gen_for_qa l.548] (22/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:29,577 DEBUG generators.py gen_for_qa l.554] (22/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:29,579 DEBUG generators.py generate l.352] (22/93) Reuse existing Prompt
[2024-03-04 11:44:29,580 DEBUG generators.py generate l.365] (22/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:29,581 DEBUG generators.py generate l.373] (22/93) Reuse post-processing
[2024-03-04 11:44:29,582 INFO generators.py gen_for_qa l.548] (22/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:29,583 DEBUG generators.py gen_for_qa l.554] (22/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:29,586 DEBUG generators.py generate l.352] (22/93) Reuse existing Prompt
[2024-03-04 11:44:29,589 DEBUG generators.py generate l.365] (22/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:29,590 DEBUG generators.py generate l.373] (22/93) Reuse post-processing
[2024-03-04 11:44:29,592 INFO generators.py gen_for_qa l.548] (22/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:29,593 DEBUG generators.py generate l.349] (22/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:29,594 DEBUG generators.py generate l.358] (22/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:30,020 ERROR generators.py complete l.400] (22/93) The following exception occurred with prompt meta={} user="Qui a inventé le transformateur électrique ?  A)  Lucien Gaulard et John Dixon Gibbs B)  Ottó Bláthy, Miksa Déri et Károly Zipernowsky .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:30,034 DEBUG generators.py generate l.373] (22/93) Reuse post-processing
[2024-03-04 11:44:30,037 INFO generators.py generate l.477] (22/93) End question "Qui a inventé le transformateur électrique ?  A)  Lucien Gaulard et John Dixon Gibbs B)  Ottó Bláthy, Miksa Déri et Károly Zipernowsky "
[2024-03-04 11:44:30,039 INFO generators.py generate l.475] (23/93) *** AnsGenerator for question "Qui a inventé le phonographe ?  A)  Thomas Edison B)  Charles Cros "
[2024-03-04 11:44:30,042 INFO generators.py gen_for_qa l.548] (23/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:30,043 DEBUG generators.py gen_for_qa l.554] (23/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:30,044 DEBUG generators.py generate l.352] (23/93) Reuse existing Prompt
[2024-03-04 11:44:30,046 DEBUG generators.py generate l.365] (23/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:30,047 DEBUG generators.py generate l.373] (23/93) Reuse post-processing
[2024-03-04 11:44:30,049 INFO generators.py gen_for_qa l.548] (23/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:30,050 DEBUG generators.py gen_for_qa l.554] (23/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:30,052 DEBUG generators.py generate l.352] (23/93) Reuse existing Prompt
[2024-03-04 11:44:30,053 DEBUG generators.py generate l.365] (23/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:30,055 DEBUG generators.py generate l.373] (23/93) Reuse post-processing
[2024-03-04 11:44:30,057 INFO generators.py gen_for_qa l.548] (23/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:30,058 DEBUG generators.py gen_for_qa l.554] (23/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:30,059 DEBUG generators.py generate l.352] (23/93) Reuse existing Prompt
[2024-03-04 11:44:30,060 DEBUG generators.py generate l.365] (23/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:30,061 DEBUG generators.py generate l.373] (23/93) Reuse post-processing
[2024-03-04 11:44:30,062 INFO generators.py gen_for_qa l.548] (23/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:30,062 DEBUG generators.py gen_for_qa l.554] (23/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:30,062 DEBUG generators.py generate l.352] (23/93) Reuse existing Prompt
[2024-03-04 11:44:30,064 DEBUG generators.py generate l.365] (23/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:30,064 DEBUG generators.py generate l.373] (23/93) Reuse post-processing
[2024-03-04 11:44:30,066 INFO generators.py gen_for_qa l.548] (23/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:30,068 DEBUG generators.py gen_for_qa l.554] (23/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:30,071 DEBUG generators.py generate l.352] (23/93) Reuse existing Prompt
[2024-03-04 11:44:30,071 DEBUG generators.py generate l.365] (23/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:30,072 DEBUG generators.py generate l.373] (23/93) Reuse post-processing
[2024-03-04 11:44:30,074 INFO generators.py gen_for_qa l.548] (23/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:30,075 DEBUG generators.py gen_for_qa l.554] (23/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:30,078 DEBUG generators.py generate l.352] (23/93) Reuse existing Prompt
[2024-03-04 11:44:30,079 DEBUG generators.py generate l.365] (23/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:30,081 DEBUG generators.py generate l.373] (23/93) Reuse post-processing
[2024-03-04 11:44:30,084 INFO generators.py gen_for_qa l.548] (23/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:30,088 DEBUG generators.py generate l.349] (23/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:30,089 DEBUG generators.py generate l.358] (23/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:30,488 ERROR generators.py complete l.400] (23/93) The following exception occurred with prompt meta={} user="Qui a inventé le phonographe ?  A)  Thomas Edison B)  Charles Cros .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:30,501 DEBUG generators.py generate l.373] (23/93) Reuse post-processing
[2024-03-04 11:44:30,504 INFO generators.py generate l.477] (23/93) End question "Qui a inventé le phonographe ?  A)  Thomas Edison B)  Charles Cros "
[2024-03-04 11:44:30,504 INFO generators.py generate l.475] (24/93) *** AnsGenerator for question " Qui a découvert l'Amérique ?  A)  Christophe Colomb B)  Leif Erikson "
[2024-03-04 11:44:30,508 INFO generators.py gen_for_qa l.548] (24/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:30,510 DEBUG generators.py gen_for_qa l.554] (24/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:30,511 DEBUG generators.py generate l.352] (24/93) Reuse existing Prompt
[2024-03-04 11:44:30,513 DEBUG generators.py generate l.365] (24/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:30,513 DEBUG generators.py generate l.373] (24/93) Reuse post-processing
[2024-03-04 11:44:30,515 INFO generators.py gen_for_qa l.548] (24/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:30,516 DEBUG generators.py gen_for_qa l.554] (24/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:30,518 DEBUG generators.py generate l.352] (24/93) Reuse existing Prompt
[2024-03-04 11:44:30,521 DEBUG generators.py generate l.365] (24/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:30,522 DEBUG generators.py generate l.373] (24/93) Reuse post-processing
[2024-03-04 11:44:30,523 INFO generators.py gen_for_qa l.548] (24/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:30,524 DEBUG generators.py gen_for_qa l.554] (24/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:30,525 DEBUG generators.py generate l.352] (24/93) Reuse existing Prompt
[2024-03-04 11:44:30,525 DEBUG generators.py generate l.365] (24/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:30,525 DEBUG generators.py generate l.373] (24/93) Reuse post-processing
[2024-03-04 11:44:30,528 INFO generators.py gen_for_qa l.548] (24/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:30,530 DEBUG generators.py gen_for_qa l.554] (24/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:30,531 DEBUG generators.py generate l.352] (24/93) Reuse existing Prompt
[2024-03-04 11:44:30,531 DEBUG generators.py generate l.365] (24/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:30,532 DEBUG generators.py generate l.373] (24/93) Reuse post-processing
[2024-03-04 11:44:30,534 INFO generators.py gen_for_qa l.548] (24/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:30,535 DEBUG generators.py gen_for_qa l.554] (24/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:30,538 DEBUG generators.py generate l.352] (24/93) Reuse existing Prompt
[2024-03-04 11:44:30,538 DEBUG generators.py generate l.365] (24/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:30,538 DEBUG generators.py generate l.373] (24/93) Reuse post-processing
[2024-03-04 11:44:30,541 INFO generators.py gen_for_qa l.548] (24/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:30,545 DEBUG generators.py gen_for_qa l.554] (24/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:30,546 DEBUG generators.py generate l.352] (24/93) Reuse existing Prompt
[2024-03-04 11:44:30,546 DEBUG generators.py generate l.365] (24/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:30,548 DEBUG generators.py generate l.373] (24/93) Reuse post-processing
[2024-03-04 11:44:30,549 INFO generators.py gen_for_qa l.548] (24/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:30,550 DEBUG generators.py generate l.349] (24/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:30,552 DEBUG generators.py generate l.358] (24/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:30,966 ERROR generators.py complete l.400] (24/93) The following exception occurred with prompt meta={} user=" Qui a découvert l'Amérique ?  A)  Christophe Colomb B)  Leif Erikson .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:30,979 DEBUG generators.py generate l.373] (24/93) Reuse post-processing
[2024-03-04 11:44:30,981 INFO generators.py generate l.477] (24/93) End question " Qui a découvert l'Amérique ?  A)  Christophe Colomb B)  Leif Erikson "
[2024-03-04 11:44:30,984 INFO generators.py generate l.475] (25/93) *** AnsGenerator for question " Qui a inventé l'avion ?  A)  Orville et Wilbur Wright B)  Clément Ader C)  Gustave Whitehead "
[2024-03-04 11:44:30,988 INFO generators.py gen_for_qa l.548] (25/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:30,989 DEBUG generators.py gen_for_qa l.554] (25/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:30,992 DEBUG generators.py generate l.352] (25/93) Reuse existing Prompt
[2024-03-04 11:44:30,992 DEBUG generators.py generate l.365] (25/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:30,992 DEBUG generators.py generate l.373] (25/93) Reuse post-processing
[2024-03-04 11:44:30,995 INFO generators.py gen_for_qa l.548] (25/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:30,996 DEBUG generators.py gen_for_qa l.554] (25/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:30,997 DEBUG generators.py generate l.352] (25/93) Reuse existing Prompt
[2024-03-04 11:44:30,998 DEBUG generators.py generate l.365] (25/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:31,000 DEBUG generators.py generate l.373] (25/93) Reuse post-processing
[2024-03-04 11:44:31,003 INFO generators.py gen_for_qa l.548] (25/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:31,005 DEBUG generators.py gen_for_qa l.554] (25/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:31,006 DEBUG generators.py generate l.352] (25/93) Reuse existing Prompt
[2024-03-04 11:44:31,007 DEBUG generators.py generate l.365] (25/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:31,007 DEBUG generators.py generate l.373] (25/93) Reuse post-processing
[2024-03-04 11:44:31,009 INFO generators.py gen_for_qa l.548] (25/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:31,009 DEBUG generators.py gen_for_qa l.554] (25/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:31,012 DEBUG generators.py generate l.352] (25/93) Reuse existing Prompt
[2024-03-04 11:44:31,013 DEBUG generators.py generate l.365] (25/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:31,014 DEBUG generators.py generate l.373] (25/93) Reuse post-processing
[2024-03-04 11:44:31,015 INFO generators.py gen_for_qa l.548] (25/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:31,015 DEBUG generators.py gen_for_qa l.554] (25/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:31,017 DEBUG generators.py generate l.352] (25/93) Reuse existing Prompt
[2024-03-04 11:44:31,019 DEBUG generators.py generate l.365] (25/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:31,021 DEBUG generators.py generate l.373] (25/93) Reuse post-processing
[2024-03-04 11:44:31,022 INFO generators.py gen_for_qa l.548] (25/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:31,023 DEBUG generators.py gen_for_qa l.554] (25/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:31,024 DEBUG generators.py generate l.352] (25/93) Reuse existing Prompt
[2024-03-04 11:44:31,025 DEBUG generators.py generate l.365] (25/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:31,025 DEBUG generators.py generate l.373] (25/93) Reuse post-processing
[2024-03-04 11:44:31,027 INFO generators.py gen_for_qa l.548] (25/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:31,028 DEBUG generators.py generate l.349] (25/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:31,028 DEBUG generators.py generate l.358] (25/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:31,419 ERROR generators.py complete l.400] (25/93) The following exception occurred with prompt meta={} user=" Qui a inventé l'avion ?  A)  Orville et Wilbur Wright B)  Clément Ader C)  Gustave Whitehead .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:31,442 DEBUG generators.py generate l.373] (25/93) Reuse post-processing
[2024-03-04 11:44:31,444 INFO generators.py generate l.477] (25/93) End question " Qui a inventé l'avion ?  A)  Orville et Wilbur Wright B)  Clément Ader C)  Gustave Whitehead "
[2024-03-04 11:44:31,447 INFO generators.py generate l.475] (26/93) *** AnsGenerator for question " Qui a inventé la radio ?  A)  Guglielmo Marconi B)  Nikola Tesla C)  Oliver Lodge "
[2024-03-04 11:44:31,450 INFO generators.py gen_for_qa l.548] (26/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:31,453 DEBUG generators.py gen_for_qa l.554] (26/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:31,456 DEBUG generators.py generate l.352] (26/93) Reuse existing Prompt
[2024-03-04 11:44:31,458 DEBUG generators.py generate l.365] (26/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:31,459 DEBUG generators.py generate l.373] (26/93) Reuse post-processing
[2024-03-04 11:44:31,460 INFO generators.py gen_for_qa l.548] (26/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:31,460 DEBUG generators.py gen_for_qa l.554] (26/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:31,460 DEBUG generators.py generate l.352] (26/93) Reuse existing Prompt
[2024-03-04 11:44:31,460 DEBUG generators.py generate l.365] (26/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:31,467 DEBUG generators.py generate l.373] (26/93) Reuse post-processing
[2024-03-04 11:44:31,469 INFO generators.py gen_for_qa l.548] (26/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:31,469 DEBUG generators.py gen_for_qa l.554] (26/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:31,469 DEBUG generators.py generate l.352] (26/93) Reuse existing Prompt
[2024-03-04 11:44:31,473 DEBUG generators.py generate l.365] (26/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:31,473 DEBUG generators.py generate l.373] (26/93) Reuse post-processing
[2024-03-04 11:44:31,473 INFO generators.py gen_for_qa l.548] (26/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:31,477 DEBUG generators.py gen_for_qa l.554] (26/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:31,477 DEBUG generators.py generate l.352] (26/93) Reuse existing Prompt
[2024-03-04 11:44:31,478 DEBUG generators.py generate l.365] (26/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:31,479 DEBUG generators.py generate l.373] (26/93) Reuse post-processing
[2024-03-04 11:44:31,481 INFO generators.py gen_for_qa l.548] (26/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:31,481 DEBUG generators.py gen_for_qa l.554] (26/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:31,483 DEBUG generators.py generate l.352] (26/93) Reuse existing Prompt
[2024-03-04 11:44:31,485 DEBUG generators.py generate l.365] (26/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:31,487 DEBUG generators.py generate l.373] (26/93) Reuse post-processing
[2024-03-04 11:44:31,488 INFO generators.py gen_for_qa l.548] (26/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:31,490 DEBUG generators.py gen_for_qa l.554] (26/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:31,490 DEBUG generators.py generate l.352] (26/93) Reuse existing Prompt
[2024-03-04 11:44:31,491 DEBUG generators.py generate l.365] (26/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:31,492 DEBUG generators.py generate l.373] (26/93) Reuse post-processing
[2024-03-04 11:44:31,493 INFO generators.py gen_for_qa l.548] (26/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:31,493 DEBUG generators.py generate l.349] (26/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:31,493 DEBUG generators.py generate l.358] (26/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:31,924 ERROR generators.py complete l.400] (26/93) The following exception occurred with prompt meta={} user=" Qui a inventé la radio ?  A)  Guglielmo Marconi B)  Nikola Tesla C)  Oliver Lodge .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:31,936 DEBUG generators.py generate l.373] (26/93) Reuse post-processing
[2024-03-04 11:44:31,940 INFO generators.py generate l.477] (26/93) End question " Qui a inventé la radio ?  A)  Guglielmo Marconi B)  Nikola Tesla C)  Oliver Lodge "
[2024-03-04 11:44:31,940 INFO generators.py generate l.475] (27/93) *** AnsGenerator for question " Qui a inventé le télescope ?  A)  Hans Lippershey B)  Zacharias Janssen C)  Galileo Galilei "
[2024-03-04 11:44:31,944 INFO generators.py gen_for_qa l.548] (27/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:31,945 DEBUG generators.py gen_for_qa l.554] (27/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:31,946 DEBUG generators.py generate l.352] (27/93) Reuse existing Prompt
[2024-03-04 11:44:31,949 DEBUG generators.py generate l.365] (27/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:31,950 DEBUG generators.py generate l.373] (27/93) Reuse post-processing
[2024-03-04 11:44:31,951 INFO generators.py gen_for_qa l.548] (27/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:31,953 DEBUG generators.py gen_for_qa l.554] (27/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:31,955 DEBUG generators.py generate l.352] (27/93) Reuse existing Prompt
[2024-03-04 11:44:31,956 DEBUG generators.py generate l.365] (27/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:31,957 DEBUG generators.py generate l.373] (27/93) Reuse post-processing
[2024-03-04 11:44:31,958 INFO generators.py gen_for_qa l.548] (27/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:31,959 DEBUG generators.py gen_for_qa l.554] (27/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:31,960 DEBUG generators.py generate l.352] (27/93) Reuse existing Prompt
[2024-03-04 11:44:31,960 DEBUG generators.py generate l.365] (27/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:31,960 DEBUG generators.py generate l.373] (27/93) Reuse post-processing
[2024-03-04 11:44:31,960 INFO generators.py gen_for_qa l.548] (27/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:31,964 DEBUG generators.py gen_for_qa l.554] (27/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:31,966 DEBUG generators.py generate l.352] (27/93) Reuse existing Prompt
[2024-03-04 11:44:31,966 DEBUG generators.py generate l.365] (27/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:31,968 DEBUG generators.py generate l.373] (27/93) Reuse post-processing
[2024-03-04 11:44:31,968 INFO generators.py gen_for_qa l.548] (27/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:31,971 DEBUG generators.py gen_for_qa l.554] (27/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:31,972 DEBUG generators.py generate l.352] (27/93) Reuse existing Prompt
[2024-03-04 11:44:31,974 DEBUG generators.py generate l.365] (27/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:31,975 DEBUG generators.py generate l.373] (27/93) Reuse post-processing
[2024-03-04 11:44:31,975 INFO generators.py gen_for_qa l.548] (27/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:31,976 DEBUG generators.py gen_for_qa l.554] (27/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:31,977 DEBUG generators.py generate l.352] (27/93) Reuse existing Prompt
[2024-03-04 11:44:31,978 DEBUG generators.py generate l.365] (27/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:31,978 DEBUG generators.py generate l.373] (27/93) Reuse post-processing
[2024-03-04 11:44:31,978 INFO generators.py gen_for_qa l.548] (27/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:31,978 DEBUG generators.py generate l.349] (27/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:31,982 DEBUG generators.py generate l.358] (27/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:32,368 ERROR generators.py complete l.400] (27/93) The following exception occurred with prompt meta={} user=" Qui a inventé le télescope ?  A)  Hans Lippershey B)  Zacharias Janssen C)  Galileo Galilei .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:32,388 DEBUG generators.py generate l.373] (27/93) Reuse post-processing
[2024-03-04 11:44:32,393 INFO generators.py generate l.477] (27/93) End question " Qui a inventé le télescope ?  A)  Hans Lippershey B)  Zacharias Janssen C)  Galileo Galilei "
[2024-03-04 11:44:32,396 INFO generators.py generate l.475] (28/93) *** AnsGenerator for question " Qui a découvert l'oxygène ?  A)  Joseph Priestley B)  Carl Wilhelm Scheele C)  Antoine Lavoisier "
[2024-03-04 11:44:32,397 INFO generators.py gen_for_qa l.548] (28/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:32,399 DEBUG generators.py gen_for_qa l.554] (28/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:32,401 DEBUG generators.py generate l.352] (28/93) Reuse existing Prompt
[2024-03-04 11:44:32,404 DEBUG generators.py generate l.365] (28/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:32,405 DEBUG generators.py generate l.373] (28/93) Reuse post-processing
[2024-03-04 11:44:32,406 INFO generators.py gen_for_qa l.548] (28/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:32,408 DEBUG generators.py gen_for_qa l.554] (28/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:32,409 DEBUG generators.py generate l.352] (28/93) Reuse existing Prompt
[2024-03-04 11:44:32,409 DEBUG generators.py generate l.365] (28/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:32,409 DEBUG generators.py generate l.373] (28/93) Reuse post-processing
[2024-03-04 11:44:32,414 INFO generators.py gen_for_qa l.548] (28/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:32,415 DEBUG generators.py gen_for_qa l.554] (28/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:32,416 DEBUG generators.py generate l.352] (28/93) Reuse existing Prompt
[2024-03-04 11:44:32,418 DEBUG generators.py generate l.365] (28/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:32,420 DEBUG generators.py generate l.373] (28/93) Reuse post-processing
[2024-03-04 11:44:32,421 INFO generators.py gen_for_qa l.548] (28/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:32,422 DEBUG generators.py gen_for_qa l.554] (28/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:32,423 DEBUG generators.py generate l.352] (28/93) Reuse existing Prompt
[2024-03-04 11:44:32,425 DEBUG generators.py generate l.365] (28/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:32,426 DEBUG generators.py generate l.373] (28/93) Reuse post-processing
[2024-03-04 11:44:32,426 INFO generators.py gen_for_qa l.548] (28/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:32,428 DEBUG generators.py gen_for_qa l.554] (28/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:32,428 DEBUG generators.py generate l.352] (28/93) Reuse existing Prompt
[2024-03-04 11:44:32,429 DEBUG generators.py generate l.365] (28/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:32,430 DEBUG generators.py generate l.373] (28/93) Reuse post-processing
[2024-03-04 11:44:32,431 INFO generators.py gen_for_qa l.548] (28/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:32,433 DEBUG generators.py gen_for_qa l.554] (28/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:32,435 DEBUG generators.py generate l.352] (28/93) Reuse existing Prompt
[2024-03-04 11:44:32,437 DEBUG generators.py generate l.365] (28/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:32,438 DEBUG generators.py generate l.373] (28/93) Reuse post-processing
[2024-03-04 11:44:32,439 INFO generators.py gen_for_qa l.548] (28/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:32,440 DEBUG generators.py generate l.349] (28/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:32,441 DEBUG generators.py generate l.358] (28/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:32,820 ERROR generators.py complete l.400] (28/93) The following exception occurred with prompt meta={} user=" Qui a découvert l'oxygène ?  A)  Joseph Priestley B)  Carl Wilhelm Scheele C)  Antoine Lavoisier .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:32,820 DEBUG generators.py generate l.373] (28/93) Reuse post-processing
[2024-03-04 11:44:32,834 INFO generators.py generate l.477] (28/93) End question " Qui a découvert l'oxygène ?  A)  Joseph Priestley B)  Carl Wilhelm Scheele C)  Antoine Lavoisier "
[2024-03-04 11:44:32,837 INFO generators.py generate l.475] (29/93) *** AnsGenerator for question " Qui a inventé le moteur à réaction ?  A)  Frank Whittle B)  Hans von Ohain "
[2024-03-04 11:44:32,838 INFO generators.py gen_for_qa l.548] (29/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:32,839 DEBUG generators.py gen_for_qa l.554] (29/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:32,840 DEBUG generators.py generate l.352] (29/93) Reuse existing Prompt
[2024-03-04 11:44:32,841 DEBUG generators.py generate l.365] (29/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:32,843 DEBUG generators.py generate l.373] (29/93) Reuse post-processing
[2024-03-04 11:44:32,844 INFO generators.py gen_for_qa l.548] (29/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:32,844 DEBUG generators.py gen_for_qa l.554] (29/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:32,844 DEBUG generators.py generate l.352] (29/93) Reuse existing Prompt
[2024-03-04 11:44:32,847 DEBUG generators.py generate l.365] (29/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:32,847 DEBUG generators.py generate l.373] (29/93) Reuse post-processing
[2024-03-04 11:44:32,849 INFO generators.py gen_for_qa l.548] (29/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:32,850 DEBUG generators.py gen_for_qa l.554] (29/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:32,852 DEBUG generators.py generate l.352] (29/93) Reuse existing Prompt
[2024-03-04 11:44:32,854 DEBUG generators.py generate l.365] (29/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:32,856 DEBUG generators.py generate l.373] (29/93) Reuse post-processing
[2024-03-04 11:44:32,856 INFO generators.py gen_for_qa l.548] (29/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:32,857 DEBUG generators.py gen_for_qa l.554] (29/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:32,858 DEBUG generators.py generate l.352] (29/93) Reuse existing Prompt
[2024-03-04 11:44:32,860 DEBUG generators.py generate l.365] (29/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:32,860 DEBUG generators.py generate l.373] (29/93) Reuse post-processing
[2024-03-04 11:44:32,860 INFO generators.py gen_for_qa l.548] (29/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:32,862 DEBUG generators.py gen_for_qa l.554] (29/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:32,862 DEBUG generators.py generate l.352] (29/93) Reuse existing Prompt
[2024-03-04 11:44:32,864 DEBUG generators.py generate l.365] (29/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:32,864 DEBUG generators.py generate l.373] (29/93) Reuse post-processing
[2024-03-04 11:44:32,865 INFO generators.py gen_for_qa l.548] (29/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:32,867 DEBUG generators.py gen_for_qa l.554] (29/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:32,869 DEBUG generators.py generate l.352] (29/93) Reuse existing Prompt
[2024-03-04 11:44:32,871 DEBUG generators.py generate l.365] (29/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:32,871 DEBUG generators.py generate l.373] (29/93) Reuse post-processing
[2024-03-04 11:44:32,872 INFO generators.py gen_for_qa l.548] (29/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:32,874 DEBUG generators.py generate l.349] (29/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:32,875 DEBUG generators.py generate l.358] (29/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:33,276 ERROR generators.py complete l.400] (29/93) The following exception occurred with prompt meta={} user=" Qui a inventé le moteur à réaction ?  A)  Frank Whittle B)  Hans von Ohain .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:33,291 DEBUG generators.py generate l.373] (29/93) Reuse post-processing
[2024-03-04 11:44:33,294 INFO generators.py generate l.477] (29/93) End question " Qui a inventé le moteur à réaction ?  A)  Frank Whittle B)  Hans von Ohain "
[2024-03-04 11:44:33,297 INFO generators.py generate l.475] (30/93) *** AnsGenerator for question " Qui a inventé le radar ?  A)  Robert Watson-Watt B)  Christian Hülsmeyer C)  Guglielmo Marconi "
[2024-03-04 11:44:33,298 INFO generators.py gen_for_qa l.548] (30/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:33,300 DEBUG generators.py gen_for_qa l.554] (30/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:33,302 DEBUG generators.py generate l.352] (30/93) Reuse existing Prompt
[2024-03-04 11:44:33,305 DEBUG generators.py generate l.365] (30/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:33,306 DEBUG generators.py generate l.373] (30/93) Reuse post-processing
[2024-03-04 11:44:33,307 INFO generators.py gen_for_qa l.548] (30/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:33,309 DEBUG generators.py gen_for_qa l.554] (30/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:33,310 DEBUG generators.py generate l.352] (30/93) Reuse existing Prompt
[2024-03-04 11:44:33,312 DEBUG generators.py generate l.365] (30/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:33,312 DEBUG generators.py generate l.373] (30/93) Reuse post-processing
[2024-03-04 11:44:33,312 INFO generators.py gen_for_qa l.548] (30/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:33,315 DEBUG generators.py gen_for_qa l.554] (30/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:33,316 DEBUG generators.py generate l.352] (30/93) Reuse existing Prompt
[2024-03-04 11:44:33,318 DEBUG generators.py generate l.365] (30/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:33,320 DEBUG generators.py generate l.373] (30/93) Reuse post-processing
[2024-03-04 11:44:33,322 INFO generators.py gen_for_qa l.548] (30/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:33,322 DEBUG generators.py gen_for_qa l.554] (30/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:33,324 DEBUG generators.py generate l.352] (30/93) Reuse existing Prompt
[2024-03-04 11:44:33,324 DEBUG generators.py generate l.365] (30/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:33,324 DEBUG generators.py generate l.373] (30/93) Reuse post-processing
[2024-03-04 11:44:33,326 INFO generators.py gen_for_qa l.548] (30/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:33,327 DEBUG generators.py gen_for_qa l.554] (30/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:33,328 DEBUG generators.py generate l.352] (30/93) Reuse existing Prompt
[2024-03-04 11:44:33,330 DEBUG generators.py generate l.365] (30/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:33,331 DEBUG generators.py generate l.373] (30/93) Reuse post-processing
[2024-03-04 11:44:33,332 INFO generators.py gen_for_qa l.548] (30/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:33,333 DEBUG generators.py gen_for_qa l.554] (30/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:33,334 DEBUG generators.py generate l.352] (30/93) Reuse existing Prompt
[2024-03-04 11:44:33,334 DEBUG generators.py generate l.365] (30/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:33,338 DEBUG generators.py generate l.373] (30/93) Reuse post-processing
[2024-03-04 11:44:33,339 INFO generators.py gen_for_qa l.548] (30/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:33,340 DEBUG generators.py generate l.349] (30/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:33,341 DEBUG generators.py generate l.358] (30/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:33,733 ERROR generators.py complete l.400] (30/93) The following exception occurred with prompt meta={} user=" Qui a inventé le radar ?  A)  Robert Watson-Watt B)  Christian Hülsmeyer C)  Guglielmo Marconi .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:33,750 DEBUG generators.py generate l.373] (30/93) Reuse post-processing
[2024-03-04 11:44:33,754 INFO generators.py generate l.477] (30/93) End question " Qui a inventé le radar ?  A)  Robert Watson-Watt B)  Christian Hülsmeyer C)  Guglielmo Marconi "
[2024-03-04 11:44:33,757 INFO generators.py generate l.475] (31/93) *** AnsGenerator for question " Qui a découvert la pénicilline ?  A)  Alexander Fleming B)  Ernest Duchesne "
[2024-03-04 11:44:33,758 INFO generators.py gen_for_qa l.548] (31/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:33,758 DEBUG generators.py gen_for_qa l.554] (31/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:33,761 DEBUG generators.py generate l.352] (31/93) Reuse existing Prompt
[2024-03-04 11:44:33,764 DEBUG generators.py generate l.365] (31/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:33,766 DEBUG generators.py generate l.373] (31/93) Reuse post-processing
[2024-03-04 11:44:33,767 INFO generators.py gen_for_qa l.548] (31/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:33,770 DEBUG generators.py gen_for_qa l.554] (31/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:33,771 DEBUG generators.py generate l.352] (31/93) Reuse existing Prompt
[2024-03-04 11:44:33,772 DEBUG generators.py generate l.365] (31/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:33,774 DEBUG generators.py generate l.373] (31/93) Reuse post-processing
[2024-03-04 11:44:33,774 INFO generators.py gen_for_qa l.548] (31/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:33,776 DEBUG generators.py gen_for_qa l.554] (31/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:33,778 DEBUG generators.py generate l.352] (31/93) Reuse existing Prompt
[2024-03-04 11:44:33,778 DEBUG generators.py generate l.365] (31/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:33,780 DEBUG generators.py generate l.373] (31/93) Reuse post-processing
[2024-03-04 11:44:33,781 INFO generators.py gen_for_qa l.548] (31/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:33,782 DEBUG generators.py gen_for_qa l.554] (31/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:33,784 DEBUG generators.py generate l.352] (31/93) Reuse existing Prompt
[2024-03-04 11:44:33,786 DEBUG generators.py generate l.365] (31/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:33,787 DEBUG generators.py generate l.373] (31/93) Reuse post-processing
[2024-03-04 11:44:33,787 INFO generators.py gen_for_qa l.548] (31/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:33,789 DEBUG generators.py gen_for_qa l.554] (31/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:33,790 DEBUG generators.py generate l.352] (31/93) Reuse existing Prompt
[2024-03-04 11:44:33,791 DEBUG generators.py generate l.365] (31/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:33,791 DEBUG generators.py generate l.373] (31/93) Reuse post-processing
[2024-03-04 11:44:33,791 INFO generators.py gen_for_qa l.548] (31/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:33,794 DEBUG generators.py gen_for_qa l.554] (31/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:33,794 DEBUG generators.py generate l.352] (31/93) Reuse existing Prompt
[2024-03-04 11:44:33,794 DEBUG generators.py generate l.365] (31/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:33,797 DEBUG generators.py generate l.373] (31/93) Reuse post-processing
[2024-03-04 11:44:33,797 INFO generators.py gen_for_qa l.548] (31/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:33,798 DEBUG generators.py generate l.349] (31/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:33,801 DEBUG generators.py generate l.358] (31/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:34,221 ERROR generators.py complete l.400] (31/93) The following exception occurred with prompt meta={} user=" Qui a découvert la pénicilline ?  A)  Alexander Fleming B)  Ernest Duchesne .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:34,237 DEBUG generators.py generate l.373] (31/93) Reuse post-processing
[2024-03-04 11:44:34,239 INFO generators.py generate l.477] (31/93) End question " Qui a découvert la pénicilline ?  A)  Alexander Fleming B)  Ernest Duchesne "
[2024-03-04 11:44:34,240 INFO generators.py generate l.475] (32/93) *** AnsGenerator for question " Qui a inventé la télévision ?  A)  John Logie Baird B)  Philo Farnsworth C)  Vladimir Zworykin "
[2024-03-04 11:44:34,242 INFO generators.py gen_for_qa l.548] (32/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:34,245 DEBUG generators.py gen_for_qa l.554] (32/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:34,248 DEBUG generators.py generate l.352] (32/93) Reuse existing Prompt
[2024-03-04 11:44:34,249 DEBUG generators.py generate l.365] (32/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:34,249 DEBUG generators.py generate l.373] (32/93) Reuse post-processing
[2024-03-04 11:44:34,252 INFO generators.py gen_for_qa l.548] (32/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:34,252 DEBUG generators.py gen_for_qa l.554] (32/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:34,255 DEBUG generators.py generate l.352] (32/93) Reuse existing Prompt
[2024-03-04 11:44:34,256 DEBUG generators.py generate l.365] (32/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:34,258 DEBUG generators.py generate l.373] (32/93) Reuse post-processing
[2024-03-04 11:44:34,259 INFO generators.py gen_for_qa l.548] (32/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:34,260 DEBUG generators.py gen_for_qa l.554] (32/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:34,261 DEBUG generators.py generate l.352] (32/93) Reuse existing Prompt
[2024-03-04 11:44:34,263 DEBUG generators.py generate l.365] (32/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:34,263 DEBUG generators.py generate l.373] (32/93) Reuse post-processing
[2024-03-04 11:44:34,264 INFO generators.py gen_for_qa l.548] (32/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:34,266 DEBUG generators.py gen_for_qa l.554] (32/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:34,267 DEBUG generators.py generate l.352] (32/93) Reuse existing Prompt
[2024-03-04 11:44:34,269 DEBUG generators.py generate l.365] (32/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:34,269 DEBUG generators.py generate l.373] (32/93) Reuse post-processing
[2024-03-04 11:44:34,271 INFO generators.py gen_for_qa l.548] (32/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:34,273 DEBUG generators.py gen_for_qa l.554] (32/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:34,274 DEBUG generators.py generate l.352] (32/93) Reuse existing Prompt
[2024-03-04 11:44:34,275 DEBUG generators.py generate l.365] (32/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:34,276 DEBUG generators.py generate l.373] (32/93) Reuse post-processing
[2024-03-04 11:44:34,276 INFO generators.py gen_for_qa l.548] (32/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:34,276 DEBUG generators.py gen_for_qa l.554] (32/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:34,279 DEBUG generators.py generate l.352] (32/93) Reuse existing Prompt
[2024-03-04 11:44:34,280 DEBUG generators.py generate l.365] (32/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:34,281 DEBUG generators.py generate l.373] (32/93) Reuse post-processing
[2024-03-04 11:44:34,282 INFO generators.py gen_for_qa l.548] (32/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:34,284 DEBUG generators.py generate l.349] (32/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:34,286 DEBUG generators.py generate l.358] (32/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:34,691 ERROR generators.py complete l.400] (32/93) The following exception occurred with prompt meta={} user=" Qui a inventé la télévision ?  A)  John Logie Baird B)  Philo Farnsworth C)  Vladimir Zworykin .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:34,708 DEBUG generators.py generate l.373] (32/93) Reuse post-processing
[2024-03-04 11:44:34,712 INFO generators.py generate l.477] (32/93) End question " Qui a inventé la télévision ?  A)  John Logie Baird B)  Philo Farnsworth C)  Vladimir Zworykin "
[2024-03-04 11:44:34,716 INFO generators.py generate l.475] (33/93) *** AnsGenerator for question " Qui a découvert l'ADN ?  A)  James Watson et Francis Crick B)  Rosalind Franklin C)  Maurice Wilkins "
[2024-03-04 11:44:34,722 INFO generators.py gen_for_qa l.548] (33/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:34,725 DEBUG generators.py gen_for_qa l.554] (33/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:34,732 DEBUG generators.py generate l.352] (33/93) Reuse existing Prompt
[2024-03-04 11:44:34,735 DEBUG generators.py generate l.365] (33/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:34,738 DEBUG generators.py generate l.373] (33/93) Reuse post-processing
[2024-03-04 11:44:34,744 INFO generators.py gen_for_qa l.548] (33/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:34,747 DEBUG generators.py gen_for_qa l.554] (33/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:34,751 DEBUG generators.py generate l.352] (33/93) Reuse existing Prompt
[2024-03-04 11:44:34,753 DEBUG generators.py generate l.365] (33/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:34,756 DEBUG generators.py generate l.373] (33/93) Reuse post-processing
[2024-03-04 11:44:34,759 INFO generators.py gen_for_qa l.548] (33/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:34,759 DEBUG generators.py gen_for_qa l.554] (33/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:34,762 DEBUG generators.py generate l.352] (33/93) Reuse existing Prompt
[2024-03-04 11:44:34,764 DEBUG generators.py generate l.365] (33/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:34,766 DEBUG generators.py generate l.373] (33/93) Reuse post-processing
[2024-03-04 11:44:34,768 INFO generators.py gen_for_qa l.548] (33/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:34,770 DEBUG generators.py gen_for_qa l.554] (33/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:34,772 DEBUG generators.py generate l.352] (33/93) Reuse existing Prompt
[2024-03-04 11:44:34,774 DEBUG generators.py generate l.365] (33/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:34,775 DEBUG generators.py generate l.373] (33/93) Reuse post-processing
[2024-03-04 11:44:34,777 INFO generators.py gen_for_qa l.548] (33/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:34,778 DEBUG generators.py gen_for_qa l.554] (33/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:34,779 DEBUG generators.py generate l.352] (33/93) Reuse existing Prompt
[2024-03-04 11:44:34,781 DEBUG generators.py generate l.365] (33/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:34,782 DEBUG generators.py generate l.373] (33/93) Reuse post-processing
[2024-03-04 11:44:34,784 INFO generators.py gen_for_qa l.548] (33/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:34,787 DEBUG generators.py gen_for_qa l.554] (33/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:34,789 DEBUG generators.py generate l.352] (33/93) Reuse existing Prompt
[2024-03-04 11:44:34,789 DEBUG generators.py generate l.365] (33/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:34,791 DEBUG generators.py generate l.373] (33/93) Reuse post-processing
[2024-03-04 11:44:34,792 INFO generators.py gen_for_qa l.548] (33/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:34,794 DEBUG generators.py generate l.349] (33/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:34,794 DEBUG generators.py generate l.358] (33/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:35,189 ERROR generators.py complete l.400] (33/93) The following exception occurred with prompt meta={} user=" Qui a découvert l'ADN ?  A)  James Watson et Francis Crick B)  Rosalind Franklin C)  Maurice Wilkins .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:35,204 DEBUG generators.py generate l.373] (33/93) Reuse post-processing
[2024-03-04 11:44:35,207 INFO generators.py generate l.477] (33/93) End question " Qui a découvert l'ADN ?  A)  James Watson et Francis Crick B)  Rosalind Franklin C)  Maurice Wilkins "
[2024-03-04 11:44:35,208 INFO generators.py generate l.475] (34/93) *** AnsGenerator for question " Qui a inventé le laser ?  A)  Gordon Gould B)  Charles Hard Townes C)  Arthur Leonard Schawlow "
[2024-03-04 11:44:35,210 INFO generators.py gen_for_qa l.548] (34/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:35,213 DEBUG generators.py gen_for_qa l.554] (34/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:35,214 DEBUG generators.py generate l.352] (34/93) Reuse existing Prompt
[2024-03-04 11:44:35,216 DEBUG generators.py generate l.365] (34/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:35,220 DEBUG generators.py generate l.373] (34/93) Reuse post-processing
[2024-03-04 11:44:35,222 INFO generators.py gen_for_qa l.548] (34/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:35,222 DEBUG generators.py gen_for_qa l.554] (34/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:35,224 DEBUG generators.py generate l.352] (34/93) Reuse existing Prompt
[2024-03-04 11:44:35,225 DEBUG generators.py generate l.365] (34/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:35,225 DEBUG generators.py generate l.373] (34/93) Reuse post-processing
[2024-03-04 11:44:35,226 INFO generators.py gen_for_qa l.548] (34/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:35,226 DEBUG generators.py gen_for_qa l.554] (34/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:35,230 DEBUG generators.py generate l.352] (34/93) Reuse existing Prompt
[2024-03-04 11:44:35,230 DEBUG generators.py generate l.365] (34/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:35,232 DEBUG generators.py generate l.373] (34/93) Reuse post-processing
[2024-03-04 11:44:35,234 INFO generators.py gen_for_qa l.548] (34/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:35,235 DEBUG generators.py gen_for_qa l.554] (34/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:35,235 DEBUG generators.py generate l.352] (34/93) Reuse existing Prompt
[2024-03-04 11:44:35,239 DEBUG generators.py generate l.365] (34/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:35,239 DEBUG generators.py generate l.373] (34/93) Reuse post-processing
[2024-03-04 11:44:35,241 INFO generators.py gen_for_qa l.548] (34/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:35,242 DEBUG generators.py gen_for_qa l.554] (34/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:35,243 DEBUG generators.py generate l.352] (34/93) Reuse existing Prompt
[2024-03-04 11:44:35,244 DEBUG generators.py generate l.365] (34/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:35,244 DEBUG generators.py generate l.373] (34/93) Reuse post-processing
[2024-03-04 11:44:35,246 INFO generators.py gen_for_qa l.548] (34/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:35,246 DEBUG generators.py gen_for_qa l.554] (34/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:35,249 DEBUG generators.py generate l.352] (34/93) Reuse existing Prompt
[2024-03-04 11:44:35,251 DEBUG generators.py generate l.365] (34/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:35,252 DEBUG generators.py generate l.373] (34/93) Reuse post-processing
[2024-03-04 11:44:35,253 INFO generators.py gen_for_qa l.548] (34/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:35,254 DEBUG generators.py generate l.349] (34/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:35,255 DEBUG generators.py generate l.358] (34/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:35,690 ERROR generators.py complete l.400] (34/93) The following exception occurred with prompt meta={} user=" Qui a inventé le laser ?  A)  Gordon Gould B)  Charles Hard Townes C)  Arthur Leonard Schawlow .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:35,705 DEBUG generators.py generate l.373] (34/93) Reuse post-processing
[2024-03-04 11:44:35,707 INFO generators.py generate l.477] (34/93) End question " Qui a inventé le laser ?  A)  Gordon Gould B)  Charles Hard Townes C)  Arthur Leonard Schawlow "
[2024-03-04 11:44:35,710 INFO generators.py generate l.475] (35/93) *** AnsGenerator for question " Qui a inventé le transistor ?  A)  John Bardeen, Walter Brattain et William Shockley B)  Julius Edgar Lilienfeld C)  Oskar Heil "
[2024-03-04 11:44:35,712 INFO generators.py gen_for_qa l.548] (35/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:35,714 DEBUG generators.py gen_for_qa l.554] (35/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:35,716 DEBUG generators.py generate l.352] (35/93) Reuse existing Prompt
[2024-03-04 11:44:35,718 DEBUG generators.py generate l.365] (35/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:35,720 DEBUG generators.py generate l.373] (35/93) Reuse post-processing
[2024-03-04 11:44:35,720 INFO generators.py gen_for_qa l.548] (35/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:35,720 DEBUG generators.py gen_for_qa l.554] (35/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:35,720 DEBUG generators.py generate l.352] (35/93) Reuse existing Prompt
[2024-03-04 11:44:35,726 DEBUG generators.py generate l.365] (35/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:35,727 DEBUG generators.py generate l.373] (35/93) Reuse post-processing
[2024-03-04 11:44:35,728 INFO generators.py gen_for_qa l.548] (35/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:35,729 DEBUG generators.py gen_for_qa l.554] (35/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:35,732 DEBUG generators.py generate l.352] (35/93) Reuse existing Prompt
[2024-03-04 11:44:35,732 DEBUG generators.py generate l.365] (35/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:35,732 DEBUG generators.py generate l.373] (35/93) Reuse post-processing
[2024-03-04 11:44:35,737 INFO generators.py gen_for_qa l.548] (35/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:35,738 DEBUG generators.py gen_for_qa l.554] (35/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:35,739 DEBUG generators.py generate l.352] (35/93) Reuse existing Prompt
[2024-03-04 11:44:35,740 DEBUG generators.py generate l.365] (35/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:35,742 DEBUG generators.py generate l.373] (35/93) Reuse post-processing
[2024-03-04 11:44:35,742 INFO generators.py gen_for_qa l.548] (35/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:35,743 DEBUG generators.py gen_for_qa l.554] (35/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:35,744 DEBUG generators.py generate l.352] (35/93) Reuse existing Prompt
[2024-03-04 11:44:35,744 DEBUG generators.py generate l.365] (35/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:35,747 DEBUG generators.py generate l.373] (35/93) Reuse post-processing
[2024-03-04 11:44:35,747 INFO generators.py gen_for_qa l.548] (35/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:35,748 DEBUG generators.py gen_for_qa l.554] (35/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:35,750 DEBUG generators.py generate l.352] (35/93) Reuse existing Prompt
[2024-03-04 11:44:35,752 DEBUG generators.py generate l.365] (35/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:35,752 DEBUG generators.py generate l.373] (35/93) Reuse post-processing
[2024-03-04 11:44:35,752 INFO generators.py gen_for_qa l.548] (35/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:35,755 DEBUG generators.py generate l.349] (35/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:35,755 DEBUG generators.py generate l.358] (35/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:36,154 ERROR generators.py complete l.400] (35/93) The following exception occurred with prompt meta={} user=" Qui a inventé le transistor ?  A)  John Bardeen, Walter Brattain et William Shockley B)  Julius Edgar Lilienfeld C)  Oskar Heil .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:36,168 DEBUG generators.py generate l.373] (35/93) Reuse post-processing
[2024-03-04 11:44:36,168 INFO generators.py generate l.477] (35/93) End question " Qui a inventé le transistor ?  A)  John Bardeen, Walter Brattain et William Shockley B)  Julius Edgar Lilienfeld C)  Oskar Heil "
[2024-03-04 11:44:36,168 INFO generators.py generate l.475] (36/93) *** AnsGenerator for question " Qui a inventé l'ordinateur ?  A)  Charles Babbage B)  Alan Turing C)  John Atanasoff et Clifford Berry "
[2024-03-04 11:44:36,168 INFO generators.py gen_for_qa l.548] (36/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:36,177 DEBUG generators.py gen_for_qa l.554] (36/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:36,179 DEBUG generators.py generate l.352] (36/93) Reuse existing Prompt
[2024-03-04 11:44:36,180 DEBUG generators.py generate l.365] (36/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:36,182 DEBUG generators.py generate l.373] (36/93) Reuse post-processing
[2024-03-04 11:44:36,182 INFO generators.py gen_for_qa l.548] (36/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:36,184 DEBUG generators.py gen_for_qa l.554] (36/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:36,186 DEBUG generators.py generate l.352] (36/93) Reuse existing Prompt
[2024-03-04 11:44:36,188 DEBUG generators.py generate l.365] (36/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:36,188 DEBUG generators.py generate l.373] (36/93) Reuse post-processing
[2024-03-04 11:44:36,189 INFO generators.py gen_for_qa l.548] (36/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:36,190 DEBUG generators.py gen_for_qa l.554] (36/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:36,191 DEBUG generators.py generate l.352] (36/93) Reuse existing Prompt
[2024-03-04 11:44:36,193 DEBUG generators.py generate l.365] (36/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:36,194 DEBUG generators.py generate l.373] (36/93) Reuse post-processing
[2024-03-04 11:44:36,194 INFO generators.py gen_for_qa l.548] (36/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:36,196 DEBUG generators.py gen_for_qa l.554] (36/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:36,196 DEBUG generators.py generate l.352] (36/93) Reuse existing Prompt
[2024-03-04 11:44:36,198 DEBUG generators.py generate l.365] (36/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:36,201 DEBUG generators.py generate l.373] (36/93) Reuse post-processing
[2024-03-04 11:44:36,202 INFO generators.py gen_for_qa l.548] (36/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:36,204 DEBUG generators.py gen_for_qa l.554] (36/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:36,205 DEBUG generators.py generate l.352] (36/93) Reuse existing Prompt
[2024-03-04 11:44:36,205 DEBUG generators.py generate l.365] (36/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:36,206 DEBUG generators.py generate l.373] (36/93) Reuse post-processing
[2024-03-04 11:44:36,206 INFO generators.py gen_for_qa l.548] (36/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:36,209 DEBUG generators.py gen_for_qa l.554] (36/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:36,210 DEBUG generators.py generate l.352] (36/93) Reuse existing Prompt
[2024-03-04 11:44:36,210 DEBUG generators.py generate l.365] (36/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:36,211 DEBUG generators.py generate l.373] (36/93) Reuse post-processing
[2024-03-04 11:44:36,212 INFO generators.py gen_for_qa l.548] (36/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:36,213 DEBUG generators.py generate l.349] (36/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:36,214 DEBUG generators.py generate l.358] (36/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:36,643 ERROR generators.py complete l.400] (36/93) The following exception occurred with prompt meta={} user=" Qui a inventé l'ordinateur ?  A)  Charles Babbage B)  Alan Turing C)  John Atanasoff et Clifford Berry .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:36,656 DEBUG generators.py generate l.373] (36/93) Reuse post-processing
[2024-03-04 11:44:36,656 INFO generators.py generate l.477] (36/93) End question " Qui a inventé l'ordinateur ?  A)  Charles Babbage B)  Alan Turing C)  John Atanasoff et Clifford Berry "
[2024-03-04 11:44:36,659 INFO generators.py generate l.475] (37/93) *** AnsGenerator for question " Qui a découvert le boson de Higgs ?  A)  Peter Higgs B)  François Englert C)  Robert Brout "
[2024-03-04 11:44:36,661 INFO generators.py gen_for_qa l.548] (37/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:36,663 DEBUG generators.py gen_for_qa l.554] (37/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:36,665 DEBUG generators.py generate l.352] (37/93) Reuse existing Prompt
[2024-03-04 11:44:36,666 DEBUG generators.py generate l.365] (37/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:36,667 DEBUG generators.py generate l.373] (37/93) Reuse post-processing
[2024-03-04 11:44:36,670 INFO generators.py gen_for_qa l.548] (37/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:36,671 DEBUG generators.py gen_for_qa l.554] (37/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:36,673 DEBUG generators.py generate l.352] (37/93) Reuse existing Prompt
[2024-03-04 11:44:36,674 DEBUG generators.py generate l.365] (37/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:36,676 DEBUG generators.py generate l.373] (37/93) Reuse post-processing
[2024-03-04 11:44:36,676 INFO generators.py gen_for_qa l.548] (37/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:36,678 DEBUG generators.py gen_for_qa l.554] (37/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:36,678 DEBUG generators.py generate l.352] (37/93) Reuse existing Prompt
[2024-03-04 11:44:36,680 DEBUG generators.py generate l.365] (37/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:36,680 DEBUG generators.py generate l.373] (37/93) Reuse post-processing
[2024-03-04 11:44:36,683 INFO generators.py gen_for_qa l.548] (37/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:36,684 DEBUG generators.py gen_for_qa l.554] (37/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:36,686 DEBUG generators.py generate l.352] (37/93) Reuse existing Prompt
[2024-03-04 11:44:36,686 DEBUG generators.py generate l.365] (37/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:36,688 DEBUG generators.py generate l.373] (37/93) Reuse post-processing
[2024-03-04 11:44:36,690 INFO generators.py gen_for_qa l.548] (37/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:36,690 DEBUG generators.py gen_for_qa l.554] (37/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:36,691 DEBUG generators.py generate l.352] (37/93) Reuse existing Prompt
[2024-03-04 11:44:36,692 DEBUG generators.py generate l.365] (37/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:36,693 DEBUG generators.py generate l.373] (37/93) Reuse post-processing
[2024-03-04 11:44:36,694 INFO generators.py gen_for_qa l.548] (37/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:36,694 DEBUG generators.py gen_for_qa l.554] (37/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:36,695 DEBUG generators.py generate l.352] (37/93) Reuse existing Prompt
[2024-03-04 11:44:36,695 DEBUG generators.py generate l.365] (37/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:36,697 DEBUG generators.py generate l.373] (37/93) Reuse post-processing
[2024-03-04 11:44:36,697 INFO generators.py gen_for_qa l.548] (37/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:36,699 DEBUG generators.py generate l.349] (37/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:36,702 DEBUG generators.py generate l.358] (37/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:37,110 ERROR generators.py complete l.400] (37/93) The following exception occurred with prompt meta={} user=" Qui a découvert le boson de Higgs ?  A)  Peter Higgs B)  François Englert C)  Robert Brout .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:37,121 DEBUG generators.py generate l.373] (37/93) Reuse post-processing
[2024-03-04 11:44:37,127 INFO generators.py generate l.477] (37/93) End question " Qui a découvert le boson de Higgs ?  A)  Peter Higgs B)  François Englert C)  Robert Brout "
[2024-03-04 11:44:37,129 INFO generators.py generate l.475] (38/93) *** AnsGenerator for question " Qui a inventé le World Wide Web ?  A)  Tim Berners-Lee "
[2024-03-04 11:44:37,130 INFO generators.py gen_for_qa l.548] (38/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:37,132 DEBUG generators.py gen_for_qa l.554] (38/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:37,135 DEBUG generators.py generate l.352] (38/93) Reuse existing Prompt
[2024-03-04 11:44:37,136 DEBUG generators.py generate l.365] (38/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:37,139 DEBUG generators.py generate l.373] (38/93) Reuse post-processing
[2024-03-04 11:44:37,140 INFO generators.py gen_for_qa l.548] (38/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:37,142 DEBUG generators.py gen_for_qa l.554] (38/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:37,142 DEBUG generators.py generate l.352] (38/93) Reuse existing Prompt
[2024-03-04 11:44:37,143 DEBUG generators.py generate l.365] (38/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:37,146 DEBUG generators.py generate l.373] (38/93) Reuse post-processing
[2024-03-04 11:44:37,147 INFO generators.py gen_for_qa l.548] (38/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:37,149 DEBUG generators.py gen_for_qa l.554] (38/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:37,151 DEBUG generators.py generate l.352] (38/93) Reuse existing Prompt
[2024-03-04 11:44:37,153 DEBUG generators.py generate l.365] (38/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:37,153 DEBUG generators.py generate l.373] (38/93) Reuse post-processing
[2024-03-04 11:44:37,155 INFO generators.py gen_for_qa l.548] (38/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:37,156 DEBUG generators.py gen_for_qa l.554] (38/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:37,156 DEBUG generators.py generate l.352] (38/93) Reuse existing Prompt
[2024-03-04 11:44:37,159 DEBUG generators.py generate l.365] (38/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:37,159 DEBUG generators.py generate l.373] (38/93) Reuse post-processing
[2024-03-04 11:44:37,160 INFO generators.py gen_for_qa l.548] (38/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:37,161 DEBUG generators.py gen_for_qa l.554] (38/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:37,161 DEBUG generators.py generate l.352] (38/93) Reuse existing Prompt
[2024-03-04 11:44:37,163 DEBUG generators.py generate l.365] (38/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:37,163 DEBUG generators.py generate l.373] (38/93) Reuse post-processing
[2024-03-04 11:44:37,164 INFO generators.py gen_for_qa l.548] (38/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:37,165 DEBUG generators.py gen_for_qa l.554] (38/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:37,168 DEBUG generators.py generate l.352] (38/93) Reuse existing Prompt
[2024-03-04 11:44:37,169 DEBUG generators.py generate l.365] (38/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:37,170 DEBUG generators.py generate l.373] (38/93) Reuse post-processing
[2024-03-04 11:44:37,171 INFO generators.py gen_for_qa l.548] (38/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:37,172 DEBUG generators.py generate l.349] (38/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:37,173 DEBUG generators.py generate l.358] (38/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:37,557 ERROR generators.py complete l.400] (38/93) The following exception occurred with prompt meta={} user=" Qui a inventé le World Wide Web ?  A)  Tim Berners-Lee .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:37,564 DEBUG generators.py generate l.373] (38/93) Reuse post-processing
[2024-03-04 11:44:37,565 INFO generators.py generate l.477] (38/93) End question " Qui a inventé le World Wide Web ?  A)  Tim Berners-Lee "
[2024-03-04 11:44:37,567 INFO generators.py generate l.475] (39/93) *** AnsGenerator for question " Qui a découvert le vaccin contre la variole ?  A)  Edward Jenner B)  Benjamin Jesty "
[2024-03-04 11:44:37,568 INFO generators.py gen_for_qa l.548] (39/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:37,569 DEBUG generators.py gen_for_qa l.554] (39/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:37,571 DEBUG generators.py generate l.352] (39/93) Reuse existing Prompt
[2024-03-04 11:44:37,572 DEBUG generators.py generate l.365] (39/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:37,573 DEBUG generators.py generate l.373] (39/93) Reuse post-processing
[2024-03-04 11:44:37,574 INFO generators.py gen_for_qa l.548] (39/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:37,575 DEBUG generators.py gen_for_qa l.554] (39/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:37,576 DEBUG generators.py generate l.352] (39/93) Reuse existing Prompt
[2024-03-04 11:44:37,577 DEBUG generators.py generate l.365] (39/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:37,579 DEBUG generators.py generate l.373] (39/93) Reuse post-processing
[2024-03-04 11:44:37,580 INFO generators.py gen_for_qa l.548] (39/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:37,581 DEBUG generators.py gen_for_qa l.554] (39/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:37,582 DEBUG generators.py generate l.352] (39/93) Reuse existing Prompt
[2024-03-04 11:44:37,583 DEBUG generators.py generate l.365] (39/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:37,586 DEBUG generators.py generate l.373] (39/93) Reuse post-processing
[2024-03-04 11:44:37,586 INFO generators.py gen_for_qa l.548] (39/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:37,588 DEBUG generators.py gen_for_qa l.554] (39/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:37,588 DEBUG generators.py generate l.352] (39/93) Reuse existing Prompt
[2024-03-04 11:44:37,588 DEBUG generators.py generate l.365] (39/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:37,591 DEBUG generators.py generate l.373] (39/93) Reuse post-processing
[2024-03-04 11:44:37,592 INFO generators.py gen_for_qa l.548] (39/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:37,592 DEBUG generators.py gen_for_qa l.554] (39/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:37,593 DEBUG generators.py generate l.352] (39/93) Reuse existing Prompt
[2024-03-04 11:44:37,594 DEBUG generators.py generate l.365] (39/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:37,595 DEBUG generators.py generate l.373] (39/93) Reuse post-processing
[2024-03-04 11:44:37,596 INFO generators.py gen_for_qa l.548] (39/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:37,597 DEBUG generators.py gen_for_qa l.554] (39/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:37,598 DEBUG generators.py generate l.352] (39/93) Reuse existing Prompt
[2024-03-04 11:44:37,600 DEBUG generators.py generate l.365] (39/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:37,602 DEBUG generators.py generate l.373] (39/93) Reuse post-processing
[2024-03-04 11:44:37,603 INFO generators.py gen_for_qa l.548] (39/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:37,603 DEBUG generators.py generate l.349] (39/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:37,604 DEBUG generators.py generate l.358] (39/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:38,006 ERROR generators.py complete l.400] (39/93) The following exception occurred with prompt meta={} user=" Qui a découvert le vaccin contre la variole ?  A)  Edward Jenner B)  Benjamin Jesty .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:38,013 DEBUG generators.py generate l.373] (39/93) Reuse post-processing
[2024-03-04 11:44:38,022 INFO generators.py generate l.477] (39/93) End question " Qui a découvert le vaccin contre la variole ?  A)  Edward Jenner B)  Benjamin Jesty "
[2024-03-04 11:44:38,023 INFO generators.py generate l.475] (40/93) *** AnsGenerator for question " Qui a inventé le sous-marin ?  A)  Cornelis Drebbel B)  David Bushnell C)  Robert Fulton "
[2024-03-04 11:44:38,027 INFO generators.py gen_for_qa l.548] (40/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:38,028 DEBUG generators.py gen_for_qa l.554] (40/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:38,029 DEBUG generators.py generate l.352] (40/93) Reuse existing Prompt
[2024-03-04 11:44:38,031 DEBUG generators.py generate l.365] (40/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:38,033 DEBUG generators.py generate l.373] (40/93) Reuse post-processing
[2024-03-04 11:44:38,036 INFO generators.py gen_for_qa l.548] (40/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:38,038 DEBUG generators.py gen_for_qa l.554] (40/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:38,038 DEBUG generators.py generate l.352] (40/93) Reuse existing Prompt
[2024-03-04 11:44:38,039 DEBUG generators.py generate l.365] (40/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:38,040 DEBUG generators.py generate l.373] (40/93) Reuse post-processing
[2024-03-04 11:44:38,042 INFO generators.py gen_for_qa l.548] (40/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:38,043 DEBUG generators.py gen_for_qa l.554] (40/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:38,043 DEBUG generators.py generate l.352] (40/93) Reuse existing Prompt
[2024-03-04 11:44:38,045 DEBUG generators.py generate l.365] (40/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:38,046 DEBUG generators.py generate l.373] (40/93) Reuse post-processing
[2024-03-04 11:44:38,047 INFO generators.py gen_for_qa l.548] (40/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:38,049 DEBUG generators.py gen_for_qa l.554] (40/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:38,051 DEBUG generators.py generate l.352] (40/93) Reuse existing Prompt
[2024-03-04 11:44:38,053 DEBUG generators.py generate l.365] (40/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:38,054 DEBUG generators.py generate l.373] (40/93) Reuse post-processing
[2024-03-04 11:44:38,055 INFO generators.py gen_for_qa l.548] (40/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:38,056 DEBUG generators.py gen_for_qa l.554] (40/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:38,057 DEBUG generators.py generate l.352] (40/93) Reuse existing Prompt
[2024-03-04 11:44:38,058 DEBUG generators.py generate l.365] (40/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:38,058 DEBUG generators.py generate l.373] (40/93) Reuse post-processing
[2024-03-04 11:44:38,059 INFO generators.py gen_for_qa l.548] (40/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:38,059 DEBUG generators.py gen_for_qa l.554] (40/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:38,061 DEBUG generators.py generate l.352] (40/93) Reuse existing Prompt
[2024-03-04 11:44:38,063 DEBUG generators.py generate l.365] (40/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:38,063 DEBUG generators.py generate l.373] (40/93) Reuse post-processing
[2024-03-04 11:44:38,065 INFO generators.py gen_for_qa l.548] (40/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:38,066 DEBUG generators.py generate l.349] (40/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:38,067 DEBUG generators.py generate l.358] (40/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:38,446 ERROR generators.py complete l.400] (40/93) The following exception occurred with prompt meta={} user=" Qui a inventé le sous-marin ?  A)  Cornelis Drebbel B)  David Bushnell C)  Robert Fulton .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:38,472 DEBUG generators.py generate l.373] (40/93) Reuse post-processing
[2024-03-04 11:44:38,476 INFO generators.py generate l.477] (40/93) End question " Qui a inventé le sous-marin ?  A)  Cornelis Drebbel B)  David Bushnell C)  Robert Fulton "
[2024-03-04 11:44:38,481 INFO generators.py generate l.475] (41/93) *** AnsGenerator for question " Qui a découvert la loi de la gravitation universelle ?  A)  Isaac Newton B)  Robert Hooke "
[2024-03-04 11:44:38,486 INFO generators.py gen_for_qa l.548] (41/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:38,489 DEBUG generators.py gen_for_qa l.554] (41/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:38,491 DEBUG generators.py generate l.352] (41/93) Reuse existing Prompt
[2024-03-04 11:44:38,493 DEBUG generators.py generate l.365] (41/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:38,494 DEBUG generators.py generate l.373] (41/93) Reuse post-processing
[2024-03-04 11:44:38,496 INFO generators.py gen_for_qa l.548] (41/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:38,498 DEBUG generators.py gen_for_qa l.554] (41/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:38,502 DEBUG generators.py generate l.352] (41/93) Reuse existing Prompt
[2024-03-04 11:44:38,503 DEBUG generators.py generate l.365] (41/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:38,505 DEBUG generators.py generate l.373] (41/93) Reuse post-processing
[2024-03-04 11:44:38,506 INFO generators.py gen_for_qa l.548] (41/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:38,507 DEBUG generators.py gen_for_qa l.554] (41/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:38,507 DEBUG generators.py generate l.352] (41/93) Reuse existing Prompt
[2024-03-04 11:44:38,510 DEBUG generators.py generate l.365] (41/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:38,511 DEBUG generators.py generate l.373] (41/93) Reuse post-processing
[2024-03-04 11:44:38,511 INFO generators.py gen_for_qa l.548] (41/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:38,513 DEBUG generators.py gen_for_qa l.554] (41/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:38,514 DEBUG generators.py generate l.352] (41/93) Reuse existing Prompt
[2024-03-04 11:44:38,517 DEBUG generators.py generate l.365] (41/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:38,517 DEBUG generators.py generate l.373] (41/93) Reuse post-processing
[2024-03-04 11:44:38,520 INFO generators.py gen_for_qa l.548] (41/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:38,521 DEBUG generators.py gen_for_qa l.554] (41/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:38,521 DEBUG generators.py generate l.352] (41/93) Reuse existing Prompt
[2024-03-04 11:44:38,522 DEBUG generators.py generate l.365] (41/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:38,524 DEBUG generators.py generate l.373] (41/93) Reuse post-processing
[2024-03-04 11:44:38,524 INFO generators.py gen_for_qa l.548] (41/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:38,524 DEBUG generators.py gen_for_qa l.554] (41/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:38,524 DEBUG generators.py generate l.352] (41/93) Reuse existing Prompt
[2024-03-04 11:44:38,527 DEBUG generators.py generate l.365] (41/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:38,527 DEBUG generators.py generate l.373] (41/93) Reuse post-processing
[2024-03-04 11:44:38,527 INFO generators.py gen_for_qa l.548] (41/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:38,530 DEBUG generators.py generate l.349] (41/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:38,532 DEBUG generators.py generate l.358] (41/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:38,921 ERROR generators.py complete l.400] (41/93) The following exception occurred with prompt meta={} user=" Qui a découvert la loi de la gravitation universelle ?  A)  Isaac Newton B)  Robert Hooke .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:38,931 DEBUG generators.py generate l.373] (41/93) Reuse post-processing
[2024-03-04 11:44:38,933 INFO generators.py generate l.477] (41/93) End question " Qui a découvert la loi de la gravitation universelle ?  A)  Isaac Newton B)  Robert Hooke "
[2024-03-04 11:44:38,936 INFO generators.py generate l.475] (42/93) *** AnsGenerator for question " Qui a inventé le microscope électronique ?  A)  Ernst Ruska et Max Knoll "
[2024-03-04 11:44:38,937 INFO generators.py gen_for_qa l.548] (42/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:38,939 DEBUG generators.py gen_for_qa l.554] (42/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:38,939 DEBUG generators.py generate l.352] (42/93) Reuse existing Prompt
[2024-03-04 11:44:38,941 DEBUG generators.py generate l.365] (42/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:38,941 DEBUG generators.py generate l.373] (42/93) Reuse post-processing
[2024-03-04 11:44:38,943 INFO generators.py gen_for_qa l.548] (42/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:38,945 DEBUG generators.py gen_for_qa l.554] (42/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:38,945 DEBUG generators.py generate l.352] (42/93) Reuse existing Prompt
[2024-03-04 11:44:38,946 DEBUG generators.py generate l.365] (42/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:38,947 DEBUG generators.py generate l.373] (42/93) Reuse post-processing
[2024-03-04 11:44:38,949 INFO generators.py gen_for_qa l.548] (42/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:38,950 DEBUG generators.py gen_for_qa l.554] (42/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:38,950 DEBUG generators.py generate l.352] (42/93) Reuse existing Prompt
[2024-03-04 11:44:38,954 DEBUG generators.py generate l.365] (42/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:38,954 DEBUG generators.py generate l.373] (42/93) Reuse post-processing
[2024-03-04 11:44:38,955 INFO generators.py gen_for_qa l.548] (42/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:38,956 DEBUG generators.py gen_for_qa l.554] (42/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:38,957 DEBUG generators.py generate l.352] (42/93) Reuse existing Prompt
[2024-03-04 11:44:38,958 DEBUG generators.py generate l.365] (42/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:38,959 DEBUG generators.py generate l.373] (42/93) Reuse post-processing
[2024-03-04 11:44:38,959 INFO generators.py gen_for_qa l.548] (42/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:38,959 DEBUG generators.py gen_for_qa l.554] (42/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:38,959 DEBUG generators.py generate l.352] (42/93) Reuse existing Prompt
[2024-03-04 11:44:38,962 DEBUG generators.py generate l.365] (42/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:38,962 DEBUG generators.py generate l.373] (42/93) Reuse post-processing
[2024-03-04 11:44:38,963 INFO generators.py gen_for_qa l.548] (42/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:38,964 DEBUG generators.py gen_for_qa l.554] (42/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:38,965 DEBUG generators.py generate l.352] (42/93) Reuse existing Prompt
[2024-03-04 11:44:38,966 DEBUG generators.py generate l.365] (42/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:38,968 DEBUG generators.py generate l.373] (42/93) Reuse post-processing
[2024-03-04 11:44:38,970 INFO generators.py gen_for_qa l.548] (42/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:38,973 DEBUG generators.py generate l.349] (42/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:38,974 DEBUG generators.py generate l.358] (42/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:39,362 ERROR generators.py complete l.400] (42/93) The following exception occurred with prompt meta={} user=" Qui a inventé le microscope électronique ?  A)  Ernst Ruska et Max Knoll .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:39,378 DEBUG generators.py generate l.373] (42/93) Reuse post-processing
[2024-03-04 11:44:39,384 INFO generators.py generate l.477] (42/93) End question " Qui a inventé le microscope électronique ?  A)  Ernst Ruska et Max Knoll "
[2024-03-04 11:44:39,386 INFO generators.py generate l.475] (43/93) *** AnsGenerator for question " Qui a découvert la structure de la molécule de benzène ?  A)  August Kekulé B)  Archibald Scott Couper "
[2024-03-04 11:44:39,388 INFO generators.py gen_for_qa l.548] (43/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:39,390 DEBUG generators.py gen_for_qa l.554] (43/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:39,393 DEBUG generators.py generate l.352] (43/93) Reuse existing Prompt
[2024-03-04 11:44:39,395 DEBUG generators.py generate l.365] (43/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:39,396 DEBUG generators.py generate l.373] (43/93) Reuse post-processing
[2024-03-04 11:44:39,398 INFO generators.py gen_for_qa l.548] (43/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:39,401 DEBUG generators.py gen_for_qa l.554] (43/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:39,402 DEBUG generators.py generate l.352] (43/93) Reuse existing Prompt
[2024-03-04 11:44:39,403 DEBUG generators.py generate l.365] (43/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:39,405 DEBUG generators.py generate l.373] (43/93) Reuse post-processing
[2024-03-04 11:44:39,405 INFO generators.py gen_for_qa l.548] (43/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:39,405 DEBUG generators.py gen_for_qa l.554] (43/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:39,405 DEBUG generators.py generate l.352] (43/93) Reuse existing Prompt
[2024-03-04 11:44:39,410 DEBUG generators.py generate l.365] (43/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:39,411 DEBUG generators.py generate l.373] (43/93) Reuse post-processing
[2024-03-04 11:44:39,412 INFO generators.py gen_for_qa l.548] (43/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:39,413 DEBUG generators.py gen_for_qa l.554] (43/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:39,415 DEBUG generators.py generate l.352] (43/93) Reuse existing Prompt
[2024-03-04 11:44:39,417 DEBUG generators.py generate l.365] (43/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:39,417 DEBUG generators.py generate l.373] (43/93) Reuse post-processing
[2024-03-04 11:44:39,420 INFO generators.py gen_for_qa l.548] (43/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:39,420 DEBUG generators.py gen_for_qa l.554] (43/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:39,422 DEBUG generators.py generate l.352] (43/93) Reuse existing Prompt
[2024-03-04 11:44:39,423 DEBUG generators.py generate l.365] (43/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:39,424 DEBUG generators.py generate l.373] (43/93) Reuse post-processing
[2024-03-04 11:44:39,425 INFO generators.py gen_for_qa l.548] (43/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:39,426 DEBUG generators.py gen_for_qa l.554] (43/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:39,427 DEBUG generators.py generate l.352] (43/93) Reuse existing Prompt
[2024-03-04 11:44:39,427 DEBUG generators.py generate l.365] (43/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:39,427 DEBUG generators.py generate l.373] (43/93) Reuse post-processing
[2024-03-04 11:44:39,430 INFO generators.py gen_for_qa l.548] (43/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:39,432 DEBUG generators.py generate l.349] (43/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:39,434 DEBUG generators.py generate l.358] (43/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:39,978 ERROR generators.py complete l.400] (43/93) The following exception occurred with prompt meta={} user=" Qui a découvert la structure de la molécule de benzène ?  A)  August Kekulé B)  Archibald Scott Couper .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:39,984 DEBUG generators.py generate l.373] (43/93) Reuse post-processing
[2024-03-04 11:44:39,985 INFO generators.py generate l.477] (43/93) End question " Qui a découvert la structure de la molécule de benzène ?  A)  August Kekulé B)  Archibald Scott Couper "
[2024-03-04 11:44:39,987 INFO generators.py generate l.475] (44/93) *** AnsGenerator for question " Qui a inventé la dynamite ?  A)  Alfred Nobel B)  Ascanio Sobrero "
[2024-03-04 11:44:39,987 INFO generators.py gen_for_qa l.548] (44/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:39,990 DEBUG generators.py gen_for_qa l.554] (44/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:39,990 DEBUG generators.py generate l.352] (44/93) Reuse existing Prompt
[2024-03-04 11:44:39,992 DEBUG generators.py generate l.365] (44/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:39,994 DEBUG generators.py generate l.373] (44/93) Reuse post-processing
[2024-03-04 11:44:39,994 INFO generators.py gen_for_qa l.548] (44/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:39,995 DEBUG generators.py gen_for_qa l.554] (44/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:39,997 DEBUG generators.py generate l.352] (44/93) Reuse existing Prompt
[2024-03-04 11:44:39,999 DEBUG generators.py generate l.365] (44/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:40,001 DEBUG generators.py generate l.373] (44/93) Reuse post-processing
[2024-03-04 11:44:40,002 INFO generators.py gen_for_qa l.548] (44/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:40,003 DEBUG generators.py gen_for_qa l.554] (44/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:40,003 DEBUG generators.py generate l.352] (44/93) Reuse existing Prompt
[2024-03-04 11:44:40,003 DEBUG generators.py generate l.365] (44/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:40,003 DEBUG generators.py generate l.373] (44/93) Reuse post-processing
[2024-03-04 11:44:40,007 INFO generators.py gen_for_qa l.548] (44/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:40,007 DEBUG generators.py gen_for_qa l.554] (44/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:40,010 DEBUG generators.py generate l.352] (44/93) Reuse existing Prompt
[2024-03-04 11:44:40,010 DEBUG generators.py generate l.365] (44/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:40,011 DEBUG generators.py generate l.373] (44/93) Reuse post-processing
[2024-03-04 11:44:40,011 INFO generators.py gen_for_qa l.548] (44/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:40,013 DEBUG generators.py gen_for_qa l.554] (44/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:40,015 DEBUG generators.py generate l.352] (44/93) Reuse existing Prompt
[2024-03-04 11:44:40,015 DEBUG generators.py generate l.365] (44/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:40,017 DEBUG generators.py generate l.373] (44/93) Reuse post-processing
[2024-03-04 11:44:40,017 INFO generators.py gen_for_qa l.548] (44/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:40,021 DEBUG generators.py gen_for_qa l.554] (44/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:40,022 DEBUG generators.py generate l.352] (44/93) Reuse existing Prompt
[2024-03-04 11:44:40,023 DEBUG generators.py generate l.365] (44/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:40,023 DEBUG generators.py generate l.373] (44/93) Reuse post-processing
[2024-03-04 11:44:40,024 INFO generators.py gen_for_qa l.548] (44/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:40,025 DEBUG generators.py generate l.349] (44/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:40,026 DEBUG generators.py generate l.358] (44/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:40,417 ERROR generators.py complete l.400] (44/93) The following exception occurred with prompt meta={} user=" Qui a inventé la dynamite ?  A)  Alfred Nobel B)  Ascanio Sobrero .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:40,432 DEBUG generators.py generate l.373] (44/93) Reuse post-processing
[2024-03-04 11:44:40,435 INFO generators.py generate l.477] (44/93) End question " Qui a inventé la dynamite ?  A)  Alfred Nobel B)  Ascanio Sobrero "
[2024-03-04 11:44:40,437 INFO generators.py generate l.475] (45/93) *** AnsGenerator for question " Qui a découvert les rayons X ?  A)  Wilhelm Röntgen B)  Thomas Edison C)  Nikola Tesla "
[2024-03-04 11:44:40,440 INFO generators.py gen_for_qa l.548] (45/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:40,441 DEBUG generators.py gen_for_qa l.554] (45/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:40,442 DEBUG generators.py generate l.352] (45/93) Reuse existing Prompt
[2024-03-04 11:44:40,445 DEBUG generators.py generate l.365] (45/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:40,446 DEBUG generators.py generate l.373] (45/93) Reuse post-processing
[2024-03-04 11:44:40,448 INFO generators.py gen_for_qa l.548] (45/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:40,450 DEBUG generators.py gen_for_qa l.554] (45/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:40,452 DEBUG generators.py generate l.352] (45/93) Reuse existing Prompt
[2024-03-04 11:44:40,452 DEBUG generators.py generate l.365] (45/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:40,452 DEBUG generators.py generate l.373] (45/93) Reuse post-processing
[2024-03-04 11:44:40,455 INFO generators.py gen_for_qa l.548] (45/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:40,456 DEBUG generators.py gen_for_qa l.554] (45/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:40,457 DEBUG generators.py generate l.352] (45/93) Reuse existing Prompt
[2024-03-04 11:44:40,458 DEBUG generators.py generate l.365] (45/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:40,460 DEBUG generators.py generate l.373] (45/93) Reuse post-processing
[2024-03-04 11:44:40,461 INFO generators.py gen_for_qa l.548] (45/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:40,461 DEBUG generators.py gen_for_qa l.554] (45/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:40,463 DEBUG generators.py generate l.352] (45/93) Reuse existing Prompt
[2024-03-04 11:44:40,464 DEBUG generators.py generate l.365] (45/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:40,465 DEBUG generators.py generate l.373] (45/93) Reuse post-processing
[2024-03-04 11:44:40,466 INFO generators.py gen_for_qa l.548] (45/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:40,468 DEBUG generators.py gen_for_qa l.554] (45/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:40,470 DEBUG generators.py generate l.352] (45/93) Reuse existing Prompt
[2024-03-04 11:44:40,472 DEBUG generators.py generate l.365] (45/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:40,473 DEBUG generators.py generate l.373] (45/93) Reuse post-processing
[2024-03-04 11:44:40,473 INFO generators.py gen_for_qa l.548] (45/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:40,474 DEBUG generators.py gen_for_qa l.554] (45/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:40,474 DEBUG generators.py generate l.352] (45/93) Reuse existing Prompt
[2024-03-04 11:44:40,477 DEBUG generators.py generate l.365] (45/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:40,478 DEBUG generators.py generate l.373] (45/93) Reuse post-processing
[2024-03-04 11:44:40,479 INFO generators.py gen_for_qa l.548] (45/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:40,480 DEBUG generators.py generate l.349] (45/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:40,482 DEBUG generators.py generate l.358] (45/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:40,981 ERROR generators.py complete l.400] (45/93) The following exception occurred with prompt meta={} user=" Qui a découvert les rayons X ?  A)  Wilhelm Röntgen B)  Thomas Edison C)  Nikola Tesla .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:40,994 DEBUG generators.py generate l.373] (45/93) Reuse post-processing
[2024-03-04 11:44:40,998 INFO generators.py generate l.477] (45/93) End question " Qui a découvert les rayons X ?  A)  Wilhelm Röntgen B)  Thomas Edison C)  Nikola Tesla "
[2024-03-04 11:44:41,002 INFO generators.py generate l.475] (46/93) *** AnsGenerator for question " Qui a inventé le gramophone ?  A)  Emile Berliner B)  Thomas Edison C)  Charles Cros "
[2024-03-04 11:44:41,003 INFO generators.py gen_for_qa l.548] (46/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:41,005 DEBUG generators.py gen_for_qa l.554] (46/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:41,005 DEBUG generators.py generate l.352] (46/93) Reuse existing Prompt
[2024-03-04 11:44:41,009 DEBUG generators.py generate l.365] (46/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:41,010 DEBUG generators.py generate l.373] (46/93) Reuse post-processing
[2024-03-04 11:44:41,011 INFO generators.py gen_for_qa l.548] (46/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:41,012 DEBUG generators.py gen_for_qa l.554] (46/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:41,015 DEBUG generators.py generate l.352] (46/93) Reuse existing Prompt
[2024-03-04 11:44:41,017 DEBUG generators.py generate l.365] (46/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:41,017 DEBUG generators.py generate l.373] (46/93) Reuse post-processing
[2024-03-04 11:44:41,020 INFO generators.py gen_for_qa l.548] (46/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:41,021 DEBUG generators.py gen_for_qa l.554] (46/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:41,022 DEBUG generators.py generate l.352] (46/93) Reuse existing Prompt
[2024-03-04 11:44:41,022 DEBUG generators.py generate l.365] (46/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:41,023 DEBUG generators.py generate l.373] (46/93) Reuse post-processing
[2024-03-04 11:44:41,025 INFO generators.py gen_for_qa l.548] (46/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:41,025 DEBUG generators.py gen_for_qa l.554] (46/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:41,027 DEBUG generators.py generate l.352] (46/93) Reuse existing Prompt
[2024-03-04 11:44:41,027 DEBUG generators.py generate l.365] (46/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:41,028 DEBUG generators.py generate l.373] (46/93) Reuse post-processing
[2024-03-04 11:44:41,030 INFO generators.py gen_for_qa l.548] (46/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:41,032 DEBUG generators.py gen_for_qa l.554] (46/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:41,034 DEBUG generators.py generate l.352] (46/93) Reuse existing Prompt
[2024-03-04 11:44:41,035 DEBUG generators.py generate l.365] (46/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:41,036 DEBUG generators.py generate l.373] (46/93) Reuse post-processing
[2024-03-04 11:44:41,037 INFO generators.py gen_for_qa l.548] (46/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:41,038 DEBUG generators.py gen_for_qa l.554] (46/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:41,039 DEBUG generators.py generate l.352] (46/93) Reuse existing Prompt
[2024-03-04 11:44:41,040 DEBUG generators.py generate l.365] (46/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:41,040 DEBUG generators.py generate l.373] (46/93) Reuse post-processing
[2024-03-04 11:44:41,041 INFO generators.py gen_for_qa l.548] (46/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:41,042 DEBUG generators.py generate l.349] (46/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:41,043 DEBUG generators.py generate l.358] (46/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:41,431 ERROR generators.py complete l.400] (46/93) The following exception occurred with prompt meta={} user=" Qui a inventé le gramophone ?  A)  Emile Berliner B)  Thomas Edison C)  Charles Cros .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:41,440 DEBUG generators.py generate l.373] (46/93) Reuse post-processing
[2024-03-04 11:44:41,443 INFO generators.py generate l.477] (46/93) End question " Qui a inventé le gramophone ?  A)  Emile Berliner B)  Thomas Edison C)  Charles Cros "
[2024-03-04 11:44:41,446 INFO generators.py generate l.475] (47/93) *** AnsGenerator for question " Qui a découvert les rayons cosmiques ?  A)  Victor Hess B)  Robert Millikan "
[2024-03-04 11:44:41,448 INFO generators.py gen_for_qa l.548] (47/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:41,452 DEBUG generators.py gen_for_qa l.554] (47/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:41,454 DEBUG generators.py generate l.352] (47/93) Reuse existing Prompt
[2024-03-04 11:44:41,456 DEBUG generators.py generate l.365] (47/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:41,458 DEBUG generators.py generate l.373] (47/93) Reuse post-processing
[2024-03-04 11:44:41,459 INFO generators.py gen_for_qa l.548] (47/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:41,461 DEBUG generators.py gen_for_qa l.554] (47/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:41,463 DEBUG generators.py generate l.352] (47/93) Reuse existing Prompt
[2024-03-04 11:44:41,465 DEBUG generators.py generate l.365] (47/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:41,466 DEBUG generators.py generate l.373] (47/93) Reuse post-processing
[2024-03-04 11:44:41,468 INFO generators.py gen_for_qa l.548] (47/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:41,470 DEBUG generators.py gen_for_qa l.554] (47/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:41,473 DEBUG generators.py generate l.352] (47/93) Reuse existing Prompt
[2024-03-04 11:44:41,474 DEBUG generators.py generate l.365] (47/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:41,475 DEBUG generators.py generate l.373] (47/93) Reuse post-processing
[2024-03-04 11:44:41,475 INFO generators.py gen_for_qa l.548] (47/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:41,477 DEBUG generators.py gen_for_qa l.554] (47/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:41,477 DEBUG generators.py generate l.352] (47/93) Reuse existing Prompt
[2024-03-04 11:44:41,480 DEBUG generators.py generate l.365] (47/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:41,482 DEBUG generators.py generate l.373] (47/93) Reuse post-processing
[2024-03-04 11:44:41,483 INFO generators.py gen_for_qa l.548] (47/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:41,486 DEBUG generators.py gen_for_qa l.554] (47/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:41,487 DEBUG generators.py generate l.352] (47/93) Reuse existing Prompt
[2024-03-04 11:44:41,487 DEBUG generators.py generate l.365] (47/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:41,488 DEBUG generators.py generate l.373] (47/93) Reuse post-processing
[2024-03-04 11:44:41,489 INFO generators.py gen_for_qa l.548] (47/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:41,489 DEBUG generators.py gen_for_qa l.554] (47/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:41,491 DEBUG generators.py generate l.352] (47/93) Reuse existing Prompt
[2024-03-04 11:44:41,491 DEBUG generators.py generate l.365] (47/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:41,493 DEBUG generators.py generate l.373] (47/93) Reuse post-processing
[2024-03-04 11:44:41,494 INFO generators.py gen_for_qa l.548] (47/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:41,495 DEBUG generators.py generate l.349] (47/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:41,496 DEBUG generators.py generate l.358] (47/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:41,896 ERROR generators.py complete l.400] (47/93) The following exception occurred with prompt meta={} user=" Qui a découvert les rayons cosmiques ?  A)  Victor Hess B)  Robert Millikan .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:41,920 DEBUG generators.py generate l.373] (47/93) Reuse post-processing
[2024-03-04 11:44:41,923 INFO generators.py generate l.477] (47/93) End question " Qui a découvert les rayons cosmiques ?  A)  Victor Hess B)  Robert Millikan "
[2024-03-04 11:44:41,925 INFO generators.py generate l.475] (48/93) *** AnsGenerator for question " Qui a inventé le premier satellite artificiel ?  A)  Sergueï Korolev B)  Wernher von Braun "
[2024-03-04 11:44:41,927 INFO generators.py gen_for_qa l.548] (48/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:41,929 DEBUG generators.py gen_for_qa l.554] (48/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:41,930 DEBUG generators.py generate l.352] (48/93) Reuse existing Prompt
[2024-03-04 11:44:41,934 DEBUG generators.py generate l.365] (48/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:41,935 DEBUG generators.py generate l.373] (48/93) Reuse post-processing
[2024-03-04 11:44:41,936 INFO generators.py gen_for_qa l.548] (48/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:41,938 DEBUG generators.py gen_for_qa l.554] (48/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:41,939 DEBUG generators.py generate l.352] (48/93) Reuse existing Prompt
[2024-03-04 11:44:41,939 DEBUG generators.py generate l.365] (48/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:41,942 DEBUG generators.py generate l.373] (48/93) Reuse post-processing
[2024-03-04 11:44:41,942 INFO generators.py gen_for_qa l.548] (48/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:41,945 DEBUG generators.py gen_for_qa l.554] (48/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:41,946 DEBUG generators.py generate l.352] (48/93) Reuse existing Prompt
[2024-03-04 11:44:41,949 DEBUG generators.py generate l.365] (48/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:41,950 DEBUG generators.py generate l.373] (48/93) Reuse post-processing
[2024-03-04 11:44:41,950 INFO generators.py gen_for_qa l.548] (48/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:41,953 DEBUG generators.py gen_for_qa l.554] (48/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:41,953 DEBUG generators.py generate l.352] (48/93) Reuse existing Prompt
[2024-03-04 11:44:41,955 DEBUG generators.py generate l.365] (48/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:41,956 DEBUG generators.py generate l.373] (48/93) Reuse post-processing
[2024-03-04 11:44:41,957 INFO generators.py gen_for_qa l.548] (48/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:41,957 DEBUG generators.py gen_for_qa l.554] (48/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:41,959 DEBUG generators.py generate l.352] (48/93) Reuse existing Prompt
[2024-03-04 11:44:41,960 DEBUG generators.py generate l.365] (48/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:41,960 DEBUG generators.py generate l.373] (48/93) Reuse post-processing
[2024-03-04 11:44:41,961 INFO generators.py gen_for_qa l.548] (48/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:41,962 DEBUG generators.py gen_for_qa l.554] (48/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:41,963 DEBUG generators.py generate l.352] (48/93) Reuse existing Prompt
[2024-03-04 11:44:41,965 DEBUG generators.py generate l.365] (48/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:41,965 DEBUG generators.py generate l.373] (48/93) Reuse post-processing
[2024-03-04 11:44:41,968 INFO generators.py gen_for_qa l.548] (48/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:41,969 DEBUG generators.py generate l.349] (48/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:41,971 DEBUG generators.py generate l.358] (48/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:42,369 ERROR generators.py complete l.400] (48/93) The following exception occurred with prompt meta={} user=" Qui a inventé le premier satellite artificiel ?  A)  Sergueï Korolev B)  Wernher von Braun .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:42,381 DEBUG generators.py generate l.373] (48/93) Reuse post-processing
[2024-03-04 11:44:42,384 INFO generators.py generate l.477] (48/93) End question " Qui a inventé le premier satellite artificiel ?  A)  Sergueï Korolev B)  Wernher von Braun "
[2024-03-04 11:44:42,387 INFO generators.py generate l.475] (49/93) *** AnsGenerator for question " Qui a découvert la loi de la conservation de l'énergie ?  A)  James Joule B)  Julius Robert Mayer C)  Hermann von Helmholtz "
[2024-03-04 11:44:42,388 INFO generators.py gen_for_qa l.548] (49/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:42,390 DEBUG generators.py gen_for_qa l.554] (49/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:42,391 DEBUG generators.py generate l.352] (49/93) Reuse existing Prompt
[2024-03-04 11:44:42,393 DEBUG generators.py generate l.365] (49/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:42,393 DEBUG generators.py generate l.373] (49/93) Reuse post-processing
[2024-03-04 11:44:42,396 INFO generators.py gen_for_qa l.548] (49/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:42,398 DEBUG generators.py gen_for_qa l.554] (49/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:42,400 DEBUG generators.py generate l.352] (49/93) Reuse existing Prompt
[2024-03-04 11:44:42,401 DEBUG generators.py generate l.365] (49/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:42,402 DEBUG generators.py generate l.373] (49/93) Reuse post-processing
[2024-03-04 11:44:42,403 INFO generators.py gen_for_qa l.548] (49/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:42,404 DEBUG generators.py gen_for_qa l.554] (49/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:42,404 DEBUG generators.py generate l.352] (49/93) Reuse existing Prompt
[2024-03-04 11:44:42,406 DEBUG generators.py generate l.365] (49/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:42,408 DEBUG generators.py generate l.373] (49/93) Reuse post-processing
[2024-03-04 11:44:42,409 INFO generators.py gen_for_qa l.548] (49/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:42,410 DEBUG generators.py gen_for_qa l.554] (49/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:42,410 DEBUG generators.py generate l.352] (49/93) Reuse existing Prompt
[2024-03-04 11:44:42,412 DEBUG generators.py generate l.365] (49/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:42,414 DEBUG generators.py generate l.373] (49/93) Reuse post-processing
[2024-03-04 11:44:42,415 INFO generators.py gen_for_qa l.548] (49/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:42,418 DEBUG generators.py gen_for_qa l.554] (49/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:42,419 DEBUG generators.py generate l.352] (49/93) Reuse existing Prompt
[2024-03-04 11:44:42,420 DEBUG generators.py generate l.365] (49/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:42,421 DEBUG generators.py generate l.373] (49/93) Reuse post-processing
[2024-03-04 11:44:42,421 INFO generators.py gen_for_qa l.548] (49/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:42,422 DEBUG generators.py gen_for_qa l.554] (49/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:42,422 DEBUG generators.py generate l.352] (49/93) Reuse existing Prompt
[2024-03-04 11:44:42,422 DEBUG generators.py generate l.365] (49/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:42,425 DEBUG generators.py generate l.373] (49/93) Reuse post-processing
[2024-03-04 11:44:42,425 INFO generators.py gen_for_qa l.548] (49/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:42,425 DEBUG generators.py generate l.349] (49/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:42,428 DEBUG generators.py generate l.358] (49/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:42,821 ERROR generators.py complete l.400] (49/93) The following exception occurred with prompt meta={} user=" Qui a découvert la loi de la conservation de l'énergie ?  A)  James Joule B)  Julius Robert Mayer C)  Hermann von Helmholtz .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:42,837 DEBUG generators.py generate l.373] (49/93) Reuse post-processing
[2024-03-04 11:44:42,841 INFO generators.py generate l.477] (49/93) End question " Qui a découvert la loi de la conservation de l'énergie ?  A)  James Joule B)  Julius Robert Mayer C)  Hermann von Helmholtz "
[2024-03-04 11:44:42,846 INFO generators.py generate l.475] (50/93) *** AnsGenerator for question " Qui a inventé la machine à calculer ?  A)  Blaise Pascal B)  Gottfried Leibniz C)  Charles Babbage "
[2024-03-04 11:44:42,847 INFO generators.py gen_for_qa l.548] (50/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:42,853 DEBUG generators.py gen_for_qa l.554] (50/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:42,856 DEBUG generators.py generate l.352] (50/93) Reuse existing Prompt
[2024-03-04 11:44:42,857 DEBUG generators.py generate l.365] (50/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:42,859 DEBUG generators.py generate l.373] (50/93) Reuse post-processing
[2024-03-04 11:44:42,860 INFO generators.py gen_for_qa l.548] (50/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:42,862 DEBUG generators.py gen_for_qa l.554] (50/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:42,865 DEBUG generators.py generate l.352] (50/93) Reuse existing Prompt
[2024-03-04 11:44:42,867 DEBUG generators.py generate l.365] (50/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:42,867 DEBUG generators.py generate l.373] (50/93) Reuse post-processing
[2024-03-04 11:44:42,867 INFO generators.py gen_for_qa l.548] (50/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:42,871 DEBUG generators.py gen_for_qa l.554] (50/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:42,872 DEBUG generators.py generate l.352] (50/93) Reuse existing Prompt
[2024-03-04 11:44:42,873 DEBUG generators.py generate l.365] (50/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:42,874 DEBUG generators.py generate l.373] (50/93) Reuse post-processing
[2024-03-04 11:44:42,875 INFO generators.py gen_for_qa l.548] (50/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:42,876 DEBUG generators.py gen_for_qa l.554] (50/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:42,876 DEBUG generators.py generate l.352] (50/93) Reuse existing Prompt
[2024-03-04 11:44:42,879 DEBUG generators.py generate l.365] (50/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:42,879 DEBUG generators.py generate l.373] (50/93) Reuse post-processing
[2024-03-04 11:44:42,882 INFO generators.py gen_for_qa l.548] (50/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:42,883 DEBUG generators.py gen_for_qa l.554] (50/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:42,885 DEBUG generators.py generate l.352] (50/93) Reuse existing Prompt
[2024-03-04 11:44:42,885 DEBUG generators.py generate l.365] (50/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:42,886 DEBUG generators.py generate l.373] (50/93) Reuse post-processing
[2024-03-04 11:44:42,887 INFO generators.py gen_for_qa l.548] (50/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:42,888 DEBUG generators.py gen_for_qa l.554] (50/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:42,890 DEBUG generators.py generate l.352] (50/93) Reuse existing Prompt
[2024-03-04 11:44:42,890 DEBUG generators.py generate l.365] (50/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:42,891 DEBUG generators.py generate l.373] (50/93) Reuse post-processing
[2024-03-04 11:44:42,892 INFO generators.py gen_for_qa l.548] (50/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:42,893 DEBUG generators.py generate l.349] (50/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:42,893 DEBUG generators.py generate l.358] (50/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:43,417 ERROR generators.py complete l.400] (50/93) The following exception occurred with prompt meta={} user=" Qui a inventé la machine à calculer ?  A)  Blaise Pascal B)  Gottfried Leibniz C)  Charles Babbage .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:43,426 DEBUG generators.py generate l.373] (50/93) Reuse post-processing
[2024-03-04 11:44:43,430 INFO generators.py generate l.477] (50/93) End question " Qui a inventé la machine à calculer ?  A)  Blaise Pascal B)  Gottfried Leibniz C)  Charles Babbage "
[2024-03-04 11:44:43,431 INFO generators.py generate l.475] (51/93) *** AnsGenerator for question " Qui a découvert la loi de la conservation de la masse ?  A)  Antoine Lavoisier B)  Mikhail Lomonosov "
[2024-03-04 11:44:43,433 INFO generators.py gen_for_qa l.548] (51/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:43,435 DEBUG generators.py gen_for_qa l.554] (51/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:43,437 DEBUG generators.py generate l.352] (51/93) Reuse existing Prompt
[2024-03-04 11:44:43,439 DEBUG generators.py generate l.365] (51/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:43,440 DEBUG generators.py generate l.373] (51/93) Reuse post-processing
[2024-03-04 11:44:43,441 INFO generators.py gen_for_qa l.548] (51/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:43,442 DEBUG generators.py gen_for_qa l.554] (51/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:43,443 DEBUG generators.py generate l.352] (51/93) Reuse existing Prompt
[2024-03-04 11:44:43,444 DEBUG generators.py generate l.365] (51/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:43,444 DEBUG generators.py generate l.373] (51/93) Reuse post-processing
[2024-03-04 11:44:43,447 INFO generators.py gen_for_qa l.548] (51/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:43,449 DEBUG generators.py gen_for_qa l.554] (51/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:43,451 DEBUG generators.py generate l.352] (51/93) Reuse existing Prompt
[2024-03-04 11:44:43,452 DEBUG generators.py generate l.365] (51/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:43,453 DEBUG generators.py generate l.373] (51/93) Reuse post-processing
[2024-03-04 11:44:43,453 INFO generators.py gen_for_qa l.548] (51/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:43,456 DEBUG generators.py gen_for_qa l.554] (51/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:43,457 DEBUG generators.py generate l.352] (51/93) Reuse existing Prompt
[2024-03-04 11:44:43,458 DEBUG generators.py generate l.365] (51/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:43,459 DEBUG generators.py generate l.373] (51/93) Reuse post-processing
[2024-03-04 11:44:43,461 INFO generators.py gen_for_qa l.548] (51/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:43,462 DEBUG generators.py gen_for_qa l.554] (51/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:43,464 DEBUG generators.py generate l.352] (51/93) Reuse existing Prompt
[2024-03-04 11:44:43,465 DEBUG generators.py generate l.365] (51/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:43,466 DEBUG generators.py generate l.373] (51/93) Reuse post-processing
[2024-03-04 11:44:43,468 INFO generators.py gen_for_qa l.548] (51/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:43,471 DEBUG generators.py gen_for_qa l.554] (51/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:43,473 DEBUG generators.py generate l.352] (51/93) Reuse existing Prompt
[2024-03-04 11:44:43,474 DEBUG generators.py generate l.365] (51/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:43,475 DEBUG generators.py generate l.373] (51/93) Reuse post-processing
[2024-03-04 11:44:43,475 INFO generators.py gen_for_qa l.548] (51/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:43,478 DEBUG generators.py generate l.349] (51/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:43,479 DEBUG generators.py generate l.358] (51/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:43,862 ERROR generators.py complete l.400] (51/93) The following exception occurred with prompt meta={} user=" Qui a découvert la loi de la conservation de la masse ?  A)  Antoine Lavoisier B)  Mikhail Lomonosov .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:43,868 DEBUG generators.py generate l.373] (51/93) Reuse post-processing
[2024-03-04 11:44:43,870 INFO generators.py generate l.477] (51/93) End question " Qui a découvert la loi de la conservation de la masse ?  A)  Antoine Lavoisier B)  Mikhail Lomonosov "
[2024-03-04 11:44:43,872 INFO generators.py generate l.475] (52/93) *** AnsGenerator for question " Qui a inventé le premier ordinateur programmable ?  A)  Konrad Zuse B)  John Atanasoff et Clifford Berry "
[2024-03-04 11:44:43,872 INFO generators.py gen_for_qa l.548] (52/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:43,875 DEBUG generators.py gen_for_qa l.554] (52/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:43,875 DEBUG generators.py generate l.352] (52/93) Reuse existing Prompt
[2024-03-04 11:44:43,877 DEBUG generators.py generate l.365] (52/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:43,878 DEBUG generators.py generate l.373] (52/93) Reuse post-processing
[2024-03-04 11:44:43,880 INFO generators.py gen_for_qa l.548] (52/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:43,882 DEBUG generators.py gen_for_qa l.554] (52/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:43,883 DEBUG generators.py generate l.352] (52/93) Reuse existing Prompt
[2024-03-04 11:44:43,885 DEBUG generators.py generate l.365] (52/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:43,885 DEBUG generators.py generate l.373] (52/93) Reuse post-processing
[2024-03-04 11:44:43,887 INFO generators.py gen_for_qa l.548] (52/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:43,887 DEBUG generators.py gen_for_qa l.554] (52/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:43,888 DEBUG generators.py generate l.352] (52/93) Reuse existing Prompt
[2024-03-04 11:44:43,888 DEBUG generators.py generate l.365] (52/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:43,888 DEBUG generators.py generate l.373] (52/93) Reuse post-processing
[2024-03-04 11:44:43,891 INFO generators.py gen_for_qa l.548] (52/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:43,893 DEBUG generators.py gen_for_qa l.554] (52/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:43,893 DEBUG generators.py generate l.352] (52/93) Reuse existing Prompt
[2024-03-04 11:44:43,894 DEBUG generators.py generate l.365] (52/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:43,895 DEBUG generators.py generate l.373] (52/93) Reuse post-processing
[2024-03-04 11:44:43,896 INFO generators.py gen_for_qa l.548] (52/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:43,897 DEBUG generators.py gen_for_qa l.554] (52/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:43,897 DEBUG generators.py generate l.352] (52/93) Reuse existing Prompt
[2024-03-04 11:44:43,900 DEBUG generators.py generate l.365] (52/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:43,902 DEBUG generators.py generate l.373] (52/93) Reuse post-processing
[2024-03-04 11:44:43,902 INFO generators.py gen_for_qa l.548] (52/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:43,903 DEBUG generators.py gen_for_qa l.554] (52/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:43,904 DEBUG generators.py generate l.352] (52/93) Reuse existing Prompt
[2024-03-04 11:44:43,904 DEBUG generators.py generate l.365] (52/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:43,905 DEBUG generators.py generate l.373] (52/93) Reuse post-processing
[2024-03-04 11:44:43,906 INFO generators.py gen_for_qa l.548] (52/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:43,906 DEBUG generators.py generate l.349] (52/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:43,906 DEBUG generators.py generate l.358] (52/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:44,298 ERROR generators.py complete l.400] (52/93) The following exception occurred with prompt meta={} user=" Qui a inventé le premier ordinateur programmable ?  A)  Konrad Zuse B)  John Atanasoff et Clifford Berry .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:44,314 DEBUG generators.py generate l.373] (52/93) Reuse post-processing
[2024-03-04 11:44:44,319 INFO generators.py generate l.477] (52/93) End question " Qui a inventé le premier ordinateur programmable ?  A)  Konrad Zuse B)  John Atanasoff et Clifford Berry "
[2024-03-04 11:44:44,322 INFO generators.py generate l.475] (53/93) *** AnsGenerator for question " Qui a découvert la structure de l'ADN ?  A)  James Watson et Francis Crick B)  Rosalind Franklin C)  Maurice Wilkins "
[2024-03-04 11:44:44,324 INFO generators.py gen_for_qa l.548] (53/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:44,326 DEBUG generators.py gen_for_qa l.554] (53/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:44,328 DEBUG generators.py generate l.352] (53/93) Reuse existing Prompt
[2024-03-04 11:44:44,329 DEBUG generators.py generate l.365] (53/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:44,331 DEBUG generators.py generate l.373] (53/93) Reuse post-processing
[2024-03-04 11:44:44,334 INFO generators.py gen_for_qa l.548] (53/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:44,335 DEBUG generators.py gen_for_qa l.554] (53/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:44,336 DEBUG generators.py generate l.352] (53/93) Reuse existing Prompt
[2024-03-04 11:44:44,337 DEBUG generators.py generate l.365] (53/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:44,338 DEBUG generators.py generate l.373] (53/93) Reuse post-processing
[2024-03-04 11:44:44,340 INFO generators.py gen_for_qa l.548] (53/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:44,340 DEBUG generators.py gen_for_qa l.554] (53/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:44,342 DEBUG generators.py generate l.352] (53/93) Reuse existing Prompt
[2024-03-04 11:44:44,342 DEBUG generators.py generate l.365] (53/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:44,345 DEBUG generators.py generate l.373] (53/93) Reuse post-processing
[2024-03-04 11:44:44,346 INFO generators.py gen_for_qa l.548] (53/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:44,348 DEBUG generators.py gen_for_qa l.554] (53/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:44,350 DEBUG generators.py generate l.352] (53/93) Reuse existing Prompt
[2024-03-04 11:44:44,350 DEBUG generators.py generate l.365] (53/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:44,352 DEBUG generators.py generate l.373] (53/93) Reuse post-processing
[2024-03-04 11:44:44,352 INFO generators.py gen_for_qa l.548] (53/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:44,352 DEBUG generators.py gen_for_qa l.554] (53/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:44,352 DEBUG generators.py generate l.352] (53/93) Reuse existing Prompt
[2024-03-04 11:44:44,356 DEBUG generators.py generate l.365] (53/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:44,357 DEBUG generators.py generate l.373] (53/93) Reuse post-processing
[2024-03-04 11:44:44,358 INFO generators.py gen_for_qa l.548] (53/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:44,359 DEBUG generators.py gen_for_qa l.554] (53/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:44,360 DEBUG generators.py generate l.352] (53/93) Reuse existing Prompt
[2024-03-04 11:44:44,361 DEBUG generators.py generate l.365] (53/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:44,362 DEBUG generators.py generate l.373] (53/93) Reuse post-processing
[2024-03-04 11:44:44,364 INFO generators.py gen_for_qa l.548] (53/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:44,366 DEBUG generators.py generate l.349] (53/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:44,367 DEBUG generators.py generate l.358] (53/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:44,784 ERROR generators.py complete l.400] (53/93) The following exception occurred with prompt meta={} user=" Qui a découvert la structure de l'ADN ?  A)  James Watson et Francis Crick B)  Rosalind Franklin C)  Maurice Wilkins .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:44,796 DEBUG generators.py generate l.373] (53/93) Reuse post-processing
[2024-03-04 11:44:44,798 INFO generators.py generate l.477] (53/93) End question " Qui a découvert la structure de l'ADN ?  A)  James Watson et Francis Crick B)  Rosalind Franklin C)  Maurice Wilkins "
[2024-03-04 11:44:44,801 INFO generators.py generate l.475] (54/93) *** AnsGenerator for question " Qui a inventé le premier laser fonctionnel ?  A)  Theodore Maiman B)  Gordon Gould C)  Charles Hard Townes "
[2024-03-04 11:44:44,803 INFO generators.py gen_for_qa l.548] (54/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:44,805 DEBUG generators.py gen_for_qa l.554] (54/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:44,806 DEBUG generators.py generate l.352] (54/93) Reuse existing Prompt
[2024-03-04 11:44:44,807 DEBUG generators.py generate l.365] (54/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:44,807 DEBUG generators.py generate l.373] (54/93) Reuse post-processing
[2024-03-04 11:44:44,810 INFO generators.py gen_for_qa l.548] (54/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:44,812 DEBUG generators.py gen_for_qa l.554] (54/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:44,812 DEBUG generators.py generate l.352] (54/93) Reuse existing Prompt
[2024-03-04 11:44:44,815 DEBUG generators.py generate l.365] (54/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:44,816 DEBUG generators.py generate l.373] (54/93) Reuse post-processing
[2024-03-04 11:44:44,818 INFO generators.py gen_for_qa l.548] (54/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:44,818 DEBUG generators.py gen_for_qa l.554] (54/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:44,820 DEBUG generators.py generate l.352] (54/93) Reuse existing Prompt
[2024-03-04 11:44:44,821 DEBUG generators.py generate l.365] (54/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:44,822 DEBUG generators.py generate l.373] (54/93) Reuse post-processing
[2024-03-04 11:44:44,823 INFO generators.py gen_for_qa l.548] (54/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:44,824 DEBUG generators.py gen_for_qa l.554] (54/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:44,825 DEBUG generators.py generate l.352] (54/93) Reuse existing Prompt
[2024-03-04 11:44:44,826 DEBUG generators.py generate l.365] (54/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:44,826 DEBUG generators.py generate l.373] (54/93) Reuse post-processing
[2024-03-04 11:44:44,828 INFO generators.py gen_for_qa l.548] (54/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:44,829 DEBUG generators.py gen_for_qa l.554] (54/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:44,831 DEBUG generators.py generate l.352] (54/93) Reuse existing Prompt
[2024-03-04 11:44:44,833 DEBUG generators.py generate l.365] (54/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:44,834 DEBUG generators.py generate l.373] (54/93) Reuse post-processing
[2024-03-04 11:44:44,834 INFO generators.py gen_for_qa l.548] (54/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:44,837 DEBUG generators.py gen_for_qa l.554] (54/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:44,837 DEBUG generators.py generate l.352] (54/93) Reuse existing Prompt
[2024-03-04 11:44:44,838 DEBUG generators.py generate l.365] (54/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:44,839 DEBUG generators.py generate l.373] (54/93) Reuse post-processing
[2024-03-04 11:44:44,839 INFO generators.py gen_for_qa l.548] (54/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:44,841 DEBUG generators.py generate l.349] (54/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:44,842 DEBUG generators.py generate l.358] (54/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:45,237 ERROR generators.py complete l.400] (54/93) The following exception occurred with prompt meta={} user=" Qui a inventé le premier laser fonctionnel ?  A)  Theodore Maiman B)  Gordon Gould C)  Charles Hard Townes .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:45,243 DEBUG generators.py generate l.373] (54/93) Reuse post-processing
[2024-03-04 11:44:45,245 INFO generators.py generate l.477] (54/93) End question " Qui a inventé le premier laser fonctionnel ?  A)  Theodore Maiman B)  Gordon Gould C)  Charles Hard Townes "
[2024-03-04 11:44:45,248 INFO generators.py generate l.475] (55/93) *** AnsGenerator for question " Qui a découvert la loi de la conservation de la quantité de mouvement ?  A)  Isaac Newton B)  René Descartes C)  Christiaan Huygens "
[2024-03-04 11:44:45,250 INFO generators.py gen_for_qa l.548] (55/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:45,251 DEBUG generators.py gen_for_qa l.554] (55/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:45,252 DEBUG generators.py generate l.352] (55/93) Reuse existing Prompt
[2024-03-04 11:44:45,254 DEBUG generators.py generate l.365] (55/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:45,254 DEBUG generators.py generate l.373] (55/93) Reuse post-processing
[2024-03-04 11:44:45,254 INFO generators.py gen_for_qa l.548] (55/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:45,258 DEBUG generators.py gen_for_qa l.554] (55/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:45,259 DEBUG generators.py generate l.352] (55/93) Reuse existing Prompt
[2024-03-04 11:44:45,260 DEBUG generators.py generate l.365] (55/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:45,262 DEBUG generators.py generate l.373] (55/93) Reuse post-processing
[2024-03-04 11:44:45,264 INFO generators.py gen_for_qa l.548] (55/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:45,266 DEBUG generators.py gen_for_qa l.554] (55/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:45,267 DEBUG generators.py generate l.352] (55/93) Reuse existing Prompt
[2024-03-04 11:44:45,267 DEBUG generators.py generate l.365] (55/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:45,269 DEBUG generators.py generate l.373] (55/93) Reuse post-processing
[2024-03-04 11:44:45,269 INFO generators.py gen_for_qa l.548] (55/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:45,269 DEBUG generators.py gen_for_qa l.554] (55/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:45,269 DEBUG generators.py generate l.352] (55/93) Reuse existing Prompt
[2024-03-04 11:44:45,273 DEBUG generators.py generate l.365] (55/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:45,275 DEBUG generators.py generate l.373] (55/93) Reuse post-processing
[2024-03-04 11:44:45,276 INFO generators.py gen_for_qa l.548] (55/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:45,277 DEBUG generators.py gen_for_qa l.554] (55/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:45,277 DEBUG generators.py generate l.352] (55/93) Reuse existing Prompt
[2024-03-04 11:44:45,279 DEBUG generators.py generate l.365] (55/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:45,281 DEBUG generators.py generate l.373] (55/93) Reuse post-processing
[2024-03-04 11:44:45,282 INFO generators.py gen_for_qa l.548] (55/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:45,284 DEBUG generators.py gen_for_qa l.554] (55/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:45,284 DEBUG generators.py generate l.352] (55/93) Reuse existing Prompt
[2024-03-04 11:44:45,286 DEBUG generators.py generate l.365] (55/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:45,286 DEBUG generators.py generate l.373] (55/93) Reuse post-processing
[2024-03-04 11:44:45,286 INFO generators.py gen_for_qa l.548] (55/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:45,289 DEBUG generators.py generate l.349] (55/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:45,289 DEBUG generators.py generate l.358] (55/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:45,686 ERROR generators.py complete l.400] (55/93) The following exception occurred with prompt meta={} user=" Qui a découvert la loi de la conservation de la quantité de mouvement ?  A)  Isaac Newton B)  René Descartes C)  Christiaan Huygens .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:45,701 DEBUG generators.py generate l.373] (55/93) Reuse post-processing
[2024-03-04 11:44:45,703 INFO generators.py generate l.477] (55/93) End question " Qui a découvert la loi de la conservation de la quantité de mouvement ?  A)  Isaac Newton B)  René Descartes C)  Christiaan Huygens "
[2024-03-04 11:44:45,706 INFO generators.py generate l.475] (56/93) *** AnsGenerator for question " Qui a inventé le premier microprocesseur ?  A)  Federico Faggin, Ted Hoff et Stanley Mazor B)  Marcian Hoff "
[2024-03-04 11:44:45,708 INFO generators.py gen_for_qa l.548] (56/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:45,708 DEBUG generators.py gen_for_qa l.554] (56/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:45,712 DEBUG generators.py generate l.352] (56/93) Reuse existing Prompt
[2024-03-04 11:44:45,714 DEBUG generators.py generate l.365] (56/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:45,715 DEBUG generators.py generate l.373] (56/93) Reuse post-processing
[2024-03-04 11:44:45,717 INFO generators.py gen_for_qa l.548] (56/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:45,718 DEBUG generators.py gen_for_qa l.554] (56/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:45,721 DEBUG generators.py generate l.352] (56/93) Reuse existing Prompt
[2024-03-04 11:44:45,721 DEBUG generators.py generate l.365] (56/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:45,721 DEBUG generators.py generate l.373] (56/93) Reuse post-processing
[2024-03-04 11:44:45,724 INFO generators.py gen_for_qa l.548] (56/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:45,725 DEBUG generators.py gen_for_qa l.554] (56/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:45,726 DEBUG generators.py generate l.352] (56/93) Reuse existing Prompt
[2024-03-04 11:44:45,727 DEBUG generators.py generate l.365] (56/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:45,728 DEBUG generators.py generate l.373] (56/93) Reuse post-processing
[2024-03-04 11:44:45,732 INFO generators.py gen_for_qa l.548] (56/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:45,732 DEBUG generators.py gen_for_qa l.554] (56/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:45,735 DEBUG generators.py generate l.352] (56/93) Reuse existing Prompt
[2024-03-04 11:44:45,736 DEBUG generators.py generate l.365] (56/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:45,737 DEBUG generators.py generate l.373] (56/93) Reuse post-processing
[2024-03-04 11:44:45,738 INFO generators.py gen_for_qa l.548] (56/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:45,739 DEBUG generators.py gen_for_qa l.554] (56/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:45,740 DEBUG generators.py generate l.352] (56/93) Reuse existing Prompt
[2024-03-04 11:44:45,740 DEBUG generators.py generate l.365] (56/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:45,740 DEBUG generators.py generate l.373] (56/93) Reuse post-processing
[2024-03-04 11:44:45,740 INFO generators.py gen_for_qa l.548] (56/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:45,744 DEBUG generators.py gen_for_qa l.554] (56/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:45,745 DEBUG generators.py generate l.352] (56/93) Reuse existing Prompt
[2024-03-04 11:44:45,748 DEBUG generators.py generate l.365] (56/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:45,749 DEBUG generators.py generate l.373] (56/93) Reuse post-processing
[2024-03-04 11:44:45,751 INFO generators.py gen_for_qa l.548] (56/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:45,752 DEBUG generators.py generate l.349] (56/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:45,752 DEBUG generators.py generate l.358] (56/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:46,145 ERROR generators.py complete l.400] (56/93) The following exception occurred with prompt meta={} user=" Qui a inventé le premier microprocesseur ?  A)  Federico Faggin, Ted Hoff et Stanley Mazor B)  Marcian Hoff .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:46,163 DEBUG generators.py generate l.373] (56/93) Reuse post-processing
[2024-03-04 11:44:46,167 INFO generators.py generate l.477] (56/93) End question " Qui a inventé le premier microprocesseur ?  A)  Federico Faggin, Ted Hoff et Stanley Mazor B)  Marcian Hoff "
[2024-03-04 11:44:46,169 INFO generators.py generate l.475] (57/93) *** AnsGenerator for question " Qui a découvert la loi de la conservation de la charge électrique ?  A)  Michael Faraday B)  Charles-Augustin de Coulomb C)  Benjamin Franklin "
[2024-03-04 11:44:46,171 INFO generators.py gen_for_qa l.548] (57/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:46,174 DEBUG generators.py gen_for_qa l.554] (57/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:46,176 DEBUG generators.py generate l.352] (57/93) Reuse existing Prompt
[2024-03-04 11:44:46,178 DEBUG generators.py generate l.365] (57/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:46,179 DEBUG generators.py generate l.373] (57/93) Reuse post-processing
[2024-03-04 11:44:46,181 INFO generators.py gen_for_qa l.548] (57/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:46,183 DEBUG generators.py gen_for_qa l.554] (57/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:46,185 DEBUG generators.py generate l.352] (57/93) Reuse existing Prompt
[2024-03-04 11:44:46,185 DEBUG generators.py generate l.365] (57/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:46,188 DEBUG generators.py generate l.373] (57/93) Reuse post-processing
[2024-03-04 11:44:46,189 INFO generators.py gen_for_qa l.548] (57/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:46,190 DEBUG generators.py gen_for_qa l.554] (57/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:46,191 DEBUG generators.py generate l.352] (57/93) Reuse existing Prompt
[2024-03-04 11:44:46,192 DEBUG generators.py generate l.365] (57/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:46,194 DEBUG generators.py generate l.373] (57/93) Reuse post-processing
[2024-03-04 11:44:46,195 INFO generators.py gen_for_qa l.548] (57/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:46,197 DEBUG generators.py gen_for_qa l.554] (57/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:46,200 DEBUG generators.py generate l.352] (57/93) Reuse existing Prompt
[2024-03-04 11:44:46,201 DEBUG generators.py generate l.365] (57/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:46,202 DEBUG generators.py generate l.373] (57/93) Reuse post-processing
[2024-03-04 11:44:46,203 INFO generators.py gen_for_qa l.548] (57/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:46,204 DEBUG generators.py gen_for_qa l.554] (57/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:46,205 DEBUG generators.py generate l.352] (57/93) Reuse existing Prompt
[2024-03-04 11:44:46,205 DEBUG generators.py generate l.365] (57/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:46,206 DEBUG generators.py generate l.373] (57/93) Reuse post-processing
[2024-03-04 11:44:46,206 INFO generators.py gen_for_qa l.548] (57/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:46,208 DEBUG generators.py gen_for_qa l.554] (57/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:46,210 DEBUG generators.py generate l.352] (57/93) Reuse existing Prompt
[2024-03-04 11:44:46,210 DEBUG generators.py generate l.365] (57/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:46,211 DEBUG generators.py generate l.373] (57/93) Reuse post-processing
[2024-03-04 11:44:46,213 INFO generators.py gen_for_qa l.548] (57/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:46,214 DEBUG generators.py generate l.349] (57/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:46,216 DEBUG generators.py generate l.358] (57/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:46,620 ERROR generators.py complete l.400] (57/93) The following exception occurred with prompt meta={} user=" Qui a découvert la loi de la conservation de la charge électrique ?  A)  Michael Faraday B)  Charles-Augustin de Coulomb C)  Benjamin Franklin .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:46,636 DEBUG generators.py generate l.373] (57/93) Reuse post-processing
[2024-03-04 11:44:46,639 INFO generators.py generate l.477] (57/93) End question " Qui a découvert la loi de la conservation de la charge électrique ?  A)  Michael Faraday B)  Charles-Augustin de Coulomb C)  Benjamin Franklin "
[2024-03-04 11:44:46,641 INFO generators.py generate l.475] (58/93) *** AnsGenerator for question " Qui a inventé le premier téléphone portable ?  A)  Martin Cooper B)  Rudy Krolopp "
[2024-03-04 11:44:46,645 INFO generators.py gen_for_qa l.548] (58/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:46,649 DEBUG generators.py gen_for_qa l.554] (58/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:46,651 DEBUG generators.py generate l.352] (58/93) Reuse existing Prompt
[2024-03-04 11:44:46,654 DEBUG generators.py generate l.365] (58/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:46,654 DEBUG generators.py generate l.373] (58/93) Reuse post-processing
[2024-03-04 11:44:46,656 INFO generators.py gen_for_qa l.548] (58/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:46,658 DEBUG generators.py gen_for_qa l.554] (58/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:46,659 DEBUG generators.py generate l.352] (58/93) Reuse existing Prompt
[2024-03-04 11:44:46,660 DEBUG generators.py generate l.365] (58/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:46,662 DEBUG generators.py generate l.373] (58/93) Reuse post-processing
[2024-03-04 11:44:46,664 INFO generators.py gen_for_qa l.548] (58/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:46,667 DEBUG generators.py gen_for_qa l.554] (58/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:46,668 DEBUG generators.py generate l.352] (58/93) Reuse existing Prompt
[2024-03-04 11:44:46,669 DEBUG generators.py generate l.365] (58/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:46,670 DEBUG generators.py generate l.373] (58/93) Reuse post-processing
[2024-03-04 11:44:46,671 INFO generators.py gen_for_qa l.548] (58/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:46,672 DEBUG generators.py gen_for_qa l.554] (58/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:46,673 DEBUG generators.py generate l.352] (58/93) Reuse existing Prompt
[2024-03-04 11:44:46,675 DEBUG generators.py generate l.365] (58/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:46,675 DEBUG generators.py generate l.373] (58/93) Reuse post-processing
[2024-03-04 11:44:46,675 INFO generators.py gen_for_qa l.548] (58/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:46,677 DEBUG generators.py gen_for_qa l.554] (58/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:46,680 DEBUG generators.py generate l.352] (58/93) Reuse existing Prompt
[2024-03-04 11:44:46,681 DEBUG generators.py generate l.365] (58/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:46,683 DEBUG generators.py generate l.373] (58/93) Reuse post-processing
[2024-03-04 11:44:46,684 INFO generators.py gen_for_qa l.548] (58/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:46,684 DEBUG generators.py gen_for_qa l.554] (58/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:46,684 DEBUG generators.py generate l.352] (58/93) Reuse existing Prompt
[2024-03-04 11:44:46,684 DEBUG generators.py generate l.365] (58/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:46,688 DEBUG generators.py generate l.373] (58/93) Reuse post-processing
[2024-03-04 11:44:46,689 INFO generators.py gen_for_qa l.548] (58/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:46,689 DEBUG generators.py generate l.349] (58/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:46,690 DEBUG generators.py generate l.358] (58/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:47,082 ERROR generators.py complete l.400] (58/93) The following exception occurred with prompt meta={} user=" Qui a inventé le premier téléphone portable ?  A)  Martin Cooper B)  Rudy Krolopp .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:47,098 DEBUG generators.py generate l.373] (58/93) Reuse post-processing
[2024-03-04 11:44:47,102 INFO generators.py generate l.477] (58/93) End question " Qui a inventé le premier téléphone portable ?  A)  Martin Cooper B)  Rudy Krolopp "
[2024-03-04 11:44:47,104 INFO generators.py generate l.475] (59/93) *** AnsGenerator for question " Où a été inventé le premier avion ?  A)  Kitty Hawk B)  Le Bourget C)  Bridgeport "
[2024-03-04 11:44:47,106 INFO generators.py gen_for_qa l.548] (59/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:47,106 DEBUG generators.py gen_for_qa l.554] (59/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:47,108 DEBUG generators.py generate l.352] (59/93) Reuse existing Prompt
[2024-03-04 11:44:47,111 DEBUG generators.py generate l.365] (59/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:47,113 DEBUG generators.py generate l.373] (59/93) Reuse post-processing
[2024-03-04 11:44:47,115 INFO generators.py gen_for_qa l.548] (59/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:47,117 DEBUG generators.py gen_for_qa l.554] (59/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:47,118 DEBUG generators.py generate l.352] (59/93) Reuse existing Prompt
[2024-03-04 11:44:47,119 DEBUG generators.py generate l.365] (59/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:47,120 DEBUG generators.py generate l.373] (59/93) Reuse post-processing
[2024-03-04 11:44:47,122 INFO generators.py gen_for_qa l.548] (59/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:47,123 DEBUG generators.py gen_for_qa l.554] (59/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:47,124 DEBUG generators.py generate l.352] (59/93) Reuse existing Prompt
[2024-03-04 11:44:47,125 DEBUG generators.py generate l.365] (59/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:47,125 DEBUG generators.py generate l.373] (59/93) Reuse post-processing
[2024-03-04 11:44:47,128 INFO generators.py gen_for_qa l.548] (59/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:47,130 DEBUG generators.py gen_for_qa l.554] (59/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:47,131 DEBUG generators.py generate l.352] (59/93) Reuse existing Prompt
[2024-03-04 11:44:47,132 DEBUG generators.py generate l.365] (59/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:47,133 DEBUG generators.py generate l.373] (59/93) Reuse post-processing
[2024-03-04 11:44:47,133 INFO generators.py gen_for_qa l.548] (59/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:47,135 DEBUG generators.py gen_for_qa l.554] (59/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:47,136 DEBUG generators.py generate l.352] (59/93) Reuse existing Prompt
[2024-03-04 11:44:47,137 DEBUG generators.py generate l.365] (59/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:47,137 DEBUG generators.py generate l.373] (59/93) Reuse post-processing
[2024-03-04 11:44:47,138 INFO generators.py gen_for_qa l.548] (59/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:47,140 DEBUG generators.py gen_for_qa l.554] (59/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:47,140 DEBUG generators.py generate l.352] (59/93) Reuse existing Prompt
[2024-03-04 11:44:47,141 DEBUG generators.py generate l.365] (59/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:47,142 DEBUG generators.py generate l.373] (59/93) Reuse post-processing
[2024-03-04 11:44:47,143 INFO generators.py gen_for_qa l.548] (59/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:47,144 DEBUG generators.py generate l.349] (59/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:47,146 DEBUG generators.py generate l.358] (59/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:47,544 ERROR generators.py complete l.400] (59/93) The following exception occurred with prompt meta={} user=" Où a été inventé le premier avion ?  A)  Kitty Hawk B)  Le Bourget C)  Bridgeport .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:47,562 DEBUG generators.py generate l.373] (59/93) Reuse post-processing
[2024-03-04 11:44:47,563 INFO generators.py generate l.477] (59/93) End question " Où a été inventé le premier avion ?  A)  Kitty Hawk B)  Le Bourget C)  Bridgeport "
[2024-03-04 11:44:47,565 INFO generators.py generate l.475] (60/93) *** AnsGenerator for question " Où a été découvert l'Amérique par Christophe Colomb ?  A)  San Salvador B)  Pointe Isabela C)  Guanahani "
[2024-03-04 11:44:47,566 INFO generators.py gen_for_qa l.548] (60/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:47,567 DEBUG generators.py gen_for_qa l.554] (60/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:47,569 DEBUG generators.py generate l.352] (60/93) Reuse existing Prompt
[2024-03-04 11:44:47,570 DEBUG generators.py generate l.365] (60/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:47,572 DEBUG generators.py generate l.373] (60/93) Reuse post-processing
[2024-03-04 11:44:47,573 INFO generators.py gen_for_qa l.548] (60/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:47,573 DEBUG generators.py gen_for_qa l.554] (60/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:47,575 DEBUG generators.py generate l.352] (60/93) Reuse existing Prompt
[2024-03-04 11:44:47,576 DEBUG generators.py generate l.365] (60/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:47,577 DEBUG generators.py generate l.373] (60/93) Reuse post-processing
[2024-03-04 11:44:47,579 INFO generators.py gen_for_qa l.548] (60/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:47,580 DEBUG generators.py gen_for_qa l.554] (60/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:47,582 DEBUG generators.py generate l.352] (60/93) Reuse existing Prompt
[2024-03-04 11:44:47,583 DEBUG generators.py generate l.365] (60/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:47,583 DEBUG generators.py generate l.373] (60/93) Reuse post-processing
[2024-03-04 11:44:47,585 INFO generators.py gen_for_qa l.548] (60/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:47,585 DEBUG generators.py gen_for_qa l.554] (60/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:47,587 DEBUG generators.py generate l.352] (60/93) Reuse existing Prompt
[2024-03-04 11:44:47,587 DEBUG generators.py generate l.365] (60/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:47,588 DEBUG generators.py generate l.373] (60/93) Reuse post-processing
[2024-03-04 11:44:47,589 INFO generators.py gen_for_qa l.548] (60/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:47,590 DEBUG generators.py gen_for_qa l.554] (60/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:47,591 DEBUG generators.py generate l.352] (60/93) Reuse existing Prompt
[2024-03-04 11:44:47,592 DEBUG generators.py generate l.365] (60/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:47,592 DEBUG generators.py generate l.373] (60/93) Reuse post-processing
[2024-03-04 11:44:47,594 INFO generators.py gen_for_qa l.548] (60/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:47,594 DEBUG generators.py gen_for_qa l.554] (60/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:47,598 DEBUG generators.py generate l.352] (60/93) Reuse existing Prompt
[2024-03-04 11:44:47,599 DEBUG generators.py generate l.365] (60/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:47,599 DEBUG generators.py generate l.373] (60/93) Reuse post-processing
[2024-03-04 11:44:47,600 INFO generators.py gen_for_qa l.548] (60/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:47,601 DEBUG generators.py generate l.349] (60/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:47,602 DEBUG generators.py generate l.358] (60/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:48,012 ERROR generators.py complete l.400] (60/93) The following exception occurred with prompt meta={} user=" Où a été découvert l'Amérique par Christophe Colomb ?  A)  San Salvador B)  Pointe Isabela C)  Guanahani .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:48,022 DEBUG generators.py generate l.373] (60/93) Reuse post-processing
[2024-03-04 11:44:48,025 INFO generators.py generate l.477] (60/93) End question " Où a été découvert l'Amérique par Christophe Colomb ?  A)  San Salvador B)  Pointe Isabela C)  Guanahani "
[2024-03-04 11:44:48,027 INFO generators.py generate l.475] (61/93) *** AnsGenerator for question " Où a été inventé le premier ordinateur programmable ?  A)  Berlin B)  Iowa C)  Bletchley Park "
[2024-03-04 11:44:48,030 INFO generators.py gen_for_qa l.548] (61/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:48,031 DEBUG generators.py gen_for_qa l.554] (61/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:48,033 DEBUG generators.py generate l.352] (61/93) Reuse existing Prompt
[2024-03-04 11:44:48,035 DEBUG generators.py generate l.365] (61/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:48,035 DEBUG generators.py generate l.373] (61/93) Reuse post-processing
[2024-03-04 11:44:48,038 INFO generators.py gen_for_qa l.548] (61/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:48,040 DEBUG generators.py gen_for_qa l.554] (61/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:48,041 DEBUG generators.py generate l.352] (61/93) Reuse existing Prompt
[2024-03-04 11:44:48,043 DEBUG generators.py generate l.365] (61/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:48,045 DEBUG generators.py generate l.373] (61/93) Reuse post-processing
[2024-03-04 11:44:48,045 INFO generators.py gen_for_qa l.548] (61/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:48,046 DEBUG generators.py gen_for_qa l.554] (61/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:48,048 DEBUG generators.py generate l.352] (61/93) Reuse existing Prompt
[2024-03-04 11:44:48,049 DEBUG generators.py generate l.365] (61/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:48,051 DEBUG generators.py generate l.373] (61/93) Reuse post-processing
[2024-03-04 11:44:48,054 INFO generators.py gen_for_qa l.548] (61/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:48,055 DEBUG generators.py gen_for_qa l.554] (61/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:48,055 DEBUG generators.py generate l.352] (61/93) Reuse existing Prompt
[2024-03-04 11:44:48,057 DEBUG generators.py generate l.365] (61/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:48,057 DEBUG generators.py generate l.373] (61/93) Reuse post-processing
[2024-03-04 11:44:48,059 INFO generators.py gen_for_qa l.548] (61/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:48,059 DEBUG generators.py gen_for_qa l.554] (61/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:48,060 DEBUG generators.py generate l.352] (61/93) Reuse existing Prompt
[2024-03-04 11:44:48,061 DEBUG generators.py generate l.365] (61/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:48,063 DEBUG generators.py generate l.373] (61/93) Reuse post-processing
[2024-03-04 11:44:48,065 INFO generators.py gen_for_qa l.548] (61/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:48,066 DEBUG generators.py gen_for_qa l.554] (61/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:48,067 DEBUG generators.py generate l.352] (61/93) Reuse existing Prompt
[2024-03-04 11:44:48,068 DEBUG generators.py generate l.365] (61/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:48,069 DEBUG generators.py generate l.373] (61/93) Reuse post-processing
[2024-03-04 11:44:48,070 INFO generators.py gen_for_qa l.548] (61/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:48,070 DEBUG generators.py generate l.349] (61/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:48,072 DEBUG generators.py generate l.358] (61/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:48,463 ERROR generators.py complete l.400] (61/93) The following exception occurred with prompt meta={} user=" Où a été inventé le premier ordinateur programmable ?  A)  Berlin B)  Iowa C)  Bletchley Park .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:48,478 DEBUG generators.py generate l.373] (61/93) Reuse post-processing
[2024-03-04 11:44:48,481 INFO generators.py generate l.477] (61/93) End question " Où a été inventé le premier ordinateur programmable ?  A)  Berlin B)  Iowa C)  Bletchley Park "
[2024-03-04 11:44:48,483 INFO generators.py generate l.475] (62/93) *** AnsGenerator for question " Où a été découverte la pénicilline ?  A)  Londres B)  Paris "
[2024-03-04 11:44:48,486 INFO generators.py gen_for_qa l.548] (62/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:48,487 DEBUG generators.py gen_for_qa l.554] (62/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:48,490 DEBUG generators.py generate l.352] (62/93) Reuse existing Prompt
[2024-03-04 11:44:48,491 DEBUG generators.py generate l.365] (62/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:48,491 DEBUG generators.py generate l.373] (62/93) Reuse post-processing
[2024-03-04 11:44:48,495 INFO generators.py gen_for_qa l.548] (62/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:48,497 DEBUG generators.py gen_for_qa l.554] (62/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:48,499 DEBUG generators.py generate l.352] (62/93) Reuse existing Prompt
[2024-03-04 11:44:48,500 DEBUG generators.py generate l.365] (62/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:48,501 DEBUG generators.py generate l.373] (62/93) Reuse post-processing
[2024-03-04 11:44:48,503 INFO generators.py gen_for_qa l.548] (62/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:48,503 DEBUG generators.py gen_for_qa l.554] (62/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:48,505 DEBUG generators.py generate l.352] (62/93) Reuse existing Prompt
[2024-03-04 11:44:48,506 DEBUG generators.py generate l.365] (62/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:48,508 DEBUG generators.py generate l.373] (62/93) Reuse post-processing
[2024-03-04 11:44:48,509 INFO generators.py gen_for_qa l.548] (62/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:48,510 DEBUG generators.py gen_for_qa l.554] (62/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:48,512 DEBUG generators.py generate l.352] (62/93) Reuse existing Prompt
[2024-03-04 11:44:48,514 DEBUG generators.py generate l.365] (62/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:48,514 DEBUG generators.py generate l.373] (62/93) Reuse post-processing
[2024-03-04 11:44:48,514 INFO generators.py gen_for_qa l.548] (62/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:48,514 DEBUG generators.py gen_for_qa l.554] (62/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:48,518 DEBUG generators.py generate l.352] (62/93) Reuse existing Prompt
[2024-03-04 11:44:48,520 DEBUG generators.py generate l.365] (62/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:48,521 DEBUG generators.py generate l.373] (62/93) Reuse post-processing
[2024-03-04 11:44:48,521 INFO generators.py gen_for_qa l.548] (62/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:48,522 DEBUG generators.py gen_for_qa l.554] (62/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:48,523 DEBUG generators.py generate l.352] (62/93) Reuse existing Prompt
[2024-03-04 11:44:48,524 DEBUG generators.py generate l.365] (62/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:48,525 DEBUG generators.py generate l.373] (62/93) Reuse post-processing
[2024-03-04 11:44:48,525 INFO generators.py gen_for_qa l.548] (62/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:48,527 DEBUG generators.py generate l.349] (62/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:48,529 DEBUG generators.py generate l.358] (62/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:48,921 ERROR generators.py complete l.400] (62/93) The following exception occurred with prompt meta={} user=" Où a été découverte la pénicilline ?  A)  Londres B)  Paris .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:48,927 DEBUG generators.py generate l.373] (62/93) Reuse post-processing
[2024-03-04 11:44:48,927 INFO generators.py generate l.477] (62/93) End question " Où a été découverte la pénicilline ?  A)  Londres B)  Paris "
[2024-03-04 11:44:48,927 INFO generators.py generate l.475] (63/93) *** AnsGenerator for question " Où a été inventé le premier satellite artificiel ?  A)  Moscou B)  Cape Canaveral C)  Baïkonour "
[2024-03-04 11:44:48,931 INFO generators.py gen_for_qa l.548] (63/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:48,931 DEBUG generators.py gen_for_qa l.554] (63/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:48,933 DEBUG generators.py generate l.352] (63/93) Reuse existing Prompt
[2024-03-04 11:44:48,933 DEBUG generators.py generate l.365] (63/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:48,936 DEBUG generators.py generate l.373] (63/93) Reuse post-processing
[2024-03-04 11:44:48,937 INFO generators.py gen_for_qa l.548] (63/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:48,939 DEBUG generators.py gen_for_qa l.554] (63/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:48,940 DEBUG generators.py generate l.352] (63/93) Reuse existing Prompt
[2024-03-04 11:44:48,940 DEBUG generators.py generate l.365] (63/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:48,941 DEBUG generators.py generate l.373] (63/93) Reuse post-processing
[2024-03-04 11:44:48,942 INFO generators.py gen_for_qa l.548] (63/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:48,942 DEBUG generators.py gen_for_qa l.554] (63/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:48,944 DEBUG generators.py generate l.352] (63/93) Reuse existing Prompt
[2024-03-04 11:44:48,947 DEBUG generators.py generate l.365] (63/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:48,948 DEBUG generators.py generate l.373] (63/93) Reuse post-processing
[2024-03-04 11:44:48,950 INFO generators.py gen_for_qa l.548] (63/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:48,951 DEBUG generators.py gen_for_qa l.554] (63/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:48,952 DEBUG generators.py generate l.352] (63/93) Reuse existing Prompt
[2024-03-04 11:44:48,953 DEBUG generators.py generate l.365] (63/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:48,955 DEBUG generators.py generate l.373] (63/93) Reuse post-processing
[2024-03-04 11:44:48,955 INFO generators.py gen_for_qa l.548] (63/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:48,956 DEBUG generators.py gen_for_qa l.554] (63/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:48,956 DEBUG generators.py generate l.352] (63/93) Reuse existing Prompt
[2024-03-04 11:44:48,958 DEBUG generators.py generate l.365] (63/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:48,960 DEBUG generators.py generate l.373] (63/93) Reuse post-processing
[2024-03-04 11:44:48,961 INFO generators.py gen_for_qa l.548] (63/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:48,962 DEBUG generators.py gen_for_qa l.554] (63/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:48,964 DEBUG generators.py generate l.352] (63/93) Reuse existing Prompt
[2024-03-04 11:44:48,965 DEBUG generators.py generate l.365] (63/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:48,965 DEBUG generators.py generate l.373] (63/93) Reuse post-processing
[2024-03-04 11:44:48,965 INFO generators.py gen_for_qa l.548] (63/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:48,965 DEBUG generators.py generate l.349] (63/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:48,970 DEBUG generators.py generate l.358] (63/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:49,362 ERROR generators.py complete l.400] (63/93) The following exception occurred with prompt meta={} user=" Où a été inventé le premier satellite artificiel ?  A)  Moscou B)  Cape Canaveral C)  Baïkonour .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:49,379 DEBUG generators.py generate l.373] (63/93) Reuse post-processing
[2024-03-04 11:44:49,383 INFO generators.py generate l.477] (63/93) End question " Où a été inventé le premier satellite artificiel ?  A)  Moscou B)  Cape Canaveral C)  Baïkonour "
[2024-03-04 11:44:49,385 INFO generators.py generate l.475] (64/93) *** AnsGenerator for question " Où a été découvert le feu par l'Homme ?  A)  Vallée du Rift B)  Grotte de Zhoukoudian C)  Grotte de Wonderwerk "
[2024-03-04 11:44:49,388 INFO generators.py gen_for_qa l.548] (64/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:49,389 DEBUG generators.py gen_for_qa l.554] (64/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:49,391 DEBUG generators.py generate l.352] (64/93) Reuse existing Prompt
[2024-03-04 11:44:49,393 DEBUG generators.py generate l.365] (64/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:49,396 DEBUG generators.py generate l.373] (64/93) Reuse post-processing
[2024-03-04 11:44:49,398 INFO generators.py gen_for_qa l.548] (64/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:49,399 DEBUG generators.py gen_for_qa l.554] (64/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:49,400 DEBUG generators.py generate l.352] (64/93) Reuse existing Prompt
[2024-03-04 11:44:49,401 DEBUG generators.py generate l.365] (64/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:49,402 DEBUG generators.py generate l.373] (64/93) Reuse post-processing
[2024-03-04 11:44:49,402 INFO generators.py gen_for_qa l.548] (64/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:49,406 DEBUG generators.py gen_for_qa l.554] (64/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:49,407 DEBUG generators.py generate l.352] (64/93) Reuse existing Prompt
[2024-03-04 11:44:49,407 DEBUG generators.py generate l.365] (64/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:49,408 DEBUG generators.py generate l.373] (64/93) Reuse post-processing
[2024-03-04 11:44:49,409 INFO generators.py gen_for_qa l.548] (64/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:49,412 DEBUG generators.py gen_for_qa l.554] (64/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:49,413 DEBUG generators.py generate l.352] (64/93) Reuse existing Prompt
[2024-03-04 11:44:49,414 DEBUG generators.py generate l.365] (64/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:49,416 DEBUG generators.py generate l.373] (64/93) Reuse post-processing
[2024-03-04 11:44:49,416 INFO generators.py gen_for_qa l.548] (64/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:49,417 DEBUG generators.py gen_for_qa l.554] (64/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:49,418 DEBUG generators.py generate l.352] (64/93) Reuse existing Prompt
[2024-03-04 11:44:49,419 DEBUG generators.py generate l.365] (64/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:49,421 DEBUG generators.py generate l.373] (64/93) Reuse post-processing
[2024-03-04 11:44:49,421 INFO generators.py gen_for_qa l.548] (64/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:49,422 DEBUG generators.py gen_for_qa l.554] (64/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:49,423 DEBUG generators.py generate l.352] (64/93) Reuse existing Prompt
[2024-03-04 11:44:49,423 DEBUG generators.py generate l.365] (64/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:49,425 DEBUG generators.py generate l.373] (64/93) Reuse post-processing
[2024-03-04 11:44:49,425 INFO generators.py gen_for_qa l.548] (64/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:49,428 DEBUG generators.py generate l.349] (64/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:49,429 DEBUG generators.py generate l.358] (64/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:49,828 ERROR generators.py complete l.400] (64/93) The following exception occurred with prompt meta={} user=" Où a été découvert le feu par l'Homme ?  A)  Vallée du Rift B)  Grotte de Zhoukoudian C)  Grotte de Wonderwerk .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:49,842 DEBUG generators.py generate l.373] (64/93) Reuse post-processing
[2024-03-04 11:44:49,844 INFO generators.py generate l.477] (64/93) End question " Où a été découvert le feu par l'Homme ?  A)  Vallée du Rift B)  Grotte de Zhoukoudian C)  Grotte de Wonderwerk "
[2024-03-04 11:44:49,847 INFO generators.py generate l.475] (65/93) *** AnsGenerator for question " Où a été inventé le premier microprocesseur ?  A)  Silicon Valley B)  Palo Alto C)  Santa Clara "
[2024-03-04 11:44:49,850 INFO generators.py gen_for_qa l.548] (65/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:49,851 DEBUG generators.py gen_for_qa l.554] (65/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:49,852 DEBUG generators.py generate l.352] (65/93) Reuse existing Prompt
[2024-03-04 11:44:49,854 DEBUG generators.py generate l.365] (65/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:49,856 DEBUG generators.py generate l.373] (65/93) Reuse post-processing
[2024-03-04 11:44:49,857 INFO generators.py gen_for_qa l.548] (65/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:49,858 DEBUG generators.py gen_for_qa l.554] (65/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:49,859 DEBUG generators.py generate l.352] (65/93) Reuse existing Prompt
[2024-03-04 11:44:49,862 DEBUG generators.py generate l.365] (65/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:49,863 DEBUG generators.py generate l.373] (65/93) Reuse post-processing
[2024-03-04 11:44:49,866 INFO generators.py gen_for_qa l.548] (65/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:49,866 DEBUG generators.py gen_for_qa l.554] (65/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:49,867 DEBUG generators.py generate l.352] (65/93) Reuse existing Prompt
[2024-03-04 11:44:49,868 DEBUG generators.py generate l.365] (65/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:49,869 DEBUG generators.py generate l.373] (65/93) Reuse post-processing
[2024-03-04 11:44:49,870 INFO generators.py gen_for_qa l.548] (65/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:49,871 DEBUG generators.py gen_for_qa l.554] (65/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:49,871 DEBUG generators.py generate l.352] (65/93) Reuse existing Prompt
[2024-03-04 11:44:49,873 DEBUG generators.py generate l.365] (65/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:49,874 DEBUG generators.py generate l.373] (65/93) Reuse post-processing
[2024-03-04 11:44:49,875 INFO generators.py gen_for_qa l.548] (65/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:49,875 DEBUG generators.py gen_for_qa l.554] (65/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:49,877 DEBUG generators.py generate l.352] (65/93) Reuse existing Prompt
[2024-03-04 11:44:49,879 DEBUG generators.py generate l.365] (65/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:49,881 DEBUG generators.py generate l.373] (65/93) Reuse post-processing
[2024-03-04 11:44:49,882 INFO generators.py gen_for_qa l.548] (65/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:49,883 DEBUG generators.py gen_for_qa l.554] (65/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:49,883 DEBUG generators.py generate l.352] (65/93) Reuse existing Prompt
[2024-03-04 11:44:49,885 DEBUG generators.py generate l.365] (65/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:49,885 DEBUG generators.py generate l.373] (65/93) Reuse post-processing
[2024-03-04 11:44:49,887 INFO generators.py gen_for_qa l.548] (65/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:49,888 DEBUG generators.py generate l.349] (65/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:49,888 DEBUG generators.py generate l.358] (65/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:50,285 ERROR generators.py complete l.400] (65/93) The following exception occurred with prompt meta={} user=" Où a été inventé le premier microprocesseur ?  A)  Silicon Valley B)  Palo Alto C)  Santa Clara .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:50,296 DEBUG generators.py generate l.373] (65/93) Reuse post-processing
[2024-03-04 11:44:50,298 INFO generators.py generate l.477] (65/93) End question " Où a été inventé le premier microprocesseur ?  A)  Silicon Valley B)  Palo Alto C)  Santa Clara "
[2024-03-04 11:44:50,302 INFO generators.py generate l.475] (66/93) *** AnsGenerator for question " Où a été découvert le premier dinosaure ?  A)  Angleterre B)  États-Unis C)  France "
[2024-03-04 11:44:50,304 INFO generators.py gen_for_qa l.548] (66/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:50,306 DEBUG generators.py gen_for_qa l.554] (66/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:50,306 DEBUG generators.py generate l.352] (66/93) Reuse existing Prompt
[2024-03-04 11:44:50,309 DEBUG generators.py generate l.365] (66/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:50,311 DEBUG generators.py generate l.373] (66/93) Reuse post-processing
[2024-03-04 11:44:50,313 INFO generators.py gen_for_qa l.548] (66/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:50,314 DEBUG generators.py gen_for_qa l.554] (66/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:50,317 DEBUG generators.py generate l.352] (66/93) Reuse existing Prompt
[2024-03-04 11:44:50,318 DEBUG generators.py generate l.365] (66/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:50,320 DEBUG generators.py generate l.373] (66/93) Reuse post-processing
[2024-03-04 11:44:50,320 INFO generators.py gen_for_qa l.548] (66/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:50,320 DEBUG generators.py gen_for_qa l.554] (66/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:50,323 DEBUG generators.py generate l.352] (66/93) Reuse existing Prompt
[2024-03-04 11:44:50,324 DEBUG generators.py generate l.365] (66/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:50,324 DEBUG generators.py generate l.373] (66/93) Reuse post-processing
[2024-03-04 11:44:50,325 INFO generators.py gen_for_qa l.548] (66/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:50,326 DEBUG generators.py gen_for_qa l.554] (66/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:50,328 DEBUG generators.py generate l.352] (66/93) Reuse existing Prompt
[2024-03-04 11:44:50,329 DEBUG generators.py generate l.365] (66/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:50,331 DEBUG generators.py generate l.373] (66/93) Reuse post-processing
[2024-03-04 11:44:50,333 INFO generators.py gen_for_qa l.548] (66/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:50,335 DEBUG generators.py gen_for_qa l.554] (66/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:50,336 DEBUG generators.py generate l.352] (66/93) Reuse existing Prompt
[2024-03-04 11:44:50,336 DEBUG generators.py generate l.365] (66/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:50,338 DEBUG generators.py generate l.373] (66/93) Reuse post-processing
[2024-03-04 11:44:50,339 INFO generators.py gen_for_qa l.548] (66/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:50,339 DEBUG generators.py gen_for_qa l.554] (66/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:50,339 DEBUG generators.py generate l.352] (66/93) Reuse existing Prompt
[2024-03-04 11:44:50,339 DEBUG generators.py generate l.365] (66/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:50,343 DEBUG generators.py generate l.373] (66/93) Reuse post-processing
[2024-03-04 11:44:50,344 INFO generators.py gen_for_qa l.548] (66/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:50,345 DEBUG generators.py generate l.349] (66/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:50,347 DEBUG generators.py generate l.358] (66/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:50,734 ERROR generators.py complete l.400] (66/93) The following exception occurred with prompt meta={} user=" Où a été découvert le premier dinosaure ?  A)  Angleterre B)  États-Unis C)  France .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:50,739 DEBUG generators.py generate l.373] (66/93) Reuse post-processing
[2024-03-04 11:44:50,740 INFO generators.py generate l.477] (66/93) End question " Où a été découvert le premier dinosaure ?  A)  Angleterre B)  États-Unis C)  France "
[2024-03-04 11:44:50,742 INFO generators.py generate l.475] (67/93) *** AnsGenerator for question " Où a été inventé le premier téléphone ?  A)  Boston B)  Philadelphie C)  Washington D.C. "
[2024-03-04 11:44:50,743 INFO generators.py gen_for_qa l.548] (67/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:50,743 DEBUG generators.py gen_for_qa l.554] (67/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:50,745 DEBUG generators.py generate l.352] (67/93) Reuse existing Prompt
[2024-03-04 11:44:50,747 DEBUG generators.py generate l.365] (67/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:50,747 DEBUG generators.py generate l.373] (67/93) Reuse post-processing
[2024-03-04 11:44:50,749 INFO generators.py gen_for_qa l.548] (67/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:50,749 DEBUG generators.py gen_for_qa l.554] (67/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:50,751 DEBUG generators.py generate l.352] (67/93) Reuse existing Prompt
[2024-03-04 11:44:50,751 DEBUG generators.py generate l.365] (67/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:50,753 DEBUG generators.py generate l.373] (67/93) Reuse post-processing
[2024-03-04 11:44:50,753 INFO generators.py gen_for_qa l.548] (67/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:50,755 DEBUG generators.py gen_for_qa l.554] (67/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:50,756 DEBUG generators.py generate l.352] (67/93) Reuse existing Prompt
[2024-03-04 11:44:50,757 DEBUG generators.py generate l.365] (67/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:50,757 DEBUG generators.py generate l.373] (67/93) Reuse post-processing
[2024-03-04 11:44:50,758 INFO generators.py gen_for_qa l.548] (67/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:50,759 DEBUG generators.py gen_for_qa l.554] (67/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:50,761 DEBUG generators.py generate l.352] (67/93) Reuse existing Prompt
[2024-03-04 11:44:50,762 DEBUG generators.py generate l.365] (67/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:50,763 DEBUG generators.py generate l.373] (67/93) Reuse post-processing
[2024-03-04 11:44:50,764 INFO generators.py gen_for_qa l.548] (67/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:50,765 DEBUG generators.py gen_for_qa l.554] (67/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:50,765 DEBUG generators.py generate l.352] (67/93) Reuse existing Prompt
[2024-03-04 11:44:50,765 DEBUG generators.py generate l.365] (67/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:50,799 DEBUG generators.py generate l.373] (67/93) Reuse post-processing
[2024-03-04 11:44:50,812 INFO generators.py gen_for_qa l.548] (67/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:50,816 DEBUG generators.py gen_for_qa l.554] (67/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:50,818 DEBUG generators.py generate l.352] (67/93) Reuse existing Prompt
[2024-03-04 11:44:50,818 DEBUG generators.py generate l.365] (67/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:50,818 DEBUG generators.py generate l.373] (67/93) Reuse post-processing
[2024-03-04 11:44:50,818 INFO generators.py gen_for_qa l.548] (67/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:50,818 DEBUG generators.py generate l.349] (67/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:50,828 DEBUG generators.py generate l.358] (67/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:51,244 ERROR generators.py complete l.400] (67/93) The following exception occurred with prompt meta={} user=" Où a été inventé le premier téléphone ?  A)  Boston B)  Philadelphie C)  Washington D.C. .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:51,251 DEBUG generators.py generate l.373] (67/93) Reuse post-processing
[2024-03-04 11:44:51,252 INFO generators.py generate l.477] (67/93) End question " Où a été inventé le premier téléphone ?  A)  Boston B)  Philadelphie C)  Washington D.C. "
[2024-03-04 11:44:51,254 INFO generators.py generate l.475] (68/93) *** AnsGenerator for question " Où a été découvert le premier gisement de pétrole ?  A)  Titusville B)  Bakou C)  Tulsa "
[2024-03-04 11:44:51,255 INFO generators.py gen_for_qa l.548] (68/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:51,256 DEBUG generators.py gen_for_qa l.554] (68/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:51,259 DEBUG generators.py generate l.352] (68/93) Reuse existing Prompt
[2024-03-04 11:44:51,260 DEBUG generators.py generate l.365] (68/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:51,262 DEBUG generators.py generate l.373] (68/93) Reuse post-processing
[2024-03-04 11:44:51,264 INFO generators.py gen_for_qa l.548] (68/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:51,266 DEBUG generators.py gen_for_qa l.554] (68/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:51,267 DEBUG generators.py generate l.352] (68/93) Reuse existing Prompt
[2024-03-04 11:44:51,268 DEBUG generators.py generate l.365] (68/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:51,270 DEBUG generators.py generate l.373] (68/93) Reuse post-processing
[2024-03-04 11:44:51,270 INFO generators.py gen_for_qa l.548] (68/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:51,273 DEBUG generators.py gen_for_qa l.554] (68/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:51,274 DEBUG generators.py generate l.352] (68/93) Reuse existing Prompt
[2024-03-04 11:44:51,275 DEBUG generators.py generate l.365] (68/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:51,277 DEBUG generators.py generate l.373] (68/93) Reuse post-processing
[2024-03-04 11:44:51,280 INFO generators.py gen_for_qa l.548] (68/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:51,282 DEBUG generators.py gen_for_qa l.554] (68/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:51,284 DEBUG generators.py generate l.352] (68/93) Reuse existing Prompt
[2024-03-04 11:44:51,285 DEBUG generators.py generate l.365] (68/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:51,286 DEBUG generators.py generate l.373] (68/93) Reuse post-processing
[2024-03-04 11:44:51,288 INFO generators.py gen_for_qa l.548] (68/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:51,290 DEBUG generators.py gen_for_qa l.554] (68/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:51,291 DEBUG generators.py generate l.352] (68/93) Reuse existing Prompt
[2024-03-04 11:44:51,295 DEBUG generators.py generate l.365] (68/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:51,296 DEBUG generators.py generate l.373] (68/93) Reuse post-processing
[2024-03-04 11:44:51,297 INFO generators.py gen_for_qa l.548] (68/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:51,301 DEBUG generators.py gen_for_qa l.554] (68/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:51,303 DEBUG generators.py generate l.352] (68/93) Reuse existing Prompt
[2024-03-04 11:44:51,304 DEBUG generators.py generate l.365] (68/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:51,306 DEBUG generators.py generate l.373] (68/93) Reuse post-processing
[2024-03-04 11:44:51,308 INFO generators.py gen_for_qa l.548] (68/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:51,309 DEBUG generators.py generate l.349] (68/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:51,312 DEBUG generators.py generate l.358] (68/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:51,706 ERROR generators.py complete l.400] (68/93) The following exception occurred with prompt meta={} user=" Où a été découvert le premier gisement de pétrole ?  A)  Titusville B)  Bakou C)  Tulsa .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:51,712 DEBUG generators.py generate l.373] (68/93) Reuse post-processing
[2024-03-04 11:44:51,714 INFO generators.py generate l.477] (68/93) End question " Où a été découvert le premier gisement de pétrole ?  A)  Titusville B)  Bakou C)  Tulsa "
[2024-03-04 11:44:51,715 INFO generators.py generate l.475] (69/93) *** AnsGenerator for question " Où a été inventé le premier sous-marin ?  A)  Londres B)  La Haye C)  New York "
[2024-03-04 11:44:51,716 INFO generators.py gen_for_qa l.548] (69/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:51,716 DEBUG generators.py gen_for_qa l.554] (69/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:51,716 DEBUG generators.py generate l.352] (69/93) Reuse existing Prompt
[2024-03-04 11:44:51,721 DEBUG generators.py generate l.365] (69/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:51,722 DEBUG generators.py generate l.373] (69/93) Reuse post-processing
[2024-03-04 11:44:51,723 INFO generators.py gen_for_qa l.548] (69/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:51,724 DEBUG generators.py gen_for_qa l.554] (69/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:51,725 DEBUG generators.py generate l.352] (69/93) Reuse existing Prompt
[2024-03-04 11:44:51,727 DEBUG generators.py generate l.365] (69/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:51,727 DEBUG generators.py generate l.373] (69/93) Reuse post-processing
[2024-03-04 11:44:51,729 INFO generators.py gen_for_qa l.548] (69/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:51,732 DEBUG generators.py gen_for_qa l.554] (69/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:51,733 DEBUG generators.py generate l.352] (69/93) Reuse existing Prompt
[2024-03-04 11:44:51,734 DEBUG generators.py generate l.365] (69/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:51,735 DEBUG generators.py generate l.373] (69/93) Reuse post-processing
[2024-03-04 11:44:51,736 INFO generators.py gen_for_qa l.548] (69/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:51,736 DEBUG generators.py gen_for_qa l.554] (69/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:51,736 DEBUG generators.py generate l.352] (69/93) Reuse existing Prompt
[2024-03-04 11:44:51,736 DEBUG generators.py generate l.365] (69/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:51,740 DEBUG generators.py generate l.373] (69/93) Reuse post-processing
[2024-03-04 11:44:51,740 INFO generators.py gen_for_qa l.548] (69/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:51,742 DEBUG generators.py gen_for_qa l.554] (69/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:51,743 DEBUG generators.py generate l.352] (69/93) Reuse existing Prompt
[2024-03-04 11:44:51,743 DEBUG generators.py generate l.365] (69/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:51,748 DEBUG generators.py generate l.373] (69/93) Reuse post-processing
[2024-03-04 11:44:51,749 INFO generators.py gen_for_qa l.548] (69/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:51,750 DEBUG generators.py gen_for_qa l.554] (69/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:51,751 DEBUG generators.py generate l.352] (69/93) Reuse existing Prompt
[2024-03-04 11:44:51,752 DEBUG generators.py generate l.365] (69/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:51,753 DEBUG generators.py generate l.373] (69/93) Reuse post-processing
[2024-03-04 11:44:51,753 INFO generators.py gen_for_qa l.548] (69/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:51,754 DEBUG generators.py generate l.349] (69/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:51,754 DEBUG generators.py generate l.358] (69/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:52,150 ERROR generators.py complete l.400] (69/93) The following exception occurred with prompt meta={} user=" Où a été inventé le premier sous-marin ?  A)  Londres B)  La Haye C)  New York .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:52,168 DEBUG generators.py generate l.373] (69/93) Reuse post-processing
[2024-03-04 11:44:52,173 INFO generators.py generate l.477] (69/93) End question " Où a été inventé le premier sous-marin ?  A)  Londres B)  La Haye C)  New York "
[2024-03-04 11:44:52,176 INFO generators.py generate l.475] (70/93) *** AnsGenerator for question " Où a été découvert le premier vaccin ?  A)  Gloucestershire B)  Dorset C)  Somerset "
[2024-03-04 11:44:52,184 INFO generators.py gen_for_qa l.548] (70/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:52,189 DEBUG generators.py gen_for_qa l.554] (70/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:52,195 DEBUG generators.py generate l.352] (70/93) Reuse existing Prompt
[2024-03-04 11:44:52,195 DEBUG generators.py generate l.365] (70/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:52,204 DEBUG generators.py generate l.373] (70/93) Reuse post-processing
[2024-03-04 11:44:52,206 INFO generators.py gen_for_qa l.548] (70/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:52,210 DEBUG generators.py gen_for_qa l.554] (70/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:52,214 DEBUG generators.py generate l.352] (70/93) Reuse existing Prompt
[2024-03-04 11:44:52,216 DEBUG generators.py generate l.365] (70/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:52,217 DEBUG generators.py generate l.373] (70/93) Reuse post-processing
[2024-03-04 11:44:52,220 INFO generators.py gen_for_qa l.548] (70/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:52,222 DEBUG generators.py gen_for_qa l.554] (70/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:52,224 DEBUG generators.py generate l.352] (70/93) Reuse existing Prompt
[2024-03-04 11:44:52,226 DEBUG generators.py generate l.365] (70/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:52,229 DEBUG generators.py generate l.373] (70/93) Reuse post-processing
[2024-03-04 11:44:52,230 INFO generators.py gen_for_qa l.548] (70/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:52,230 DEBUG generators.py gen_for_qa l.554] (70/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:52,233 DEBUG generators.py generate l.352] (70/93) Reuse existing Prompt
[2024-03-04 11:44:52,235 DEBUG generators.py generate l.365] (70/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:52,236 DEBUG generators.py generate l.373] (70/93) Reuse post-processing
[2024-03-04 11:44:52,237 INFO generators.py gen_for_qa l.548] (70/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:52,238 DEBUG generators.py gen_for_qa l.554] (70/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:52,239 DEBUG generators.py generate l.352] (70/93) Reuse existing Prompt
[2024-03-04 11:44:52,240 DEBUG generators.py generate l.365] (70/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:52,242 DEBUG generators.py generate l.373] (70/93) Reuse post-processing
[2024-03-04 11:44:52,243 INFO generators.py gen_for_qa l.548] (70/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:52,246 DEBUG generators.py gen_for_qa l.554] (70/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:52,247 DEBUG generators.py generate l.352] (70/93) Reuse existing Prompt
[2024-03-04 11:44:52,249 DEBUG generators.py generate l.365] (70/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:52,249 DEBUG generators.py generate l.373] (70/93) Reuse post-processing
[2024-03-04 11:44:52,250 INFO generators.py gen_for_qa l.548] (70/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:52,252 DEBUG generators.py generate l.349] (70/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:52,253 DEBUG generators.py generate l.358] (70/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:52,652 ERROR generators.py complete l.400] (70/93) The following exception occurred with prompt meta={} user=" Où a été découvert le premier vaccin ?  A)  Gloucestershire B)  Dorset C)  Somerset .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:52,683 DEBUG generators.py generate l.373] (70/93) Reuse post-processing
[2024-03-04 11:44:52,687 INFO generators.py generate l.477] (70/93) End question " Où a été découvert le premier vaccin ?  A)  Gloucestershire B)  Dorset C)  Somerset "
[2024-03-04 11:44:52,689 INFO generators.py generate l.475] (71/93) *** AnsGenerator for question " Où a été inventé le premier moteur à combustion interne ?  A)  Paris B)  Francfort C)  Londres "
[2024-03-04 11:44:52,691 INFO generators.py gen_for_qa l.548] (71/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:52,694 DEBUG generators.py gen_for_qa l.554] (71/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:52,696 DEBUG generators.py generate l.352] (71/93) Reuse existing Prompt
[2024-03-04 11:44:52,698 DEBUG generators.py generate l.365] (71/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:52,700 DEBUG generators.py generate l.373] (71/93) Reuse post-processing
[2024-03-04 11:44:52,701 INFO generators.py gen_for_qa l.548] (71/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:52,703 DEBUG generators.py gen_for_qa l.554] (71/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:52,705 DEBUG generators.py generate l.352] (71/93) Reuse existing Prompt
[2024-03-04 11:44:52,706 DEBUG generators.py generate l.365] (71/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:52,707 DEBUG generators.py generate l.373] (71/93) Reuse post-processing
[2024-03-04 11:44:52,709 INFO generators.py gen_for_qa l.548] (71/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:52,712 DEBUG generators.py gen_for_qa l.554] (71/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:52,713 DEBUG generators.py generate l.352] (71/93) Reuse existing Prompt
[2024-03-04 11:44:52,714 DEBUG generators.py generate l.365] (71/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:52,715 DEBUG generators.py generate l.373] (71/93) Reuse post-processing
[2024-03-04 11:44:52,716 INFO generators.py gen_for_qa l.548] (71/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:52,717 DEBUG generators.py gen_for_qa l.554] (71/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:52,718 DEBUG generators.py generate l.352] (71/93) Reuse existing Prompt
[2024-03-04 11:44:52,718 DEBUG generators.py generate l.365] (71/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:52,718 DEBUG generators.py generate l.373] (71/93) Reuse post-processing
[2024-03-04 11:44:52,718 INFO generators.py gen_for_qa l.548] (71/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:52,718 DEBUG generators.py gen_for_qa l.554] (71/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:52,724 DEBUG generators.py generate l.352] (71/93) Reuse existing Prompt
[2024-03-04 11:44:52,725 DEBUG generators.py generate l.365] (71/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:52,726 DEBUG generators.py generate l.373] (71/93) Reuse post-processing
[2024-03-04 11:44:52,728 INFO generators.py gen_for_qa l.548] (71/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:52,730 DEBUG generators.py gen_for_qa l.554] (71/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:52,731 DEBUG generators.py generate l.352] (71/93) Reuse existing Prompt
[2024-03-04 11:44:52,731 DEBUG generators.py generate l.365] (71/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:52,732 DEBUG generators.py generate l.373] (71/93) Reuse post-processing
[2024-03-04 11:44:52,733 INFO generators.py gen_for_qa l.548] (71/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:52,734 DEBUG generators.py generate l.349] (71/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:52,735 DEBUG generators.py generate l.358] (71/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:53,121 ERROR generators.py complete l.400] (71/93) The following exception occurred with prompt meta={} user=" Où a été inventé le premier moteur à combustion interne ?  A)  Paris B)  Francfort C)  Londres .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:53,142 DEBUG generators.py generate l.373] (71/93) Reuse post-processing
[2024-03-04 11:44:53,142 INFO generators.py generate l.477] (71/93) End question " Où a été inventé le premier moteur à combustion interne ?  A)  Paris B)  Francfort C)  Londres "
[2024-03-04 11:44:53,149 INFO generators.py generate l.475] (72/93) *** AnsGenerator for question " Où a été découvert le premier code de lois écrites ?  A)  Ur B)  Babylone C)  Ebla "
[2024-03-04 11:44:53,152 INFO generators.py gen_for_qa l.548] (72/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:53,154 DEBUG generators.py gen_for_qa l.554] (72/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:53,155 DEBUG generators.py generate l.352] (72/93) Reuse existing Prompt
[2024-03-04 11:44:53,157 DEBUG generators.py generate l.365] (72/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:53,158 DEBUG generators.py generate l.373] (72/93) Reuse post-processing
[2024-03-04 11:44:53,159 INFO generators.py gen_for_qa l.548] (72/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:53,162 DEBUG generators.py gen_for_qa l.554] (72/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:53,163 DEBUG generators.py generate l.352] (72/93) Reuse existing Prompt
[2024-03-04 11:44:53,164 DEBUG generators.py generate l.365] (72/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:53,165 DEBUG generators.py generate l.373] (72/93) Reuse post-processing
[2024-03-04 11:44:53,166 INFO generators.py gen_for_qa l.548] (72/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:53,168 DEBUG generators.py gen_for_qa l.554] (72/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:53,169 DEBUG generators.py generate l.352] (72/93) Reuse existing Prompt
[2024-03-04 11:44:53,170 DEBUG generators.py generate l.365] (72/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:53,171 DEBUG generators.py generate l.373] (72/93) Reuse post-processing
[2024-03-04 11:44:53,172 INFO generators.py gen_for_qa l.548] (72/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:53,173 DEBUG generators.py gen_for_qa l.554] (72/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:53,173 DEBUG generators.py generate l.352] (72/93) Reuse existing Prompt
[2024-03-04 11:44:53,176 DEBUG generators.py generate l.365] (72/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:53,176 DEBUG generators.py generate l.373] (72/93) Reuse post-processing
[2024-03-04 11:44:53,180 INFO generators.py gen_for_qa l.548] (72/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:53,180 DEBUG generators.py gen_for_qa l.554] (72/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:53,181 DEBUG generators.py generate l.352] (72/93) Reuse existing Prompt
[2024-03-04 11:44:53,182 DEBUG generators.py generate l.365] (72/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:53,184 DEBUG generators.py generate l.373] (72/93) Reuse post-processing
[2024-03-04 11:44:53,184 INFO generators.py gen_for_qa l.548] (72/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:53,184 DEBUG generators.py gen_for_qa l.554] (72/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:53,187 DEBUG generators.py generate l.352] (72/93) Reuse existing Prompt
[2024-03-04 11:44:53,187 DEBUG generators.py generate l.365] (72/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:53,187 DEBUG generators.py generate l.373] (72/93) Reuse post-processing
[2024-03-04 11:44:53,187 INFO generators.py gen_for_qa l.548] (72/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:53,190 DEBUG generators.py generate l.349] (72/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:53,191 DEBUG generators.py generate l.358] (72/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:53,592 ERROR generators.py complete l.400] (72/93) The following exception occurred with prompt meta={} user=" Où a été découvert le premier code de lois écrites ?  A)  Ur B)  Babylone C)  Ebla .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:53,611 DEBUG generators.py generate l.373] (72/93) Reuse post-processing
[2024-03-04 11:44:53,614 INFO generators.py generate l.477] (72/93) End question " Où a été découvert le premier code de lois écrites ?  A)  Ur B)  Babylone C)  Ebla "
[2024-03-04 11:44:53,616 INFO generators.py generate l.475] (73/93) *** AnsGenerator for question " Où a été inventé le premier télescope ?  A)  Middelburg B)  Gdansk C)  Florence "
[2024-03-04 11:44:53,618 INFO generators.py gen_for_qa l.548] (73/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:53,621 DEBUG generators.py gen_for_qa l.554] (73/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:53,622 DEBUG generators.py generate l.352] (73/93) Reuse existing Prompt
[2024-03-04 11:44:53,624 DEBUG generators.py generate l.365] (73/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:53,626 DEBUG generators.py generate l.373] (73/93) Reuse post-processing
[2024-03-04 11:44:53,627 INFO generators.py gen_for_qa l.548] (73/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:53,629 DEBUG generators.py gen_for_qa l.554] (73/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:53,631 DEBUG generators.py generate l.352] (73/93) Reuse existing Prompt
[2024-03-04 11:44:53,632 DEBUG generators.py generate l.365] (73/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:53,633 DEBUG generators.py generate l.373] (73/93) Reuse post-processing
[2024-03-04 11:44:53,635 INFO generators.py gen_for_qa l.548] (73/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:53,636 DEBUG generators.py gen_for_qa l.554] (73/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:53,637 DEBUG generators.py generate l.352] (73/93) Reuse existing Prompt
[2024-03-04 11:44:53,638 DEBUG generators.py generate l.365] (73/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:53,638 DEBUG generators.py generate l.373] (73/93) Reuse post-processing
[2024-03-04 11:44:53,638 INFO generators.py gen_for_qa l.548] (73/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:53,642 DEBUG generators.py gen_for_qa l.554] (73/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:53,644 DEBUG generators.py generate l.352] (73/93) Reuse existing Prompt
[2024-03-04 11:44:53,645 DEBUG generators.py generate l.365] (73/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:53,646 DEBUG generators.py generate l.373] (73/93) Reuse post-processing
[2024-03-04 11:44:53,646 INFO generators.py gen_for_qa l.548] (73/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:53,649 DEBUG generators.py gen_for_qa l.554] (73/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:53,649 DEBUG generators.py generate l.352] (73/93) Reuse existing Prompt
[2024-03-04 11:44:53,651 DEBUG generators.py generate l.365] (73/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:53,652 DEBUG generators.py generate l.373] (73/93) Reuse post-processing
[2024-03-04 11:44:53,653 INFO generators.py gen_for_qa l.548] (73/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:53,654 DEBUG generators.py gen_for_qa l.554] (73/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:53,655 DEBUG generators.py generate l.352] (73/93) Reuse existing Prompt
[2024-03-04 11:44:53,655 DEBUG generators.py generate l.365] (73/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:53,657 DEBUG generators.py generate l.373] (73/93) Reuse post-processing
[2024-03-04 11:44:53,657 INFO generators.py gen_for_qa l.548] (73/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:53,660 DEBUG generators.py generate l.349] (73/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:53,662 DEBUG generators.py generate l.358] (73/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:54,061 ERROR generators.py complete l.400] (73/93) The following exception occurred with prompt meta={} user=" Où a été inventé le premier télescope ?  A)  Middelburg B)  Gdansk C)  Florence .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:54,076 DEBUG generators.py generate l.373] (73/93) Reuse post-processing
[2024-03-04 11:44:54,080 INFO generators.py generate l.477] (73/93) End question " Où a été inventé le premier télescope ?  A)  Middelburg B)  Gdansk C)  Florence "
[2024-03-04 11:44:54,082 INFO generators.py generate l.475] (74/93) *** AnsGenerator for question " Où a été découvert le premier site préhistorique ?  A)  Vallée de la Vézère B)  Vallée du Nil C)  Vallée de l'Omo "
[2024-03-04 11:44:54,084 INFO generators.py gen_for_qa l.548] (74/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:54,086 DEBUG generators.py gen_for_qa l.554] (74/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:54,087 DEBUG generators.py generate l.352] (74/93) Reuse existing Prompt
[2024-03-04 11:44:54,088 DEBUG generators.py generate l.365] (74/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:54,090 DEBUG generators.py generate l.373] (74/93) Reuse post-processing
[2024-03-04 11:44:54,093 INFO generators.py gen_for_qa l.548] (74/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:54,094 DEBUG generators.py gen_for_qa l.554] (74/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:54,097 DEBUG generators.py generate l.352] (74/93) Reuse existing Prompt
[2024-03-04 11:44:54,097 DEBUG generators.py generate l.365] (74/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:54,099 DEBUG generators.py generate l.373] (74/93) Reuse post-processing
[2024-03-04 11:44:54,100 INFO generators.py gen_for_qa l.548] (74/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:54,101 DEBUG generators.py gen_for_qa l.554] (74/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:54,101 DEBUG generators.py generate l.352] (74/93) Reuse existing Prompt
[2024-03-04 11:44:54,101 DEBUG generators.py generate l.365] (74/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:54,104 DEBUG generators.py generate l.373] (74/93) Reuse post-processing
[2024-03-04 11:44:54,106 INFO generators.py gen_for_qa l.548] (74/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:54,107 DEBUG generators.py gen_for_qa l.554] (74/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:54,107 DEBUG generators.py generate l.352] (74/93) Reuse existing Prompt
[2024-03-04 11:44:54,109 DEBUG generators.py generate l.365] (74/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:54,111 DEBUG generators.py generate l.373] (74/93) Reuse post-processing
[2024-03-04 11:44:54,113 INFO generators.py gen_for_qa l.548] (74/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:54,113 DEBUG generators.py gen_for_qa l.554] (74/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:54,113 DEBUG generators.py generate l.352] (74/93) Reuse existing Prompt
[2024-03-04 11:44:54,116 DEBUG generators.py generate l.365] (74/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:54,116 DEBUG generators.py generate l.373] (74/93) Reuse post-processing
[2024-03-04 11:44:54,117 INFO generators.py gen_for_qa l.548] (74/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:54,119 DEBUG generators.py gen_for_qa l.554] (74/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:54,119 DEBUG generators.py generate l.352] (74/93) Reuse existing Prompt
[2024-03-04 11:44:54,120 DEBUG generators.py generate l.365] (74/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:54,121 DEBUG generators.py generate l.373] (74/93) Reuse post-processing
[2024-03-04 11:44:54,122 INFO generators.py gen_for_qa l.548] (74/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:54,122 DEBUG generators.py generate l.349] (74/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:54,122 DEBUG generators.py generate l.358] (74/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:54,516 ERROR generators.py complete l.400] (74/93) The following exception occurred with prompt meta={} user=" Où a été découvert le premier site préhistorique ?  A)  Vallée de la Vézère B)  Vallée du Nil C)  Vallée de l'Omo .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:54,523 DEBUG generators.py generate l.373] (74/93) Reuse post-processing
[2024-03-04 11:44:54,526 INFO generators.py generate l.477] (74/93) End question " Où a été découvert le premier site préhistorique ?  A)  Vallée de la Vézère B)  Vallée du Nil C)  Vallée de l'Omo "
[2024-03-04 11:44:54,527 INFO generators.py generate l.475] (75/93) *** AnsGenerator for question " Où a été inventé le premier appareil photographique ?  A)  Paris B)  Lacock C)  Nice "
[2024-03-04 11:44:54,529 INFO generators.py gen_for_qa l.548] (75/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:54,530 DEBUG generators.py gen_for_qa l.554] (75/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:54,533 DEBUG generators.py generate l.352] (75/93) Reuse existing Prompt
[2024-03-04 11:44:54,534 DEBUG generators.py generate l.365] (75/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:54,536 DEBUG generators.py generate l.373] (75/93) Reuse post-processing
[2024-03-04 11:44:54,536 INFO generators.py gen_for_qa l.548] (75/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:54,538 DEBUG generators.py gen_for_qa l.554] (75/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:54,538 DEBUG generators.py generate l.352] (75/93) Reuse existing Prompt
[2024-03-04 11:44:54,540 DEBUG generators.py generate l.365] (75/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:54,543 DEBUG generators.py generate l.373] (75/93) Reuse post-processing
[2024-03-04 11:44:54,544 INFO generators.py gen_for_qa l.548] (75/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:54,545 DEBUG generators.py gen_for_qa l.554] (75/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:54,548 DEBUG generators.py generate l.352] (75/93) Reuse existing Prompt
[2024-03-04 11:44:54,549 DEBUG generators.py generate l.365] (75/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:54,550 DEBUG generators.py generate l.373] (75/93) Reuse post-processing
[2024-03-04 11:44:54,552 INFO generators.py gen_for_qa l.548] (75/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:54,552 DEBUG generators.py gen_for_qa l.554] (75/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:54,553 DEBUG generators.py generate l.352] (75/93) Reuse existing Prompt
[2024-03-04 11:44:54,553 DEBUG generators.py generate l.365] (75/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:54,553 DEBUG generators.py generate l.373] (75/93) Reuse post-processing
[2024-03-04 11:44:54,557 INFO generators.py gen_for_qa l.548] (75/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:54,557 DEBUG generators.py gen_for_qa l.554] (75/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:54,559 DEBUG generators.py generate l.352] (75/93) Reuse existing Prompt
[2024-03-04 11:44:54,560 DEBUG generators.py generate l.365] (75/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:54,561 DEBUG generators.py generate l.373] (75/93) Reuse post-processing
[2024-03-04 11:44:54,563 INFO generators.py gen_for_qa l.548] (75/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:54,565 DEBUG generators.py gen_for_qa l.554] (75/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:54,566 DEBUG generators.py generate l.352] (75/93) Reuse existing Prompt
[2024-03-04 11:44:54,567 DEBUG generators.py generate l.365] (75/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:54,568 DEBUG generators.py generate l.373] (75/93) Reuse post-processing
[2024-03-04 11:44:54,569 INFO generators.py gen_for_qa l.548] (75/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:54,570 DEBUG generators.py generate l.349] (75/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:54,571 DEBUG generators.py generate l.358] (75/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:54,964 ERROR generators.py complete l.400] (75/93) The following exception occurred with prompt meta={} user=" Où a été inventé le premier appareil photographique ?  A)  Paris B)  Lacock C)  Nice .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:54,982 DEBUG generators.py generate l.373] (75/93) Reuse post-processing
[2024-03-04 11:44:54,983 INFO generators.py generate l.477] (75/93) End question " Où a été inventé le premier appareil photographique ?  A)  Paris B)  Lacock C)  Nice "
[2024-03-04 11:44:54,983 INFO generators.py generate l.475] (76/93) *** AnsGenerator for question " Où a été découvert le premier gisement de charbon ?  A)  Newcastle upon Tyne B)  Liège C)  Essen "
[2024-03-04 11:44:54,983 INFO generators.py gen_for_qa l.548] (76/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:54,992 DEBUG generators.py gen_for_qa l.554] (76/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:54,994 DEBUG generators.py generate l.352] (76/93) Reuse existing Prompt
[2024-03-04 11:44:54,994 DEBUG generators.py generate l.365] (76/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:54,999 DEBUG generators.py generate l.373] (76/93) Reuse post-processing
[2024-03-04 11:44:55,000 INFO generators.py gen_for_qa l.548] (76/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:55,001 DEBUG generators.py gen_for_qa l.554] (76/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:55,002 DEBUG generators.py generate l.352] (76/93) Reuse existing Prompt
[2024-03-04 11:44:55,005 DEBUG generators.py generate l.365] (76/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:55,005 DEBUG generators.py generate l.373] (76/93) Reuse post-processing
[2024-03-04 11:44:55,008 INFO generators.py gen_for_qa l.548] (76/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:55,009 DEBUG generators.py gen_for_qa l.554] (76/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:55,011 DEBUG generators.py generate l.352] (76/93) Reuse existing Prompt
[2024-03-04 11:44:55,013 DEBUG generators.py generate l.365] (76/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:55,014 DEBUG generators.py generate l.373] (76/93) Reuse post-processing
[2024-03-04 11:44:55,016 INFO generators.py gen_for_qa l.548] (76/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:55,017 DEBUG generators.py gen_for_qa l.554] (76/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:55,018 DEBUG generators.py generate l.352] (76/93) Reuse existing Prompt
[2024-03-04 11:44:55,019 DEBUG generators.py generate l.365] (76/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:55,020 DEBUG generators.py generate l.373] (76/93) Reuse post-processing
[2024-03-04 11:44:55,021 INFO generators.py gen_for_qa l.548] (76/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:55,023 DEBUG generators.py gen_for_qa l.554] (76/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:55,024 DEBUG generators.py generate l.352] (76/93) Reuse existing Prompt
[2024-03-04 11:44:55,025 DEBUG generators.py generate l.365] (76/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:55,025 DEBUG generators.py generate l.373] (76/93) Reuse post-processing
[2024-03-04 11:44:55,027 INFO generators.py gen_for_qa l.548] (76/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:55,027 DEBUG generators.py gen_for_qa l.554] (76/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:55,030 DEBUG generators.py generate l.352] (76/93) Reuse existing Prompt
[2024-03-04 11:44:55,030 DEBUG generators.py generate l.365] (76/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:55,033 DEBUG generators.py generate l.373] (76/93) Reuse post-processing
[2024-03-04 11:44:55,034 INFO generators.py gen_for_qa l.548] (76/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:55,034 DEBUG generators.py generate l.349] (76/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:55,035 DEBUG generators.py generate l.358] (76/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:55,455 ERROR generators.py complete l.400] (76/93) The following exception occurred with prompt meta={} user=" Où a été découvert le premier gisement de charbon ?  A)  Newcastle upon Tyne B)  Liège C)  Essen .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:55,464 DEBUG generators.py generate l.373] (76/93) Reuse post-processing
[2024-03-04 11:44:55,464 INFO generators.py generate l.477] (76/93) End question " Où a été découvert le premier gisement de charbon ?  A)  Newcastle upon Tyne B)  Liège C)  Essen "
[2024-03-04 11:44:55,465 INFO generators.py generate l.475] (77/93) *** AnsGenerator for question " Où a été inventé le premier bateau à vapeur ?  A)  Paris B)  Glasgow C)  Philadelphie "
[2024-03-04 11:44:55,467 INFO generators.py gen_for_qa l.548] (77/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:55,469 DEBUG generators.py gen_for_qa l.554] (77/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:55,470 DEBUG generators.py generate l.352] (77/93) Reuse existing Prompt
[2024-03-04 11:44:55,473 DEBUG generators.py generate l.365] (77/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:55,474 DEBUG generators.py generate l.373] (77/93) Reuse post-processing
[2024-03-04 11:44:55,475 INFO generators.py gen_for_qa l.548] (77/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:55,475 DEBUG generators.py gen_for_qa l.554] (77/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:55,477 DEBUG generators.py generate l.352] (77/93) Reuse existing Prompt
[2024-03-04 11:44:55,478 DEBUG generators.py generate l.365] (77/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:55,483 DEBUG generators.py generate l.373] (77/93) Reuse post-processing
[2024-03-04 11:44:55,484 INFO generators.py gen_for_qa l.548] (77/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:55,485 DEBUG generators.py gen_for_qa l.554] (77/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:55,485 DEBUG generators.py generate l.352] (77/93) Reuse existing Prompt
[2024-03-04 11:44:55,485 DEBUG generators.py generate l.365] (77/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:55,487 DEBUG generators.py generate l.373] (77/93) Reuse post-processing
[2024-03-04 11:44:55,487 INFO generators.py gen_for_qa l.548] (77/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:55,490 DEBUG generators.py gen_for_qa l.554] (77/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:55,491 DEBUG generators.py generate l.352] (77/93) Reuse existing Prompt
[2024-03-04 11:44:55,492 DEBUG generators.py generate l.365] (77/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:55,493 DEBUG generators.py generate l.373] (77/93) Reuse post-processing
[2024-03-04 11:44:55,494 INFO generators.py gen_for_qa l.548] (77/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:55,495 DEBUG generators.py gen_for_qa l.554] (77/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:55,496 DEBUG generators.py generate l.352] (77/93) Reuse existing Prompt
[2024-03-04 11:44:55,498 DEBUG generators.py generate l.365] (77/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:55,501 DEBUG generators.py generate l.373] (77/93) Reuse post-processing
[2024-03-04 11:44:55,502 INFO generators.py gen_for_qa l.548] (77/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:55,502 DEBUG generators.py gen_for_qa l.554] (77/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:55,503 DEBUG generators.py generate l.352] (77/93) Reuse existing Prompt
[2024-03-04 11:44:55,504 DEBUG generators.py generate l.365] (77/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:55,505 DEBUG generators.py generate l.373] (77/93) Reuse post-processing
[2024-03-04 11:44:55,506 INFO generators.py gen_for_qa l.548] (77/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:55,506 DEBUG generators.py generate l.349] (77/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:55,508 DEBUG generators.py generate l.358] (77/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:55,917 ERROR generators.py complete l.400] (77/93) The following exception occurred with prompt meta={} user=" Où a été inventé le premier bateau à vapeur ?  A)  Paris B)  Glasgow C)  Philadelphie .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:55,925 DEBUG generators.py generate l.373] (77/93) Reuse post-processing
[2024-03-04 11:44:55,928 INFO generators.py generate l.477] (77/93) End question " Où a été inventé le premier bateau à vapeur ?  A)  Paris B)  Glasgow C)  Philadelphie "
[2024-03-04 11:44:55,929 INFO generators.py generate l.475] (78/93) *** AnsGenerator for question " Où a été découvert le premier système d'écriture ?  A)  Uruk B)  Sumer C)  Susa "
[2024-03-04 11:44:55,932 INFO generators.py gen_for_qa l.548] (78/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:55,935 DEBUG generators.py gen_for_qa l.554] (78/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:55,937 DEBUG generators.py generate l.352] (78/93) Reuse existing Prompt
[2024-03-04 11:44:55,938 DEBUG generators.py generate l.365] (78/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:55,938 DEBUG generators.py generate l.373] (78/93) Reuse post-processing
[2024-03-04 11:44:55,939 INFO generators.py gen_for_qa l.548] (78/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:55,941 DEBUG generators.py gen_for_qa l.554] (78/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:55,943 DEBUG generators.py generate l.352] (78/93) Reuse existing Prompt
[2024-03-04 11:44:55,945 DEBUG generators.py generate l.365] (78/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:55,946 DEBUG generators.py generate l.373] (78/93) Reuse post-processing
[2024-03-04 11:44:55,948 INFO generators.py gen_for_qa l.548] (78/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:55,949 DEBUG generators.py gen_for_qa l.554] (78/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:55,952 DEBUG generators.py generate l.352] (78/93) Reuse existing Prompt
[2024-03-04 11:44:55,953 DEBUG generators.py generate l.365] (78/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:55,954 DEBUG generators.py generate l.373] (78/93) Reuse post-processing
[2024-03-04 11:44:55,955 INFO generators.py gen_for_qa l.548] (78/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:55,956 DEBUG generators.py gen_for_qa l.554] (78/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:55,957 DEBUG generators.py generate l.352] (78/93) Reuse existing Prompt
[2024-03-04 11:44:55,958 DEBUG generators.py generate l.365] (78/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:55,959 DEBUG generators.py generate l.373] (78/93) Reuse post-processing
[2024-03-04 11:44:55,960 INFO generators.py gen_for_qa l.548] (78/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:55,961 DEBUG generators.py gen_for_qa l.554] (78/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:55,963 DEBUG generators.py generate l.352] (78/93) Reuse existing Prompt
[2024-03-04 11:44:55,967 DEBUG generators.py generate l.365] (78/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:55,968 DEBUG generators.py generate l.373] (78/93) Reuse post-processing
[2024-03-04 11:44:55,969 INFO generators.py gen_for_qa l.548] (78/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:55,970 DEBUG generators.py gen_for_qa l.554] (78/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:55,972 DEBUG generators.py generate l.352] (78/93) Reuse existing Prompt
[2024-03-04 11:44:55,973 DEBUG generators.py generate l.365] (78/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:55,973 DEBUG generators.py generate l.373] (78/93) Reuse post-processing
[2024-03-04 11:44:55,975 INFO generators.py gen_for_qa l.548] (78/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:55,976 DEBUG generators.py generate l.349] (78/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:55,978 DEBUG generators.py generate l.358] (78/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:56,377 ERROR generators.py complete l.400] (78/93) The following exception occurred with prompt meta={} user=" Où a été découvert le premier système d'écriture ?  A)  Uruk B)  Sumer C)  Susa .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:56,390 DEBUG generators.py generate l.373] (78/93) Reuse post-processing
[2024-03-04 11:44:56,393 INFO generators.py generate l.477] (78/93) End question " Où a été découvert le premier système d'écriture ?  A)  Uruk B)  Sumer C)  Susa "
[2024-03-04 11:44:56,397 INFO generators.py generate l.475] (79/93) *** AnsGenerator for question " Où a été inventé le premier réseau ferroviaire ?  A)  Stockton et Darlington B)  Liverpool et Manchester C)  Baltimore et Ohio "
[2024-03-04 11:44:56,400 INFO generators.py gen_for_qa l.548] (79/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:56,402 DEBUG generators.py gen_for_qa l.554] (79/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:56,403 DEBUG generators.py generate l.352] (79/93) Reuse existing Prompt
[2024-03-04 11:44:56,405 DEBUG generators.py generate l.365] (79/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:56,407 DEBUG generators.py generate l.373] (79/93) Reuse post-processing
[2024-03-04 11:44:56,410 INFO generators.py gen_for_qa l.548] (79/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:56,410 DEBUG generators.py gen_for_qa l.554] (79/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:56,412 DEBUG generators.py generate l.352] (79/93) Reuse existing Prompt
[2024-03-04 11:44:56,415 DEBUG generators.py generate l.365] (79/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:56,416 DEBUG generators.py generate l.373] (79/93) Reuse post-processing
[2024-03-04 11:44:56,417 INFO generators.py gen_for_qa l.548] (79/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:56,418 DEBUG generators.py gen_for_qa l.554] (79/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:56,419 DEBUG generators.py generate l.352] (79/93) Reuse existing Prompt
[2024-03-04 11:44:56,419 DEBUG generators.py generate l.365] (79/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:56,419 DEBUG generators.py generate l.373] (79/93) Reuse post-processing
[2024-03-04 11:44:56,419 INFO generators.py gen_for_qa l.548] (79/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:56,424 DEBUG generators.py gen_for_qa l.554] (79/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:56,425 DEBUG generators.py generate l.352] (79/93) Reuse existing Prompt
[2024-03-04 11:44:56,427 DEBUG generators.py generate l.365] (79/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:56,427 DEBUG generators.py generate l.373] (79/93) Reuse post-processing
[2024-03-04 11:44:56,431 INFO generators.py gen_for_qa l.548] (79/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:56,432 DEBUG generators.py gen_for_qa l.554] (79/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:56,433 DEBUG generators.py generate l.352] (79/93) Reuse existing Prompt
[2024-03-04 11:44:56,434 DEBUG generators.py generate l.365] (79/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:56,435 DEBUG generators.py generate l.373] (79/93) Reuse post-processing
[2024-03-04 11:44:56,436 INFO generators.py gen_for_qa l.548] (79/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:56,437 DEBUG generators.py gen_for_qa l.554] (79/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:56,438 DEBUG generators.py generate l.352] (79/93) Reuse existing Prompt
[2024-03-04 11:44:56,438 DEBUG generators.py generate l.365] (79/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:56,439 DEBUG generators.py generate l.373] (79/93) Reuse post-processing
[2024-03-04 11:44:56,443 INFO generators.py gen_for_qa l.548] (79/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:56,443 DEBUG generators.py generate l.349] (79/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:56,447 DEBUG generators.py generate l.358] (79/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:56,835 ERROR generators.py complete l.400] (79/93) The following exception occurred with prompt meta={} user=" Où a été inventé le premier réseau ferroviaire ?  A)  Stockton et Darlington B)  Liverpool et Manchester C)  Baltimore et Ohio .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:56,844 DEBUG generators.py generate l.373] (79/93) Reuse post-processing
[2024-03-04 11:44:56,846 INFO generators.py generate l.477] (79/93) End question " Où a été inventé le premier réseau ferroviaire ?  A)  Stockton et Darlington B)  Liverpool et Manchester C)  Baltimore et Ohio "
[2024-03-04 11:44:56,848 INFO generators.py generate l.475] (80/93) *** AnsGenerator for question " Qui a découvert l'Amérique (pour les Européens) ?  A)  Christophe Colomb B)  Leif Erikson C)  Amerigo Vespucci "
[2024-03-04 11:44:56,851 INFO generators.py gen_for_qa l.548] (80/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:56,854 DEBUG generators.py gen_for_qa l.554] (80/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:56,855 DEBUG generators.py generate l.352] (80/93) Reuse existing Prompt
[2024-03-04 11:44:56,856 DEBUG generators.py generate l.365] (80/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:56,858 DEBUG generators.py generate l.373] (80/93) Reuse post-processing
[2024-03-04 11:44:56,859 INFO generators.py gen_for_qa l.548] (80/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:56,861 DEBUG generators.py gen_for_qa l.554] (80/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:56,863 DEBUG generators.py generate l.352] (80/93) Reuse existing Prompt
[2024-03-04 11:44:56,865 DEBUG generators.py generate l.365] (80/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:56,867 DEBUG generators.py generate l.373] (80/93) Reuse post-processing
[2024-03-04 11:44:56,868 INFO generators.py gen_for_qa l.548] (80/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:56,868 DEBUG generators.py gen_for_qa l.554] (80/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:56,871 DEBUG generators.py generate l.352] (80/93) Reuse existing Prompt
[2024-03-04 11:44:56,871 DEBUG generators.py generate l.365] (80/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:56,871 DEBUG generators.py generate l.373] (80/93) Reuse post-processing
[2024-03-04 11:44:56,874 INFO generators.py gen_for_qa l.548] (80/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:56,875 DEBUG generators.py gen_for_qa l.554] (80/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:56,876 DEBUG generators.py generate l.352] (80/93) Reuse existing Prompt
[2024-03-04 11:44:56,878 DEBUG generators.py generate l.365] (80/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:56,880 DEBUG generators.py generate l.373] (80/93) Reuse post-processing
[2024-03-04 11:44:56,881 INFO generators.py gen_for_qa l.548] (80/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:56,884 DEBUG generators.py gen_for_qa l.554] (80/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:56,885 DEBUG generators.py generate l.352] (80/93) Reuse existing Prompt
[2024-03-04 11:44:56,885 DEBUG generators.py generate l.365] (80/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:56,885 DEBUG generators.py generate l.373] (80/93) Reuse post-processing
[2024-03-04 11:44:56,885 INFO generators.py gen_for_qa l.548] (80/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:56,885 DEBUG generators.py gen_for_qa l.554] (80/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:56,890 DEBUG generators.py generate l.352] (80/93) Reuse existing Prompt
[2024-03-04 11:44:56,891 DEBUG generators.py generate l.365] (80/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:56,892 DEBUG generators.py generate l.373] (80/93) Reuse post-processing
[2024-03-04 11:44:56,894 INFO generators.py gen_for_qa l.548] (80/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:56,895 DEBUG generators.py generate l.349] (80/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:56,896 DEBUG generators.py generate l.358] (80/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:57,293 ERROR generators.py complete l.400] (80/93) The following exception occurred with prompt meta={} user=" Qui a découvert l'Amérique (pour les Européens) ?  A)  Christophe Colomb B)  Leif Erikson C)  Amerigo Vespucci .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:57,312 DEBUG generators.py generate l.373] (80/93) Reuse post-processing
[2024-03-04 11:44:57,316 INFO generators.py generate l.477] (80/93) End question " Qui a découvert l'Amérique (pour les Européens) ?  A)  Christophe Colomb B)  Leif Erikson C)  Amerigo Vespucci "
[2024-03-04 11:44:57,324 INFO generators.py generate l.475] (81/93) *** AnsGenerator for question " Qui a découvert l'Australie (pour les Européens) ?  A)  James Cook B)  Willem Janszoon C)  Abel Tasman "
[2024-03-04 11:44:57,329 INFO generators.py gen_for_qa l.548] (81/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:57,335 DEBUG generators.py gen_for_qa l.554] (81/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:57,335 DEBUG generators.py generate l.352] (81/93) Reuse existing Prompt
[2024-03-04 11:44:57,343 DEBUG generators.py generate l.365] (81/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:57,349 DEBUG generators.py generate l.373] (81/93) Reuse post-processing
[2024-03-04 11:44:57,349 INFO generators.py gen_for_qa l.548] (81/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:57,357 DEBUG generators.py gen_for_qa l.554] (81/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:57,359 DEBUG generators.py generate l.352] (81/93) Reuse existing Prompt
[2024-03-04 11:44:57,363 DEBUG generators.py generate l.365] (81/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:57,364 DEBUG generators.py generate l.373] (81/93) Reuse post-processing
[2024-03-04 11:44:57,364 INFO generators.py gen_for_qa l.548] (81/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:57,364 DEBUG generators.py gen_for_qa l.554] (81/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:57,364 DEBUG generators.py generate l.352] (81/93) Reuse existing Prompt
[2024-03-04 11:44:57,364 DEBUG generators.py generate l.365] (81/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:57,364 DEBUG generators.py generate l.373] (81/93) Reuse post-processing
[2024-03-04 11:44:57,374 INFO generators.py gen_for_qa l.548] (81/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:57,375 DEBUG generators.py gen_for_qa l.554] (81/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:57,376 DEBUG generators.py generate l.352] (81/93) Reuse existing Prompt
[2024-03-04 11:44:57,378 DEBUG generators.py generate l.365] (81/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:57,379 DEBUG generators.py generate l.373] (81/93) Reuse post-processing
[2024-03-04 11:44:57,381 INFO generators.py gen_for_qa l.548] (81/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:57,382 DEBUG generators.py gen_for_qa l.554] (81/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:57,383 DEBUG generators.py generate l.352] (81/93) Reuse existing Prompt
[2024-03-04 11:44:57,384 DEBUG generators.py generate l.365] (81/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:57,386 DEBUG generators.py generate l.373] (81/93) Reuse post-processing
[2024-03-04 11:44:57,387 INFO generators.py gen_for_qa l.548] (81/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:57,388 DEBUG generators.py gen_for_qa l.554] (81/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:57,389 DEBUG generators.py generate l.352] (81/93) Reuse existing Prompt
[2024-03-04 11:44:57,391 DEBUG generators.py generate l.365] (81/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:57,392 DEBUG generators.py generate l.373] (81/93) Reuse post-processing
[2024-03-04 11:44:57,393 INFO generators.py gen_for_qa l.548] (81/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:57,394 DEBUG generators.py generate l.349] (81/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:57,396 DEBUG generators.py generate l.358] (81/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:57,775 ERROR generators.py complete l.400] (81/93) The following exception occurred with prompt meta={} user=" Qui a découvert l'Australie (pour les Européens) ?  A)  James Cook B)  Willem Janszoon C)  Abel Tasman .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:57,802 DEBUG generators.py generate l.373] (81/93) Reuse post-processing
[2024-03-04 11:44:57,809 INFO generators.py generate l.477] (81/93) End question " Qui a découvert l'Australie (pour les Européens) ?  A)  James Cook B)  Willem Janszoon C)  Abel Tasman "
[2024-03-04 11:44:57,811 INFO generators.py generate l.475] (82/93) *** AnsGenerator for question " Qui a découvert l'Antarctique ?  A)  Fabian von Bellingshausen B)  Edward Bransfield C)  Nathaniel Palmer "
[2024-03-04 11:44:57,813 INFO generators.py gen_for_qa l.548] (82/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:57,815 DEBUG generators.py gen_for_qa l.554] (82/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:57,817 DEBUG generators.py generate l.352] (82/93) Reuse existing Prompt
[2024-03-04 11:44:57,818 DEBUG generators.py generate l.365] (82/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:57,820 DEBUG generators.py generate l.373] (82/93) Reuse post-processing
[2024-03-04 11:44:57,822 INFO generators.py gen_for_qa l.548] (82/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:57,822 DEBUG generators.py gen_for_qa l.554] (82/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:57,825 DEBUG generators.py generate l.352] (82/93) Reuse existing Prompt
[2024-03-04 11:44:57,827 DEBUG generators.py generate l.365] (82/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:57,828 DEBUG generators.py generate l.373] (82/93) Reuse post-processing
[2024-03-04 11:44:57,830 INFO generators.py gen_for_qa l.548] (82/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:57,832 DEBUG generators.py gen_for_qa l.554] (82/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:57,833 DEBUG generators.py generate l.352] (82/93) Reuse existing Prompt
[2024-03-04 11:44:57,834 DEBUG generators.py generate l.365] (82/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:57,835 DEBUG generators.py generate l.373] (82/93) Reuse post-processing
[2024-03-04 11:44:57,836 INFO generators.py gen_for_qa l.548] (82/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:57,836 DEBUG generators.py gen_for_qa l.554] (82/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:57,839 DEBUG generators.py generate l.352] (82/93) Reuse existing Prompt
[2024-03-04 11:44:57,840 DEBUG generators.py generate l.365] (82/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:57,841 DEBUG generators.py generate l.373] (82/93) Reuse post-processing
[2024-03-04 11:44:57,842 INFO generators.py gen_for_qa l.548] (82/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:57,843 DEBUG generators.py gen_for_qa l.554] (82/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:57,845 DEBUG generators.py generate l.352] (82/93) Reuse existing Prompt
[2024-03-04 11:44:57,847 DEBUG generators.py generate l.365] (82/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:57,848 DEBUG generators.py generate l.373] (82/93) Reuse post-processing
[2024-03-04 11:44:57,850 INFO generators.py gen_for_qa l.548] (82/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:57,850 DEBUG generators.py gen_for_qa l.554] (82/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:57,851 DEBUG generators.py generate l.352] (82/93) Reuse existing Prompt
[2024-03-04 11:44:57,852 DEBUG generators.py generate l.365] (82/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:57,854 DEBUG generators.py generate l.373] (82/93) Reuse post-processing
[2024-03-04 11:44:57,855 INFO generators.py gen_for_qa l.548] (82/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:57,856 DEBUG generators.py generate l.349] (82/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:57,857 DEBUG generators.py generate l.358] (82/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:58,251 ERROR generators.py complete l.400] (82/93) The following exception occurred with prompt meta={} user=" Qui a découvert l'Antarctique ?  A)  Fabian von Bellingshausen B)  Edward Bransfield C)  Nathaniel Palmer .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:58,268 DEBUG generators.py generate l.373] (82/93) Reuse post-processing
[2024-03-04 11:44:58,270 INFO generators.py generate l.477] (82/93) End question " Qui a découvert l'Antarctique ?  A)  Fabian von Bellingshausen B)  Edward Bransfield C)  Nathaniel Palmer "
[2024-03-04 11:44:58,276 INFO generators.py generate l.475] (83/93) *** AnsGenerator for question " Qui a découvert le Canada (pour les Européens) ?  A)  John Cabot B)  Jacques Cartier C)  Giovanni Verrazzano "
[2024-03-04 11:44:58,278 INFO generators.py gen_for_qa l.548] (83/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:58,279 DEBUG generators.py gen_for_qa l.554] (83/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:58,281 DEBUG generators.py generate l.352] (83/93) Reuse existing Prompt
[2024-03-04 11:44:58,283 DEBUG generators.py generate l.365] (83/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:58,284 DEBUG generators.py generate l.373] (83/93) Reuse post-processing
[2024-03-04 11:44:58,287 INFO generators.py gen_for_qa l.548] (83/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:58,288 DEBUG generators.py gen_for_qa l.554] (83/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:58,289 DEBUG generators.py generate l.352] (83/93) Reuse existing Prompt
[2024-03-04 11:44:58,291 DEBUG generators.py generate l.365] (83/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:58,292 DEBUG generators.py generate l.373] (83/93) Reuse post-processing
[2024-03-04 11:44:58,294 INFO generators.py gen_for_qa l.548] (83/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:58,295 DEBUG generators.py gen_for_qa l.554] (83/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:58,297 DEBUG generators.py generate l.352] (83/93) Reuse existing Prompt
[2024-03-04 11:44:58,298 DEBUG generators.py generate l.365] (83/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:58,299 DEBUG generators.py generate l.373] (83/93) Reuse post-processing
[2024-03-04 11:44:58,300 INFO generators.py gen_for_qa l.548] (83/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:58,301 DEBUG generators.py gen_for_qa l.554] (83/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:58,303 DEBUG generators.py generate l.352] (83/93) Reuse existing Prompt
[2024-03-04 11:44:58,303 DEBUG generators.py generate l.365] (83/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:58,305 DEBUG generators.py generate l.373] (83/93) Reuse post-processing
[2024-03-04 11:44:58,306 INFO generators.py gen_for_qa l.548] (83/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:58,308 DEBUG generators.py gen_for_qa l.554] (83/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:58,309 DEBUG generators.py generate l.352] (83/93) Reuse existing Prompt
[2024-03-04 11:44:58,311 DEBUG generators.py generate l.365] (83/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:58,312 DEBUG generators.py generate l.373] (83/93) Reuse post-processing
[2024-03-04 11:44:58,313 INFO generators.py gen_for_qa l.548] (83/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:58,315 DEBUG generators.py gen_for_qa l.554] (83/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:58,316 DEBUG generators.py generate l.352] (83/93) Reuse existing Prompt
[2024-03-04 11:44:58,317 DEBUG generators.py generate l.365] (83/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:58,318 DEBUG generators.py generate l.373] (83/93) Reuse post-processing
[2024-03-04 11:44:58,318 INFO generators.py gen_for_qa l.548] (83/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:58,320 DEBUG generators.py generate l.349] (83/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:58,322 DEBUG generators.py generate l.358] (83/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:58,700 ERROR generators.py complete l.400] (83/93) The following exception occurred with prompt meta={} user=" Qui a découvert le Canada (pour les Européens) ?  A)  John Cabot B)  Jacques Cartier C)  Giovanni Verrazzano .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:58,718 DEBUG generators.py generate l.373] (83/93) Reuse post-processing
[2024-03-04 11:44:58,721 INFO generators.py generate l.477] (83/93) End question " Qui a découvert le Canada (pour les Européens) ?  A)  John Cabot B)  Jacques Cartier C)  Giovanni Verrazzano "
[2024-03-04 11:44:58,725 INFO generators.py generate l.475] (84/93) *** AnsGenerator for question " Qui a découvert les îles Galápagos ?  A)  Tomás de Berlanga B)  Francis Drake "
[2024-03-04 11:44:58,727 INFO generators.py gen_for_qa l.548] (84/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:58,730 DEBUG generators.py gen_for_qa l.554] (84/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:58,733 DEBUG generators.py generate l.352] (84/93) Reuse existing Prompt
[2024-03-04 11:44:58,734 DEBUG generators.py generate l.365] (84/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:58,736 DEBUG generators.py generate l.373] (84/93) Reuse post-processing
[2024-03-04 11:44:58,738 INFO generators.py gen_for_qa l.548] (84/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:58,739 DEBUG generators.py gen_for_qa l.554] (84/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:58,741 DEBUG generators.py generate l.352] (84/93) Reuse existing Prompt
[2024-03-04 11:44:58,744 DEBUG generators.py generate l.365] (84/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:58,745 DEBUG generators.py generate l.373] (84/93) Reuse post-processing
[2024-03-04 11:44:58,747 INFO generators.py gen_for_qa l.548] (84/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:58,748 DEBUG generators.py gen_for_qa l.554] (84/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:58,750 DEBUG generators.py generate l.352] (84/93) Reuse existing Prompt
[2024-03-04 11:44:58,750 DEBUG generators.py generate l.365] (84/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:58,751 DEBUG generators.py generate l.373] (84/93) Reuse post-processing
[2024-03-04 11:44:58,752 INFO generators.py gen_for_qa l.548] (84/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:58,754 DEBUG generators.py gen_for_qa l.554] (84/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:58,755 DEBUG generators.py generate l.352] (84/93) Reuse existing Prompt
[2024-03-04 11:44:58,756 DEBUG generators.py generate l.365] (84/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:58,757 DEBUG generators.py generate l.373] (84/93) Reuse post-processing
[2024-03-04 11:44:58,760 INFO generators.py gen_for_qa l.548] (84/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:58,761 DEBUG generators.py gen_for_qa l.554] (84/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:58,762 DEBUG generators.py generate l.352] (84/93) Reuse existing Prompt
[2024-03-04 11:44:58,764 DEBUG generators.py generate l.365] (84/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:58,765 DEBUG generators.py generate l.373] (84/93) Reuse post-processing
[2024-03-04 11:44:58,766 INFO generators.py gen_for_qa l.548] (84/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:58,767 DEBUG generators.py gen_for_qa l.554] (84/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:58,768 DEBUG generators.py generate l.352] (84/93) Reuse existing Prompt
[2024-03-04 11:44:58,769 DEBUG generators.py generate l.365] (84/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:58,770 DEBUG generators.py generate l.373] (84/93) Reuse post-processing
[2024-03-04 11:44:58,771 INFO generators.py gen_for_qa l.548] (84/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:58,772 DEBUG generators.py generate l.349] (84/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:58,772 DEBUG generators.py generate l.358] (84/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:59,173 ERROR generators.py complete l.400] (84/93) The following exception occurred with prompt meta={} user=" Qui a découvert les îles Galápagos ?  A)  Tomás de Berlanga B)  Francis Drake .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:59,180 DEBUG generators.py generate l.373] (84/93) Reuse post-processing
[2024-03-04 11:44:59,182 INFO generators.py generate l.477] (84/93) End question " Qui a découvert les îles Galápagos ?  A)  Tomás de Berlanga B)  Francis Drake "
[2024-03-04 11:44:59,184 INFO generators.py generate l.475] (85/93) *** AnsGenerator for question " Qui a découvert l'Amérique centrale (pour les Européens) ?  A)  Christophe Colomb B)  Vasco Núñez de Balboa "
[2024-03-04 11:44:59,186 INFO generators.py gen_for_qa l.548] (85/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:59,188 DEBUG generators.py gen_for_qa l.554] (85/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:59,190 DEBUG generators.py generate l.352] (85/93) Reuse existing Prompt
[2024-03-04 11:44:59,191 DEBUG generators.py generate l.365] (85/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:59,192 DEBUG generators.py generate l.373] (85/93) Reuse post-processing
[2024-03-04 11:44:59,193 INFO generators.py gen_for_qa l.548] (85/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:59,195 DEBUG generators.py gen_for_qa l.554] (85/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:59,197 DEBUG generators.py generate l.352] (85/93) Reuse existing Prompt
[2024-03-04 11:44:59,199 DEBUG generators.py generate l.365] (85/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:59,200 DEBUG generators.py generate l.373] (85/93) Reuse post-processing
[2024-03-04 11:44:59,201 INFO generators.py gen_for_qa l.548] (85/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:59,201 DEBUG generators.py gen_for_qa l.554] (85/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:59,203 DEBUG generators.py generate l.352] (85/93) Reuse existing Prompt
[2024-03-04 11:44:59,203 DEBUG generators.py generate l.365] (85/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:59,205 DEBUG generators.py generate l.373] (85/93) Reuse post-processing
[2024-03-04 11:44:59,205 INFO generators.py gen_for_qa l.548] (85/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:59,206 DEBUG generators.py gen_for_qa l.554] (85/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:59,208 DEBUG generators.py generate l.352] (85/93) Reuse existing Prompt
[2024-03-04 11:44:59,208 DEBUG generators.py generate l.365] (85/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:59,209 DEBUG generators.py generate l.373] (85/93) Reuse post-processing
[2024-03-04 11:44:59,211 INFO generators.py gen_for_qa l.548] (85/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:59,213 DEBUG generators.py gen_for_qa l.554] (85/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:59,215 DEBUG generators.py generate l.352] (85/93) Reuse existing Prompt
[2024-03-04 11:44:59,216 DEBUG generators.py generate l.365] (85/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:59,216 DEBUG generators.py generate l.373] (85/93) Reuse post-processing
[2024-03-04 11:44:59,218 INFO generators.py gen_for_qa l.548] (85/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:59,219 DEBUG generators.py gen_for_qa l.554] (85/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:59,220 DEBUG generators.py generate l.352] (85/93) Reuse existing Prompt
[2024-03-04 11:44:59,220 DEBUG generators.py generate l.365] (85/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:59,220 DEBUG generators.py generate l.373] (85/93) Reuse post-processing
[2024-03-04 11:44:59,223 INFO generators.py gen_for_qa l.548] (85/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:59,224 DEBUG generators.py generate l.349] (85/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:59,225 DEBUG generators.py generate l.358] (85/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:44:59,607 ERROR generators.py complete l.400] (85/93) The following exception occurred with prompt meta={} user=" Qui a découvert l'Amérique centrale (pour les Européens) ?  A)  Christophe Colomb B)  Vasco Núñez de Balboa .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:44:59,611 DEBUG generators.py generate l.373] (85/93) Reuse post-processing
[2024-03-04 11:44:59,612 INFO generators.py generate l.477] (85/93) End question " Qui a découvert l'Amérique centrale (pour les Européens) ?  A)  Christophe Colomb B)  Vasco Núñez de Balboa "
[2024-03-04 11:44:59,613 INFO generators.py generate l.475] (86/93) *** AnsGenerator for question " Qui a découvert la Nouvelle-Zélande (pour les Européens) ?  A)  Abel Tasman B)  James Cook C)  Jean-François-Marie de Surville "
[2024-03-04 11:44:59,615 INFO generators.py gen_for_qa l.548] (86/93) * Start with LLM "gpt-4"
[2024-03-04 11:44:59,616 DEBUG generators.py gen_for_qa l.554] (86/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:59,616 DEBUG generators.py generate l.352] (86/93) Reuse existing Prompt
[2024-03-04 11:44:59,617 DEBUG generators.py generate l.365] (86/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:59,617 DEBUG generators.py generate l.373] (86/93) Reuse post-processing
[2024-03-04 11:44:59,620 INFO generators.py gen_for_qa l.548] (86/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:44:59,620 DEBUG generators.py gen_for_qa l.554] (86/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:59,622 DEBUG generators.py generate l.352] (86/93) Reuse existing Prompt
[2024-03-04 11:44:59,623 DEBUG generators.py generate l.365] (86/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:59,623 DEBUG generators.py generate l.373] (86/93) Reuse post-processing
[2024-03-04 11:44:59,623 INFO generators.py gen_for_qa l.548] (86/93) * Start with LLM "gemini-pro"
[2024-03-04 11:44:59,625 DEBUG generators.py gen_for_qa l.554] (86/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:59,626 DEBUG generators.py generate l.352] (86/93) Reuse existing Prompt
[2024-03-04 11:44:59,627 DEBUG generators.py generate l.365] (86/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:59,627 DEBUG generators.py generate l.373] (86/93) Reuse post-processing
[2024-03-04 11:44:59,627 INFO generators.py gen_for_qa l.548] (86/93) * Start with LLM "claude-2.1"
[2024-03-04 11:44:59,627 DEBUG generators.py gen_for_qa l.554] (86/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:59,627 DEBUG generators.py generate l.352] (86/93) Reuse existing Prompt
[2024-03-04 11:44:59,627 DEBUG generators.py generate l.365] (86/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:59,627 DEBUG generators.py generate l.373] (86/93) Reuse post-processing
[2024-03-04 11:44:59,635 INFO generators.py gen_for_qa l.548] (86/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:44:59,635 DEBUG generators.py gen_for_qa l.554] (86/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:59,637 DEBUG generators.py generate l.352] (86/93) Reuse existing Prompt
[2024-03-04 11:44:59,638 DEBUG generators.py generate l.365] (86/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:59,639 DEBUG generators.py generate l.373] (86/93) Reuse post-processing
[2024-03-04 11:44:59,640 INFO generators.py gen_for_qa l.548] (86/93) * Start with LLM "command-nightly"
[2024-03-04 11:44:59,641 DEBUG generators.py gen_for_qa l.554] (86/93) An Answer has already been generated with this LLM
[2024-03-04 11:44:59,642 DEBUG generators.py generate l.352] (86/93) Reuse existing Prompt
[2024-03-04 11:44:59,643 DEBUG generators.py generate l.365] (86/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:44:59,644 DEBUG generators.py generate l.373] (86/93) Reuse post-processing
[2024-03-04 11:44:59,645 INFO generators.py gen_for_qa l.548] (86/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:44:59,647 DEBUG generators.py generate l.349] (86/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:44:59,648 DEBUG generators.py generate l.358] (86/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:45:00,043 ERROR generators.py complete l.400] (86/93) The following exception occurred with prompt meta={} user=" Qui a découvert la Nouvelle-Zélande (pour les Européens) ?  A)  Abel Tasman B)  James Cook C)  Jean-François-Marie de Surville .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:45:00,048 DEBUG generators.py generate l.373] (86/93) Reuse post-processing
[2024-03-04 11:45:00,049 INFO generators.py generate l.477] (86/93) End question " Qui a découvert la Nouvelle-Zélande (pour les Européens) ?  A)  Abel Tasman B)  James Cook C)  Jean-François-Marie de Surville "
[2024-03-04 11:45:00,051 INFO generators.py generate l.475] (87/93) *** AnsGenerator for question " Qui a découvert le fleuve Amazone ?  A)  Francisco de Orellana B)  Vicente Yáñez Pinzón C)  Diego de Ordaz "
[2024-03-04 11:45:00,052 INFO generators.py gen_for_qa l.548] (87/93) * Start with LLM "gpt-4"
[2024-03-04 11:45:00,053 DEBUG generators.py gen_for_qa l.554] (87/93) An Answer has already been generated with this LLM
[2024-03-04 11:45:00,055 DEBUG generators.py generate l.352] (87/93) Reuse existing Prompt
[2024-03-04 11:45:00,057 DEBUG generators.py generate l.365] (87/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:45:00,058 DEBUG generators.py generate l.373] (87/93) Reuse post-processing
[2024-03-04 11:45:00,059 INFO generators.py gen_for_qa l.548] (87/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:45:00,061 DEBUG generators.py gen_for_qa l.554] (87/93) An Answer has already been generated with this LLM
[2024-03-04 11:45:00,062 DEBUG generators.py generate l.352] (87/93) Reuse existing Prompt
[2024-03-04 11:45:00,062 DEBUG generators.py generate l.365] (87/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:45:00,065 DEBUG generators.py generate l.373] (87/93) Reuse post-processing
[2024-03-04 11:45:00,066 INFO generators.py gen_for_qa l.548] (87/93) * Start with LLM "gemini-pro"
[2024-03-04 11:45:00,067 DEBUG generators.py gen_for_qa l.554] (87/93) An Answer has already been generated with this LLM
[2024-03-04 11:45:00,068 DEBUG generators.py generate l.352] (87/93) Reuse existing Prompt
[2024-03-04 11:45:00,070 DEBUG generators.py generate l.365] (87/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:45:00,070 DEBUG generators.py generate l.373] (87/93) Reuse post-processing
[2024-03-04 11:45:00,071 INFO generators.py gen_for_qa l.548] (87/93) * Start with LLM "claude-2.1"
[2024-03-04 11:45:00,073 DEBUG generators.py gen_for_qa l.554] (87/93) An Answer has already been generated with this LLM
[2024-03-04 11:45:00,073 DEBUG generators.py generate l.352] (87/93) Reuse existing Prompt
[2024-03-04 11:45:00,074 DEBUG generators.py generate l.365] (87/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:45:00,076 DEBUG generators.py generate l.373] (87/93) Reuse post-processing
[2024-03-04 11:45:00,077 INFO generators.py gen_for_qa l.548] (87/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:45:00,078 DEBUG generators.py gen_for_qa l.554] (87/93) An Answer has already been generated with this LLM
[2024-03-04 11:45:00,081 DEBUG generators.py generate l.352] (87/93) Reuse existing Prompt
[2024-03-04 11:45:00,082 DEBUG generators.py generate l.365] (87/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:45:00,082 DEBUG generators.py generate l.373] (87/93) Reuse post-processing
[2024-03-04 11:45:00,084 INFO generators.py gen_for_qa l.548] (87/93) * Start with LLM "command-nightly"
[2024-03-04 11:45:00,086 DEBUG generators.py gen_for_qa l.554] (87/93) An Answer has already been generated with this LLM
[2024-03-04 11:45:00,087 DEBUG generators.py generate l.352] (87/93) Reuse existing Prompt
[2024-03-04 11:45:00,088 DEBUG generators.py generate l.365] (87/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:45:00,088 DEBUG generators.py generate l.373] (87/93) Reuse post-processing
[2024-03-04 11:45:00,089 INFO generators.py gen_for_qa l.548] (87/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:45:00,091 DEBUG generators.py generate l.349] (87/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:45:00,092 DEBUG generators.py generate l.358] (87/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:45:00,478 ERROR generators.py complete l.400] (87/93) The following exception occurred with prompt meta={} user=" Qui a découvert le fleuve Amazone ?  A)  Francisco de Orellana B)  Vicente Yáñez Pinzón C)  Diego de Ordaz .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:45:00,484 DEBUG generators.py generate l.373] (87/93) Reuse post-processing
[2024-03-04 11:45:00,488 INFO generators.py generate l.477] (87/93) End question " Qui a découvert le fleuve Amazone ?  A)  Francisco de Orellana B)  Vicente Yáñez Pinzón C)  Diego de Ordaz "
[2024-03-04 11:45:00,489 INFO generators.py generate l.475] (88/93) *** AnsGenerator for question " Qui a découvert le détroit de Magellan ?  A)  Fernand de Magellan B)  Francisco de Almeida C)  Estêvão Gomes "
[2024-03-04 11:45:00,491 INFO generators.py gen_for_qa l.548] (88/93) * Start with LLM "gpt-4"
[2024-03-04 11:45:00,492 DEBUG generators.py gen_for_qa l.554] (88/93) An Answer has already been generated with this LLM
[2024-03-04 11:45:00,493 DEBUG generators.py generate l.352] (88/93) Reuse existing Prompt
[2024-03-04 11:45:00,496 DEBUG generators.py generate l.365] (88/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:45:00,499 DEBUG generators.py generate l.373] (88/93) Reuse post-processing
[2024-03-04 11:45:00,500 INFO generators.py gen_for_qa l.548] (88/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:45:00,501 DEBUG generators.py gen_for_qa l.554] (88/93) An Answer has already been generated with this LLM
[2024-03-04 11:45:00,502 DEBUG generators.py generate l.352] (88/93) Reuse existing Prompt
[2024-03-04 11:45:00,504 DEBUG generators.py generate l.365] (88/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:45:00,505 DEBUG generators.py generate l.373] (88/93) Reuse post-processing
[2024-03-04 11:45:00,506 INFO generators.py gen_for_qa l.548] (88/93) * Start with LLM "gemini-pro"
[2024-03-04 11:45:00,507 DEBUG generators.py gen_for_qa l.554] (88/93) An Answer has already been generated with this LLM
[2024-03-04 11:45:00,510 DEBUG generators.py generate l.352] (88/93) Reuse existing Prompt
[2024-03-04 11:45:00,511 DEBUG generators.py generate l.365] (88/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:45:00,513 DEBUG generators.py generate l.373] (88/93) Reuse post-processing
[2024-03-04 11:45:00,516 INFO generators.py gen_for_qa l.548] (88/93) * Start with LLM "claude-2.1"
[2024-03-04 11:45:00,518 DEBUG generators.py gen_for_qa l.554] (88/93) An Answer has already been generated with this LLM
[2024-03-04 11:45:00,519 DEBUG generators.py generate l.352] (88/93) Reuse existing Prompt
[2024-03-04 11:45:00,520 DEBUG generators.py generate l.365] (88/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:45:00,523 DEBUG generators.py generate l.373] (88/93) Reuse post-processing
[2024-03-04 11:45:00,525 INFO generators.py gen_for_qa l.548] (88/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:45:00,526 DEBUG generators.py gen_for_qa l.554] (88/93) An Answer has already been generated with this LLM
[2024-03-04 11:45:00,528 DEBUG generators.py generate l.352] (88/93) Reuse existing Prompt
[2024-03-04 11:45:00,529 DEBUG generators.py generate l.365] (88/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:45:00,535 DEBUG generators.py generate l.373] (88/93) Reuse post-processing
[2024-03-04 11:45:00,536 INFO generators.py gen_for_qa l.548] (88/93) * Start with LLM "command-nightly"
[2024-03-04 11:45:00,536 DEBUG generators.py gen_for_qa l.554] (88/93) An Answer has already been generated with this LLM
[2024-03-04 11:45:00,540 DEBUG generators.py generate l.352] (88/93) Reuse existing Prompt
[2024-03-04 11:45:00,542 DEBUG generators.py generate l.365] (88/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:45:00,544 DEBUG generators.py generate l.373] (88/93) Reuse post-processing
[2024-03-04 11:45:00,545 INFO generators.py gen_for_qa l.548] (88/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:45:00,548 DEBUG generators.py generate l.349] (88/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:45:00,551 DEBUG generators.py generate l.358] (88/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:45:00,948 ERROR generators.py complete l.400] (88/93) The following exception occurred with prompt meta={} user=" Qui a découvert le détroit de Magellan ?  A)  Fernand de Magellan B)  Francisco de Almeida C)  Estêvão Gomes .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:45:00,957 DEBUG generators.py generate l.373] (88/93) Reuse post-processing
[2024-03-04 11:45:00,959 INFO generators.py generate l.477] (88/93) End question " Qui a découvert le détroit de Magellan ?  A)  Fernand de Magellan B)  Francisco de Almeida C)  Estêvão Gomes "
[2024-03-04 11:45:00,961 INFO generators.py generate l.475] (89/93) *** AnsGenerator for question " Qui a découvert l'archipel d'Hawaï (pour les Européens) ?  A)  James Cook B)  Juan Gaetano "
[2024-03-04 11:45:00,963 INFO generators.py gen_for_qa l.548] (89/93) * Start with LLM "gpt-4"
[2024-03-04 11:45:00,966 DEBUG generators.py gen_for_qa l.554] (89/93) An Answer has already been generated with this LLM
[2024-03-04 11:45:00,968 DEBUG generators.py generate l.352] (89/93) Reuse existing Prompt
[2024-03-04 11:45:00,970 DEBUG generators.py generate l.365] (89/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:45:00,971 DEBUG generators.py generate l.373] (89/93) Reuse post-processing
[2024-03-04 11:45:00,972 INFO generators.py gen_for_qa l.548] (89/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:45:00,975 DEBUG generators.py gen_for_qa l.554] (89/93) An Answer has already been generated with this LLM
[2024-03-04 11:45:00,977 DEBUG generators.py generate l.352] (89/93) Reuse existing Prompt
[2024-03-04 11:45:00,978 DEBUG generators.py generate l.365] (89/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:45:00,980 DEBUG generators.py generate l.373] (89/93) Reuse post-processing
[2024-03-04 11:45:00,983 INFO generators.py gen_for_qa l.548] (89/93) * Start with LLM "gemini-pro"
[2024-03-04 11:45:00,985 DEBUG generators.py gen_for_qa l.554] (89/93) An Answer has already been generated with this LLM
[2024-03-04 11:45:00,987 DEBUG generators.py generate l.352] (89/93) Reuse existing Prompt
[2024-03-04 11:45:00,989 DEBUG generators.py generate l.365] (89/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:45:00,992 DEBUG generators.py generate l.373] (89/93) Reuse post-processing
[2024-03-04 11:45:00,993 INFO generators.py gen_for_qa l.548] (89/93) * Start with LLM "claude-2.1"
[2024-03-04 11:45:00,995 DEBUG generators.py gen_for_qa l.554] (89/93) An Answer has already been generated with this LLM
[2024-03-04 11:45:00,997 DEBUG generators.py generate l.352] (89/93) Reuse existing Prompt
[2024-03-04 11:45:00,999 DEBUG generators.py generate l.365] (89/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:45:01,001 DEBUG generators.py generate l.373] (89/93) Reuse post-processing
[2024-03-04 11:45:01,003 INFO generators.py gen_for_qa l.548] (89/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:45:01,005 DEBUG generators.py gen_for_qa l.554] (89/93) An Answer has already been generated with this LLM
[2024-03-04 11:45:01,005 DEBUG generators.py generate l.352] (89/93) Reuse existing Prompt
[2024-03-04 11:45:01,006 DEBUG generators.py generate l.365] (89/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:45:01,009 DEBUG generators.py generate l.373] (89/93) Reuse post-processing
[2024-03-04 11:45:01,010 INFO generators.py gen_for_qa l.548] (89/93) * Start with LLM "command-nightly"
[2024-03-04 11:45:01,011 DEBUG generators.py gen_for_qa l.554] (89/93) An Answer has already been generated with this LLM
[2024-03-04 11:45:01,014 DEBUG generators.py generate l.352] (89/93) Reuse existing Prompt
[2024-03-04 11:45:01,016 DEBUG generators.py generate l.365] (89/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:45:01,018 DEBUG generators.py generate l.373] (89/93) Reuse post-processing
[2024-03-04 11:45:01,019 INFO generators.py gen_for_qa l.548] (89/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:45:01,019 DEBUG generators.py generate l.349] (89/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:45:01,021 DEBUG generators.py generate l.358] (89/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:45:01,416 ERROR generators.py complete l.400] (89/93) The following exception occurred with prompt meta={} user=" Qui a découvert l'archipel d'Hawaï (pour les Européens) ?  A)  James Cook B)  Juan Gaetano .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:45:01,433 DEBUG generators.py generate l.373] (89/93) Reuse post-processing
[2024-03-04 11:45:01,436 INFO generators.py generate l.477] (89/93) End question " Qui a découvert l'archipel d'Hawaï (pour les Européens) ?  A)  James Cook B)  Juan Gaetano "
[2024-03-04 11:45:01,440 INFO generators.py generate l.475] (90/93) *** AnsGenerator for question " Qui a découvert le passage du Nord-Ouest ?  A)  Roald Amundsen B)  John Franklin C)  Robert McClure "
[2024-03-04 11:45:01,444 INFO generators.py gen_for_qa l.548] (90/93) * Start with LLM "gpt-4"
[2024-03-04 11:45:01,446 DEBUG generators.py gen_for_qa l.554] (90/93) An Answer has already been generated with this LLM
[2024-03-04 11:45:01,447 DEBUG generators.py generate l.352] (90/93) Reuse existing Prompt
[2024-03-04 11:45:01,450 DEBUG generators.py generate l.365] (90/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:45:01,452 DEBUG generators.py generate l.373] (90/93) Reuse post-processing
[2024-03-04 11:45:01,453 INFO generators.py gen_for_qa l.548] (90/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:45:01,455 DEBUG generators.py gen_for_qa l.554] (90/93) An Answer has already been generated with this LLM
[2024-03-04 11:45:01,458 DEBUG generators.py generate l.352] (90/93) Reuse existing Prompt
[2024-03-04 11:45:01,459 DEBUG generators.py generate l.365] (90/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:45:01,460 DEBUG generators.py generate l.373] (90/93) Reuse post-processing
[2024-03-04 11:45:01,462 INFO generators.py gen_for_qa l.548] (90/93) * Start with LLM "gemini-pro"
[2024-03-04 11:45:01,464 DEBUG generators.py gen_for_qa l.554] (90/93) An Answer has already been generated with this LLM
[2024-03-04 11:45:01,465 DEBUG generators.py generate l.352] (90/93) Reuse existing Prompt
[2024-03-04 11:45:01,466 DEBUG generators.py generate l.365] (90/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:45:01,467 DEBUG generators.py generate l.373] (90/93) Reuse post-processing
[2024-03-04 11:45:01,468 INFO generators.py gen_for_qa l.548] (90/93) * Start with LLM "claude-2.1"
[2024-03-04 11:45:01,469 DEBUG generators.py gen_for_qa l.554] (90/93) An Answer has already been generated with this LLM
[2024-03-04 11:45:01,470 DEBUG generators.py generate l.352] (90/93) Reuse existing Prompt
[2024-03-04 11:45:01,471 DEBUG generators.py generate l.365] (90/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:45:01,474 DEBUG generators.py generate l.373] (90/93) Reuse post-processing
[2024-03-04 11:45:01,475 INFO generators.py gen_for_qa l.548] (90/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:45:01,476 DEBUG generators.py gen_for_qa l.554] (90/93) An Answer has already been generated with this LLM
[2024-03-04 11:45:01,477 DEBUG generators.py generate l.352] (90/93) Reuse existing Prompt
[2024-03-04 11:45:01,479 DEBUG generators.py generate l.365] (90/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:45:01,481 DEBUG generators.py generate l.373] (90/93) Reuse post-processing
[2024-03-04 11:45:01,482 INFO generators.py gen_for_qa l.548] (90/93) * Start with LLM "command-nightly"
[2024-03-04 11:45:01,483 DEBUG generators.py gen_for_qa l.554] (90/93) An Answer has already been generated with this LLM
[2024-03-04 11:45:01,486 DEBUG generators.py generate l.352] (90/93) Reuse existing Prompt
[2024-03-04 11:45:01,487 DEBUG generators.py generate l.365] (90/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:45:01,488 DEBUG generators.py generate l.373] (90/93) Reuse post-processing
[2024-03-04 11:45:01,490 INFO generators.py gen_for_qa l.548] (90/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:45:01,493 DEBUG generators.py generate l.349] (90/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:45:01,495 DEBUG generators.py generate l.358] (90/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:45:01,900 ERROR generators.py complete l.400] (90/93) The following exception occurred with prompt meta={} user=" Qui a découvert le passage du Nord-Ouest ?  A)  Roald Amundsen B)  John Franklin C)  Robert McClure .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:45:01,911 DEBUG generators.py generate l.373] (90/93) Reuse post-processing
[2024-03-04 11:45:01,914 INFO generators.py generate l.477] (90/93) End question " Qui a découvert le passage du Nord-Ouest ?  A)  Roald Amundsen B)  John Franklin C)  Robert McClure "
[2024-03-04 11:45:01,917 INFO generators.py generate l.475] (91/93) *** AnsGenerator for question " Qui a découvert le cap de Bonne-Espérance ?  A)  Bartolomeu Dias B)  Vasco da Gama "
[2024-03-04 11:45:01,920 INFO generators.py gen_for_qa l.548] (91/93) * Start with LLM "gpt-4"
[2024-03-04 11:45:01,921 DEBUG generators.py gen_for_qa l.554] (91/93) An Answer has already been generated with this LLM
[2024-03-04 11:45:01,923 DEBUG generators.py generate l.352] (91/93) Reuse existing Prompt
[2024-03-04 11:45:01,926 DEBUG generators.py generate l.365] (91/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:45:01,927 DEBUG generators.py generate l.373] (91/93) Reuse post-processing
[2024-03-04 11:45:01,929 INFO generators.py gen_for_qa l.548] (91/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:45:01,931 DEBUG generators.py gen_for_qa l.554] (91/93) An Answer has already been generated with this LLM
[2024-03-04 11:45:01,931 DEBUG generators.py generate l.352] (91/93) Reuse existing Prompt
[2024-03-04 11:45:01,931 DEBUG generators.py generate l.365] (91/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:45:01,935 DEBUG generators.py generate l.373] (91/93) Reuse post-processing
[2024-03-04 11:45:01,937 INFO generators.py gen_for_qa l.548] (91/93) * Start with LLM "gemini-pro"
[2024-03-04 11:45:01,938 DEBUG generators.py gen_for_qa l.554] (91/93) An Answer has already been generated with this LLM
[2024-03-04 11:45:01,939 DEBUG generators.py generate l.352] (91/93) Reuse existing Prompt
[2024-03-04 11:45:01,940 DEBUG generators.py generate l.365] (91/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:45:01,941 DEBUG generators.py generate l.373] (91/93) Reuse post-processing
[2024-03-04 11:45:01,943 INFO generators.py gen_for_qa l.548] (91/93) * Start with LLM "claude-2.1"
[2024-03-04 11:45:01,945 DEBUG generators.py gen_for_qa l.554] (91/93) An Answer has already been generated with this LLM
[2024-03-04 11:45:01,946 DEBUG generators.py generate l.352] (91/93) Reuse existing Prompt
[2024-03-04 11:45:01,947 DEBUG generators.py generate l.365] (91/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:45:01,948 DEBUG generators.py generate l.373] (91/93) Reuse post-processing
[2024-03-04 11:45:01,949 INFO generators.py gen_for_qa l.548] (91/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:45:01,950 DEBUG generators.py gen_for_qa l.554] (91/93) An Answer has already been generated with this LLM
[2024-03-04 11:45:01,950 DEBUG generators.py generate l.352] (91/93) Reuse existing Prompt
[2024-03-04 11:45:01,952 DEBUG generators.py generate l.365] (91/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:45:01,952 DEBUG generators.py generate l.373] (91/93) Reuse post-processing
[2024-03-04 11:45:01,955 INFO generators.py gen_for_qa l.548] (91/93) * Start with LLM "command-nightly"
[2024-03-04 11:45:01,956 DEBUG generators.py gen_for_qa l.554] (91/93) An Answer has already been generated with this LLM
[2024-03-04 11:45:01,957 DEBUG generators.py generate l.352] (91/93) Reuse existing Prompt
[2024-03-04 11:45:01,959 DEBUG generators.py generate l.365] (91/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:45:01,961 DEBUG generators.py generate l.373] (91/93) Reuse post-processing
[2024-03-04 11:45:01,962 INFO generators.py gen_for_qa l.548] (91/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:45:01,962 DEBUG generators.py generate l.349] (91/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:45:01,964 DEBUG generators.py generate l.358] (91/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:45:02,353 ERROR generators.py complete l.400] (91/93) The following exception occurred with prompt meta={} user=" Qui a découvert le cap de Bonne-Espérance ?  A)  Bartolomeu Dias B)  Vasco da Gama .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:45:02,362 DEBUG generators.py generate l.373] (91/93) Reuse post-processing
[2024-03-04 11:45:02,364 INFO generators.py generate l.477] (91/93) End question " Qui a découvert le cap de Bonne-Espérance ?  A)  Bartolomeu Dias B)  Vasco da Gama "
[2024-03-04 11:45:02,366 INFO generators.py generate l.475] (92/93) *** AnsGenerator for question " Qui a découvert la source du Nil ?  A)  John Hanning Speke B)  Richard Francis Burton C)  Samuel Baker "
[2024-03-04 11:45:02,368 INFO generators.py gen_for_qa l.548] (92/93) * Start with LLM "gpt-4"
[2024-03-04 11:45:02,370 DEBUG generators.py gen_for_qa l.554] (92/93) An Answer has already been generated with this LLM
[2024-03-04 11:45:02,372 DEBUG generators.py generate l.352] (92/93) Reuse existing Prompt
[2024-03-04 11:45:02,373 DEBUG generators.py generate l.365] (92/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:45:02,375 DEBUG generators.py generate l.373] (92/93) Reuse post-processing
[2024-03-04 11:45:02,377 INFO generators.py gen_for_qa l.548] (92/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:45:02,378 DEBUG generators.py gen_for_qa l.554] (92/93) An Answer has already been generated with this LLM
[2024-03-04 11:45:02,380 DEBUG generators.py generate l.352] (92/93) Reuse existing Prompt
[2024-03-04 11:45:02,380 DEBUG generators.py generate l.365] (92/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:45:02,382 DEBUG generators.py generate l.373] (92/93) Reuse post-processing
[2024-03-04 11:45:02,382 INFO generators.py gen_for_qa l.548] (92/93) * Start with LLM "gemini-pro"
[2024-03-04 11:45:02,382 DEBUG generators.py gen_for_qa l.554] (92/93) An Answer has already been generated with this LLM
[2024-03-04 11:45:02,385 DEBUG generators.py generate l.352] (92/93) Reuse existing Prompt
[2024-03-04 11:45:02,387 DEBUG generators.py generate l.365] (92/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:45:02,388 DEBUG generators.py generate l.373] (92/93) Reuse post-processing
[2024-03-04 11:45:02,389 INFO generators.py gen_for_qa l.548] (92/93) * Start with LLM "claude-2.1"
[2024-03-04 11:45:02,390 DEBUG generators.py gen_for_qa l.554] (92/93) An Answer has already been generated with this LLM
[2024-03-04 11:45:02,392 DEBUG generators.py generate l.352] (92/93) Reuse existing Prompt
[2024-03-04 11:45:02,392 DEBUG generators.py generate l.365] (92/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:45:02,395 DEBUG generators.py generate l.373] (92/93) Reuse post-processing
[2024-03-04 11:45:02,396 INFO generators.py gen_for_qa l.548] (92/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:45:02,397 DEBUG generators.py gen_for_qa l.554] (92/93) An Answer has already been generated with this LLM
[2024-03-04 11:45:02,398 DEBUG generators.py generate l.352] (92/93) Reuse existing Prompt
[2024-03-04 11:45:02,399 DEBUG generators.py generate l.365] (92/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:45:02,400 DEBUG generators.py generate l.373] (92/93) Reuse post-processing
[2024-03-04 11:45:02,401 INFO generators.py gen_for_qa l.548] (92/93) * Start with LLM "command-nightly"
[2024-03-04 11:45:02,402 DEBUG generators.py gen_for_qa l.554] (92/93) An Answer has already been generated with this LLM
[2024-03-04 11:45:02,402 DEBUG generators.py generate l.352] (92/93) Reuse existing Prompt
[2024-03-04 11:45:02,402 DEBUG generators.py generate l.365] (92/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:45:02,405 DEBUG generators.py generate l.373] (92/93) Reuse post-processing
[2024-03-04 11:45:02,405 INFO generators.py gen_for_qa l.548] (92/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:45:02,407 DEBUG generators.py generate l.349] (92/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:45:02,409 DEBUG generators.py generate l.358] (92/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:45:02,760 ERROR generators.py complete l.400] (92/93) The following exception occurred with prompt meta={} user=" Qui a découvert la source du Nil ?  A)  John Hanning Speke B)  Richard Francis Burton C)  Samuel Baker .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:45:02,778 DEBUG generators.py generate l.373] (92/93) Reuse post-processing
[2024-03-04 11:45:02,782 INFO generators.py generate l.477] (92/93) End question " Qui a découvert la source du Nil ?  A)  John Hanning Speke B)  Richard Francis Burton C)  Samuel Baker "
[2024-03-04 11:45:02,784 INFO generators.py generate l.475] (93/93) *** AnsGenerator for question " Qui a découvert la Terre de Feu ?  A)  Ferdinand Magellan B)  Hernando de Magallanes "
[2024-03-04 11:45:02,787 INFO generators.py gen_for_qa l.548] (93/93) * Start with LLM "gpt-4"
[2024-03-04 11:45:02,790 DEBUG generators.py gen_for_qa l.554] (93/93) An Answer has already been generated with this LLM
[2024-03-04 11:45:02,792 DEBUG generators.py generate l.352] (93/93) Reuse existing Prompt
[2024-03-04 11:45:02,795 DEBUG generators.py generate l.365] (93/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:45:02,796 DEBUG generators.py generate l.373] (93/93) Reuse post-processing
[2024-03-04 11:45:02,798 INFO generators.py gen_for_qa l.548] (93/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 11:45:02,800 DEBUG generators.py gen_for_qa l.554] (93/93) An Answer has already been generated with this LLM
[2024-03-04 11:45:02,801 DEBUG generators.py generate l.352] (93/93) Reuse existing Prompt
[2024-03-04 11:45:02,802 DEBUG generators.py generate l.365] (93/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:45:02,803 DEBUG generators.py generate l.373] (93/93) Reuse post-processing
[2024-03-04 11:45:02,805 INFO generators.py gen_for_qa l.548] (93/93) * Start with LLM "gemini-pro"
[2024-03-04 11:45:02,806 DEBUG generators.py gen_for_qa l.554] (93/93) An Answer has already been generated with this LLM
[2024-03-04 11:45:02,808 DEBUG generators.py generate l.352] (93/93) Reuse existing Prompt
[2024-03-04 11:45:02,811 DEBUG generators.py generate l.365] (93/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:45:02,811 DEBUG generators.py generate l.373] (93/93) Reuse post-processing
[2024-03-04 11:45:02,812 INFO generators.py gen_for_qa l.548] (93/93) * Start with LLM "claude-2.1"
[2024-03-04 11:45:02,813 DEBUG generators.py gen_for_qa l.554] (93/93) An Answer has already been generated with this LLM
[2024-03-04 11:45:02,815 DEBUG generators.py generate l.352] (93/93) Reuse existing Prompt
[2024-03-04 11:45:02,816 DEBUG generators.py generate l.365] (93/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:45:02,816 DEBUG generators.py generate l.373] (93/93) Reuse post-processing
[2024-03-04 11:45:02,816 INFO generators.py gen_for_qa l.548] (93/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 11:45:02,819 DEBUG generators.py gen_for_qa l.554] (93/93) An Answer has already been generated with this LLM
[2024-03-04 11:45:02,820 DEBUG generators.py generate l.352] (93/93) Reuse existing Prompt
[2024-03-04 11:45:02,821 DEBUG generators.py generate l.365] (93/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:45:02,822 DEBUG generators.py generate l.373] (93/93) Reuse post-processing
[2024-03-04 11:45:02,823 INFO generators.py gen_for_qa l.548] (93/93) * Start with LLM "command-nightly"
[2024-03-04 11:45:02,825 DEBUG generators.py gen_for_qa l.554] (93/93) An Answer has already been generated with this LLM
[2024-03-04 11:45:02,828 DEBUG generators.py generate l.352] (93/93) Reuse existing Prompt
[2024-03-04 11:45:02,828 DEBUG generators.py generate l.365] (93/93) Reuse existing LLMAnswer in Answer
[2024-03-04 11:45:02,830 DEBUG generators.py generate l.373] (93/93) Reuse post-processing
[2024-03-04 11:45:02,831 INFO generators.py gen_for_qa l.548] (93/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 11:45:02,831 DEBUG generators.py generate l.349] (93/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 11:45:02,832 DEBUG generators.py generate l.358] (93/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 11:45:03,212 ERROR generators.py complete l.400] (93/93) The following exception occurred with prompt meta={} user=" Qui a découvert la Terre de Feu ?  A)  Ferdinand Magellan B)  Hernando de Magallanes .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
isinstance expected 2 arguments, got 1
Traceback (most recent call last):
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1908, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7768, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7558, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1123, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2588, in wrapper
    or isinstance(openai.APIConnectionError)
TypeError: isinstance expected 2 arguments, got 1
[2024-03-04 11:45:03,242 DEBUG generators.py generate l.373] (93/93) Reuse post-processing
[2024-03-04 11:45:03,245 INFO generators.py generate l.477] (93/93) End question " Qui a découvert la Terre de Feu ?  A)  Ferdinand Magellan B)  Hernando de Magallanes "
[2024-03-04 11:45:03,267 INFO expe.py save_to_json l.283] (93/93) Expe saved as JSON to expe\Answers\culture_fr_v1_gen_with_LeChat--93Q_0C_0F_7M_558A_0HE_0AE_2024-03-04_11,45,03.json
[2024-03-04 11:45:03,271 INFO main.py <module> l.101] (93/93) MAIN ENDS
[2024-03-04 21:34:20,834 INFO main.py <module> l.87] MAIN STARTS
[2024-03-04 21:34:20,867 INFO generators.py generate l.475] (1/93) *** AnsGenerator for question "Qui a inventé le télégraphe électrique ?  A)  Samuel Morse B)  Charles Wheatstone C)  William Fothergill Cooke "
[2024-03-04 21:34:20,869 INFO generators.py gen_for_qa l.548] (1/93) * Start with LLM "gpt-4"
[2024-03-04 21:34:20,871 DEBUG generators.py gen_for_qa l.554] (1/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:20,873 DEBUG generators.py generate l.352] (1/93) Reuse existing Prompt
[2024-03-04 21:34:20,875 DEBUG generators.py generate l.365] (1/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:20,876 DEBUG generators.py generate l.373] (1/93) Reuse post-processing
[2024-03-04 21:34:20,876 INFO generators.py gen_for_qa l.548] (1/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:34:20,877 DEBUG generators.py gen_for_qa l.554] (1/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:20,879 DEBUG generators.py generate l.352] (1/93) Reuse existing Prompt
[2024-03-04 21:34:20,879 DEBUG generators.py generate l.365] (1/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:20,879 DEBUG generators.py generate l.373] (1/93) Reuse post-processing
[2024-03-04 21:34:20,881 INFO generators.py gen_for_qa l.548] (1/93) * Start with LLM "gemini-pro"
[2024-03-04 21:34:20,882 DEBUG generators.py gen_for_qa l.554] (1/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:20,883 DEBUG generators.py generate l.352] (1/93) Reuse existing Prompt
[2024-03-04 21:34:20,884 DEBUG generators.py generate l.365] (1/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:20,885 DEBUG generators.py generate l.373] (1/93) Reuse post-processing
[2024-03-04 21:34:20,886 INFO generators.py gen_for_qa l.548] (1/93) * Start with LLM "claude-2.1"
[2024-03-04 21:34:20,886 DEBUG generators.py gen_for_qa l.554] (1/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:20,887 DEBUG generators.py generate l.352] (1/93) Reuse existing Prompt
[2024-03-04 21:34:20,890 DEBUG generators.py generate l.365] (1/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:20,892 DEBUG generators.py generate l.373] (1/93) Reuse post-processing
[2024-03-04 21:34:20,893 INFO generators.py gen_for_qa l.548] (1/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:34:20,894 DEBUG generators.py gen_for_qa l.554] (1/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:20,895 DEBUG generators.py generate l.352] (1/93) Reuse existing Prompt
[2024-03-04 21:34:20,896 DEBUG generators.py generate l.365] (1/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:20,897 DEBUG generators.py generate l.373] (1/93) Reuse post-processing
[2024-03-04 21:34:20,898 INFO generators.py gen_for_qa l.548] (1/93) * Start with LLM "command-nightly"
[2024-03-04 21:34:20,898 DEBUG generators.py gen_for_qa l.554] (1/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:20,899 DEBUG generators.py generate l.352] (1/93) Reuse existing Prompt
[2024-03-04 21:34:20,899 DEBUG generators.py generate l.365] (1/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:20,901 DEBUG generators.py generate l.373] (1/93) Reuse post-processing
[2024-03-04 21:34:20,901 INFO generators.py gen_for_qa l.548] (1/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:34:20,903 DEBUG generators.py generate l.349] (1/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:34:20,904 DEBUG generators.py generate l.358] (1/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:34:27,871 ERROR generators.py complete l.400] (1/93) The following exception occurred with prompt meta={} user="Qui a inventé le télégraphe électrique ?  A)  Samuel Morse B)  Charles Wheatstone C)  William Fothergill Cooke .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:34:27,890 DEBUG generators.py generate l.373] (1/93) Reuse post-processing
[2024-03-04 21:34:27,891 INFO generators.py generate l.477] (1/93) End question "Qui a inventé le télégraphe électrique ?  A)  Samuel Morse B)  Charles Wheatstone C)  William Fothergill Cooke "
[2024-03-04 21:34:27,894 INFO generators.py generate l.475] (2/93) *** AnsGenerator for question "Qui a inventé la photographie ?  A)  Louis Daguerre B)  William Henry Fox Talbot "
[2024-03-04 21:34:27,894 INFO generators.py gen_for_qa l.548] (2/93) * Start with LLM "gpt-4"
[2024-03-04 21:34:27,897 DEBUG generators.py gen_for_qa l.554] (2/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:27,897 DEBUG generators.py generate l.352] (2/93) Reuse existing Prompt
[2024-03-04 21:34:27,899 DEBUG generators.py generate l.365] (2/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:27,900 DEBUG generators.py generate l.373] (2/93) Reuse post-processing
[2024-03-04 21:34:27,900 INFO generators.py gen_for_qa l.548] (2/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:34:27,901 DEBUG generators.py gen_for_qa l.554] (2/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:27,904 DEBUG generators.py generate l.352] (2/93) Reuse existing Prompt
[2024-03-04 21:34:27,904 DEBUG generators.py generate l.365] (2/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:27,904 DEBUG generators.py generate l.373] (2/93) Reuse post-processing
[2024-03-04 21:34:27,904 INFO generators.py gen_for_qa l.548] (2/93) * Start with LLM "gemini-pro"
[2024-03-04 21:34:27,904 DEBUG generators.py gen_for_qa l.554] (2/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:27,910 DEBUG generators.py generate l.352] (2/93) Reuse existing Prompt
[2024-03-04 21:34:27,910 DEBUG generators.py generate l.365] (2/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:27,911 DEBUG generators.py generate l.373] (2/93) Reuse post-processing
[2024-03-04 21:34:27,912 INFO generators.py gen_for_qa l.548] (2/93) * Start with LLM "claude-2.1"
[2024-03-04 21:34:27,913 DEBUG generators.py gen_for_qa l.554] (2/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:27,915 DEBUG generators.py generate l.352] (2/93) Reuse existing Prompt
[2024-03-04 21:34:27,916 DEBUG generators.py generate l.365] (2/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:27,916 DEBUG generators.py generate l.373] (2/93) Reuse post-processing
[2024-03-04 21:34:27,916 INFO generators.py gen_for_qa l.548] (2/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:34:27,916 DEBUG generators.py gen_for_qa l.554] (2/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:27,919 DEBUG generators.py generate l.352] (2/93) Reuse existing Prompt
[2024-03-04 21:34:27,920 DEBUG generators.py generate l.365] (2/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:27,921 DEBUG generators.py generate l.373] (2/93) Reuse post-processing
[2024-03-04 21:34:27,924 INFO generators.py gen_for_qa l.548] (2/93) * Start with LLM "command-nightly"
[2024-03-04 21:34:27,925 DEBUG generators.py gen_for_qa l.554] (2/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:27,925 DEBUG generators.py generate l.352] (2/93) Reuse existing Prompt
[2024-03-04 21:34:27,925 DEBUG generators.py generate l.365] (2/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:27,927 DEBUG generators.py generate l.373] (2/93) Reuse post-processing
[2024-03-04 21:34:27,927 INFO generators.py gen_for_qa l.548] (2/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:34:27,929 DEBUG generators.py generate l.349] (2/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:34:27,929 DEBUG generators.py generate l.358] (2/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:34:32,112 ERROR generators.py complete l.400] (2/93) The following exception occurred with prompt meta={} user="Qui a inventé la photographie ?  A)  Louis Daguerre B)  William Henry Fox Talbot .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:34:32,125 DEBUG generators.py generate l.373] (2/93) Reuse post-processing
[2024-03-04 21:34:32,126 INFO generators.py generate l.477] (2/93) End question "Qui a inventé la photographie ?  A)  Louis Daguerre B)  William Henry Fox Talbot "
[2024-03-04 21:34:32,128 INFO generators.py generate l.475] (3/93) *** AnsGenerator for question "Qui a inventé le moteur à vapeur ?  A)  Thomas Newcomen B)  Denis Papin "
[2024-03-04 21:34:32,128 INFO generators.py gen_for_qa l.548] (3/93) * Start with LLM "gpt-4"
[2024-03-04 21:34:32,128 DEBUG generators.py gen_for_qa l.554] (3/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:32,131 DEBUG generators.py generate l.352] (3/93) Reuse existing Prompt
[2024-03-04 21:34:32,132 DEBUG generators.py generate l.365] (3/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:32,132 DEBUG generators.py generate l.373] (3/93) Reuse post-processing
[2024-03-04 21:34:32,134 INFO generators.py gen_for_qa l.548] (3/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:34:32,134 DEBUG generators.py gen_for_qa l.554] (3/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:32,137 DEBUG generators.py generate l.352] (3/93) Reuse existing Prompt
[2024-03-04 21:34:32,137 DEBUG generators.py generate l.365] (3/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:32,137 DEBUG generators.py generate l.373] (3/93) Reuse post-processing
[2024-03-04 21:34:32,139 INFO generators.py gen_for_qa l.548] (3/93) * Start with LLM "gemini-pro"
[2024-03-04 21:34:32,139 DEBUG generators.py gen_for_qa l.554] (3/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:32,142 DEBUG generators.py generate l.352] (3/93) Reuse existing Prompt
[2024-03-04 21:34:32,143 DEBUG generators.py generate l.365] (3/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:32,143 DEBUG generators.py generate l.373] (3/93) Reuse post-processing
[2024-03-04 21:34:32,144 INFO generators.py gen_for_qa l.548] (3/93) * Start with LLM "claude-2.1"
[2024-03-04 21:34:32,145 DEBUG generators.py gen_for_qa l.554] (3/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:32,146 DEBUG generators.py generate l.352] (3/93) Reuse existing Prompt
[2024-03-04 21:34:32,147 DEBUG generators.py generate l.365] (3/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:32,147 DEBUG generators.py generate l.373] (3/93) Reuse post-processing
[2024-03-04 21:34:32,148 INFO generators.py gen_for_qa l.548] (3/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:34:32,148 DEBUG generators.py gen_for_qa l.554] (3/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:32,151 DEBUG generators.py generate l.352] (3/93) Reuse existing Prompt
[2024-03-04 21:34:32,153 DEBUG generators.py generate l.365] (3/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:32,154 DEBUG generators.py generate l.373] (3/93) Reuse post-processing
[2024-03-04 21:34:32,156 INFO generators.py gen_for_qa l.548] (3/93) * Start with LLM "command-nightly"
[2024-03-04 21:34:32,157 DEBUG generators.py gen_for_qa l.554] (3/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:32,157 DEBUG generators.py generate l.352] (3/93) Reuse existing Prompt
[2024-03-04 21:34:32,158 DEBUG generators.py generate l.365] (3/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:32,158 DEBUG generators.py generate l.373] (3/93) Reuse post-processing
[2024-03-04 21:34:32,158 INFO generators.py gen_for_qa l.548] (3/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:34:32,162 DEBUG generators.py generate l.349] (3/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:34:32,163 DEBUG generators.py generate l.358] (3/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:34:33,673 ERROR generators.py complete l.400] (3/93) The following exception occurred with prompt meta={} user="Qui a inventé le moteur à vapeur ?  A)  Thomas Newcomen B)  Denis Papin .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:34:33,696 DEBUG generators.py generate l.373] (3/93) Reuse post-processing
[2024-03-04 21:34:33,697 INFO generators.py generate l.477] (3/93) End question "Qui a inventé le moteur à vapeur ?  A)  Thomas Newcomen B)  Denis Papin "
[2024-03-04 21:34:33,697 INFO generators.py generate l.475] (4/93) *** AnsGenerator for question "Qui a inventé le bateau à vapeur ?  A)  James Watt B)  Claude de Jouffroy d'Abbans "
[2024-03-04 21:34:33,701 INFO generators.py gen_for_qa l.548] (4/93) * Start with LLM "gpt-4"
[2024-03-04 21:34:33,704 DEBUG generators.py gen_for_qa l.554] (4/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:33,706 DEBUG generators.py generate l.352] (4/93) Reuse existing Prompt
[2024-03-04 21:34:33,707 DEBUG generators.py generate l.365] (4/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:33,708 DEBUG generators.py generate l.373] (4/93) Reuse post-processing
[2024-03-04 21:34:33,708 INFO generators.py gen_for_qa l.548] (4/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:34:33,710 DEBUG generators.py gen_for_qa l.554] (4/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:33,710 DEBUG generators.py generate l.352] (4/93) Reuse existing Prompt
[2024-03-04 21:34:33,713 DEBUG generators.py generate l.365] (4/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:33,713 DEBUG generators.py generate l.373] (4/93) Reuse post-processing
[2024-03-04 21:34:33,714 INFO generators.py gen_for_qa l.548] (4/93) * Start with LLM "gemini-pro"
[2024-03-04 21:34:33,715 DEBUG generators.py gen_for_qa l.554] (4/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:33,717 DEBUG generators.py generate l.352] (4/93) Reuse existing Prompt
[2024-03-04 21:34:33,718 DEBUG generators.py generate l.365] (4/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:33,720 DEBUG generators.py generate l.373] (4/93) Reuse post-processing
[2024-03-04 21:34:33,721 INFO generators.py gen_for_qa l.548] (4/93) * Start with LLM "claude-2.1"
[2024-03-04 21:34:33,721 DEBUG generators.py gen_for_qa l.554] (4/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:33,724 DEBUG generators.py generate l.352] (4/93) Reuse existing Prompt
[2024-03-04 21:34:33,725 DEBUG generators.py generate l.365] (4/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:33,726 DEBUG generators.py generate l.373] (4/93) Reuse post-processing
[2024-03-04 21:34:33,727 INFO generators.py gen_for_qa l.548] (4/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:34:33,728 DEBUG generators.py gen_for_qa l.554] (4/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:33,728 DEBUG generators.py generate l.352] (4/93) Reuse existing Prompt
[2024-03-04 21:34:33,730 DEBUG generators.py generate l.365] (4/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:33,730 DEBUG generators.py generate l.373] (4/93) Reuse post-processing
[2024-03-04 21:34:33,731 INFO generators.py gen_for_qa l.548] (4/93) * Start with LLM "command-nightly"
[2024-03-04 21:34:33,731 DEBUG generators.py gen_for_qa l.554] (4/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:33,733 DEBUG generators.py generate l.352] (4/93) Reuse existing Prompt
[2024-03-04 21:34:33,734 DEBUG generators.py generate l.365] (4/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:33,736 DEBUG generators.py generate l.373] (4/93) Reuse post-processing
[2024-03-04 21:34:33,738 INFO generators.py gen_for_qa l.548] (4/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:34:33,738 DEBUG generators.py generate l.349] (4/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:34:33,738 DEBUG generators.py generate l.358] (4/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:34:35,808 ERROR generators.py complete l.400] (4/93) The following exception occurred with prompt meta={} user="Qui a inventé le bateau à vapeur ?  A)  James Watt B)  Claude de Jouffroy d'Abbans .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:34:35,823 DEBUG generators.py generate l.373] (4/93) Reuse post-processing
[2024-03-04 21:34:35,825 INFO generators.py generate l.477] (4/93) End question "Qui a inventé le bateau à vapeur ?  A)  James Watt B)  Claude de Jouffroy d'Abbans "
[2024-03-04 21:34:35,826 INFO generators.py generate l.475] (5/93) *** AnsGenerator for question "Qui a inventé le parachute ?  A)  Louis-Sébastien Lenormand B)  Sir George Cayley "
[2024-03-04 21:34:35,827 INFO generators.py gen_for_qa l.548] (5/93) * Start with LLM "gpt-4"
[2024-03-04 21:34:35,829 DEBUG generators.py gen_for_qa l.554] (5/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:35,830 DEBUG generators.py generate l.352] (5/93) Reuse existing Prompt
[2024-03-04 21:34:35,832 DEBUG generators.py generate l.365] (5/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:35,833 DEBUG generators.py generate l.373] (5/93) Reuse post-processing
[2024-03-04 21:34:35,837 INFO generators.py gen_for_qa l.548] (5/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:34:35,839 DEBUG generators.py gen_for_qa l.554] (5/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:35,839 DEBUG generators.py generate l.352] (5/93) Reuse existing Prompt
[2024-03-04 21:34:35,840 DEBUG generators.py generate l.365] (5/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:35,842 DEBUG generators.py generate l.373] (5/93) Reuse post-processing
[2024-03-04 21:34:35,842 INFO generators.py gen_for_qa l.548] (5/93) * Start with LLM "gemini-pro"
[2024-03-04 21:34:35,842 DEBUG generators.py gen_for_qa l.554] (5/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:35,845 DEBUG generators.py generate l.352] (5/93) Reuse existing Prompt
[2024-03-04 21:34:35,845 DEBUG generators.py generate l.365] (5/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:35,845 DEBUG generators.py generate l.373] (5/93) Reuse post-processing
[2024-03-04 21:34:35,848 INFO generators.py gen_for_qa l.548] (5/93) * Start with LLM "claude-2.1"
[2024-03-04 21:34:35,848 DEBUG generators.py gen_for_qa l.554] (5/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:35,851 DEBUG generators.py generate l.352] (5/93) Reuse existing Prompt
[2024-03-04 21:34:35,853 DEBUG generators.py generate l.365] (5/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:35,853 DEBUG generators.py generate l.373] (5/93) Reuse post-processing
[2024-03-04 21:34:35,853 INFO generators.py gen_for_qa l.548] (5/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:34:35,856 DEBUG generators.py gen_for_qa l.554] (5/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:35,856 DEBUG generators.py generate l.352] (5/93) Reuse existing Prompt
[2024-03-04 21:34:35,858 DEBUG generators.py generate l.365] (5/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:35,859 DEBUG generators.py generate l.373] (5/93) Reuse post-processing
[2024-03-04 21:34:35,859 INFO generators.py gen_for_qa l.548] (5/93) * Start with LLM "command-nightly"
[2024-03-04 21:34:35,860 DEBUG generators.py gen_for_qa l.554] (5/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:35,861 DEBUG generators.py generate l.352] (5/93) Reuse existing Prompt
[2024-03-04 21:34:35,863 DEBUG generators.py generate l.365] (5/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:35,863 DEBUG generators.py generate l.373] (5/93) Reuse post-processing
[2024-03-04 21:34:35,864 INFO generators.py gen_for_qa l.548] (5/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:34:35,864 DEBUG generators.py generate l.349] (5/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:34:35,866 DEBUG generators.py generate l.358] (5/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:34:37,710 ERROR generators.py complete l.400] (5/93) The following exception occurred with prompt meta={} user="Qui a inventé le parachute ?  A)  Louis-Sébastien Lenormand B)  Sir George Cayley .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:34:37,724 DEBUG generators.py generate l.373] (5/93) Reuse post-processing
[2024-03-04 21:34:37,726 INFO generators.py generate l.477] (5/93) End question "Qui a inventé le parachute ?  A)  Louis-Sébastien Lenormand B)  Sir George Cayley "
[2024-03-04 21:34:37,728 INFO generators.py generate l.475] (6/93) *** AnsGenerator for question "Qui a inventé le vélocipède ?  A)  Pierre Michaux B)  Kirkpatrick Macmillan "
[2024-03-04 21:34:37,729 INFO generators.py gen_for_qa l.548] (6/93) * Start with LLM "gpt-4"
[2024-03-04 21:34:37,731 DEBUG generators.py gen_for_qa l.554] (6/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:37,733 DEBUG generators.py generate l.352] (6/93) Reuse existing Prompt
[2024-03-04 21:34:37,736 DEBUG generators.py generate l.365] (6/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:37,737 DEBUG generators.py generate l.373] (6/93) Reuse post-processing
[2024-03-04 21:34:37,739 INFO generators.py gen_for_qa l.548] (6/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:34:37,740 DEBUG generators.py gen_for_qa l.554] (6/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:37,740 DEBUG generators.py generate l.352] (6/93) Reuse existing Prompt
[2024-03-04 21:34:37,741 DEBUG generators.py generate l.365] (6/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:37,743 DEBUG generators.py generate l.373] (6/93) Reuse post-processing
[2024-03-04 21:34:37,744 INFO generators.py gen_for_qa l.548] (6/93) * Start with LLM "gemini-pro"
[2024-03-04 21:34:37,745 DEBUG generators.py gen_for_qa l.554] (6/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:37,745 DEBUG generators.py generate l.352] (6/93) Reuse existing Prompt
[2024-03-04 21:34:37,747 DEBUG generators.py generate l.365] (6/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:37,747 DEBUG generators.py generate l.373] (6/93) Reuse post-processing
[2024-03-04 21:34:37,748 INFO generators.py gen_for_qa l.548] (6/93) * Start with LLM "claude-2.1"
[2024-03-04 21:34:37,749 DEBUG generators.py gen_for_qa l.554] (6/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:37,751 DEBUG generators.py generate l.352] (6/93) Reuse existing Prompt
[2024-03-04 21:34:37,754 DEBUG generators.py generate l.365] (6/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:37,755 DEBUG generators.py generate l.373] (6/93) Reuse post-processing
[2024-03-04 21:34:37,756 INFO generators.py gen_for_qa l.548] (6/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:34:37,756 DEBUG generators.py gen_for_qa l.554] (6/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:37,757 DEBUG generators.py generate l.352] (6/93) Reuse existing Prompt
[2024-03-04 21:34:37,759 DEBUG generators.py generate l.365] (6/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:37,759 DEBUG generators.py generate l.373] (6/93) Reuse post-processing
[2024-03-04 21:34:37,759 INFO generators.py gen_for_qa l.548] (6/93) * Start with LLM "command-nightly"
[2024-03-04 21:34:37,761 DEBUG generators.py gen_for_qa l.554] (6/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:37,762 DEBUG generators.py generate l.352] (6/93) Reuse existing Prompt
[2024-03-04 21:34:37,762 DEBUG generators.py generate l.365] (6/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:37,764 DEBUG generators.py generate l.373] (6/93) Reuse post-processing
[2024-03-04 21:34:37,765 INFO generators.py gen_for_qa l.548] (6/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:34:37,766 DEBUG generators.py generate l.349] (6/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:34:37,768 DEBUG generators.py generate l.358] (6/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:34:40,692 ERROR generators.py complete l.400] (6/93) The following exception occurred with prompt meta={} user="Qui a inventé le vélocipède ?  A)  Pierre Michaux B)  Kirkpatrick Macmillan .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:34:40,704 DEBUG generators.py generate l.373] (6/93) Reuse post-processing
[2024-03-04 21:34:40,705 INFO generators.py generate l.477] (6/93) End question "Qui a inventé le vélocipède ?  A)  Pierre Michaux B)  Kirkpatrick Macmillan "
[2024-03-04 21:34:40,705 INFO generators.py generate l.475] (7/93) *** AnsGenerator for question "Qui a inventé le cinématographe ?  A)  Auguste et Louis Lumière B)  William Friese-Greene "
[2024-03-04 21:34:40,708 INFO generators.py gen_for_qa l.548] (7/93) * Start with LLM "gpt-4"
[2024-03-04 21:34:40,708 DEBUG generators.py gen_for_qa l.554] (7/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:40,708 DEBUG generators.py generate l.352] (7/93) Reuse existing Prompt
[2024-03-04 21:34:40,710 DEBUG generators.py generate l.365] (7/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:40,712 DEBUG generators.py generate l.373] (7/93) Reuse post-processing
[2024-03-04 21:34:40,713 INFO generators.py gen_for_qa l.548] (7/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:34:40,715 DEBUG generators.py gen_for_qa l.554] (7/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:40,717 DEBUG generators.py generate l.352] (7/93) Reuse existing Prompt
[2024-03-04 21:34:40,717 DEBUG generators.py generate l.365] (7/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:40,717 DEBUG generators.py generate l.373] (7/93) Reuse post-processing
[2024-03-04 21:34:40,721 INFO generators.py gen_for_qa l.548] (7/93) * Start with LLM "gemini-pro"
[2024-03-04 21:34:40,722 DEBUG generators.py gen_for_qa l.554] (7/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:40,723 DEBUG generators.py generate l.352] (7/93) Reuse existing Prompt
[2024-03-04 21:34:40,724 DEBUG generators.py generate l.365] (7/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:40,724 DEBUG generators.py generate l.373] (7/93) Reuse post-processing
[2024-03-04 21:34:40,726 INFO generators.py gen_for_qa l.548] (7/93) * Start with LLM "claude-2.1"
[2024-03-04 21:34:40,726 DEBUG generators.py gen_for_qa l.554] (7/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:40,728 DEBUG generators.py generate l.352] (7/93) Reuse existing Prompt
[2024-03-04 21:34:40,728 DEBUG generators.py generate l.365] (7/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:40,728 DEBUG generators.py generate l.373] (7/93) Reuse post-processing
[2024-03-04 21:34:40,731 INFO generators.py gen_for_qa l.548] (7/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:34:40,732 DEBUG generators.py gen_for_qa l.554] (7/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:40,735 DEBUG generators.py generate l.352] (7/93) Reuse existing Prompt
[2024-03-04 21:34:40,736 DEBUG generators.py generate l.365] (7/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:40,736 DEBUG generators.py generate l.373] (7/93) Reuse post-processing
[2024-03-04 21:34:40,738 INFO generators.py gen_for_qa l.548] (7/93) * Start with LLM "command-nightly"
[2024-03-04 21:34:40,738 DEBUG generators.py gen_for_qa l.554] (7/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:40,740 DEBUG generators.py generate l.352] (7/93) Reuse existing Prompt
[2024-03-04 21:34:40,740 DEBUG generators.py generate l.365] (7/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:40,741 DEBUG generators.py generate l.373] (7/93) Reuse post-processing
[2024-03-04 21:34:40,742 INFO generators.py gen_for_qa l.548] (7/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:34:40,743 DEBUG generators.py generate l.349] (7/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:34:40,744 DEBUG generators.py generate l.358] (7/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:34:44,515 ERROR generators.py complete l.400] (7/93) The following exception occurred with prompt meta={} user="Qui a inventé le cinématographe ?  A)  Auguste et Louis Lumière B)  William Friese-Greene .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:34:44,531 DEBUG generators.py generate l.373] (7/93) Reuse post-processing
[2024-03-04 21:34:44,533 INFO generators.py generate l.477] (7/93) End question "Qui a inventé le cinématographe ?  A)  Auguste et Louis Lumière B)  William Friese-Greene "
[2024-03-04 21:34:44,536 INFO generators.py generate l.475] (8/93) *** AnsGenerator for question "Qui a inventé la machine à coudre ?  A)  Barthélemy Thimonnier B)  Thomas Saint "
[2024-03-04 21:34:44,537 INFO generators.py gen_for_qa l.548] (8/93) * Start with LLM "gpt-4"
[2024-03-04 21:34:44,539 DEBUG generators.py gen_for_qa l.554] (8/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:44,539 DEBUG generators.py generate l.352] (8/93) Reuse existing Prompt
[2024-03-04 21:34:44,539 DEBUG generators.py generate l.365] (8/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:44,539 DEBUG generators.py generate l.373] (8/93) Reuse post-processing
[2024-03-04 21:34:44,544 INFO generators.py gen_for_qa l.548] (8/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:34:44,545 DEBUG generators.py gen_for_qa l.554] (8/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:44,546 DEBUG generators.py generate l.352] (8/93) Reuse existing Prompt
[2024-03-04 21:34:44,547 DEBUG generators.py generate l.365] (8/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:44,549 DEBUG generators.py generate l.373] (8/93) Reuse post-processing
[2024-03-04 21:34:44,551 INFO generators.py gen_for_qa l.548] (8/93) * Start with LLM "gemini-pro"
[2024-03-04 21:34:44,552 DEBUG generators.py gen_for_qa l.554] (8/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:44,554 DEBUG generators.py generate l.352] (8/93) Reuse existing Prompt
[2024-03-04 21:34:44,555 DEBUG generators.py generate l.365] (8/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:44,555 DEBUG generators.py generate l.373] (8/93) Reuse post-processing
[2024-03-04 21:34:44,556 INFO generators.py gen_for_qa l.548] (8/93) * Start with LLM "claude-2.1"
[2024-03-04 21:34:44,557 DEBUG generators.py gen_for_qa l.554] (8/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:44,558 DEBUG generators.py generate l.352] (8/93) Reuse existing Prompt
[2024-03-04 21:34:44,558 DEBUG generators.py generate l.365] (8/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:44,560 DEBUG generators.py generate l.373] (8/93) Reuse post-processing
[2024-03-04 21:34:44,562 INFO generators.py gen_for_qa l.548] (8/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:34:44,563 DEBUG generators.py gen_for_qa l.554] (8/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:44,564 DEBUG generators.py generate l.352] (8/93) Reuse existing Prompt
[2024-03-04 21:34:44,565 DEBUG generators.py generate l.365] (8/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:44,567 DEBUG generators.py generate l.373] (8/93) Reuse post-processing
[2024-03-04 21:34:44,568 INFO generators.py gen_for_qa l.548] (8/93) * Start with LLM "command-nightly"
[2024-03-04 21:34:44,569 DEBUG generators.py gen_for_qa l.554] (8/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:44,571 DEBUG generators.py generate l.352] (8/93) Reuse existing Prompt
[2024-03-04 21:34:44,571 DEBUG generators.py generate l.365] (8/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:44,571 DEBUG generators.py generate l.373] (8/93) Reuse post-processing
[2024-03-04 21:34:44,571 INFO generators.py gen_for_qa l.548] (8/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:34:44,574 DEBUG generators.py generate l.349] (8/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:34:44,575 DEBUG generators.py generate l.358] (8/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:34:47,663 ERROR generators.py complete l.400] (8/93) The following exception occurred with prompt meta={} user="Qui a inventé la machine à coudre ?  A)  Barthélemy Thimonnier B)  Thomas Saint .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:34:47,673 DEBUG generators.py generate l.373] (8/93) Reuse post-processing
[2024-03-04 21:34:47,676 INFO generators.py generate l.477] (8/93) End question "Qui a inventé la machine à coudre ?  A)  Barthélemy Thimonnier B)  Thomas Saint "
[2024-03-04 21:34:47,677 INFO generators.py generate l.475] (9/93) *** AnsGenerator for question "Qui a inventé l'ampoule électrique ?  A)  Joseph Swan B)  Thomas Edison C)  Hiram Maxim "
[2024-03-04 21:34:47,677 INFO generators.py gen_for_qa l.548] (9/93) * Start with LLM "gpt-4"
[2024-03-04 21:34:47,678 DEBUG generators.py gen_for_qa l.554] (9/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:47,680 DEBUG generators.py generate l.352] (9/93) Reuse existing Prompt
[2024-03-04 21:34:47,681 DEBUG generators.py generate l.365] (9/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:47,683 DEBUG generators.py generate l.373] (9/93) Reuse post-processing
[2024-03-04 21:34:47,685 INFO generators.py gen_for_qa l.548] (9/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:34:47,687 DEBUG generators.py gen_for_qa l.554] (9/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:47,688 DEBUG generators.py generate l.352] (9/93) Reuse existing Prompt
[2024-03-04 21:34:47,689 DEBUG generators.py generate l.365] (9/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:47,690 DEBUG generators.py generate l.373] (9/93) Reuse post-processing
[2024-03-04 21:34:47,691 INFO generators.py gen_for_qa l.548] (9/93) * Start with LLM "gemini-pro"
[2024-03-04 21:34:47,693 DEBUG generators.py gen_for_qa l.554] (9/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:47,693 DEBUG generators.py generate l.352] (9/93) Reuse existing Prompt
[2024-03-04 21:34:47,694 DEBUG generators.py generate l.365] (9/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:47,695 DEBUG generators.py generate l.373] (9/93) Reuse post-processing
[2024-03-04 21:34:47,696 INFO generators.py gen_for_qa l.548] (9/93) * Start with LLM "claude-2.1"
[2024-03-04 21:34:47,697 DEBUG generators.py gen_for_qa l.554] (9/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:47,700 DEBUG generators.py generate l.352] (9/93) Reuse existing Prompt
[2024-03-04 21:34:47,701 DEBUG generators.py generate l.365] (9/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:47,701 DEBUG generators.py generate l.373] (9/93) Reuse post-processing
[2024-03-04 21:34:47,703 INFO generators.py gen_for_qa l.548] (9/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:34:47,703 DEBUG generators.py gen_for_qa l.554] (9/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:47,704 DEBUG generators.py generate l.352] (9/93) Reuse existing Prompt
[2024-03-04 21:34:47,706 DEBUG generators.py generate l.365] (9/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:47,706 DEBUG generators.py generate l.373] (9/93) Reuse post-processing
[2024-03-04 21:34:47,707 INFO generators.py gen_for_qa l.548] (9/93) * Start with LLM "command-nightly"
[2024-03-04 21:34:47,708 DEBUG generators.py gen_for_qa l.554] (9/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:47,708 DEBUG generators.py generate l.352] (9/93) Reuse existing Prompt
[2024-03-04 21:34:47,709 DEBUG generators.py generate l.365] (9/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:47,711 DEBUG generators.py generate l.373] (9/93) Reuse post-processing
[2024-03-04 21:34:47,712 INFO generators.py gen_for_qa l.548] (9/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:34:47,713 DEBUG generators.py generate l.349] (9/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:34:47,713 DEBUG generators.py generate l.358] (9/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:34:54,662 ERROR generators.py complete l.400] (9/93) The following exception occurred with prompt meta={} user="Qui a inventé l'ampoule électrique ?  A)  Joseph Swan B)  Thomas Edison C)  Hiram Maxim .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:34:54,674 DEBUG generators.py generate l.373] (9/93) Reuse post-processing
[2024-03-04 21:34:54,676 INFO generators.py generate l.477] (9/93) End question "Qui a inventé l'ampoule électrique ?  A)  Joseph Swan B)  Thomas Edison C)  Hiram Maxim "
[2024-03-04 21:34:54,678 INFO generators.py generate l.475] (10/93) *** AnsGenerator for question "Qui a inventé le téléphone ?  A)  Alexander Graham Bell B)  Elisha Gray C)  Antonio Meucci "
[2024-03-04 21:34:54,680 INFO generators.py gen_for_qa l.548] (10/93) * Start with LLM "gpt-4"
[2024-03-04 21:34:54,682 DEBUG generators.py gen_for_qa l.554] (10/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:54,683 DEBUG generators.py generate l.352] (10/93) Reuse existing Prompt
[2024-03-04 21:34:54,684 DEBUG generators.py generate l.365] (10/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:54,686 DEBUG generators.py generate l.373] (10/93) Reuse post-processing
[2024-03-04 21:34:54,686 INFO generators.py gen_for_qa l.548] (10/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:34:54,687 DEBUG generators.py gen_for_qa l.554] (10/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:54,688 DEBUG generators.py generate l.352] (10/93) Reuse existing Prompt
[2024-03-04 21:34:54,689 DEBUG generators.py generate l.365] (10/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:54,690 DEBUG generators.py generate l.373] (10/93) Reuse post-processing
[2024-03-04 21:34:54,691 INFO generators.py gen_for_qa l.548] (10/93) * Start with LLM "gemini-pro"
[2024-03-04 21:34:54,693 DEBUG generators.py gen_for_qa l.554] (10/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:54,694 DEBUG generators.py generate l.352] (10/93) Reuse existing Prompt
[2024-03-04 21:34:54,695 DEBUG generators.py generate l.365] (10/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:54,696 DEBUG generators.py generate l.373] (10/93) Reuse post-processing
[2024-03-04 21:34:54,698 INFO generators.py gen_for_qa l.548] (10/93) * Start with LLM "claude-2.1"
[2024-03-04 21:34:54,699 DEBUG generators.py gen_for_qa l.554] (10/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:54,700 DEBUG generators.py generate l.352] (10/93) Reuse existing Prompt
[2024-03-04 21:34:54,701 DEBUG generators.py generate l.365] (10/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:54,702 DEBUG generators.py generate l.373] (10/93) Reuse post-processing
[2024-03-04 21:34:54,702 INFO generators.py gen_for_qa l.548] (10/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:34:54,704 DEBUG generators.py gen_for_qa l.554] (10/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:54,704 DEBUG generators.py generate l.352] (10/93) Reuse existing Prompt
[2024-03-04 21:34:54,706 DEBUG generators.py generate l.365] (10/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:54,706 DEBUG generators.py generate l.373] (10/93) Reuse post-processing
[2024-03-04 21:34:54,708 INFO generators.py gen_for_qa l.548] (10/93) * Start with LLM "command-nightly"
[2024-03-04 21:34:54,709 DEBUG generators.py gen_for_qa l.554] (10/93) An Answer has already been generated with this LLM
[2024-03-04 21:34:54,710 DEBUG generators.py generate l.352] (10/93) Reuse existing Prompt
[2024-03-04 21:34:54,711 DEBUG generators.py generate l.365] (10/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:34:54,712 DEBUG generators.py generate l.373] (10/93) Reuse post-processing
[2024-03-04 21:34:54,713 INFO generators.py gen_for_qa l.548] (10/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:34:54,713 DEBUG generators.py generate l.349] (10/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:34:54,716 DEBUG generators.py generate l.358] (10/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:35:00,387 ERROR generators.py complete l.400] (10/93) The following exception occurred with prompt meta={} user="Qui a inventé le téléphone ?  A)  Alexander Graham Bell B)  Elisha Gray C)  Antonio Meucci .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:35:00,411 DEBUG generators.py generate l.373] (10/93) Reuse post-processing
[2024-03-04 21:35:00,412 INFO generators.py generate l.477] (10/93) End question "Qui a inventé le téléphone ?  A)  Alexander Graham Bell B)  Elisha Gray C)  Antonio Meucci "
[2024-03-04 21:35:00,415 INFO generators.py generate l.475] (11/93) *** AnsGenerator for question "Qui a inventé la montgolfière ?  A)  Les frères Montgolfier B)  Richard Crosbie "
[2024-03-04 21:35:00,417 INFO generators.py gen_for_qa l.548] (11/93) * Start with LLM "gpt-4"
[2024-03-04 21:35:00,418 DEBUG generators.py gen_for_qa l.554] (11/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:00,418 DEBUG generators.py generate l.352] (11/93) Reuse existing Prompt
[2024-03-04 21:35:00,418 DEBUG generators.py generate l.365] (11/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:00,423 DEBUG generators.py generate l.373] (11/93) Reuse post-processing
[2024-03-04 21:35:00,424 INFO generators.py gen_for_qa l.548] (11/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:35:00,426 DEBUG generators.py gen_for_qa l.554] (11/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:00,428 DEBUG generators.py generate l.352] (11/93) Reuse existing Prompt
[2024-03-04 21:35:00,430 DEBUG generators.py generate l.365] (11/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:00,432 DEBUG generators.py generate l.373] (11/93) Reuse post-processing
[2024-03-04 21:35:00,433 INFO generators.py gen_for_qa l.548] (11/93) * Start with LLM "gemini-pro"
[2024-03-04 21:35:00,434 DEBUG generators.py gen_for_qa l.554] (11/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:00,434 DEBUG generators.py generate l.352] (11/93) Reuse existing Prompt
[2024-03-04 21:35:00,436 DEBUG generators.py generate l.365] (11/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:00,437 DEBUG generators.py generate l.373] (11/93) Reuse post-processing
[2024-03-04 21:35:00,438 INFO generators.py gen_for_qa l.548] (11/93) * Start with LLM "claude-2.1"
[2024-03-04 21:35:00,438 DEBUG generators.py gen_for_qa l.554] (11/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:00,441 DEBUG generators.py generate l.352] (11/93) Reuse existing Prompt
[2024-03-04 21:35:00,442 DEBUG generators.py generate l.365] (11/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:00,443 DEBUG generators.py generate l.373] (11/93) Reuse post-processing
[2024-03-04 21:35:00,445 INFO generators.py gen_for_qa l.548] (11/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:35:00,446 DEBUG generators.py gen_for_qa l.554] (11/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:00,449 DEBUG generators.py generate l.352] (11/93) Reuse existing Prompt
[2024-03-04 21:35:00,451 DEBUG generators.py generate l.365] (11/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:00,452 DEBUG generators.py generate l.373] (11/93) Reuse post-processing
[2024-03-04 21:35:00,452 INFO generators.py gen_for_qa l.548] (11/93) * Start with LLM "command-nightly"
[2024-03-04 21:35:00,453 DEBUG generators.py gen_for_qa l.554] (11/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:00,453 DEBUG generators.py generate l.352] (11/93) Reuse existing Prompt
[2024-03-04 21:35:00,455 DEBUG generators.py generate l.365] (11/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:00,456 DEBUG generators.py generate l.373] (11/93) Reuse post-processing
[2024-03-04 21:35:00,457 INFO generators.py gen_for_qa l.548] (11/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:35:00,459 DEBUG generators.py generate l.349] (11/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:35:00,460 DEBUG generators.py generate l.358] (11/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:35:02,791 ERROR generators.py complete l.400] (11/93) The following exception occurred with prompt meta={} user="Qui a inventé la montgolfière ?  A)  Les frères Montgolfier B)  Richard Crosbie .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:35:02,813 DEBUG generators.py generate l.373] (11/93) Reuse post-processing
[2024-03-04 21:35:02,817 INFO generators.py generate l.477] (11/93) End question "Qui a inventé la montgolfière ?  A)  Les frères Montgolfier B)  Richard Crosbie "
[2024-03-04 21:35:02,818 INFO generators.py generate l.475] (12/93) *** AnsGenerator for question "Qui a inventé le dirigeable ?  A)  Henri Giffard B)  George Cayley "
[2024-03-04 21:35:02,819 INFO generators.py gen_for_qa l.548] (12/93) * Start with LLM "gpt-4"
[2024-03-04 21:35:02,822 DEBUG generators.py gen_for_qa l.554] (12/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:02,824 DEBUG generators.py generate l.352] (12/93) Reuse existing Prompt
[2024-03-04 21:35:02,825 DEBUG generators.py generate l.365] (12/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:02,826 DEBUG generators.py generate l.373] (12/93) Reuse post-processing
[2024-03-04 21:35:02,828 INFO generators.py gen_for_qa l.548] (12/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:35:02,830 DEBUG generators.py gen_for_qa l.554] (12/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:02,832 DEBUG generators.py generate l.352] (12/93) Reuse existing Prompt
[2024-03-04 21:35:02,832 DEBUG generators.py generate l.365] (12/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:02,834 DEBUG generators.py generate l.373] (12/93) Reuse post-processing
[2024-03-04 21:35:02,835 INFO generators.py gen_for_qa l.548] (12/93) * Start with LLM "gemini-pro"
[2024-03-04 21:35:02,835 DEBUG generators.py gen_for_qa l.554] (12/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:02,836 DEBUG generators.py generate l.352] (12/93) Reuse existing Prompt
[2024-03-04 21:35:02,836 DEBUG generators.py generate l.365] (12/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:02,836 DEBUG generators.py generate l.373] (12/93) Reuse post-processing
[2024-03-04 21:35:02,840 INFO generators.py gen_for_qa l.548] (12/93) * Start with LLM "claude-2.1"
[2024-03-04 21:35:02,841 DEBUG generators.py gen_for_qa l.554] (12/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:02,842 DEBUG generators.py generate l.352] (12/93) Reuse existing Prompt
[2024-03-04 21:35:02,845 DEBUG generators.py generate l.365] (12/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:02,846 DEBUG generators.py generate l.373] (12/93) Reuse post-processing
[2024-03-04 21:35:02,846 INFO generators.py gen_for_qa l.548] (12/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:35:02,846 DEBUG generators.py gen_for_qa l.554] (12/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:02,846 DEBUG generators.py generate l.352] (12/93) Reuse existing Prompt
[2024-03-04 21:35:02,851 DEBUG generators.py generate l.365] (12/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:02,851 DEBUG generators.py generate l.373] (12/93) Reuse post-processing
[2024-03-04 21:35:02,852 INFO generators.py gen_for_qa l.548] (12/93) * Start with LLM "command-nightly"
[2024-03-04 21:35:02,853 DEBUG generators.py gen_for_qa l.554] (12/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:02,854 DEBUG generators.py generate l.352] (12/93) Reuse existing Prompt
[2024-03-04 21:35:02,855 DEBUG generators.py generate l.365] (12/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:02,855 DEBUG generators.py generate l.373] (12/93) Reuse post-processing
[2024-03-04 21:35:02,857 INFO generators.py gen_for_qa l.548] (12/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:35:02,858 DEBUG generators.py generate l.349] (12/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:35:02,860 DEBUG generators.py generate l.358] (12/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:35:04,179 ERROR generators.py complete l.400] (12/93) The following exception occurred with prompt meta={} user="Qui a inventé le dirigeable ?  A)  Henri Giffard B)  George Cayley .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:35:04,193 DEBUG generators.py generate l.373] (12/93) Reuse post-processing
[2024-03-04 21:35:04,195 INFO generators.py generate l.477] (12/93) End question "Qui a inventé le dirigeable ?  A)  Henri Giffard B)  George Cayley "
[2024-03-04 21:35:04,197 INFO generators.py generate l.475] (13/93) *** AnsGenerator for question "Qui a inventé la pile électrique ?  A)  Alessandro Volta B)  Luigi Galvani C)  Charles-François de Cisternay du Fay "
[2024-03-04 21:35:04,199 INFO generators.py gen_for_qa l.548] (13/93) * Start with LLM "gpt-4"
[2024-03-04 21:35:04,199 DEBUG generators.py gen_for_qa l.554] (13/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:04,199 DEBUG generators.py generate l.352] (13/93) Reuse existing Prompt
[2024-03-04 21:35:04,202 DEBUG generators.py generate l.365] (13/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:04,203 DEBUG generators.py generate l.373] (13/93) Reuse post-processing
[2024-03-04 21:35:04,205 INFO generators.py gen_for_qa l.548] (13/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:35:04,207 DEBUG generators.py gen_for_qa l.554] (13/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:04,208 DEBUG generators.py generate l.352] (13/93) Reuse existing Prompt
[2024-03-04 21:35:04,210 DEBUG generators.py generate l.365] (13/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:04,213 DEBUG generators.py generate l.373] (13/93) Reuse post-processing
[2024-03-04 21:35:04,216 INFO generators.py gen_for_qa l.548] (13/93) * Start with LLM "gemini-pro"
[2024-03-04 21:35:04,217 DEBUG generators.py gen_for_qa l.554] (13/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:04,218 DEBUG generators.py generate l.352] (13/93) Reuse existing Prompt
[2024-03-04 21:35:04,219 DEBUG generators.py generate l.365] (13/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:04,220 DEBUG generators.py generate l.373] (13/93) Reuse post-processing
[2024-03-04 21:35:04,222 INFO generators.py gen_for_qa l.548] (13/93) * Start with LLM "claude-2.1"
[2024-03-04 21:35:04,223 DEBUG generators.py gen_for_qa l.554] (13/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:04,225 DEBUG generators.py generate l.352] (13/93) Reuse existing Prompt
[2024-03-04 21:35:04,227 DEBUG generators.py generate l.365] (13/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:04,228 DEBUG generators.py generate l.373] (13/93) Reuse post-processing
[2024-03-04 21:35:04,229 INFO generators.py gen_for_qa l.548] (13/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:35:04,231 DEBUG generators.py gen_for_qa l.554] (13/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:04,231 DEBUG generators.py generate l.352] (13/93) Reuse existing Prompt
[2024-03-04 21:35:04,231 DEBUG generators.py generate l.365] (13/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:04,231 DEBUG generators.py generate l.373] (13/93) Reuse post-processing
[2024-03-04 21:35:04,231 INFO generators.py gen_for_qa l.548] (13/93) * Start with LLM "command-nightly"
[2024-03-04 21:35:04,236 DEBUG generators.py gen_for_qa l.554] (13/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:04,237 DEBUG generators.py generate l.352] (13/93) Reuse existing Prompt
[2024-03-04 21:35:04,238 DEBUG generators.py generate l.365] (13/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:04,238 DEBUG generators.py generate l.373] (13/93) Reuse post-processing
[2024-03-04 21:35:04,240 INFO generators.py gen_for_qa l.548] (13/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:35:04,241 DEBUG generators.py generate l.349] (13/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:35:04,243 DEBUG generators.py generate l.358] (13/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:35:05,585 ERROR generators.py complete l.400] (13/93) The following exception occurred with prompt meta={} user="Qui a inventé la pile électrique ?  A)  Alessandro Volta B)  Luigi Galvani C)  Charles-François de Cisternay du Fay .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:35:05,597 DEBUG generators.py generate l.373] (13/93) Reuse post-processing
[2024-03-04 21:35:05,604 INFO generators.py generate l.477] (13/93) End question "Qui a inventé la pile électrique ?  A)  Alessandro Volta B)  Luigi Galvani C)  Charles-François de Cisternay du Fay "
[2024-03-04 21:35:05,605 INFO generators.py generate l.475] (14/93) *** AnsGenerator for question "Qui a inventé le stéthoscope ?  A)  René Laennec B)  David Littmann "
[2024-03-04 21:35:05,607 INFO generators.py gen_for_qa l.548] (14/93) * Start with LLM "gpt-4"
[2024-03-04 21:35:05,608 DEBUG generators.py gen_for_qa l.554] (14/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:05,609 DEBUG generators.py generate l.352] (14/93) Reuse existing Prompt
[2024-03-04 21:35:05,611 DEBUG generators.py generate l.365] (14/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:05,614 DEBUG generators.py generate l.373] (14/93) Reuse post-processing
[2024-03-04 21:35:05,615 INFO generators.py gen_for_qa l.548] (14/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:35:05,616 DEBUG generators.py gen_for_qa l.554] (14/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:05,617 DEBUG generators.py generate l.352] (14/93) Reuse existing Prompt
[2024-03-04 21:35:05,619 DEBUG generators.py generate l.365] (14/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:05,619 DEBUG generators.py generate l.373] (14/93) Reuse post-processing
[2024-03-04 21:35:05,619 INFO generators.py gen_for_qa l.548] (14/93) * Start with LLM "gemini-pro"
[2024-03-04 21:35:05,621 DEBUG generators.py gen_for_qa l.554] (14/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:05,622 DEBUG generators.py generate l.352] (14/93) Reuse existing Prompt
[2024-03-04 21:35:05,623 DEBUG generators.py generate l.365] (14/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:05,626 DEBUG generators.py generate l.373] (14/93) Reuse post-processing
[2024-03-04 21:35:05,627 INFO generators.py gen_for_qa l.548] (14/93) * Start with LLM "claude-2.1"
[2024-03-04 21:35:05,629 DEBUG generators.py gen_for_qa l.554] (14/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:05,629 DEBUG generators.py generate l.352] (14/93) Reuse existing Prompt
[2024-03-04 21:35:05,631 DEBUG generators.py generate l.365] (14/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:05,631 DEBUG generators.py generate l.373] (14/93) Reuse post-processing
[2024-03-04 21:35:05,631 INFO generators.py gen_for_qa l.548] (14/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:35:05,634 DEBUG generators.py gen_for_qa l.554] (14/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:05,635 DEBUG generators.py generate l.352] (14/93) Reuse existing Prompt
[2024-03-04 21:35:05,636 DEBUG generators.py generate l.365] (14/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:05,636 DEBUG generators.py generate l.373] (14/93) Reuse post-processing
[2024-03-04 21:35:05,638 INFO generators.py gen_for_qa l.548] (14/93) * Start with LLM "command-nightly"
[2024-03-04 21:35:05,638 DEBUG generators.py gen_for_qa l.554] (14/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:05,639 DEBUG generators.py generate l.352] (14/93) Reuse existing Prompt
[2024-03-04 21:35:05,641 DEBUG generators.py generate l.365] (14/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:05,643 DEBUG generators.py generate l.373] (14/93) Reuse post-processing
[2024-03-04 21:35:05,644 INFO generators.py gen_for_qa l.548] (14/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:35:05,645 DEBUG generators.py generate l.349] (14/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:35:05,646 DEBUG generators.py generate l.358] (14/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:35:08,851 ERROR generators.py complete l.400] (14/93) The following exception occurred with prompt meta={} user="Qui a inventé le stéthoscope ?  A)  René Laennec B)  David Littmann .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:35:08,863 DEBUG generators.py generate l.373] (14/93) Reuse post-processing
[2024-03-04 21:35:08,864 INFO generators.py generate l.477] (14/93) End question "Qui a inventé le stéthoscope ?  A)  René Laennec B)  David Littmann "
[2024-03-04 21:35:08,865 INFO generators.py generate l.475] (15/93) *** AnsGenerator for question "Qui a inventé le microscope ?  A)  Zacharias Janssen B)  Galileo Galilei C)  Robert Hooke "
[2024-03-04 21:35:08,866 INFO generators.py gen_for_qa l.548] (15/93) * Start with LLM "gpt-4"
[2024-03-04 21:35:08,867 DEBUG generators.py gen_for_qa l.554] (15/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:08,868 DEBUG generators.py generate l.352] (15/93) Reuse existing Prompt
[2024-03-04 21:35:08,869 DEBUG generators.py generate l.365] (15/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:08,870 DEBUG generators.py generate l.373] (15/93) Reuse post-processing
[2024-03-04 21:35:08,872 INFO generators.py gen_for_qa l.548] (15/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:35:08,873 DEBUG generators.py gen_for_qa l.554] (15/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:08,873 DEBUG generators.py generate l.352] (15/93) Reuse existing Prompt
[2024-03-04 21:35:08,877 DEBUG generators.py generate l.365] (15/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:08,878 DEBUG generators.py generate l.373] (15/93) Reuse post-processing
[2024-03-04 21:35:08,879 INFO generators.py gen_for_qa l.548] (15/93) * Start with LLM "gemini-pro"
[2024-03-04 21:35:08,880 DEBUG generators.py gen_for_qa l.554] (15/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:08,881 DEBUG generators.py generate l.352] (15/93) Reuse existing Prompt
[2024-03-04 21:35:08,881 DEBUG generators.py generate l.365] (15/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:08,883 DEBUG generators.py generate l.373] (15/93) Reuse post-processing
[2024-03-04 21:35:08,884 INFO generators.py gen_for_qa l.548] (15/93) * Start with LLM "claude-2.1"
[2024-03-04 21:35:08,884 DEBUG generators.py gen_for_qa l.554] (15/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:08,886 DEBUG generators.py generate l.352] (15/93) Reuse existing Prompt
[2024-03-04 21:35:08,887 DEBUG generators.py generate l.365] (15/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:08,887 DEBUG generators.py generate l.373] (15/93) Reuse post-processing
[2024-03-04 21:35:08,888 INFO generators.py gen_for_qa l.548] (15/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:35:08,889 DEBUG generators.py gen_for_qa l.554] (15/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:08,892 DEBUG generators.py generate l.352] (15/93) Reuse existing Prompt
[2024-03-04 21:35:08,893 DEBUG generators.py generate l.365] (15/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:08,893 DEBUG generators.py generate l.373] (15/93) Reuse post-processing
[2024-03-04 21:35:08,893 INFO generators.py gen_for_qa l.548] (15/93) * Start with LLM "command-nightly"
[2024-03-04 21:35:08,897 DEBUG generators.py gen_for_qa l.554] (15/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:08,898 DEBUG generators.py generate l.352] (15/93) Reuse existing Prompt
[2024-03-04 21:35:08,898 DEBUG generators.py generate l.365] (15/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:08,899 DEBUG generators.py generate l.373] (15/93) Reuse post-processing
[2024-03-04 21:35:08,900 INFO generators.py gen_for_qa l.548] (15/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:35:08,901 DEBUG generators.py generate l.349] (15/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:35:08,902 DEBUG generators.py generate l.358] (15/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:35:12,850 ERROR generators.py complete l.400] (15/93) The following exception occurred with prompt meta={} user="Qui a inventé le microscope ?  A)  Zacharias Janssen B)  Galileo Galilei C)  Robert Hooke .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:35:12,874 DEBUG generators.py generate l.373] (15/93) Reuse post-processing
[2024-03-04 21:35:12,874 INFO generators.py generate l.477] (15/93) End question "Qui a inventé le microscope ?  A)  Zacharias Janssen B)  Galileo Galilei C)  Robert Hooke "
[2024-03-04 21:35:12,879 INFO generators.py generate l.475] (16/93) *** AnsGenerator for question "Qui a inventé le thermomètre ?  A)  Galileo Galilei B)  Daniel Gabriel Fahrenheit C)  Cornelis Drebbel "
[2024-03-04 21:35:12,880 INFO generators.py gen_for_qa l.548] (16/93) * Start with LLM "gpt-4"
[2024-03-04 21:35:12,881 DEBUG generators.py gen_for_qa l.554] (16/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:12,881 DEBUG generators.py generate l.352] (16/93) Reuse existing Prompt
[2024-03-04 21:35:12,884 DEBUG generators.py generate l.365] (16/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:12,884 DEBUG generators.py generate l.373] (16/93) Reuse post-processing
[2024-03-04 21:35:12,887 INFO generators.py gen_for_qa l.548] (16/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:35:12,890 DEBUG generators.py gen_for_qa l.554] (16/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:12,892 DEBUG generators.py generate l.352] (16/93) Reuse existing Prompt
[2024-03-04 21:35:12,893 DEBUG generators.py generate l.365] (16/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:12,894 DEBUG generators.py generate l.373] (16/93) Reuse post-processing
[2024-03-04 21:35:12,894 INFO generators.py gen_for_qa l.548] (16/93) * Start with LLM "gemini-pro"
[2024-03-04 21:35:12,897 DEBUG generators.py gen_for_qa l.554] (16/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:12,898 DEBUG generators.py generate l.352] (16/93) Reuse existing Prompt
[2024-03-04 21:35:12,899 DEBUG generators.py generate l.365] (16/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:12,900 DEBUG generators.py generate l.373] (16/93) Reuse post-processing
[2024-03-04 21:35:12,901 INFO generators.py gen_for_qa l.548] (16/93) * Start with LLM "claude-2.1"
[2024-03-04 21:35:12,902 DEBUG generators.py gen_for_qa l.554] (16/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:12,904 DEBUG generators.py generate l.352] (16/93) Reuse existing Prompt
[2024-03-04 21:35:12,906 DEBUG generators.py generate l.365] (16/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:12,908 DEBUG generators.py generate l.373] (16/93) Reuse post-processing
[2024-03-04 21:35:12,909 INFO generators.py gen_for_qa l.548] (16/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:35:12,911 DEBUG generators.py gen_for_qa l.554] (16/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:12,911 DEBUG generators.py generate l.352] (16/93) Reuse existing Prompt
[2024-03-04 21:35:12,912 DEBUG generators.py generate l.365] (16/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:12,913 DEBUG generators.py generate l.373] (16/93) Reuse post-processing
[2024-03-04 21:35:12,914 INFO generators.py gen_for_qa l.548] (16/93) * Start with LLM "command-nightly"
[2024-03-04 21:35:12,915 DEBUG generators.py gen_for_qa l.554] (16/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:12,915 DEBUG generators.py generate l.352] (16/93) Reuse existing Prompt
[2024-03-04 21:35:12,915 DEBUG generators.py generate l.365] (16/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:12,918 DEBUG generators.py generate l.373] (16/93) Reuse post-processing
[2024-03-04 21:35:12,918 INFO generators.py gen_for_qa l.548] (16/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:35:12,919 DEBUG generators.py generate l.349] (16/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:35:12,922 DEBUG generators.py generate l.358] (16/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:35:15,728 ERROR generators.py complete l.400] (16/93) The following exception occurred with prompt meta={} user="Qui a inventé le thermomètre ?  A)  Galileo Galilei B)  Daniel Gabriel Fahrenheit C)  Cornelis Drebbel .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:35:15,744 DEBUG generators.py generate l.373] (16/93) Reuse post-processing
[2024-03-04 21:35:15,745 INFO generators.py generate l.477] (16/93) End question "Qui a inventé le thermomètre ?  A)  Galileo Galilei B)  Daniel Gabriel Fahrenheit C)  Cornelis Drebbel "
[2024-03-04 21:35:15,745 INFO generators.py generate l.475] (17/93) *** AnsGenerator for question "Qui a inventé le baromètre ?  A)  Evangelista Torricelli B)  Blaise Pascal "
[2024-03-04 21:35:15,748 INFO generators.py gen_for_qa l.548] (17/93) * Start with LLM "gpt-4"
[2024-03-04 21:35:15,748 DEBUG generators.py gen_for_qa l.554] (17/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:15,751 DEBUG generators.py generate l.352] (17/93) Reuse existing Prompt
[2024-03-04 21:35:15,752 DEBUG generators.py generate l.365] (17/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:15,754 DEBUG generators.py generate l.373] (17/93) Reuse post-processing
[2024-03-04 21:35:15,756 INFO generators.py gen_for_qa l.548] (17/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:35:15,757 DEBUG generators.py gen_for_qa l.554] (17/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:15,759 DEBUG generators.py generate l.352] (17/93) Reuse existing Prompt
[2024-03-04 21:35:15,759 DEBUG generators.py generate l.365] (17/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:15,761 DEBUG generators.py generate l.373] (17/93) Reuse post-processing
[2024-03-04 21:35:15,761 INFO generators.py gen_for_qa l.548] (17/93) * Start with LLM "gemini-pro"
[2024-03-04 21:35:15,762 DEBUG generators.py gen_for_qa l.554] (17/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:15,764 DEBUG generators.py generate l.352] (17/93) Reuse existing Prompt
[2024-03-04 21:35:15,765 DEBUG generators.py generate l.365] (17/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:15,766 DEBUG generators.py generate l.373] (17/93) Reuse post-processing
[2024-03-04 21:35:15,766 INFO generators.py gen_for_qa l.548] (17/93) * Start with LLM "claude-2.1"
[2024-03-04 21:35:15,767 DEBUG generators.py gen_for_qa l.554] (17/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:15,767 DEBUG generators.py generate l.352] (17/93) Reuse existing Prompt
[2024-03-04 21:35:15,770 DEBUG generators.py generate l.365] (17/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:15,772 DEBUG generators.py generate l.373] (17/93) Reuse post-processing
[2024-03-04 21:35:15,774 INFO generators.py gen_for_qa l.548] (17/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:35:15,775 DEBUG generators.py gen_for_qa l.554] (17/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:15,776 DEBUG generators.py generate l.352] (17/93) Reuse existing Prompt
[2024-03-04 21:35:15,776 DEBUG generators.py generate l.365] (17/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:15,776 DEBUG generators.py generate l.373] (17/93) Reuse post-processing
[2024-03-04 21:35:15,778 INFO generators.py gen_for_qa l.548] (17/93) * Start with LLM "command-nightly"
[2024-03-04 21:35:15,778 DEBUG generators.py gen_for_qa l.554] (17/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:15,778 DEBUG generators.py generate l.352] (17/93) Reuse existing Prompt
[2024-03-04 21:35:15,781 DEBUG generators.py generate l.365] (17/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:15,782 DEBUG generators.py generate l.373] (17/93) Reuse post-processing
[2024-03-04 21:35:15,782 INFO generators.py gen_for_qa l.548] (17/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:35:15,783 DEBUG generators.py generate l.349] (17/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:35:15,784 DEBUG generators.py generate l.358] (17/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:35:21,587 ERROR generators.py complete l.400] (17/93) The following exception occurred with prompt meta={} user="Qui a inventé le baromètre ?  A)  Evangelista Torricelli B)  Blaise Pascal .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:35:21,603 DEBUG generators.py generate l.373] (17/93) Reuse post-processing
[2024-03-04 21:35:21,605 INFO generators.py generate l.477] (17/93) End question "Qui a inventé le baromètre ?  A)  Evangelista Torricelli B)  Blaise Pascal "
[2024-03-04 21:35:21,607 INFO generators.py generate l.475] (18/93) *** AnsGenerator for question "Qui a inventé la machine à vapeur à piston ?  A)  Thomas Savery B)  Denis Papin "
[2024-03-04 21:35:21,609 INFO generators.py gen_for_qa l.548] (18/93) * Start with LLM "gpt-4"
[2024-03-04 21:35:21,611 DEBUG generators.py gen_for_qa l.554] (18/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:21,612 DEBUG generators.py generate l.352] (18/93) Reuse existing Prompt
[2024-03-04 21:35:21,613 DEBUG generators.py generate l.365] (18/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:21,615 DEBUG generators.py generate l.373] (18/93) Reuse post-processing
[2024-03-04 21:35:21,616 INFO generators.py gen_for_qa l.548] (18/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:35:21,617 DEBUG generators.py gen_for_qa l.554] (18/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:21,619 DEBUG generators.py generate l.352] (18/93) Reuse existing Prompt
[2024-03-04 21:35:21,622 DEBUG generators.py generate l.365] (18/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:21,623 DEBUG generators.py generate l.373] (18/93) Reuse post-processing
[2024-03-04 21:35:21,624 INFO generators.py gen_for_qa l.548] (18/93) * Start with LLM "gemini-pro"
[2024-03-04 21:35:21,626 DEBUG generators.py gen_for_qa l.554] (18/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:21,627 DEBUG generators.py generate l.352] (18/93) Reuse existing Prompt
[2024-03-04 21:35:21,627 DEBUG generators.py generate l.365] (18/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:21,629 DEBUG generators.py generate l.373] (18/93) Reuse post-processing
[2024-03-04 21:35:21,629 INFO generators.py gen_for_qa l.548] (18/93) * Start with LLM "claude-2.1"
[2024-03-04 21:35:21,631 DEBUG generators.py gen_for_qa l.554] (18/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:21,632 DEBUG generators.py generate l.352] (18/93) Reuse existing Prompt
[2024-03-04 21:35:21,632 DEBUG generators.py generate l.365] (18/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:21,633 DEBUG generators.py generate l.373] (18/93) Reuse post-processing
[2024-03-04 21:35:21,634 INFO generators.py gen_for_qa l.548] (18/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:35:21,636 DEBUG generators.py gen_for_qa l.554] (18/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:21,638 DEBUG generators.py generate l.352] (18/93) Reuse existing Prompt
[2024-03-04 21:35:21,638 DEBUG generators.py generate l.365] (18/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:21,638 DEBUG generators.py generate l.373] (18/93) Reuse post-processing
[2024-03-04 21:35:21,641 INFO generators.py gen_for_qa l.548] (18/93) * Start with LLM "command-nightly"
[2024-03-04 21:35:21,642 DEBUG generators.py gen_for_qa l.554] (18/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:21,642 DEBUG generators.py generate l.352] (18/93) Reuse existing Prompt
[2024-03-04 21:35:21,643 DEBUG generators.py generate l.365] (18/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:21,645 DEBUG generators.py generate l.373] (18/93) Reuse post-processing
[2024-03-04 21:35:21,646 INFO generators.py gen_for_qa l.548] (18/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:35:21,647 DEBUG generators.py generate l.349] (18/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:35:21,647 DEBUG generators.py generate l.358] (18/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:35:26,944 ERROR generators.py complete l.400] (18/93) The following exception occurred with prompt meta={} user="Qui a inventé la machine à vapeur à piston ?  A)  Thomas Savery B)  Denis Papin .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:35:26,957 DEBUG generators.py generate l.373] (18/93) Reuse post-processing
[2024-03-04 21:35:26,959 INFO generators.py generate l.477] (18/93) End question "Qui a inventé la machine à vapeur à piston ?  A)  Thomas Savery B)  Denis Papin "
[2024-03-04 21:35:26,960 INFO generators.py generate l.475] (19/93) *** AnsGenerator for question "Qui a inventé la locomotive à vapeur ?  A)  Richard Trevithick B)  George Stephenson C)  Nicolas-Joseph Cugnot "
[2024-03-04 21:35:26,961 INFO generators.py gen_for_qa l.548] (19/93) * Start with LLM "gpt-4"
[2024-03-04 21:35:26,961 DEBUG generators.py gen_for_qa l.554] (19/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:26,964 DEBUG generators.py generate l.352] (19/93) Reuse existing Prompt
[2024-03-04 21:35:26,965 DEBUG generators.py generate l.365] (19/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:26,968 DEBUG generators.py generate l.373] (19/93) Reuse post-processing
[2024-03-04 21:35:26,970 INFO generators.py gen_for_qa l.548] (19/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:35:26,972 DEBUG generators.py gen_for_qa l.554] (19/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:26,973 DEBUG generators.py generate l.352] (19/93) Reuse existing Prompt
[2024-03-04 21:35:26,973 DEBUG generators.py generate l.365] (19/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:26,975 DEBUG generators.py generate l.373] (19/93) Reuse post-processing
[2024-03-04 21:35:26,975 INFO generators.py gen_for_qa l.548] (19/93) * Start with LLM "gemini-pro"
[2024-03-04 21:35:26,977 DEBUG generators.py gen_for_qa l.554] (19/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:26,977 DEBUG generators.py generate l.352] (19/93) Reuse existing Prompt
[2024-03-04 21:35:26,980 DEBUG generators.py generate l.365] (19/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:26,981 DEBUG generators.py generate l.373] (19/93) Reuse post-processing
[2024-03-04 21:35:26,982 INFO generators.py gen_for_qa l.548] (19/93) * Start with LLM "claude-2.1"
[2024-03-04 21:35:26,984 DEBUG generators.py gen_for_qa l.554] (19/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:26,985 DEBUG generators.py generate l.352] (19/93) Reuse existing Prompt
[2024-03-04 21:35:26,985 DEBUG generators.py generate l.365] (19/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:26,990 DEBUG generators.py generate l.373] (19/93) Reuse post-processing
[2024-03-04 21:35:26,991 INFO generators.py gen_for_qa l.548] (19/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:35:26,991 DEBUG generators.py gen_for_qa l.554] (19/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:26,992 DEBUG generators.py generate l.352] (19/93) Reuse existing Prompt
[2024-03-04 21:35:26,993 DEBUG generators.py generate l.365] (19/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:26,994 DEBUG generators.py generate l.373] (19/93) Reuse post-processing
[2024-03-04 21:35:26,994 INFO generators.py gen_for_qa l.548] (19/93) * Start with LLM "command-nightly"
[2024-03-04 21:35:26,994 DEBUG generators.py gen_for_qa l.554] (19/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:26,994 DEBUG generators.py generate l.352] (19/93) Reuse existing Prompt
[2024-03-04 21:35:26,999 DEBUG generators.py generate l.365] (19/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:27,001 DEBUG generators.py generate l.373] (19/93) Reuse post-processing
[2024-03-04 21:35:27,003 INFO generators.py gen_for_qa l.548] (19/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:35:27,004 DEBUG generators.py generate l.349] (19/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:35:27,006 DEBUG generators.py generate l.358] (19/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:35:28,568 ERROR generators.py complete l.400] (19/93) The following exception occurred with prompt meta={} user="Qui a inventé la locomotive à vapeur ?  A)  Richard Trevithick B)  George Stephenson C)  Nicolas-Joseph Cugnot .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:35:28,585 DEBUG generators.py generate l.373] (19/93) Reuse post-processing
[2024-03-04 21:35:28,587 INFO generators.py generate l.477] (19/93) End question "Qui a inventé la locomotive à vapeur ?  A)  Richard Trevithick B)  George Stephenson C)  Nicolas-Joseph Cugnot "
[2024-03-04 21:35:28,587 INFO generators.py generate l.475] (20/93) *** AnsGenerator for question "Qui a inventé le moteur à combustion interne ?  A)  Étienne Lenoir B)  Nikolaus Otto "
[2024-03-04 21:35:28,588 INFO generators.py gen_for_qa l.548] (20/93) * Start with LLM "gpt-4"
[2024-03-04 21:35:28,590 DEBUG generators.py gen_for_qa l.554] (20/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:28,591 DEBUG generators.py generate l.352] (20/93) Reuse existing Prompt
[2024-03-04 21:35:28,593 DEBUG generators.py generate l.365] (20/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:28,594 DEBUG generators.py generate l.373] (20/93) Reuse post-processing
[2024-03-04 21:35:28,594 INFO generators.py gen_for_qa l.548] (20/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:35:28,595 DEBUG generators.py gen_for_qa l.554] (20/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:28,596 DEBUG generators.py generate l.352] (20/93) Reuse existing Prompt
[2024-03-04 21:35:28,597 DEBUG generators.py generate l.365] (20/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:28,598 DEBUG generators.py generate l.373] (20/93) Reuse post-processing
[2024-03-04 21:35:28,600 INFO generators.py gen_for_qa l.548] (20/93) * Start with LLM "gemini-pro"
[2024-03-04 21:35:28,601 DEBUG generators.py gen_for_qa l.554] (20/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:28,601 DEBUG generators.py generate l.352] (20/93) Reuse existing Prompt
[2024-03-04 21:35:28,604 DEBUG generators.py generate l.365] (20/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:28,605 DEBUG generators.py generate l.373] (20/93) Reuse post-processing
[2024-03-04 21:35:28,606 INFO generators.py gen_for_qa l.548] (20/93) * Start with LLM "claude-2.1"
[2024-03-04 21:35:28,607 DEBUG generators.py gen_for_qa l.554] (20/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:28,607 DEBUG generators.py generate l.352] (20/93) Reuse existing Prompt
[2024-03-04 21:35:28,609 DEBUG generators.py generate l.365] (20/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:28,609 DEBUG generators.py generate l.373] (20/93) Reuse post-processing
[2024-03-04 21:35:28,610 INFO generators.py gen_for_qa l.548] (20/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:35:28,612 DEBUG generators.py gen_for_qa l.554] (20/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:28,612 DEBUG generators.py generate l.352] (20/93) Reuse existing Prompt
[2024-03-04 21:35:28,612 DEBUG generators.py generate l.365] (20/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:28,614 DEBUG generators.py generate l.373] (20/93) Reuse post-processing
[2024-03-04 21:35:28,616 INFO generators.py gen_for_qa l.548] (20/93) * Start with LLM "command-nightly"
[2024-03-04 21:35:28,617 DEBUG generators.py gen_for_qa l.554] (20/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:28,619 DEBUG generators.py generate l.352] (20/93) Reuse existing Prompt
[2024-03-04 21:35:28,620 DEBUG generators.py generate l.365] (20/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:28,621 DEBUG generators.py generate l.373] (20/93) Reuse post-processing
[2024-03-04 21:35:28,621 INFO generators.py gen_for_qa l.548] (20/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:35:28,622 DEBUG generators.py generate l.349] (20/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:35:28,624 DEBUG generators.py generate l.358] (20/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:35:31,596 ERROR generators.py complete l.400] (20/93) The following exception occurred with prompt meta={} user="Qui a inventé le moteur à combustion interne ?  A)  Étienne Lenoir B)  Nikolaus Otto .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:35:31,625 DEBUG generators.py generate l.373] (20/93) Reuse post-processing
[2024-03-04 21:35:31,626 INFO generators.py generate l.477] (20/93) End question "Qui a inventé le moteur à combustion interne ?  A)  Étienne Lenoir B)  Nikolaus Otto "
[2024-03-04 21:35:31,628 INFO generators.py generate l.475] (21/93) *** AnsGenerator for question "Qui a inventé le réfrigérateur ?  A)  John Gorrie B)  Carl von Linde C)  James Harrison "
[2024-03-04 21:35:31,630 INFO generators.py gen_for_qa l.548] (21/93) * Start with LLM "gpt-4"
[2024-03-04 21:35:31,632 DEBUG generators.py gen_for_qa l.554] (21/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:31,634 DEBUG generators.py generate l.352] (21/93) Reuse existing Prompt
[2024-03-04 21:35:31,636 DEBUG generators.py generate l.365] (21/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:31,637 DEBUG generators.py generate l.373] (21/93) Reuse post-processing
[2024-03-04 21:35:31,638 INFO generators.py gen_for_qa l.548] (21/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:35:31,638 DEBUG generators.py gen_for_qa l.554] (21/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:31,638 DEBUG generators.py generate l.352] (21/93) Reuse existing Prompt
[2024-03-04 21:35:31,642 DEBUG generators.py generate l.365] (21/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:31,642 DEBUG generators.py generate l.373] (21/93) Reuse post-processing
[2024-03-04 21:35:31,644 INFO generators.py gen_for_qa l.548] (21/93) * Start with LLM "gemini-pro"
[2024-03-04 21:35:31,645 DEBUG generators.py gen_for_qa l.554] (21/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:31,646 DEBUG generators.py generate l.352] (21/93) Reuse existing Prompt
[2024-03-04 21:35:31,649 DEBUG generators.py generate l.365] (21/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:31,650 DEBUG generators.py generate l.373] (21/93) Reuse post-processing
[2024-03-04 21:35:31,652 INFO generators.py gen_for_qa l.548] (21/93) * Start with LLM "claude-2.1"
[2024-03-04 21:35:31,653 DEBUG generators.py gen_for_qa l.554] (21/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:31,655 DEBUG generators.py generate l.352] (21/93) Reuse existing Prompt
[2024-03-04 21:35:31,656 DEBUG generators.py generate l.365] (21/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:31,656 DEBUG generators.py generate l.373] (21/93) Reuse post-processing
[2024-03-04 21:35:31,657 INFO generators.py gen_for_qa l.548] (21/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:35:31,658 DEBUG generators.py gen_for_qa l.554] (21/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:31,659 DEBUG generators.py generate l.352] (21/93) Reuse existing Prompt
[2024-03-04 21:35:31,660 DEBUG generators.py generate l.365] (21/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:31,660 DEBUG generators.py generate l.373] (21/93) Reuse post-processing
[2024-03-04 21:35:31,662 INFO generators.py gen_for_qa l.548] (21/93) * Start with LLM "command-nightly"
[2024-03-04 21:35:31,662 DEBUG generators.py gen_for_qa l.554] (21/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:31,663 DEBUG generators.py generate l.352] (21/93) Reuse existing Prompt
[2024-03-04 21:35:31,666 DEBUG generators.py generate l.365] (21/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:31,667 DEBUG generators.py generate l.373] (21/93) Reuse post-processing
[2024-03-04 21:35:31,669 INFO generators.py gen_for_qa l.548] (21/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:35:31,670 DEBUG generators.py generate l.349] (21/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:35:31,671 DEBUG generators.py generate l.358] (21/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:35:33,052 ERROR generators.py complete l.400] (21/93) The following exception occurred with prompt meta={} user="Qui a inventé le réfrigérateur ?  A)  John Gorrie B)  Carl von Linde C)  James Harrison .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:35:33,063 DEBUG generators.py generate l.373] (21/93) Reuse post-processing
[2024-03-04 21:35:33,065 INFO generators.py generate l.477] (21/93) End question "Qui a inventé le réfrigérateur ?  A)  John Gorrie B)  Carl von Linde C)  James Harrison "
[2024-03-04 21:35:33,066 INFO generators.py generate l.475] (22/93) *** AnsGenerator for question "Qui a inventé le transformateur électrique ?  A)  Lucien Gaulard et John Dixon Gibbs B)  Ottó Bláthy, Miksa Déri et Károly Zipernowsky "
[2024-03-04 21:35:33,068 INFO generators.py gen_for_qa l.548] (22/93) * Start with LLM "gpt-4"
[2024-03-04 21:35:33,069 DEBUG generators.py gen_for_qa l.554] (22/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:33,071 DEBUG generators.py generate l.352] (22/93) Reuse existing Prompt
[2024-03-04 21:35:33,071 DEBUG generators.py generate l.365] (22/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:33,072 DEBUG generators.py generate l.373] (22/93) Reuse post-processing
[2024-03-04 21:35:33,074 INFO generators.py gen_for_qa l.548] (22/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:35:33,074 DEBUG generators.py gen_for_qa l.554] (22/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:33,075 DEBUG generators.py generate l.352] (22/93) Reuse existing Prompt
[2024-03-04 21:35:33,075 DEBUG generators.py generate l.365] (22/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:33,077 DEBUG generators.py generate l.373] (22/93) Reuse post-processing
[2024-03-04 21:35:33,077 INFO generators.py gen_for_qa l.548] (22/93) * Start with LLM "gemini-pro"
[2024-03-04 21:35:33,080 DEBUG generators.py gen_for_qa l.554] (22/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:33,081 DEBUG generators.py generate l.352] (22/93) Reuse existing Prompt
[2024-03-04 21:35:33,082 DEBUG generators.py generate l.365] (22/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:33,082 DEBUG generators.py generate l.373] (22/93) Reuse post-processing
[2024-03-04 21:35:33,084 INFO generators.py gen_for_qa l.548] (22/93) * Start with LLM "claude-2.1"
[2024-03-04 21:35:33,085 DEBUG generators.py gen_for_qa l.554] (22/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:33,087 DEBUG generators.py generate l.352] (22/93) Reuse existing Prompt
[2024-03-04 21:35:33,087 DEBUG generators.py generate l.365] (22/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:33,089 DEBUG generators.py generate l.373] (22/93) Reuse post-processing
[2024-03-04 21:35:33,089 INFO generators.py gen_for_qa l.548] (22/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:35:33,089 DEBUG generators.py gen_for_qa l.554] (22/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:33,092 DEBUG generators.py generate l.352] (22/93) Reuse existing Prompt
[2024-03-04 21:35:33,092 DEBUG generators.py generate l.365] (22/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:33,093 DEBUG generators.py generate l.373] (22/93) Reuse post-processing
[2024-03-04 21:35:33,095 INFO generators.py gen_for_qa l.548] (22/93) * Start with LLM "command-nightly"
[2024-03-04 21:35:33,095 DEBUG generators.py gen_for_qa l.554] (22/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:33,096 DEBUG generators.py generate l.352] (22/93) Reuse existing Prompt
[2024-03-04 21:35:33,097 DEBUG generators.py generate l.365] (22/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:33,098 DEBUG generators.py generate l.373] (22/93) Reuse post-processing
[2024-03-04 21:35:33,098 INFO generators.py gen_for_qa l.548] (22/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:35:33,102 DEBUG generators.py generate l.349] (22/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:35:33,103 DEBUG generators.py generate l.358] (22/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:35:35,512 ERROR generators.py complete l.400] (22/93) The following exception occurred with prompt meta={} user="Qui a inventé le transformateur électrique ?  A)  Lucien Gaulard et John Dixon Gibbs B)  Ottó Bláthy, Miksa Déri et Károly Zipernowsky .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:35:35,529 DEBUG generators.py generate l.373] (22/93) Reuse post-processing
[2024-03-04 21:35:35,531 INFO generators.py generate l.477] (22/93) End question "Qui a inventé le transformateur électrique ?  A)  Lucien Gaulard et John Dixon Gibbs B)  Ottó Bláthy, Miksa Déri et Károly Zipernowsky "
[2024-03-04 21:35:35,533 INFO generators.py generate l.475] (23/93) *** AnsGenerator for question "Qui a inventé le phonographe ?  A)  Thomas Edison B)  Charles Cros "
[2024-03-04 21:35:35,534 INFO generators.py gen_for_qa l.548] (23/93) * Start with LLM "gpt-4"
[2024-03-04 21:35:35,537 DEBUG generators.py gen_for_qa l.554] (23/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:35,540 DEBUG generators.py generate l.352] (23/93) Reuse existing Prompt
[2024-03-04 21:35:35,540 DEBUG generators.py generate l.365] (23/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:35,542 DEBUG generators.py generate l.373] (23/93) Reuse post-processing
[2024-03-04 21:35:35,543 INFO generators.py gen_for_qa l.548] (23/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:35:35,544 DEBUG generators.py gen_for_qa l.554] (23/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:35,546 DEBUG generators.py generate l.352] (23/93) Reuse existing Prompt
[2024-03-04 21:35:35,548 DEBUG generators.py generate l.365] (23/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:35,550 DEBUG generators.py generate l.373] (23/93) Reuse post-processing
[2024-03-04 21:35:35,552 INFO generators.py gen_for_qa l.548] (23/93) * Start with LLM "gemini-pro"
[2024-03-04 21:35:35,553 DEBUG generators.py gen_for_qa l.554] (23/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:35,553 DEBUG generators.py generate l.352] (23/93) Reuse existing Prompt
[2024-03-04 21:35:35,555 DEBUG generators.py generate l.365] (23/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:35,556 DEBUG generators.py generate l.373] (23/93) Reuse post-processing
[2024-03-04 21:35:35,556 INFO generators.py gen_for_qa l.548] (23/93) * Start with LLM "claude-2.1"
[2024-03-04 21:35:35,557 DEBUG generators.py gen_for_qa l.554] (23/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:35,559 DEBUG generators.py generate l.352] (23/93) Reuse existing Prompt
[2024-03-04 21:35:35,559 DEBUG generators.py generate l.365] (23/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:35,561 DEBUG generators.py generate l.373] (23/93) Reuse post-processing
[2024-03-04 21:35:35,561 INFO generators.py gen_for_qa l.548] (23/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:35:35,562 DEBUG generators.py gen_for_qa l.554] (23/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:35,563 DEBUG generators.py generate l.352] (23/93) Reuse existing Prompt
[2024-03-04 21:35:35,564 DEBUG generators.py generate l.365] (23/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:35,566 DEBUG generators.py generate l.373] (23/93) Reuse post-processing
[2024-03-04 21:35:35,568 INFO generators.py gen_for_qa l.548] (23/93) * Start with LLM "command-nightly"
[2024-03-04 21:35:35,569 DEBUG generators.py gen_for_qa l.554] (23/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:35,570 DEBUG generators.py generate l.352] (23/93) Reuse existing Prompt
[2024-03-04 21:35:35,571 DEBUG generators.py generate l.365] (23/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:35,572 DEBUG generators.py generate l.373] (23/93) Reuse post-processing
[2024-03-04 21:35:35,573 INFO generators.py gen_for_qa l.548] (23/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:35:35,573 DEBUG generators.py generate l.349] (23/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:35:35,574 DEBUG generators.py generate l.358] (23/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:35:38,567 ERROR generators.py complete l.400] (23/93) The following exception occurred with prompt meta={} user="Qui a inventé le phonographe ?  A)  Thomas Edison B)  Charles Cros .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:35:38,578 DEBUG generators.py generate l.373] (23/93) Reuse post-processing
[2024-03-04 21:35:38,580 INFO generators.py generate l.477] (23/93) End question "Qui a inventé le phonographe ?  A)  Thomas Edison B)  Charles Cros "
[2024-03-04 21:35:38,581 INFO generators.py generate l.475] (24/93) *** AnsGenerator for question " Qui a découvert l'Amérique ?  A)  Christophe Colomb B)  Leif Erikson "
[2024-03-04 21:35:38,583 INFO generators.py gen_for_qa l.548] (24/93) * Start with LLM "gpt-4"
[2024-03-04 21:35:38,585 DEBUG generators.py gen_for_qa l.554] (24/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:38,585 DEBUG generators.py generate l.352] (24/93) Reuse existing Prompt
[2024-03-04 21:35:38,586 DEBUG generators.py generate l.365] (24/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:38,586 DEBUG generators.py generate l.373] (24/93) Reuse post-processing
[2024-03-04 21:35:38,588 INFO generators.py gen_for_qa l.548] (24/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:35:38,590 DEBUG generators.py gen_for_qa l.554] (24/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:38,591 DEBUG generators.py generate l.352] (24/93) Reuse existing Prompt
[2024-03-04 21:35:38,591 DEBUG generators.py generate l.365] (24/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:38,592 DEBUG generators.py generate l.373] (24/93) Reuse post-processing
[2024-03-04 21:35:38,593 INFO generators.py gen_for_qa l.548] (24/93) * Start with LLM "gemini-pro"
[2024-03-04 21:35:38,594 DEBUG generators.py gen_for_qa l.554] (24/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:38,594 DEBUG generators.py generate l.352] (24/93) Reuse existing Prompt
[2024-03-04 21:35:38,596 DEBUG generators.py generate l.365] (24/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:38,596 DEBUG generators.py generate l.373] (24/93) Reuse post-processing
[2024-03-04 21:35:38,596 INFO generators.py gen_for_qa l.548] (24/93) * Start with LLM "claude-2.1"
[2024-03-04 21:35:38,601 DEBUG generators.py gen_for_qa l.554] (24/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:38,601 DEBUG generators.py generate l.352] (24/93) Reuse existing Prompt
[2024-03-04 21:35:38,602 DEBUG generators.py generate l.365] (24/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:38,603 DEBUG generators.py generate l.373] (24/93) Reuse post-processing
[2024-03-04 21:35:38,604 INFO generators.py gen_for_qa l.548] (24/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:35:38,604 DEBUG generators.py gen_for_qa l.554] (24/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:38,605 DEBUG generators.py generate l.352] (24/93) Reuse existing Prompt
[2024-03-04 21:35:38,606 DEBUG generators.py generate l.365] (24/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:38,607 DEBUG generators.py generate l.373] (24/93) Reuse post-processing
[2024-03-04 21:35:38,608 INFO generators.py gen_for_qa l.548] (24/93) * Start with LLM "command-nightly"
[2024-03-04 21:35:38,609 DEBUG generators.py gen_for_qa l.554] (24/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:38,609 DEBUG generators.py generate l.352] (24/93) Reuse existing Prompt
[2024-03-04 21:35:38,611 DEBUG generators.py generate l.365] (24/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:38,611 DEBUG generators.py generate l.373] (24/93) Reuse post-processing
[2024-03-04 21:35:38,613 INFO generators.py gen_for_qa l.548] (24/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:35:38,614 DEBUG generators.py generate l.349] (24/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:35:38,616 DEBUG generators.py generate l.358] (24/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:35:42,071 ERROR generators.py complete l.400] (24/93) The following exception occurred with prompt meta={} user=" Qui a découvert l'Amérique ?  A)  Christophe Colomb B)  Leif Erikson .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:35:42,086 DEBUG generators.py generate l.373] (24/93) Reuse post-processing
[2024-03-04 21:35:42,088 INFO generators.py generate l.477] (24/93) End question " Qui a découvert l'Amérique ?  A)  Christophe Colomb B)  Leif Erikson "
[2024-03-04 21:35:42,088 INFO generators.py generate l.475] (25/93) *** AnsGenerator for question " Qui a inventé l'avion ?  A)  Orville et Wilbur Wright B)  Clément Ader C)  Gustave Whitehead "
[2024-03-04 21:35:42,090 INFO generators.py gen_for_qa l.548] (25/93) * Start with LLM "gpt-4"
[2024-03-04 21:35:42,090 DEBUG generators.py gen_for_qa l.554] (25/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:42,093 DEBUG generators.py generate l.352] (25/93) Reuse existing Prompt
[2024-03-04 21:35:42,094 DEBUG generators.py generate l.365] (25/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:42,095 DEBUG generators.py generate l.373] (25/93) Reuse post-processing
[2024-03-04 21:35:42,097 INFO generators.py gen_for_qa l.548] (25/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:35:42,099 DEBUG generators.py gen_for_qa l.554] (25/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:42,100 DEBUG generators.py generate l.352] (25/93) Reuse existing Prompt
[2024-03-04 21:35:42,102 DEBUG generators.py generate l.365] (25/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:42,103 DEBUG generators.py generate l.373] (25/93) Reuse post-processing
[2024-03-04 21:35:42,104 INFO generators.py gen_for_qa l.548] (25/93) * Start with LLM "gemini-pro"
[2024-03-04 21:35:42,104 DEBUG generators.py gen_for_qa l.554] (25/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:42,106 DEBUG generators.py generate l.352] (25/93) Reuse existing Prompt
[2024-03-04 21:35:42,106 DEBUG generators.py generate l.365] (25/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:42,106 DEBUG generators.py generate l.373] (25/93) Reuse post-processing
[2024-03-04 21:35:42,109 INFO generators.py gen_for_qa l.548] (25/93) * Start with LLM "claude-2.1"
[2024-03-04 21:35:42,109 DEBUG generators.py gen_for_qa l.554] (25/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:42,111 DEBUG generators.py generate l.352] (25/93) Reuse existing Prompt
[2024-03-04 21:35:42,112 DEBUG generators.py generate l.365] (25/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:42,113 DEBUG generators.py generate l.373] (25/93) Reuse post-processing
[2024-03-04 21:35:42,115 INFO generators.py gen_for_qa l.548] (25/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:35:42,116 DEBUG generators.py gen_for_qa l.554] (25/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:42,117 DEBUG generators.py generate l.352] (25/93) Reuse existing Prompt
[2024-03-04 21:35:42,118 DEBUG generators.py generate l.365] (25/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:42,119 DEBUG generators.py generate l.373] (25/93) Reuse post-processing
[2024-03-04 21:35:42,120 INFO generators.py gen_for_qa l.548] (25/93) * Start with LLM "command-nightly"
[2024-03-04 21:35:42,120 DEBUG generators.py gen_for_qa l.554] (25/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:42,121 DEBUG generators.py generate l.352] (25/93) Reuse existing Prompt
[2024-03-04 21:35:42,121 DEBUG generators.py generate l.365] (25/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:42,121 DEBUG generators.py generate l.373] (25/93) Reuse post-processing
[2024-03-04 21:35:42,121 INFO generators.py gen_for_qa l.548] (25/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:35:42,125 DEBUG generators.py generate l.349] (25/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:35:42,126 DEBUG generators.py generate l.358] (25/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:35:44,674 ERROR generators.py complete l.400] (25/93) The following exception occurred with prompt meta={} user=" Qui a inventé l'avion ?  A)  Orville et Wilbur Wright B)  Clément Ader C)  Gustave Whitehead .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:35:44,684 DEBUG generators.py generate l.373] (25/93) Reuse post-processing
[2024-03-04 21:35:44,685 INFO generators.py generate l.477] (25/93) End question " Qui a inventé l'avion ?  A)  Orville et Wilbur Wright B)  Clément Ader C)  Gustave Whitehead "
[2024-03-04 21:35:44,685 INFO generators.py generate l.475] (26/93) *** AnsGenerator for question " Qui a inventé la radio ?  A)  Guglielmo Marconi B)  Nikola Tesla C)  Oliver Lodge "
[2024-03-04 21:35:44,687 INFO generators.py gen_for_qa l.548] (26/93) * Start with LLM "gpt-4"
[2024-03-04 21:35:44,687 DEBUG generators.py gen_for_qa l.554] (26/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:44,689 DEBUG generators.py generate l.352] (26/93) Reuse existing Prompt
[2024-03-04 21:35:44,689 DEBUG generators.py generate l.365] (26/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:44,691 DEBUG generators.py generate l.373] (26/93) Reuse post-processing
[2024-03-04 21:35:44,692 INFO generators.py gen_for_qa l.548] (26/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:35:44,693 DEBUG generators.py gen_for_qa l.554] (26/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:44,695 DEBUG generators.py generate l.352] (26/93) Reuse existing Prompt
[2024-03-04 21:35:44,696 DEBUG generators.py generate l.365] (26/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:44,698 DEBUG generators.py generate l.373] (26/93) Reuse post-processing
[2024-03-04 21:35:44,698 INFO generators.py gen_for_qa l.548] (26/93) * Start with LLM "gemini-pro"
[2024-03-04 21:35:44,698 DEBUG generators.py gen_for_qa l.554] (26/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:44,698 DEBUG generators.py generate l.352] (26/93) Reuse existing Prompt
[2024-03-04 21:35:44,702 DEBUG generators.py generate l.365] (26/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:44,703 DEBUG generators.py generate l.373] (26/93) Reuse post-processing
[2024-03-04 21:35:44,704 INFO generators.py gen_for_qa l.548] (26/93) * Start with LLM "claude-2.1"
[2024-03-04 21:35:44,704 DEBUG generators.py gen_for_qa l.554] (26/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:44,705 DEBUG generators.py generate l.352] (26/93) Reuse existing Prompt
[2024-03-04 21:35:44,706 DEBUG generators.py generate l.365] (26/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:44,707 DEBUG generators.py generate l.373] (26/93) Reuse post-processing
[2024-03-04 21:35:44,707 INFO generators.py gen_for_qa l.548] (26/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:35:44,709 DEBUG generators.py gen_for_qa l.554] (26/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:44,710 DEBUG generators.py generate l.352] (26/93) Reuse existing Prompt
[2024-03-04 21:35:44,712 DEBUG generators.py generate l.365] (26/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:44,714 DEBUG generators.py generate l.373] (26/93) Reuse post-processing
[2024-03-04 21:35:44,715 INFO generators.py gen_for_qa l.548] (26/93) * Start with LLM "command-nightly"
[2024-03-04 21:35:44,716 DEBUG generators.py gen_for_qa l.554] (26/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:44,716 DEBUG generators.py generate l.352] (26/93) Reuse existing Prompt
[2024-03-04 21:35:44,717 DEBUG generators.py generate l.365] (26/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:44,718 DEBUG generators.py generate l.373] (26/93) Reuse post-processing
[2024-03-04 21:35:44,719 INFO generators.py gen_for_qa l.548] (26/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:35:44,719 DEBUG generators.py generate l.349] (26/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:35:44,719 DEBUG generators.py generate l.358] (26/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:35:47,390 ERROR generators.py complete l.400] (26/93) The following exception occurred with prompt meta={} user=" Qui a inventé la radio ?  A)  Guglielmo Marconi B)  Nikola Tesla C)  Oliver Lodge .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:35:47,397 DEBUG generators.py generate l.373] (26/93) Reuse post-processing
[2024-03-04 21:35:47,402 INFO generators.py generate l.477] (26/93) End question " Qui a inventé la radio ?  A)  Guglielmo Marconi B)  Nikola Tesla C)  Oliver Lodge "
[2024-03-04 21:35:47,403 INFO generators.py generate l.475] (27/93) *** AnsGenerator for question " Qui a inventé le télescope ?  A)  Hans Lippershey B)  Zacharias Janssen C)  Galileo Galilei "
[2024-03-04 21:35:47,404 INFO generators.py gen_for_qa l.548] (27/93) * Start with LLM "gpt-4"
[2024-03-04 21:35:47,405 DEBUG generators.py gen_for_qa l.554] (27/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:47,407 DEBUG generators.py generate l.352] (27/93) Reuse existing Prompt
[2024-03-04 21:35:47,408 DEBUG generators.py generate l.365] (27/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:47,408 DEBUG generators.py generate l.373] (27/93) Reuse post-processing
[2024-03-04 21:35:47,408 INFO generators.py gen_for_qa l.548] (27/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:35:47,414 DEBUG generators.py gen_for_qa l.554] (27/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:47,415 DEBUG generators.py generate l.352] (27/93) Reuse existing Prompt
[2024-03-04 21:35:47,416 DEBUG generators.py generate l.365] (27/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:47,417 DEBUG generators.py generate l.373] (27/93) Reuse post-processing
[2024-03-04 21:35:47,418 INFO generators.py gen_for_qa l.548] (27/93) * Start with LLM "gemini-pro"
[2024-03-04 21:35:47,419 DEBUG generators.py gen_for_qa l.554] (27/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:47,419 DEBUG generators.py generate l.352] (27/93) Reuse existing Prompt
[2024-03-04 21:35:47,419 DEBUG generators.py generate l.365] (27/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:47,421 DEBUG generators.py generate l.373] (27/93) Reuse post-processing
[2024-03-04 21:35:47,423 INFO generators.py gen_for_qa l.548] (27/93) * Start with LLM "claude-2.1"
[2024-03-04 21:35:47,424 DEBUG generators.py gen_for_qa l.554] (27/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:47,425 DEBUG generators.py generate l.352] (27/93) Reuse existing Prompt
[2024-03-04 21:35:47,426 DEBUG generators.py generate l.365] (27/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:47,429 DEBUG generators.py generate l.373] (27/93) Reuse post-processing
[2024-03-04 21:35:47,429 INFO generators.py gen_for_qa l.548] (27/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:35:47,432 DEBUG generators.py gen_for_qa l.554] (27/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:47,433 DEBUG generators.py generate l.352] (27/93) Reuse existing Prompt
[2024-03-04 21:35:47,433 DEBUG generators.py generate l.365] (27/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:47,435 DEBUG generators.py generate l.373] (27/93) Reuse post-processing
[2024-03-04 21:35:47,436 INFO generators.py gen_for_qa l.548] (27/93) * Start with LLM "command-nightly"
[2024-03-04 21:35:47,437 DEBUG generators.py gen_for_qa l.554] (27/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:47,438 DEBUG generators.py generate l.352] (27/93) Reuse existing Prompt
[2024-03-04 21:35:47,439 DEBUG generators.py generate l.365] (27/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:47,439 DEBUG generators.py generate l.373] (27/93) Reuse post-processing
[2024-03-04 21:35:47,440 INFO generators.py gen_for_qa l.548] (27/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:35:47,442 DEBUG generators.py generate l.349] (27/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:35:47,443 DEBUG generators.py generate l.358] (27/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:35:49,729 ERROR generators.py complete l.400] (27/93) The following exception occurred with prompt meta={} user=" Qui a inventé le télescope ?  A)  Hans Lippershey B)  Zacharias Janssen C)  Galileo Galilei .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:35:49,754 DEBUG generators.py generate l.373] (27/93) Reuse post-processing
[2024-03-04 21:35:49,756 INFO generators.py generate l.477] (27/93) End question " Qui a inventé le télescope ?  A)  Hans Lippershey B)  Zacharias Janssen C)  Galileo Galilei "
[2024-03-04 21:35:49,757 INFO generators.py generate l.475] (28/93) *** AnsGenerator for question " Qui a découvert l'oxygène ?  A)  Joseph Priestley B)  Carl Wilhelm Scheele C)  Antoine Lavoisier "
[2024-03-04 21:35:49,760 INFO generators.py gen_for_qa l.548] (28/93) * Start with LLM "gpt-4"
[2024-03-04 21:35:49,761 DEBUG generators.py gen_for_qa l.554] (28/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:49,763 DEBUG generators.py generate l.352] (28/93) Reuse existing Prompt
[2024-03-04 21:35:49,765 DEBUG generators.py generate l.365] (28/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:49,766 DEBUG generators.py generate l.373] (28/93) Reuse post-processing
[2024-03-04 21:35:49,767 INFO generators.py gen_for_qa l.548] (28/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:35:49,768 DEBUG generators.py gen_for_qa l.554] (28/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:49,768 DEBUG generators.py generate l.352] (28/93) Reuse existing Prompt
[2024-03-04 21:35:49,770 DEBUG generators.py generate l.365] (28/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:49,771 DEBUG generators.py generate l.373] (28/93) Reuse post-processing
[2024-03-04 21:35:49,771 INFO generators.py gen_for_qa l.548] (28/93) * Start with LLM "gemini-pro"
[2024-03-04 21:35:49,771 DEBUG generators.py gen_for_qa l.554] (28/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:49,775 DEBUG generators.py generate l.352] (28/93) Reuse existing Prompt
[2024-03-04 21:35:49,777 DEBUG generators.py generate l.365] (28/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:49,778 DEBUG generators.py generate l.373] (28/93) Reuse post-processing
[2024-03-04 21:35:49,780 INFO generators.py gen_for_qa l.548] (28/93) * Start with LLM "claude-2.1"
[2024-03-04 21:35:49,781 DEBUG generators.py gen_for_qa l.554] (28/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:49,782 DEBUG generators.py generate l.352] (28/93) Reuse existing Prompt
[2024-03-04 21:35:49,782 DEBUG generators.py generate l.365] (28/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:49,782 DEBUG generators.py generate l.373] (28/93) Reuse post-processing
[2024-03-04 21:35:49,782 INFO generators.py gen_for_qa l.548] (28/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:35:49,782 DEBUG generators.py gen_for_qa l.554] (28/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:49,787 DEBUG generators.py generate l.352] (28/93) Reuse existing Prompt
[2024-03-04 21:35:49,788 DEBUG generators.py generate l.365] (28/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:49,789 DEBUG generators.py generate l.373] (28/93) Reuse post-processing
[2024-03-04 21:35:49,790 INFO generators.py gen_for_qa l.548] (28/93) * Start with LLM "command-nightly"
[2024-03-04 21:35:49,790 DEBUG generators.py gen_for_qa l.554] (28/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:49,793 DEBUG generators.py generate l.352] (28/93) Reuse existing Prompt
[2024-03-04 21:35:49,793 DEBUG generators.py generate l.365] (28/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:49,795 DEBUG generators.py generate l.373] (28/93) Reuse post-processing
[2024-03-04 21:35:49,797 INFO generators.py gen_for_qa l.548] (28/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:35:49,798 DEBUG generators.py generate l.349] (28/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:35:49,799 DEBUG generators.py generate l.358] (28/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:35:53,150 ERROR generators.py complete l.400] (28/93) The following exception occurred with prompt meta={} user=" Qui a découvert l'oxygène ?  A)  Joseph Priestley B)  Carl Wilhelm Scheele C)  Antoine Lavoisier .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:35:53,174 DEBUG generators.py generate l.373] (28/93) Reuse post-processing
[2024-03-04 21:35:53,175 INFO generators.py generate l.477] (28/93) End question " Qui a découvert l'oxygène ?  A)  Joseph Priestley B)  Carl Wilhelm Scheele C)  Antoine Lavoisier "
[2024-03-04 21:35:53,179 INFO generators.py generate l.475] (29/93) *** AnsGenerator for question " Qui a inventé le moteur à réaction ?  A)  Frank Whittle B)  Hans von Ohain "
[2024-03-04 21:35:53,179 INFO generators.py gen_for_qa l.548] (29/93) * Start with LLM "gpt-4"
[2024-03-04 21:35:53,182 DEBUG generators.py gen_for_qa l.554] (29/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:53,183 DEBUG generators.py generate l.352] (29/93) Reuse existing Prompt
[2024-03-04 21:35:53,184 DEBUG generators.py generate l.365] (29/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:53,185 DEBUG generators.py generate l.373] (29/93) Reuse post-processing
[2024-03-04 21:35:53,186 INFO generators.py gen_for_qa l.548] (29/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:35:53,188 DEBUG generators.py gen_for_qa l.554] (29/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:53,188 DEBUG generators.py generate l.352] (29/93) Reuse existing Prompt
[2024-03-04 21:35:53,189 DEBUG generators.py generate l.365] (29/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:53,191 DEBUG generators.py generate l.373] (29/93) Reuse post-processing
[2024-03-04 21:35:53,192 INFO generators.py gen_for_qa l.548] (29/93) * Start with LLM "gemini-pro"
[2024-03-04 21:35:53,193 DEBUG generators.py gen_for_qa l.554] (29/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:53,195 DEBUG generators.py generate l.352] (29/93) Reuse existing Prompt
[2024-03-04 21:35:53,197 DEBUG generators.py generate l.365] (29/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:53,197 DEBUG generators.py generate l.373] (29/93) Reuse post-processing
[2024-03-04 21:35:53,198 INFO generators.py gen_for_qa l.548] (29/93) * Start with LLM "claude-2.1"
[2024-03-04 21:35:53,200 DEBUG generators.py gen_for_qa l.554] (29/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:53,200 DEBUG generators.py generate l.352] (29/93) Reuse existing Prompt
[2024-03-04 21:35:53,201 DEBUG generators.py generate l.365] (29/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:53,202 DEBUG generators.py generate l.373] (29/93) Reuse post-processing
[2024-03-04 21:35:53,203 INFO generators.py gen_for_qa l.548] (29/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:35:53,204 DEBUG generators.py gen_for_qa l.554] (29/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:53,204 DEBUG generators.py generate l.352] (29/93) Reuse existing Prompt
[2024-03-04 21:35:53,205 DEBUG generators.py generate l.365] (29/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:53,206 DEBUG generators.py generate l.373] (29/93) Reuse post-processing
[2024-03-04 21:35:53,208 INFO generators.py gen_for_qa l.548] (29/93) * Start with LLM "command-nightly"
[2024-03-04 21:35:53,209 DEBUG generators.py gen_for_qa l.554] (29/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:53,211 DEBUG generators.py generate l.352] (29/93) Reuse existing Prompt
[2024-03-04 21:35:53,212 DEBUG generators.py generate l.365] (29/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:53,213 DEBUG generators.py generate l.373] (29/93) Reuse post-processing
[2024-03-04 21:35:53,213 INFO generators.py gen_for_qa l.548] (29/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:35:53,215 DEBUG generators.py generate l.349] (29/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:35:53,215 DEBUG generators.py generate l.358] (29/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:35:55,116 ERROR generators.py complete l.400] (29/93) The following exception occurred with prompt meta={} user=" Qui a inventé le moteur à réaction ?  A)  Frank Whittle B)  Hans von Ohain .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:35:55,130 DEBUG generators.py generate l.373] (29/93) Reuse post-processing
[2024-03-04 21:35:55,132 INFO generators.py generate l.477] (29/93) End question " Qui a inventé le moteur à réaction ?  A)  Frank Whittle B)  Hans von Ohain "
[2024-03-04 21:35:55,133 INFO generators.py generate l.475] (30/93) *** AnsGenerator for question " Qui a inventé le radar ?  A)  Robert Watson-Watt B)  Christian Hülsmeyer C)  Guglielmo Marconi "
[2024-03-04 21:35:55,134 INFO generators.py gen_for_qa l.548] (30/93) * Start with LLM "gpt-4"
[2024-03-04 21:35:55,135 DEBUG generators.py gen_for_qa l.554] (30/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:55,136 DEBUG generators.py generate l.352] (30/93) Reuse existing Prompt
[2024-03-04 21:35:55,136 DEBUG generators.py generate l.365] (30/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:55,139 DEBUG generators.py generate l.373] (30/93) Reuse post-processing
[2024-03-04 21:35:55,141 INFO generators.py gen_for_qa l.548] (30/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:35:55,143 DEBUG generators.py gen_for_qa l.554] (30/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:55,144 DEBUG generators.py generate l.352] (30/93) Reuse existing Prompt
[2024-03-04 21:35:55,145 DEBUG generators.py generate l.365] (30/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:55,146 DEBUG generators.py generate l.373] (30/93) Reuse post-processing
[2024-03-04 21:35:55,148 INFO generators.py gen_for_qa l.548] (30/93) * Start with LLM "gemini-pro"
[2024-03-04 21:35:55,148 DEBUG generators.py gen_for_qa l.554] (30/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:55,150 DEBUG generators.py generate l.352] (30/93) Reuse existing Prompt
[2024-03-04 21:35:55,150 DEBUG generators.py generate l.365] (30/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:55,152 DEBUG generators.py generate l.373] (30/93) Reuse post-processing
[2024-03-04 21:35:55,153 INFO generators.py gen_for_qa l.548] (30/93) * Start with LLM "claude-2.1"
[2024-03-04 21:35:55,154 DEBUG generators.py gen_for_qa l.554] (30/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:55,155 DEBUG generators.py generate l.352] (30/93) Reuse existing Prompt
[2024-03-04 21:35:55,156 DEBUG generators.py generate l.365] (30/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:55,157 DEBUG generators.py generate l.373] (30/93) Reuse post-processing
[2024-03-04 21:35:55,160 INFO generators.py gen_for_qa l.548] (30/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:35:55,160 DEBUG generators.py gen_for_qa l.554] (30/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:55,160 DEBUG generators.py generate l.352] (30/93) Reuse existing Prompt
[2024-03-04 21:35:55,163 DEBUG generators.py generate l.365] (30/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:55,163 DEBUG generators.py generate l.373] (30/93) Reuse post-processing
[2024-03-04 21:35:55,163 INFO generators.py gen_for_qa l.548] (30/93) * Start with LLM "command-nightly"
[2024-03-04 21:35:55,165 DEBUG generators.py gen_for_qa l.554] (30/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:55,167 DEBUG generators.py generate l.352] (30/93) Reuse existing Prompt
[2024-03-04 21:35:55,168 DEBUG generators.py generate l.365] (30/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:55,169 DEBUG generators.py generate l.373] (30/93) Reuse post-processing
[2024-03-04 21:35:55,170 INFO generators.py gen_for_qa l.548] (30/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:35:55,170 DEBUG generators.py generate l.349] (30/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:35:55,172 DEBUG generators.py generate l.358] (30/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:35:58,353 ERROR generators.py complete l.400] (30/93) The following exception occurred with prompt meta={} user=" Qui a inventé le radar ?  A)  Robert Watson-Watt B)  Christian Hülsmeyer C)  Guglielmo Marconi .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:35:58,366 DEBUG generators.py generate l.373] (30/93) Reuse post-processing
[2024-03-04 21:35:58,366 INFO generators.py generate l.477] (30/93) End question " Qui a inventé le radar ?  A)  Robert Watson-Watt B)  Christian Hülsmeyer C)  Guglielmo Marconi "
[2024-03-04 21:35:58,369 INFO generators.py generate l.475] (31/93) *** AnsGenerator for question " Qui a découvert la pénicilline ?  A)  Alexander Fleming B)  Ernest Duchesne "
[2024-03-04 21:35:58,371 INFO generators.py gen_for_qa l.548] (31/93) * Start with LLM "gpt-4"
[2024-03-04 21:35:58,374 DEBUG generators.py gen_for_qa l.554] (31/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:58,375 DEBUG generators.py generate l.352] (31/93) Reuse existing Prompt
[2024-03-04 21:35:58,378 DEBUG generators.py generate l.365] (31/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:58,379 DEBUG generators.py generate l.373] (31/93) Reuse post-processing
[2024-03-04 21:35:58,380 INFO generators.py gen_for_qa l.548] (31/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:35:58,381 DEBUG generators.py gen_for_qa l.554] (31/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:58,383 DEBUG generators.py generate l.352] (31/93) Reuse existing Prompt
[2024-03-04 21:35:58,383 DEBUG generators.py generate l.365] (31/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:58,385 DEBUG generators.py generate l.373] (31/93) Reuse post-processing
[2024-03-04 21:35:58,385 INFO generators.py gen_for_qa l.548] (31/93) * Start with LLM "gemini-pro"
[2024-03-04 21:35:58,388 DEBUG generators.py gen_for_qa l.554] (31/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:58,389 DEBUG generators.py generate l.352] (31/93) Reuse existing Prompt
[2024-03-04 21:35:58,392 DEBUG generators.py generate l.365] (31/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:58,395 DEBUG generators.py generate l.373] (31/93) Reuse post-processing
[2024-03-04 21:35:58,395 INFO generators.py gen_for_qa l.548] (31/93) * Start with LLM "claude-2.1"
[2024-03-04 21:35:58,396 DEBUG generators.py gen_for_qa l.554] (31/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:58,397 DEBUG generators.py generate l.352] (31/93) Reuse existing Prompt
[2024-03-04 21:35:58,398 DEBUG generators.py generate l.365] (31/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:58,399 DEBUG generators.py generate l.373] (31/93) Reuse post-processing
[2024-03-04 21:35:58,400 INFO generators.py gen_for_qa l.548] (31/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:35:58,400 DEBUG generators.py gen_for_qa l.554] (31/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:58,402 DEBUG generators.py generate l.352] (31/93) Reuse existing Prompt
[2024-03-04 21:35:58,402 DEBUG generators.py generate l.365] (31/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:58,404 DEBUG generators.py generate l.373] (31/93) Reuse post-processing
[2024-03-04 21:35:58,405 INFO generators.py gen_for_qa l.548] (31/93) * Start with LLM "command-nightly"
[2024-03-04 21:35:58,406 DEBUG generators.py gen_for_qa l.554] (31/93) An Answer has already been generated with this LLM
[2024-03-04 21:35:58,408 DEBUG generators.py generate l.352] (31/93) Reuse existing Prompt
[2024-03-04 21:35:58,409 DEBUG generators.py generate l.365] (31/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:35:58,409 DEBUG generators.py generate l.373] (31/93) Reuse post-processing
[2024-03-04 21:35:58,411 INFO generators.py gen_for_qa l.548] (31/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:35:58,411 DEBUG generators.py generate l.349] (31/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:35:58,413 DEBUG generators.py generate l.358] (31/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:36:01,135 ERROR generators.py complete l.400] (31/93) The following exception occurred with prompt meta={} user=" Qui a découvert la pénicilline ?  A)  Alexander Fleming B)  Ernest Duchesne .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:36:01,146 DEBUG generators.py generate l.373] (31/93) Reuse post-processing
[2024-03-04 21:36:01,147 INFO generators.py generate l.477] (31/93) End question " Qui a découvert la pénicilline ?  A)  Alexander Fleming B)  Ernest Duchesne "
[2024-03-04 21:36:01,149 INFO generators.py generate l.475] (32/93) *** AnsGenerator for question " Qui a inventé la télévision ?  A)  John Logie Baird B)  Philo Farnsworth C)  Vladimir Zworykin "
[2024-03-04 21:36:01,150 INFO generators.py gen_for_qa l.548] (32/93) * Start with LLM "gpt-4"
[2024-03-04 21:36:01,151 DEBUG generators.py gen_for_qa l.554] (32/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:01,153 DEBUG generators.py generate l.352] (32/93) Reuse existing Prompt
[2024-03-04 21:36:01,154 DEBUG generators.py generate l.365] (32/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:01,156 DEBUG generators.py generate l.373] (32/93) Reuse post-processing
[2024-03-04 21:36:01,156 INFO generators.py gen_for_qa l.548] (32/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:36:01,159 DEBUG generators.py gen_for_qa l.554] (32/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:01,160 DEBUG generators.py generate l.352] (32/93) Reuse existing Prompt
[2024-03-04 21:36:01,161 DEBUG generators.py generate l.365] (32/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:01,162 DEBUG generators.py generate l.373] (32/93) Reuse post-processing
[2024-03-04 21:36:01,164 INFO generators.py gen_for_qa l.548] (32/93) * Start with LLM "gemini-pro"
[2024-03-04 21:36:01,165 DEBUG generators.py gen_for_qa l.554] (32/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:01,165 DEBUG generators.py generate l.352] (32/93) Reuse existing Prompt
[2024-03-04 21:36:01,167 DEBUG generators.py generate l.365] (32/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:01,167 DEBUG generators.py generate l.373] (32/93) Reuse post-processing
[2024-03-04 21:36:01,169 INFO generators.py gen_for_qa l.548] (32/93) * Start with LLM "claude-2.1"
[2024-03-04 21:36:01,170 DEBUG generators.py gen_for_qa l.554] (32/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:01,172 DEBUG generators.py generate l.352] (32/93) Reuse existing Prompt
[2024-03-04 21:36:01,173 DEBUG generators.py generate l.365] (32/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:01,175 DEBUG generators.py generate l.373] (32/93) Reuse post-processing
[2024-03-04 21:36:01,176 INFO generators.py gen_for_qa l.548] (32/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:36:01,176 DEBUG generators.py gen_for_qa l.554] (32/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:01,178 DEBUG generators.py generate l.352] (32/93) Reuse existing Prompt
[2024-03-04 21:36:01,179 DEBUG generators.py generate l.365] (32/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:01,180 DEBUG generators.py generate l.373] (32/93) Reuse post-processing
[2024-03-04 21:36:01,180 INFO generators.py gen_for_qa l.548] (32/93) * Start with LLM "command-nightly"
[2024-03-04 21:36:01,180 DEBUG generators.py gen_for_qa l.554] (32/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:01,183 DEBUG generators.py generate l.352] (32/93) Reuse existing Prompt
[2024-03-04 21:36:01,183 DEBUG generators.py generate l.365] (32/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:01,183 DEBUG generators.py generate l.373] (32/93) Reuse post-processing
[2024-03-04 21:36:01,185 INFO generators.py gen_for_qa l.548] (32/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:36:01,185 DEBUG generators.py generate l.349] (32/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:36:01,186 DEBUG generators.py generate l.358] (32/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:36:05,254 ERROR generators.py complete l.400] (32/93) The following exception occurred with prompt meta={} user=" Qui a inventé la télévision ?  A)  John Logie Baird B)  Philo Farnsworth C)  Vladimir Zworykin .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:36:05,268 DEBUG generators.py generate l.373] (32/93) Reuse post-processing
[2024-03-04 21:36:05,270 INFO generators.py generate l.477] (32/93) End question " Qui a inventé la télévision ?  A)  John Logie Baird B)  Philo Farnsworth C)  Vladimir Zworykin "
[2024-03-04 21:36:05,271 INFO generators.py generate l.475] (33/93) *** AnsGenerator for question " Qui a découvert l'ADN ?  A)  James Watson et Francis Crick B)  Rosalind Franklin C)  Maurice Wilkins "
[2024-03-04 21:36:05,271 INFO generators.py gen_for_qa l.548] (33/93) * Start with LLM "gpt-4"
[2024-03-04 21:36:05,276 DEBUG generators.py gen_for_qa l.554] (33/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:05,276 DEBUG generators.py generate l.352] (33/93) Reuse existing Prompt
[2024-03-04 21:36:05,277 DEBUG generators.py generate l.365] (33/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:05,279 DEBUG generators.py generate l.373] (33/93) Reuse post-processing
[2024-03-04 21:36:05,280 INFO generators.py gen_for_qa l.548] (33/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:36:05,281 DEBUG generators.py gen_for_qa l.554] (33/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:05,281 DEBUG generators.py generate l.352] (33/93) Reuse existing Prompt
[2024-03-04 21:36:05,283 DEBUG generators.py generate l.365] (33/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:05,283 DEBUG generators.py generate l.373] (33/93) Reuse post-processing
[2024-03-04 21:36:05,285 INFO generators.py gen_for_qa l.548] (33/93) * Start with LLM "gemini-pro"
[2024-03-04 21:36:05,286 DEBUG generators.py gen_for_qa l.554] (33/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:05,288 DEBUG generators.py generate l.352] (33/93) Reuse existing Prompt
[2024-03-04 21:36:05,290 DEBUG generators.py generate l.365] (33/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:05,290 DEBUG generators.py generate l.373] (33/93) Reuse post-processing
[2024-03-04 21:36:05,293 INFO generators.py gen_for_qa l.548] (33/93) * Start with LLM "claude-2.1"
[2024-03-04 21:36:05,295 DEBUG generators.py gen_for_qa l.554] (33/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:05,296 DEBUG generators.py generate l.352] (33/93) Reuse existing Prompt
[2024-03-04 21:36:05,297 DEBUG generators.py generate l.365] (33/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:05,297 DEBUG generators.py generate l.373] (33/93) Reuse post-processing
[2024-03-04 21:36:05,298 INFO generators.py gen_for_qa l.548] (33/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:36:05,299 DEBUG generators.py gen_for_qa l.554] (33/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:05,300 DEBUG generators.py generate l.352] (33/93) Reuse existing Prompt
[2024-03-04 21:36:05,300 DEBUG generators.py generate l.365] (33/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:05,302 DEBUG generators.py generate l.373] (33/93) Reuse post-processing
[2024-03-04 21:36:05,304 INFO generators.py gen_for_qa l.548] (33/93) * Start with LLM "command-nightly"
[2024-03-04 21:36:05,306 DEBUG generators.py gen_for_qa l.554] (33/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:05,307 DEBUG generators.py generate l.352] (33/93) Reuse existing Prompt
[2024-03-04 21:36:05,308 DEBUG generators.py generate l.365] (33/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:05,309 DEBUG generators.py generate l.373] (33/93) Reuse post-processing
[2024-03-04 21:36:05,310 INFO generators.py gen_for_qa l.548] (33/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:36:05,311 DEBUG generators.py generate l.349] (33/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:36:05,311 DEBUG generators.py generate l.358] (33/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:36:08,364 ERROR generators.py complete l.400] (33/93) The following exception occurred with prompt meta={} user=" Qui a découvert l'ADN ?  A)  James Watson et Francis Crick B)  Rosalind Franklin C)  Maurice Wilkins .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:36:08,380 DEBUG generators.py generate l.373] (33/93) Reuse post-processing
[2024-03-04 21:36:08,384 INFO generators.py generate l.477] (33/93) End question " Qui a découvert l'ADN ?  A)  James Watson et Francis Crick B)  Rosalind Franklin C)  Maurice Wilkins "
[2024-03-04 21:36:08,385 INFO generators.py generate l.475] (34/93) *** AnsGenerator for question " Qui a inventé le laser ?  A)  Gordon Gould B)  Charles Hard Townes C)  Arthur Leonard Schawlow "
[2024-03-04 21:36:08,387 INFO generators.py gen_for_qa l.548] (34/93) * Start with LLM "gpt-4"
[2024-03-04 21:36:08,387 DEBUG generators.py gen_for_qa l.554] (34/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:08,389 DEBUG generators.py generate l.352] (34/93) Reuse existing Prompt
[2024-03-04 21:36:08,393 DEBUG generators.py generate l.365] (34/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:08,395 DEBUG generators.py generate l.373] (34/93) Reuse post-processing
[2024-03-04 21:36:08,396 INFO generators.py gen_for_qa l.548] (34/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:36:08,397 DEBUG generators.py gen_for_qa l.554] (34/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:08,399 DEBUG generators.py generate l.352] (34/93) Reuse existing Prompt
[2024-03-04 21:36:08,401 DEBUG generators.py generate l.365] (34/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:08,402 DEBUG generators.py generate l.373] (34/93) Reuse post-processing
[2024-03-04 21:36:08,404 INFO generators.py gen_for_qa l.548] (34/93) * Start with LLM "gemini-pro"
[2024-03-04 21:36:08,407 DEBUG generators.py gen_for_qa l.554] (34/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:08,409 DEBUG generators.py generate l.352] (34/93) Reuse existing Prompt
[2024-03-04 21:36:08,411 DEBUG generators.py generate l.365] (34/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:08,412 DEBUG generators.py generate l.373] (34/93) Reuse post-processing
[2024-03-04 21:36:08,414 INFO generators.py gen_for_qa l.548] (34/93) * Start with LLM "claude-2.1"
[2024-03-04 21:36:08,415 DEBUG generators.py gen_for_qa l.554] (34/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:08,417 DEBUG generators.py generate l.352] (34/93) Reuse existing Prompt
[2024-03-04 21:36:08,419 DEBUG generators.py generate l.365] (34/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:08,421 DEBUG generators.py generate l.373] (34/93) Reuse post-processing
[2024-03-04 21:36:08,422 INFO generators.py gen_for_qa l.548] (34/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:36:08,426 DEBUG generators.py gen_for_qa l.554] (34/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:08,428 DEBUG generators.py generate l.352] (34/93) Reuse existing Prompt
[2024-03-04 21:36:08,428 DEBUG generators.py generate l.365] (34/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:08,430 DEBUG generators.py generate l.373] (34/93) Reuse post-processing
[2024-03-04 21:36:08,431 INFO generators.py gen_for_qa l.548] (34/93) * Start with LLM "command-nightly"
[2024-03-04 21:36:08,432 DEBUG generators.py gen_for_qa l.554] (34/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:08,434 DEBUG generators.py generate l.352] (34/93) Reuse existing Prompt
[2024-03-04 21:36:08,435 DEBUG generators.py generate l.365] (34/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:08,437 DEBUG generators.py generate l.373] (34/93) Reuse post-processing
[2024-03-04 21:36:08,438 INFO generators.py gen_for_qa l.548] (34/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:36:08,440 DEBUG generators.py generate l.349] (34/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:36:08,443 DEBUG generators.py generate l.358] (34/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:36:10,695 ERROR generators.py complete l.400] (34/93) The following exception occurred with prompt meta={} user=" Qui a inventé le laser ?  A)  Gordon Gould B)  Charles Hard Townes C)  Arthur Leonard Schawlow .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:36:10,707 DEBUG generators.py generate l.373] (34/93) Reuse post-processing
[2024-03-04 21:36:10,711 INFO generators.py generate l.477] (34/93) End question " Qui a inventé le laser ?  A)  Gordon Gould B)  Charles Hard Townes C)  Arthur Leonard Schawlow "
[2024-03-04 21:36:10,712 INFO generators.py generate l.475] (35/93) *** AnsGenerator for question " Qui a inventé le transistor ?  A)  John Bardeen, Walter Brattain et William Shockley B)  Julius Edgar Lilienfeld C)  Oskar Heil "
[2024-03-04 21:36:10,712 INFO generators.py gen_for_qa l.548] (35/93) * Start with LLM "gpt-4"
[2024-03-04 21:36:10,713 DEBUG generators.py gen_for_qa l.554] (35/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:10,714 DEBUG generators.py generate l.352] (35/93) Reuse existing Prompt
[2024-03-04 21:36:10,716 DEBUG generators.py generate l.365] (35/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:10,717 DEBUG generators.py generate l.373] (35/93) Reuse post-processing
[2024-03-04 21:36:10,719 INFO generators.py gen_for_qa l.548] (35/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:36:10,722 DEBUG generators.py gen_for_qa l.554] (35/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:10,724 DEBUG generators.py generate l.352] (35/93) Reuse existing Prompt
[2024-03-04 21:36:10,724 DEBUG generators.py generate l.365] (35/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:10,726 DEBUG generators.py generate l.373] (35/93) Reuse post-processing
[2024-03-04 21:36:10,727 INFO generators.py gen_for_qa l.548] (35/93) * Start with LLM "gemini-pro"
[2024-03-04 21:36:10,727 DEBUG generators.py gen_for_qa l.554] (35/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:10,728 DEBUG generators.py generate l.352] (35/93) Reuse existing Prompt
[2024-03-04 21:36:10,728 DEBUG generators.py generate l.365] (35/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:10,731 DEBUG generators.py generate l.373] (35/93) Reuse post-processing
[2024-03-04 21:36:10,731 INFO generators.py gen_for_qa l.548] (35/93) * Start with LLM "claude-2.1"
[2024-03-04 21:36:10,732 DEBUG generators.py gen_for_qa l.554] (35/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:10,733 DEBUG generators.py generate l.352] (35/93) Reuse existing Prompt
[2024-03-04 21:36:10,735 DEBUG generators.py generate l.365] (35/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:10,738 DEBUG generators.py generate l.373] (35/93) Reuse post-processing
[2024-03-04 21:36:10,739 INFO generators.py gen_for_qa l.548] (35/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:36:10,740 DEBUG generators.py gen_for_qa l.554] (35/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:10,742 DEBUG generators.py generate l.352] (35/93) Reuse existing Prompt
[2024-03-04 21:36:10,742 DEBUG generators.py generate l.365] (35/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:10,743 DEBUG generators.py generate l.373] (35/93) Reuse post-processing
[2024-03-04 21:36:10,743 INFO generators.py gen_for_qa l.548] (35/93) * Start with LLM "command-nightly"
[2024-03-04 21:36:10,744 DEBUG generators.py gen_for_qa l.554] (35/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:10,746 DEBUG generators.py generate l.352] (35/93) Reuse existing Prompt
[2024-03-04 21:36:10,747 DEBUG generators.py generate l.365] (35/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:10,747 DEBUG generators.py generate l.373] (35/93) Reuse post-processing
[2024-03-04 21:36:10,748 INFO generators.py gen_for_qa l.548] (35/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:36:10,749 DEBUG generators.py generate l.349] (35/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:36:10,750 DEBUG generators.py generate l.358] (35/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:36:12,754 ERROR generators.py complete l.400] (35/93) The following exception occurred with prompt meta={} user=" Qui a inventé le transistor ?  A)  John Bardeen, Walter Brattain et William Shockley B)  Julius Edgar Lilienfeld C)  Oskar Heil .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:36:12,768 DEBUG generators.py generate l.373] (35/93) Reuse post-processing
[2024-03-04 21:36:12,771 INFO generators.py generate l.477] (35/93) End question " Qui a inventé le transistor ?  A)  John Bardeen, Walter Brattain et William Shockley B)  Julius Edgar Lilienfeld C)  Oskar Heil "
[2024-03-04 21:36:12,773 INFO generators.py generate l.475] (36/93) *** AnsGenerator for question " Qui a inventé l'ordinateur ?  A)  Charles Babbage B)  Alan Turing C)  John Atanasoff et Clifford Berry "
[2024-03-04 21:36:12,775 INFO generators.py gen_for_qa l.548] (36/93) * Start with LLM "gpt-4"
[2024-03-04 21:36:12,776 DEBUG generators.py gen_for_qa l.554] (36/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:12,777 DEBUG generators.py generate l.352] (36/93) Reuse existing Prompt
[2024-03-04 21:36:12,778 DEBUG generators.py generate l.365] (36/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:12,779 DEBUG generators.py generate l.373] (36/93) Reuse post-processing
[2024-03-04 21:36:12,781 INFO generators.py gen_for_qa l.548] (36/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:36:12,783 DEBUG generators.py gen_for_qa l.554] (36/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:12,784 DEBUG generators.py generate l.352] (36/93) Reuse existing Prompt
[2024-03-04 21:36:12,785 DEBUG generators.py generate l.365] (36/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:12,787 DEBUG generators.py generate l.373] (36/93) Reuse post-processing
[2024-03-04 21:36:12,788 INFO generators.py gen_for_qa l.548] (36/93) * Start with LLM "gemini-pro"
[2024-03-04 21:36:12,790 DEBUG generators.py gen_for_qa l.554] (36/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:12,791 DEBUG generators.py generate l.352] (36/93) Reuse existing Prompt
[2024-03-04 21:36:12,792 DEBUG generators.py generate l.365] (36/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:12,793 DEBUG generators.py generate l.373] (36/93) Reuse post-processing
[2024-03-04 21:36:12,793 INFO generators.py gen_for_qa l.548] (36/93) * Start with LLM "claude-2.1"
[2024-03-04 21:36:12,795 DEBUG generators.py gen_for_qa l.554] (36/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:12,796 DEBUG generators.py generate l.352] (36/93) Reuse existing Prompt
[2024-03-04 21:36:12,796 DEBUG generators.py generate l.365] (36/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:12,798 DEBUG generators.py generate l.373] (36/93) Reuse post-processing
[2024-03-04 21:36:12,798 INFO generators.py gen_for_qa l.548] (36/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:36:12,799 DEBUG generators.py gen_for_qa l.554] (36/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:12,801 DEBUG generators.py generate l.352] (36/93) Reuse existing Prompt
[2024-03-04 21:36:12,803 DEBUG generators.py generate l.365] (36/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:12,805 DEBUG generators.py generate l.373] (36/93) Reuse post-processing
[2024-03-04 21:36:12,805 INFO generators.py gen_for_qa l.548] (36/93) * Start with LLM "command-nightly"
[2024-03-04 21:36:12,806 DEBUG generators.py gen_for_qa l.554] (36/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:12,807 DEBUG generators.py generate l.352] (36/93) Reuse existing Prompt
[2024-03-04 21:36:12,808 DEBUG generators.py generate l.365] (36/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:12,808 DEBUG generators.py generate l.373] (36/93) Reuse post-processing
[2024-03-04 21:36:12,809 INFO generators.py gen_for_qa l.548] (36/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:36:12,811 DEBUG generators.py generate l.349] (36/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:36:12,811 DEBUG generators.py generate l.358] (36/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:36:14,887 ERROR generators.py complete l.400] (36/93) The following exception occurred with prompt meta={} user=" Qui a inventé l'ordinateur ?  A)  Charles Babbage B)  Alan Turing C)  John Atanasoff et Clifford Berry .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:36:14,911 DEBUG generators.py generate l.373] (36/93) Reuse post-processing
[2024-03-04 21:36:14,913 INFO generators.py generate l.477] (36/93) End question " Qui a inventé l'ordinateur ?  A)  Charles Babbage B)  Alan Turing C)  John Atanasoff et Clifford Berry "
[2024-03-04 21:36:14,914 INFO generators.py generate l.475] (37/93) *** AnsGenerator for question " Qui a découvert le boson de Higgs ?  A)  Peter Higgs B)  François Englert C)  Robert Brout "
[2024-03-04 21:36:14,916 INFO generators.py gen_for_qa l.548] (37/93) * Start with LLM "gpt-4"
[2024-03-04 21:36:14,919 DEBUG generators.py gen_for_qa l.554] (37/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:14,922 DEBUG generators.py generate l.352] (37/93) Reuse existing Prompt
[2024-03-04 21:36:14,923 DEBUG generators.py generate l.365] (37/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:14,924 DEBUG generators.py generate l.373] (37/93) Reuse post-processing
[2024-03-04 21:36:14,924 INFO generators.py gen_for_qa l.548] (37/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:36:14,926 DEBUG generators.py gen_for_qa l.554] (37/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:14,926 DEBUG generators.py generate l.352] (37/93) Reuse existing Prompt
[2024-03-04 21:36:14,926 DEBUG generators.py generate l.365] (37/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:14,929 DEBUG generators.py generate l.373] (37/93) Reuse post-processing
[2024-03-04 21:36:14,930 INFO generators.py gen_for_qa l.548] (37/93) * Start with LLM "gemini-pro"
[2024-03-04 21:36:14,931 DEBUG generators.py gen_for_qa l.554] (37/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:14,933 DEBUG generators.py generate l.352] (37/93) Reuse existing Prompt
[2024-03-04 21:36:14,934 DEBUG generators.py generate l.365] (37/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:14,935 DEBUG generators.py generate l.373] (37/93) Reuse post-processing
[2024-03-04 21:36:14,937 INFO generators.py gen_for_qa l.548] (37/93) * Start with LLM "claude-2.1"
[2024-03-04 21:36:14,938 DEBUG generators.py gen_for_qa l.554] (37/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:14,940 DEBUG generators.py generate l.352] (37/93) Reuse existing Prompt
[2024-03-04 21:36:14,943 DEBUG generators.py generate l.365] (37/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:14,945 DEBUG generators.py generate l.373] (37/93) Reuse post-processing
[2024-03-04 21:36:14,946 INFO generators.py gen_for_qa l.548] (37/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:36:14,947 DEBUG generators.py gen_for_qa l.554] (37/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:14,947 DEBUG generators.py generate l.352] (37/93) Reuse existing Prompt
[2024-03-04 21:36:14,949 DEBUG generators.py generate l.365] (37/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:14,951 DEBUG generators.py generate l.373] (37/93) Reuse post-processing
[2024-03-04 21:36:14,952 INFO generators.py gen_for_qa l.548] (37/93) * Start with LLM "command-nightly"
[2024-03-04 21:36:14,954 DEBUG generators.py gen_for_qa l.554] (37/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:14,955 DEBUG generators.py generate l.352] (37/93) Reuse existing Prompt
[2024-03-04 21:36:14,956 DEBUG generators.py generate l.365] (37/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:14,956 DEBUG generators.py generate l.373] (37/93) Reuse post-processing
[2024-03-04 21:36:14,956 INFO generators.py gen_for_qa l.548] (37/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:36:14,956 DEBUG generators.py generate l.349] (37/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:36:14,956 DEBUG generators.py generate l.358] (37/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:36:16,331 ERROR generators.py complete l.400] (37/93) The following exception occurred with prompt meta={} user=" Qui a découvert le boson de Higgs ?  A)  Peter Higgs B)  François Englert C)  Robert Brout .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:36:16,355 DEBUG generators.py generate l.373] (37/93) Reuse post-processing
[2024-03-04 21:36:16,358 INFO generators.py generate l.477] (37/93) End question " Qui a découvert le boson de Higgs ?  A)  Peter Higgs B)  François Englert C)  Robert Brout "
[2024-03-04 21:36:16,360 INFO generators.py generate l.475] (38/93) *** AnsGenerator for question " Qui a inventé le World Wide Web ?  A)  Tim Berners-Lee "
[2024-03-04 21:36:16,362 INFO generators.py gen_for_qa l.548] (38/93) * Start with LLM "gpt-4"
[2024-03-04 21:36:16,362 DEBUG generators.py gen_for_qa l.554] (38/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:16,364 DEBUG generators.py generate l.352] (38/93) Reuse existing Prompt
[2024-03-04 21:36:16,365 DEBUG generators.py generate l.365] (38/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:16,367 DEBUG generators.py generate l.373] (38/93) Reuse post-processing
[2024-03-04 21:36:16,369 INFO generators.py gen_for_qa l.548] (38/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:36:16,371 DEBUG generators.py gen_for_qa l.554] (38/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:16,373 DEBUG generators.py generate l.352] (38/93) Reuse existing Prompt
[2024-03-04 21:36:16,374 DEBUG generators.py generate l.365] (38/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:16,375 DEBUG generators.py generate l.373] (38/93) Reuse post-processing
[2024-03-04 21:36:16,376 INFO generators.py gen_for_qa l.548] (38/93) * Start with LLM "gemini-pro"
[2024-03-04 21:36:16,378 DEBUG generators.py gen_for_qa l.554] (38/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:16,379 DEBUG generators.py generate l.352] (38/93) Reuse existing Prompt
[2024-03-04 21:36:16,379 DEBUG generators.py generate l.365] (38/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:16,381 DEBUG generators.py generate l.373] (38/93) Reuse post-processing
[2024-03-04 21:36:16,381 INFO generators.py gen_for_qa l.548] (38/93) * Start with LLM "claude-2.1"
[2024-03-04 21:36:16,383 DEBUG generators.py gen_for_qa l.554] (38/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:16,384 DEBUG generators.py generate l.352] (38/93) Reuse existing Prompt
[2024-03-04 21:36:16,387 DEBUG generators.py generate l.365] (38/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:16,388 DEBUG generators.py generate l.373] (38/93) Reuse post-processing
[2024-03-04 21:36:16,388 INFO generators.py gen_for_qa l.548] (38/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:36:16,388 DEBUG generators.py gen_for_qa l.554] (38/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:16,388 DEBUG generators.py generate l.352] (38/93) Reuse existing Prompt
[2024-03-04 21:36:16,388 DEBUG generators.py generate l.365] (38/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:16,388 DEBUG generators.py generate l.373] (38/93) Reuse post-processing
[2024-03-04 21:36:16,394 INFO generators.py gen_for_qa l.548] (38/93) * Start with LLM "command-nightly"
[2024-03-04 21:36:16,395 DEBUG generators.py gen_for_qa l.554] (38/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:16,395 DEBUG generators.py generate l.352] (38/93) Reuse existing Prompt
[2024-03-04 21:36:16,396 DEBUG generators.py generate l.365] (38/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:16,396 DEBUG generators.py generate l.373] (38/93) Reuse post-processing
[2024-03-04 21:36:16,397 INFO generators.py gen_for_qa l.548] (38/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:36:16,399 DEBUG generators.py generate l.349] (38/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:36:16,400 DEBUG generators.py generate l.358] (38/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:36:18,073 ERROR generators.py complete l.400] (38/93) The following exception occurred with prompt meta={} user=" Qui a inventé le World Wide Web ?  A)  Tim Berners-Lee .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:36:18,088 DEBUG generators.py generate l.373] (38/93) Reuse post-processing
[2024-03-04 21:36:18,090 INFO generators.py generate l.477] (38/93) End question " Qui a inventé le World Wide Web ?  A)  Tim Berners-Lee "
[2024-03-04 21:36:18,090 INFO generators.py generate l.475] (39/93) *** AnsGenerator for question " Qui a découvert le vaccin contre la variole ?  A)  Edward Jenner B)  Benjamin Jesty "
[2024-03-04 21:36:18,093 INFO generators.py gen_for_qa l.548] (39/93) * Start with LLM "gpt-4"
[2024-03-04 21:36:18,095 DEBUG generators.py gen_for_qa l.554] (39/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:18,096 DEBUG generators.py generate l.352] (39/93) Reuse existing Prompt
[2024-03-04 21:36:18,097 DEBUG generators.py generate l.365] (39/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:18,100 DEBUG generators.py generate l.373] (39/93) Reuse post-processing
[2024-03-04 21:36:18,102 INFO generators.py gen_for_qa l.548] (39/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:36:18,102 DEBUG generators.py gen_for_qa l.554] (39/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:18,102 DEBUG generators.py generate l.352] (39/93) Reuse existing Prompt
[2024-03-04 21:36:18,105 DEBUG generators.py generate l.365] (39/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:18,105 DEBUG generators.py generate l.373] (39/93) Reuse post-processing
[2024-03-04 21:36:18,107 INFO generators.py gen_for_qa l.548] (39/93) * Start with LLM "gemini-pro"
[2024-03-04 21:36:18,108 DEBUG generators.py gen_for_qa l.554] (39/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:18,109 DEBUG generators.py generate l.352] (39/93) Reuse existing Prompt
[2024-03-04 21:36:18,110 DEBUG generators.py generate l.365] (39/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:18,112 DEBUG generators.py generate l.373] (39/93) Reuse post-processing
[2024-03-04 21:36:18,112 INFO generators.py gen_for_qa l.548] (39/93) * Start with LLM "claude-2.1"
[2024-03-04 21:36:18,114 DEBUG generators.py gen_for_qa l.554] (39/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:18,117 DEBUG generators.py generate l.352] (39/93) Reuse existing Prompt
[2024-03-04 21:36:18,118 DEBUG generators.py generate l.365] (39/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:18,119 DEBUG generators.py generate l.373] (39/93) Reuse post-processing
[2024-03-04 21:36:18,119 INFO generators.py gen_for_qa l.548] (39/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:36:18,121 DEBUG generators.py gen_for_qa l.554] (39/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:18,122 DEBUG generators.py generate l.352] (39/93) Reuse existing Prompt
[2024-03-04 21:36:18,123 DEBUG generators.py generate l.365] (39/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:18,123 DEBUG generators.py generate l.373] (39/93) Reuse post-processing
[2024-03-04 21:36:18,125 INFO generators.py gen_for_qa l.548] (39/93) * Start with LLM "command-nightly"
[2024-03-04 21:36:18,125 DEBUG generators.py gen_for_qa l.554] (39/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:18,127 DEBUG generators.py generate l.352] (39/93) Reuse existing Prompt
[2024-03-04 21:36:18,127 DEBUG generators.py generate l.365] (39/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:18,128 DEBUG generators.py generate l.373] (39/93) Reuse post-processing
[2024-03-04 21:36:18,129 INFO generators.py gen_for_qa l.548] (39/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:36:18,129 DEBUG generators.py generate l.349] (39/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:36:18,131 DEBUG generators.py generate l.358] (39/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:36:21,528 ERROR generators.py complete l.400] (39/93) The following exception occurred with prompt meta={} user=" Qui a découvert le vaccin contre la variole ?  A)  Edward Jenner B)  Benjamin Jesty .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:36:21,537 DEBUG generators.py generate l.373] (39/93) Reuse post-processing
[2024-03-04 21:36:21,542 INFO generators.py generate l.477] (39/93) End question " Qui a découvert le vaccin contre la variole ?  A)  Edward Jenner B)  Benjamin Jesty "
[2024-03-04 21:36:21,543 INFO generators.py generate l.475] (40/93) *** AnsGenerator for question " Qui a inventé le sous-marin ?  A)  Cornelis Drebbel B)  David Bushnell C)  Robert Fulton "
[2024-03-04 21:36:21,544 INFO generators.py gen_for_qa l.548] (40/93) * Start with LLM "gpt-4"
[2024-03-04 21:36:21,544 DEBUG generators.py gen_for_qa l.554] (40/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:21,547 DEBUG generators.py generate l.352] (40/93) Reuse existing Prompt
[2024-03-04 21:36:21,548 DEBUG generators.py generate l.365] (40/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:21,549 DEBUG generators.py generate l.373] (40/93) Reuse post-processing
[2024-03-04 21:36:21,552 INFO generators.py gen_for_qa l.548] (40/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:36:21,553 DEBUG generators.py gen_for_qa l.554] (40/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:21,554 DEBUG generators.py generate l.352] (40/93) Reuse existing Prompt
[2024-03-04 21:36:21,554 DEBUG generators.py generate l.365] (40/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:21,557 DEBUG generators.py generate l.373] (40/93) Reuse post-processing
[2024-03-04 21:36:21,558 INFO generators.py gen_for_qa l.548] (40/93) * Start with LLM "gemini-pro"
[2024-03-04 21:36:21,559 DEBUG generators.py gen_for_qa l.554] (40/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:21,560 DEBUG generators.py generate l.352] (40/93) Reuse existing Prompt
[2024-03-04 21:36:21,561 DEBUG generators.py generate l.365] (40/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:21,562 DEBUG generators.py generate l.373] (40/93) Reuse post-processing
[2024-03-04 21:36:21,563 INFO generators.py gen_for_qa l.548] (40/93) * Start with LLM "claude-2.1"
[2024-03-04 21:36:21,564 DEBUG generators.py gen_for_qa l.554] (40/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:21,565 DEBUG generators.py generate l.352] (40/93) Reuse existing Prompt
[2024-03-04 21:36:21,565 DEBUG generators.py generate l.365] (40/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:21,570 DEBUG generators.py generate l.373] (40/93) Reuse post-processing
[2024-03-04 21:36:21,571 INFO generators.py gen_for_qa l.548] (40/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:36:21,573 DEBUG generators.py gen_for_qa l.554] (40/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:21,574 DEBUG generators.py generate l.352] (40/93) Reuse existing Prompt
[2024-03-04 21:36:21,574 DEBUG generators.py generate l.365] (40/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:21,575 DEBUG generators.py generate l.373] (40/93) Reuse post-processing
[2024-03-04 21:36:21,575 INFO generators.py gen_for_qa l.548] (40/93) * Start with LLM "command-nightly"
[2024-03-04 21:36:21,575 DEBUG generators.py gen_for_qa l.554] (40/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:21,577 DEBUG generators.py generate l.352] (40/93) Reuse existing Prompt
[2024-03-04 21:36:21,579 DEBUG generators.py generate l.365] (40/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:21,579 DEBUG generators.py generate l.373] (40/93) Reuse post-processing
[2024-03-04 21:36:21,581 INFO generators.py gen_for_qa l.548] (40/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:36:21,582 DEBUG generators.py generate l.349] (40/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:36:21,584 DEBUG generators.py generate l.358] (40/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:36:25,618 ERROR generators.py complete l.400] (40/93) The following exception occurred with prompt meta={} user=" Qui a inventé le sous-marin ?  A)  Cornelis Drebbel B)  David Bushnell C)  Robert Fulton .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:36:25,640 DEBUG generators.py generate l.373] (40/93) Reuse post-processing
[2024-03-04 21:36:25,642 INFO generators.py generate l.477] (40/93) End question " Qui a inventé le sous-marin ?  A)  Cornelis Drebbel B)  David Bushnell C)  Robert Fulton "
[2024-03-04 21:36:25,644 INFO generators.py generate l.475] (41/93) *** AnsGenerator for question " Qui a découvert la loi de la gravitation universelle ?  A)  Isaac Newton B)  Robert Hooke "
[2024-03-04 21:36:25,646 INFO generators.py gen_for_qa l.548] (41/93) * Start with LLM "gpt-4"
[2024-03-04 21:36:25,649 DEBUG generators.py gen_for_qa l.554] (41/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:25,651 DEBUG generators.py generate l.352] (41/93) Reuse existing Prompt
[2024-03-04 21:36:25,652 DEBUG generators.py generate l.365] (41/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:25,655 DEBUG generators.py generate l.373] (41/93) Reuse post-processing
[2024-03-04 21:36:25,657 INFO generators.py gen_for_qa l.548] (41/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:36:25,658 DEBUG generators.py gen_for_qa l.554] (41/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:25,660 DEBUG generators.py generate l.352] (41/93) Reuse existing Prompt
[2024-03-04 21:36:25,660 DEBUG generators.py generate l.365] (41/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:25,662 DEBUG generators.py generate l.373] (41/93) Reuse post-processing
[2024-03-04 21:36:25,663 INFO generators.py gen_for_qa l.548] (41/93) * Start with LLM "gemini-pro"
[2024-03-04 21:36:25,665 DEBUG generators.py gen_for_qa l.554] (41/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:25,667 DEBUG generators.py generate l.352] (41/93) Reuse existing Prompt
[2024-03-04 21:36:25,668 DEBUG generators.py generate l.365] (41/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:25,670 DEBUG generators.py generate l.373] (41/93) Reuse post-processing
[2024-03-04 21:36:25,670 INFO generators.py gen_for_qa l.548] (41/93) * Start with LLM "claude-2.1"
[2024-03-04 21:36:25,670 DEBUG generators.py gen_for_qa l.554] (41/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:25,672 DEBUG generators.py generate l.352] (41/93) Reuse existing Prompt
[2024-03-04 21:36:25,673 DEBUG generators.py generate l.365] (41/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:25,673 DEBUG generators.py generate l.373] (41/93) Reuse post-processing
[2024-03-04 21:36:25,674 INFO generators.py gen_for_qa l.548] (41/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:36:25,674 DEBUG generators.py gen_for_qa l.554] (41/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:25,674 DEBUG generators.py generate l.352] (41/93) Reuse existing Prompt
[2024-03-04 21:36:25,674 DEBUG generators.py generate l.365] (41/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:25,678 DEBUG generators.py generate l.373] (41/93) Reuse post-processing
[2024-03-04 21:36:25,679 INFO generators.py gen_for_qa l.548] (41/93) * Start with LLM "command-nightly"
[2024-03-04 21:36:25,682 DEBUG generators.py gen_for_qa l.554] (41/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:25,683 DEBUG generators.py generate l.352] (41/93) Reuse existing Prompt
[2024-03-04 21:36:25,683 DEBUG generators.py generate l.365] (41/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:25,683 DEBUG generators.py generate l.373] (41/93) Reuse post-processing
[2024-03-04 21:36:25,683 INFO generators.py gen_for_qa l.548] (41/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:36:25,683 DEBUG generators.py generate l.349] (41/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:36:25,688 DEBUG generators.py generate l.358] (41/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:36:27,133 ERROR generators.py complete l.400] (41/93) The following exception occurred with prompt meta={} user=" Qui a découvert la loi de la gravitation universelle ?  A)  Isaac Newton B)  Robert Hooke .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:36:27,153 DEBUG generators.py generate l.373] (41/93) Reuse post-processing
[2024-03-04 21:36:27,155 INFO generators.py generate l.477] (41/93) End question " Qui a découvert la loi de la gravitation universelle ?  A)  Isaac Newton B)  Robert Hooke "
[2024-03-04 21:36:27,157 INFO generators.py generate l.475] (42/93) *** AnsGenerator for question " Qui a inventé le microscope électronique ?  A)  Ernst Ruska et Max Knoll "
[2024-03-04 21:36:27,159 INFO generators.py gen_for_qa l.548] (42/93) * Start with LLM "gpt-4"
[2024-03-04 21:36:27,160 DEBUG generators.py gen_for_qa l.554] (42/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:27,161 DEBUG generators.py generate l.352] (42/93) Reuse existing Prompt
[2024-03-04 21:36:27,163 DEBUG generators.py generate l.365] (42/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:27,165 DEBUG generators.py generate l.373] (42/93) Reuse post-processing
[2024-03-04 21:36:27,166 INFO generators.py gen_for_qa l.548] (42/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:36:27,166 DEBUG generators.py gen_for_qa l.554] (42/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:27,170 DEBUG generators.py generate l.352] (42/93) Reuse existing Prompt
[2024-03-04 21:36:27,171 DEBUG generators.py generate l.365] (42/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:27,172 DEBUG generators.py generate l.373] (42/93) Reuse post-processing
[2024-03-04 21:36:27,173 INFO generators.py gen_for_qa l.548] (42/93) * Start with LLM "gemini-pro"
[2024-03-04 21:36:27,174 DEBUG generators.py gen_for_qa l.554] (42/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:27,176 DEBUG generators.py generate l.352] (42/93) Reuse existing Prompt
[2024-03-04 21:36:27,176 DEBUG generators.py generate l.365] (42/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:27,178 DEBUG generators.py generate l.373] (42/93) Reuse post-processing
[2024-03-04 21:36:27,180 INFO generators.py gen_for_qa l.548] (42/93) * Start with LLM "claude-2.1"
[2024-03-04 21:36:27,181 DEBUG generators.py gen_for_qa l.554] (42/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:27,183 DEBUG generators.py generate l.352] (42/93) Reuse existing Prompt
[2024-03-04 21:36:27,184 DEBUG generators.py generate l.365] (42/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:27,184 DEBUG generators.py generate l.373] (42/93) Reuse post-processing
[2024-03-04 21:36:27,186 INFO generators.py gen_for_qa l.548] (42/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:36:27,187 DEBUG generators.py gen_for_qa l.554] (42/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:27,188 DEBUG generators.py generate l.352] (42/93) Reuse existing Prompt
[2024-03-04 21:36:27,189 DEBUG generators.py generate l.365] (42/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:27,189 DEBUG generators.py generate l.373] (42/93) Reuse post-processing
[2024-03-04 21:36:27,191 INFO generators.py gen_for_qa l.548] (42/93) * Start with LLM "command-nightly"
[2024-03-04 21:36:27,191 DEBUG generators.py gen_for_qa l.554] (42/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:27,192 DEBUG generators.py generate l.352] (42/93) Reuse existing Prompt
[2024-03-04 21:36:27,193 DEBUG generators.py generate l.365] (42/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:27,194 DEBUG generators.py generate l.373] (42/93) Reuse post-processing
[2024-03-04 21:36:27,195 INFO generators.py gen_for_qa l.548] (42/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:36:27,197 DEBUG generators.py generate l.349] (42/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:36:27,199 DEBUG generators.py generate l.358] (42/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:36:29,261 ERROR generators.py complete l.400] (42/93) The following exception occurred with prompt meta={} user=" Qui a inventé le microscope électronique ?  A)  Ernst Ruska et Max Knoll .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:36:29,280 DEBUG generators.py generate l.373] (42/93) Reuse post-processing
[2024-03-04 21:36:29,282 INFO generators.py generate l.477] (42/93) End question " Qui a inventé le microscope électronique ?  A)  Ernst Ruska et Max Knoll "
[2024-03-04 21:36:29,282 INFO generators.py generate l.475] (43/93) *** AnsGenerator for question " Qui a découvert la structure de la molécule de benzène ?  A)  August Kekulé B)  Archibald Scott Couper "
[2024-03-04 21:36:29,286 INFO generators.py gen_for_qa l.548] (43/93) * Start with LLM "gpt-4"
[2024-03-04 21:36:29,287 DEBUG generators.py gen_for_qa l.554] (43/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:29,288 DEBUG generators.py generate l.352] (43/93) Reuse existing Prompt
[2024-03-04 21:36:29,289 DEBUG generators.py generate l.365] (43/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:29,290 DEBUG generators.py generate l.373] (43/93) Reuse post-processing
[2024-03-04 21:36:29,292 INFO generators.py gen_for_qa l.548] (43/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:36:29,292 DEBUG generators.py gen_for_qa l.554] (43/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:29,295 DEBUG generators.py generate l.352] (43/93) Reuse existing Prompt
[2024-03-04 21:36:29,296 DEBUG generators.py generate l.365] (43/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:29,296 DEBUG generators.py generate l.373] (43/93) Reuse post-processing
[2024-03-04 21:36:29,296 INFO generators.py gen_for_qa l.548] (43/93) * Start with LLM "gemini-pro"
[2024-03-04 21:36:29,296 DEBUG generators.py gen_for_qa l.554] (43/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:29,302 DEBUG generators.py generate l.352] (43/93) Reuse existing Prompt
[2024-03-04 21:36:29,302 DEBUG generators.py generate l.365] (43/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:29,303 DEBUG generators.py generate l.373] (43/93) Reuse post-processing
[2024-03-04 21:36:29,304 INFO generators.py gen_for_qa l.548] (43/93) * Start with LLM "claude-2.1"
[2024-03-04 21:36:29,305 DEBUG generators.py gen_for_qa l.554] (43/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:29,307 DEBUG generators.py generate l.352] (43/93) Reuse existing Prompt
[2024-03-04 21:36:29,308 DEBUG generators.py generate l.365] (43/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:29,308 DEBUG generators.py generate l.373] (43/93) Reuse post-processing
[2024-03-04 21:36:29,309 INFO generators.py gen_for_qa l.548] (43/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:36:29,310 DEBUG generators.py gen_for_qa l.554] (43/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:29,311 DEBUG generators.py generate l.352] (43/93) Reuse existing Prompt
[2024-03-04 21:36:29,313 DEBUG generators.py generate l.365] (43/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:29,314 DEBUG generators.py generate l.373] (43/93) Reuse post-processing
[2024-03-04 21:36:29,316 INFO generators.py gen_for_qa l.548] (43/93) * Start with LLM "command-nightly"
[2024-03-04 21:36:29,317 DEBUG generators.py gen_for_qa l.554] (43/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:29,318 DEBUG generators.py generate l.352] (43/93) Reuse existing Prompt
[2024-03-04 21:36:29,318 DEBUG generators.py generate l.365] (43/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:29,318 DEBUG generators.py generate l.373] (43/93) Reuse post-processing
[2024-03-04 21:36:29,318 INFO generators.py gen_for_qa l.548] (43/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:36:29,318 DEBUG generators.py generate l.349] (43/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:36:29,318 DEBUG generators.py generate l.358] (43/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:36:32,623 ERROR generators.py complete l.400] (43/93) The following exception occurred with prompt meta={} user=" Qui a découvert la structure de la molécule de benzène ?  A)  August Kekulé B)  Archibald Scott Couper .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:36:32,639 DEBUG generators.py generate l.373] (43/93) Reuse post-processing
[2024-03-04 21:36:32,640 INFO generators.py generate l.477] (43/93) End question " Qui a découvert la structure de la molécule de benzène ?  A)  August Kekulé B)  Archibald Scott Couper "
[2024-03-04 21:36:32,641 INFO generators.py generate l.475] (44/93) *** AnsGenerator for question " Qui a inventé la dynamite ?  A)  Alfred Nobel B)  Ascanio Sobrero "
[2024-03-04 21:36:32,644 INFO generators.py gen_for_qa l.548] (44/93) * Start with LLM "gpt-4"
[2024-03-04 21:36:32,646 DEBUG generators.py gen_for_qa l.554] (44/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:32,648 DEBUG generators.py generate l.352] (44/93) Reuse existing Prompt
[2024-03-04 21:36:32,649 DEBUG generators.py generate l.365] (44/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:32,650 DEBUG generators.py generate l.373] (44/93) Reuse post-processing
[2024-03-04 21:36:32,650 INFO generators.py gen_for_qa l.548] (44/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:36:32,650 DEBUG generators.py gen_for_qa l.554] (44/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:32,654 DEBUG generators.py generate l.352] (44/93) Reuse existing Prompt
[2024-03-04 21:36:32,655 DEBUG generators.py generate l.365] (44/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:32,655 DEBUG generators.py generate l.373] (44/93) Reuse post-processing
[2024-03-04 21:36:32,657 INFO generators.py gen_for_qa l.548] (44/93) * Start with LLM "gemini-pro"
[2024-03-04 21:36:32,658 DEBUG generators.py gen_for_qa l.554] (44/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:32,659 DEBUG generators.py generate l.352] (44/93) Reuse existing Prompt
[2024-03-04 21:36:32,660 DEBUG generators.py generate l.365] (44/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:32,661 DEBUG generators.py generate l.373] (44/93) Reuse post-processing
[2024-03-04 21:36:32,663 INFO generators.py gen_for_qa l.548] (44/93) * Start with LLM "claude-2.1"
[2024-03-04 21:36:32,663 DEBUG generators.py gen_for_qa l.554] (44/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:32,666 DEBUG generators.py generate l.352] (44/93) Reuse existing Prompt
[2024-03-04 21:36:32,667 DEBUG generators.py generate l.365] (44/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:32,668 DEBUG generators.py generate l.373] (44/93) Reuse post-processing
[2024-03-04 21:36:32,668 INFO generators.py gen_for_qa l.548] (44/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:36:32,670 DEBUG generators.py gen_for_qa l.554] (44/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:32,671 DEBUG generators.py generate l.352] (44/93) Reuse existing Prompt
[2024-03-04 21:36:32,672 DEBUG generators.py generate l.365] (44/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:32,672 DEBUG generators.py generate l.373] (44/93) Reuse post-processing
[2024-03-04 21:36:32,672 INFO generators.py gen_for_qa l.548] (44/93) * Start with LLM "command-nightly"
[2024-03-04 21:36:32,675 DEBUG generators.py gen_for_qa l.554] (44/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:32,675 DEBUG generators.py generate l.352] (44/93) Reuse existing Prompt
[2024-03-04 21:36:32,676 DEBUG generators.py generate l.365] (44/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:32,678 DEBUG generators.py generate l.373] (44/93) Reuse post-processing
[2024-03-04 21:36:32,679 INFO generators.py gen_for_qa l.548] (44/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:36:32,681 DEBUG generators.py generate l.349] (44/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:36:32,682 DEBUG generators.py generate l.358] (44/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:36:33,578 ERROR generators.py complete l.400] (44/93) The following exception occurred with prompt meta={} user=" Qui a inventé la dynamite ?  A)  Alfred Nobel B)  Ascanio Sobrero .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:36:33,586 DEBUG generators.py generate l.373] (44/93) Reuse post-processing
[2024-03-04 21:36:33,592 INFO generators.py generate l.477] (44/93) End question " Qui a inventé la dynamite ?  A)  Alfred Nobel B)  Ascanio Sobrero "
[2024-03-04 21:36:33,593 INFO generators.py generate l.475] (45/93) *** AnsGenerator for question " Qui a découvert les rayons X ?  A)  Wilhelm Röntgen B)  Thomas Edison C)  Nikola Tesla "
[2024-03-04 21:36:33,595 INFO generators.py gen_for_qa l.548] (45/93) * Start with LLM "gpt-4"
[2024-03-04 21:36:33,596 DEBUG generators.py gen_for_qa l.554] (45/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:33,598 DEBUG generators.py generate l.352] (45/93) Reuse existing Prompt
[2024-03-04 21:36:33,598 DEBUG generators.py generate l.365] (45/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:33,603 DEBUG generators.py generate l.373] (45/93) Reuse post-processing
[2024-03-04 21:36:33,604 INFO generators.py gen_for_qa l.548] (45/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:36:33,604 DEBUG generators.py gen_for_qa l.554] (45/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:33,605 DEBUG generators.py generate l.352] (45/93) Reuse existing Prompt
[2024-03-04 21:36:33,606 DEBUG generators.py generate l.365] (45/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:33,608 DEBUG generators.py generate l.373] (45/93) Reuse post-processing
[2024-03-04 21:36:33,609 INFO generators.py gen_for_qa l.548] (45/93) * Start with LLM "gemini-pro"
[2024-03-04 21:36:33,610 DEBUG generators.py gen_for_qa l.554] (45/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:33,611 DEBUG generators.py generate l.352] (45/93) Reuse existing Prompt
[2024-03-04 21:36:33,615 DEBUG generators.py generate l.365] (45/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:33,616 DEBUG generators.py generate l.373] (45/93) Reuse post-processing
[2024-03-04 21:36:33,617 INFO generators.py gen_for_qa l.548] (45/93) * Start with LLM "claude-2.1"
[2024-03-04 21:36:33,618 DEBUG generators.py gen_for_qa l.554] (45/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:33,620 DEBUG generators.py generate l.352] (45/93) Reuse existing Prompt
[2024-03-04 21:36:33,621 DEBUG generators.py generate l.365] (45/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:33,621 DEBUG generators.py generate l.373] (45/93) Reuse post-processing
[2024-03-04 21:36:33,623 INFO generators.py gen_for_qa l.548] (45/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:36:33,623 DEBUG generators.py gen_for_qa l.554] (45/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:33,624 DEBUG generators.py generate l.352] (45/93) Reuse existing Prompt
[2024-03-04 21:36:33,625 DEBUG generators.py generate l.365] (45/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:33,627 DEBUG generators.py generate l.373] (45/93) Reuse post-processing
[2024-03-04 21:36:33,628 INFO generators.py gen_for_qa l.548] (45/93) * Start with LLM "command-nightly"
[2024-03-04 21:36:33,628 DEBUG generators.py gen_for_qa l.554] (45/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:33,631 DEBUG generators.py generate l.352] (45/93) Reuse existing Prompt
[2024-03-04 21:36:33,631 DEBUG generators.py generate l.365] (45/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:33,635 DEBUG generators.py generate l.373] (45/93) Reuse post-processing
[2024-03-04 21:36:33,636 INFO generators.py gen_for_qa l.548] (45/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:36:33,637 DEBUG generators.py generate l.349] (45/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:36:33,638 DEBUG generators.py generate l.358] (45/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:36:34,438 ERROR generators.py complete l.400] (45/93) The following exception occurred with prompt meta={} user=" Qui a découvert les rayons X ?  A)  Wilhelm Röntgen B)  Thomas Edison C)  Nikola Tesla .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:36:34,461 DEBUG generators.py generate l.373] (45/93) Reuse post-processing
[2024-03-04 21:36:34,463 INFO generators.py generate l.477] (45/93) End question " Qui a découvert les rayons X ?  A)  Wilhelm Röntgen B)  Thomas Edison C)  Nikola Tesla "
[2024-03-04 21:36:34,466 INFO generators.py generate l.475] (46/93) *** AnsGenerator for question " Qui a inventé le gramophone ?  A)  Emile Berliner B)  Thomas Edison C)  Charles Cros "
[2024-03-04 21:36:34,468 INFO generators.py gen_for_qa l.548] (46/93) * Start with LLM "gpt-4"
[2024-03-04 21:36:34,469 DEBUG generators.py gen_for_qa l.554] (46/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:34,471 DEBUG generators.py generate l.352] (46/93) Reuse existing Prompt
[2024-03-04 21:36:34,472 DEBUG generators.py generate l.365] (46/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:34,473 DEBUG generators.py generate l.373] (46/93) Reuse post-processing
[2024-03-04 21:36:34,474 INFO generators.py gen_for_qa l.548] (46/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:36:34,476 DEBUG generators.py gen_for_qa l.554] (46/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:34,478 DEBUG generators.py generate l.352] (46/93) Reuse existing Prompt
[2024-03-04 21:36:34,480 DEBUG generators.py generate l.365] (46/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:34,482 DEBUG generators.py generate l.373] (46/93) Reuse post-processing
[2024-03-04 21:36:34,482 INFO generators.py gen_for_qa l.548] (46/93) * Start with LLM "gemini-pro"
[2024-03-04 21:36:34,482 DEBUG generators.py gen_for_qa l.554] (46/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:34,486 DEBUG generators.py generate l.352] (46/93) Reuse existing Prompt
[2024-03-04 21:36:34,487 DEBUG generators.py generate l.365] (46/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:34,488 DEBUG generators.py generate l.373] (46/93) Reuse post-processing
[2024-03-04 21:36:34,489 INFO generators.py gen_for_qa l.548] (46/93) * Start with LLM "claude-2.1"
[2024-03-04 21:36:34,490 DEBUG generators.py gen_for_qa l.554] (46/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:34,491 DEBUG generators.py generate l.352] (46/93) Reuse existing Prompt
[2024-03-04 21:36:34,492 DEBUG generators.py generate l.365] (46/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:34,493 DEBUG generators.py generate l.373] (46/93) Reuse post-processing
[2024-03-04 21:36:34,496 INFO generators.py gen_for_qa l.548] (46/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:36:34,497 DEBUG generators.py gen_for_qa l.554] (46/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:34,499 DEBUG generators.py generate l.352] (46/93) Reuse existing Prompt
[2024-03-04 21:36:34,499 DEBUG generators.py generate l.365] (46/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:34,501 DEBUG generators.py generate l.373] (46/93) Reuse post-processing
[2024-03-04 21:36:34,501 INFO generators.py gen_for_qa l.548] (46/93) * Start with LLM "command-nightly"
[2024-03-04 21:36:34,501 DEBUG generators.py gen_for_qa l.554] (46/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:34,501 DEBUG generators.py generate l.352] (46/93) Reuse existing Prompt
[2024-03-04 21:36:34,504 DEBUG generators.py generate l.365] (46/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:34,504 DEBUG generators.py generate l.373] (46/93) Reuse post-processing
[2024-03-04 21:36:34,506 INFO generators.py gen_for_qa l.548] (46/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:36:34,507 DEBUG generators.py generate l.349] (46/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:36:34,507 DEBUG generators.py generate l.358] (46/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:36:35,798 ERROR generators.py complete l.400] (46/93) The following exception occurred with prompt meta={} user=" Qui a inventé le gramophone ?  A)  Emile Berliner B)  Thomas Edison C)  Charles Cros .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:36:35,816 DEBUG generators.py generate l.373] (46/93) Reuse post-processing
[2024-03-04 21:36:35,817 INFO generators.py generate l.477] (46/93) End question " Qui a inventé le gramophone ?  A)  Emile Berliner B)  Thomas Edison C)  Charles Cros "
[2024-03-04 21:36:35,819 INFO generators.py generate l.475] (47/93) *** AnsGenerator for question " Qui a découvert les rayons cosmiques ?  A)  Victor Hess B)  Robert Millikan "
[2024-03-04 21:36:35,819 INFO generators.py gen_for_qa l.548] (47/93) * Start with LLM "gpt-4"
[2024-03-04 21:36:35,821 DEBUG generators.py gen_for_qa l.554] (47/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:35,821 DEBUG generators.py generate l.352] (47/93) Reuse existing Prompt
[2024-03-04 21:36:35,824 DEBUG generators.py generate l.365] (47/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:35,825 DEBUG generators.py generate l.373] (47/93) Reuse post-processing
[2024-03-04 21:36:35,827 INFO generators.py gen_for_qa l.548] (47/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:36:35,828 DEBUG generators.py gen_for_qa l.554] (47/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:35,831 DEBUG generators.py generate l.352] (47/93) Reuse existing Prompt
[2024-03-04 21:36:35,832 DEBUG generators.py generate l.365] (47/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:35,832 DEBUG generators.py generate l.373] (47/93) Reuse post-processing
[2024-03-04 21:36:35,834 INFO generators.py gen_for_qa l.548] (47/93) * Start with LLM "gemini-pro"
[2024-03-04 21:36:35,836 DEBUG generators.py gen_for_qa l.554] (47/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:35,837 DEBUG generators.py generate l.352] (47/93) Reuse existing Prompt
[2024-03-04 21:36:35,837 DEBUG generators.py generate l.365] (47/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:35,837 DEBUG generators.py generate l.373] (47/93) Reuse post-processing
[2024-03-04 21:36:35,841 INFO generators.py gen_for_qa l.548] (47/93) * Start with LLM "claude-2.1"
[2024-03-04 21:36:35,842 DEBUG generators.py gen_for_qa l.554] (47/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:35,844 DEBUG generators.py generate l.352] (47/93) Reuse existing Prompt
[2024-03-04 21:36:35,846 DEBUG generators.py generate l.365] (47/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:35,848 DEBUG generators.py generate l.373] (47/93) Reuse post-processing
[2024-03-04 21:36:35,849 INFO generators.py gen_for_qa l.548] (47/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:36:35,849 DEBUG generators.py gen_for_qa l.554] (47/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:35,850 DEBUG generators.py generate l.352] (47/93) Reuse existing Prompt
[2024-03-04 21:36:35,851 DEBUG generators.py generate l.365] (47/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:35,852 DEBUG generators.py generate l.373] (47/93) Reuse post-processing
[2024-03-04 21:36:35,853 INFO generators.py gen_for_qa l.548] (47/93) * Start with LLM "command-nightly"
[2024-03-04 21:36:35,854 DEBUG generators.py gen_for_qa l.554] (47/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:35,854 DEBUG generators.py generate l.352] (47/93) Reuse existing Prompt
[2024-03-04 21:36:35,854 DEBUG generators.py generate l.365] (47/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:35,854 DEBUG generators.py generate l.373] (47/93) Reuse post-processing
[2024-03-04 21:36:35,857 INFO generators.py gen_for_qa l.548] (47/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:36:35,858 DEBUG generators.py generate l.349] (47/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:36:35,859 DEBUG generators.py generate l.358] (47/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:36:37,713 ERROR generators.py complete l.400] (47/93) The following exception occurred with prompt meta={} user=" Qui a découvert les rayons cosmiques ?  A)  Victor Hess B)  Robert Millikan .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:36:37,731 DEBUG generators.py generate l.373] (47/93) Reuse post-processing
[2024-03-04 21:36:37,731 INFO generators.py generate l.477] (47/93) End question " Qui a découvert les rayons cosmiques ?  A)  Victor Hess B)  Robert Millikan "
[2024-03-04 21:36:37,731 INFO generators.py generate l.475] (48/93) *** AnsGenerator for question " Qui a inventé le premier satellite artificiel ?  A)  Sergueï Korolev B)  Wernher von Braun "
[2024-03-04 21:36:37,735 INFO generators.py gen_for_qa l.548] (48/93) * Start with LLM "gpt-4"
[2024-03-04 21:36:37,735 DEBUG generators.py gen_for_qa l.554] (48/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:37,738 DEBUG generators.py generate l.352] (48/93) Reuse existing Prompt
[2024-03-04 21:36:37,739 DEBUG generators.py generate l.365] (48/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:37,739 DEBUG generators.py generate l.373] (48/93) Reuse post-processing
[2024-03-04 21:36:37,741 INFO generators.py gen_for_qa l.548] (48/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:36:37,742 DEBUG generators.py gen_for_qa l.554] (48/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:37,744 DEBUG generators.py generate l.352] (48/93) Reuse existing Prompt
[2024-03-04 21:36:37,746 DEBUG generators.py generate l.365] (48/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:37,747 DEBUG generators.py generate l.373] (48/93) Reuse post-processing
[2024-03-04 21:36:37,749 INFO generators.py gen_for_qa l.548] (48/93) * Start with LLM "gemini-pro"
[2024-03-04 21:36:37,750 DEBUG generators.py gen_for_qa l.554] (48/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:37,750 DEBUG generators.py generate l.352] (48/93) Reuse existing Prompt
[2024-03-04 21:36:37,752 DEBUG generators.py generate l.365] (48/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:37,752 DEBUG generators.py generate l.373] (48/93) Reuse post-processing
[2024-03-04 21:36:37,754 INFO generators.py gen_for_qa l.548] (48/93) * Start with LLM "claude-2.1"
[2024-03-04 21:36:37,755 DEBUG generators.py gen_for_qa l.554] (48/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:37,755 DEBUG generators.py generate l.352] (48/93) Reuse existing Prompt
[2024-03-04 21:36:37,756 DEBUG generators.py generate l.365] (48/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:37,757 DEBUG generators.py generate l.373] (48/93) Reuse post-processing
[2024-03-04 21:36:37,758 INFO generators.py gen_for_qa l.548] (48/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:36:37,761 DEBUG generators.py gen_for_qa l.554] (48/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:37,763 DEBUG generators.py generate l.352] (48/93) Reuse existing Prompt
[2024-03-04 21:36:37,764 DEBUG generators.py generate l.365] (48/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:37,764 DEBUG generators.py generate l.373] (48/93) Reuse post-processing
[2024-03-04 21:36:37,765 INFO generators.py gen_for_qa l.548] (48/93) * Start with LLM "command-nightly"
[2024-03-04 21:36:37,766 DEBUG generators.py gen_for_qa l.554] (48/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:37,766 DEBUG generators.py generate l.352] (48/93) Reuse existing Prompt
[2024-03-04 21:36:37,766 DEBUG generators.py generate l.365] (48/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:37,769 DEBUG generators.py generate l.373] (48/93) Reuse post-processing
[2024-03-04 21:36:37,769 INFO generators.py gen_for_qa l.548] (48/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:36:37,770 DEBUG generators.py generate l.349] (48/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:36:37,771 DEBUG generators.py generate l.358] (48/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:36:38,786 ERROR generators.py complete l.400] (48/93) The following exception occurred with prompt meta={} user=" Qui a inventé le premier satellite artificiel ?  A)  Sergueï Korolev B)  Wernher von Braun .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:36:38,796 DEBUG generators.py generate l.373] (48/93) Reuse post-processing
[2024-03-04 21:36:38,798 INFO generators.py generate l.477] (48/93) End question " Qui a inventé le premier satellite artificiel ?  A)  Sergueï Korolev B)  Wernher von Braun "
[2024-03-04 21:36:38,799 INFO generators.py generate l.475] (49/93) *** AnsGenerator for question " Qui a découvert la loi de la conservation de l'énergie ?  A)  James Joule B)  Julius Robert Mayer C)  Hermann von Helmholtz "
[2024-03-04 21:36:38,800 INFO generators.py gen_for_qa l.548] (49/93) * Start with LLM "gpt-4"
[2024-03-04 21:36:38,801 DEBUG generators.py gen_for_qa l.554] (49/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:38,801 DEBUG generators.py generate l.352] (49/93) Reuse existing Prompt
[2024-03-04 21:36:38,803 DEBUG generators.py generate l.365] (49/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:38,803 DEBUG generators.py generate l.373] (49/93) Reuse post-processing
[2024-03-04 21:36:38,805 INFO generators.py gen_for_qa l.548] (49/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:36:38,805 DEBUG generators.py gen_for_qa l.554] (49/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:38,807 DEBUG generators.py generate l.352] (49/93) Reuse existing Prompt
[2024-03-04 21:36:38,810 DEBUG generators.py generate l.365] (49/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:38,811 DEBUG generators.py generate l.373] (49/93) Reuse post-processing
[2024-03-04 21:36:38,813 INFO generators.py gen_for_qa l.548] (49/93) * Start with LLM "gemini-pro"
[2024-03-04 21:36:38,813 DEBUG generators.py gen_for_qa l.554] (49/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:38,814 DEBUG generators.py generate l.352] (49/93) Reuse existing Prompt
[2024-03-04 21:36:38,815 DEBUG generators.py generate l.365] (49/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:38,816 DEBUG generators.py generate l.373] (49/93) Reuse post-processing
[2024-03-04 21:36:38,816 INFO generators.py gen_for_qa l.548] (49/93) * Start with LLM "claude-2.1"
[2024-03-04 21:36:38,818 DEBUG generators.py gen_for_qa l.554] (49/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:38,819 DEBUG generators.py generate l.352] (49/93) Reuse existing Prompt
[2024-03-04 21:36:38,819 DEBUG generators.py generate l.365] (49/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:38,821 DEBUG generators.py generate l.373] (49/93) Reuse post-processing
[2024-03-04 21:36:38,822 INFO generators.py gen_for_qa l.548] (49/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:36:38,822 DEBUG generators.py gen_for_qa l.554] (49/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:38,824 DEBUG generators.py generate l.352] (49/93) Reuse existing Prompt
[2024-03-04 21:36:38,824 DEBUG generators.py generate l.365] (49/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:38,826 DEBUG generators.py generate l.373] (49/93) Reuse post-processing
[2024-03-04 21:36:38,828 INFO generators.py gen_for_qa l.548] (49/93) * Start with LLM "command-nightly"
[2024-03-04 21:36:38,830 DEBUG generators.py gen_for_qa l.554] (49/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:38,831 DEBUG generators.py generate l.352] (49/93) Reuse existing Prompt
[2024-03-04 21:36:38,831 DEBUG generators.py generate l.365] (49/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:38,832 DEBUG generators.py generate l.373] (49/93) Reuse post-processing
[2024-03-04 21:36:38,833 INFO generators.py gen_for_qa l.548] (49/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:36:38,834 DEBUG generators.py generate l.349] (49/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:36:38,835 DEBUG generators.py generate l.358] (49/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:36:40,424 ERROR generators.py complete l.400] (49/93) The following exception occurred with prompt meta={} user=" Qui a découvert la loi de la conservation de l'énergie ?  A)  James Joule B)  Julius Robert Mayer C)  Hermann von Helmholtz .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:36:40,455 DEBUG generators.py generate l.373] (49/93) Reuse post-processing
[2024-03-04 21:36:40,458 INFO generators.py generate l.477] (49/93) End question " Qui a découvert la loi de la conservation de l'énergie ?  A)  James Joule B)  Julius Robert Mayer C)  Hermann von Helmholtz "
[2024-03-04 21:36:40,461 INFO generators.py generate l.475] (50/93) *** AnsGenerator for question " Qui a inventé la machine à calculer ?  A)  Blaise Pascal B)  Gottfried Leibniz C)  Charles Babbage "
[2024-03-04 21:36:40,465 INFO generators.py gen_for_qa l.548] (50/93) * Start with LLM "gpt-4"
[2024-03-04 21:36:40,466 DEBUG generators.py gen_for_qa l.554] (50/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:40,467 DEBUG generators.py generate l.352] (50/93) Reuse existing Prompt
[2024-03-04 21:36:40,468 DEBUG generators.py generate l.365] (50/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:40,470 DEBUG generators.py generate l.373] (50/93) Reuse post-processing
[2024-03-04 21:36:40,471 INFO generators.py gen_for_qa l.548] (50/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:36:40,472 DEBUG generators.py gen_for_qa l.554] (50/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:40,473 DEBUG generators.py generate l.352] (50/93) Reuse existing Prompt
[2024-03-04 21:36:40,475 DEBUG generators.py generate l.365] (50/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:40,476 DEBUG generators.py generate l.373] (50/93) Reuse post-processing
[2024-03-04 21:36:40,480 INFO generators.py gen_for_qa l.548] (50/93) * Start with LLM "gemini-pro"
[2024-03-04 21:36:40,481 DEBUG generators.py gen_for_qa l.554] (50/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:40,482 DEBUG generators.py generate l.352] (50/93) Reuse existing Prompt
[2024-03-04 21:36:40,482 DEBUG generators.py generate l.365] (50/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:40,483 DEBUG generators.py generate l.373] (50/93) Reuse post-processing
[2024-03-04 21:36:40,484 INFO generators.py gen_for_qa l.548] (50/93) * Start with LLM "claude-2.1"
[2024-03-04 21:36:40,485 DEBUG generators.py gen_for_qa l.554] (50/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:40,487 DEBUG generators.py generate l.352] (50/93) Reuse existing Prompt
[2024-03-04 21:36:40,488 DEBUG generators.py generate l.365] (50/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:40,488 DEBUG generators.py generate l.373] (50/93) Reuse post-processing
[2024-03-04 21:36:40,488 INFO generators.py gen_for_qa l.548] (50/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:36:40,491 DEBUG generators.py gen_for_qa l.554] (50/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:40,493 DEBUG generators.py generate l.352] (50/93) Reuse existing Prompt
[2024-03-04 21:36:40,495 DEBUG generators.py generate l.365] (50/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:40,496 DEBUG generators.py generate l.373] (50/93) Reuse post-processing
[2024-03-04 21:36:40,497 INFO generators.py gen_for_qa l.548] (50/93) * Start with LLM "command-nightly"
[2024-03-04 21:36:40,497 DEBUG generators.py gen_for_qa l.554] (50/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:40,497 DEBUG generators.py generate l.352] (50/93) Reuse existing Prompt
[2024-03-04 21:36:40,500 DEBUG generators.py generate l.365] (50/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:40,500 DEBUG generators.py generate l.373] (50/93) Reuse post-processing
[2024-03-04 21:36:40,501 INFO generators.py gen_for_qa l.548] (50/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:36:40,502 DEBUG generators.py generate l.349] (50/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:36:40,503 DEBUG generators.py generate l.358] (50/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:36:42,124 ERROR generators.py complete l.400] (50/93) The following exception occurred with prompt meta={} user=" Qui a inventé la machine à calculer ?  A)  Blaise Pascal B)  Gottfried Leibniz C)  Charles Babbage .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:36:42,147 DEBUG generators.py generate l.373] (50/93) Reuse post-processing
[2024-03-04 21:36:42,149 INFO generators.py generate l.477] (50/93) End question " Qui a inventé la machine à calculer ?  A)  Blaise Pascal B)  Gottfried Leibniz C)  Charles Babbage "
[2024-03-04 21:36:42,150 INFO generators.py generate l.475] (51/93) *** AnsGenerator for question " Qui a découvert la loi de la conservation de la masse ?  A)  Antoine Lavoisier B)  Mikhail Lomonosov "
[2024-03-04 21:36:42,152 INFO generators.py gen_for_qa l.548] (51/93) * Start with LLM "gpt-4"
[2024-03-04 21:36:42,152 DEBUG generators.py gen_for_qa l.554] (51/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:42,154 DEBUG generators.py generate l.352] (51/93) Reuse existing Prompt
[2024-03-04 21:36:42,156 DEBUG generators.py generate l.365] (51/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:42,157 DEBUG generators.py generate l.373] (51/93) Reuse post-processing
[2024-03-04 21:36:42,161 INFO generators.py gen_for_qa l.548] (51/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:36:42,162 DEBUG generators.py gen_for_qa l.554] (51/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:42,164 DEBUG generators.py generate l.352] (51/93) Reuse existing Prompt
[2024-03-04 21:36:42,165 DEBUG generators.py generate l.365] (51/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:42,166 DEBUG generators.py generate l.373] (51/93) Reuse post-processing
[2024-03-04 21:36:42,167 INFO generators.py gen_for_qa l.548] (51/93) * Start with LLM "gemini-pro"
[2024-03-04 21:36:42,167 DEBUG generators.py gen_for_qa l.554] (51/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:42,168 DEBUG generators.py generate l.352] (51/93) Reuse existing Prompt
[2024-03-04 21:36:42,168 DEBUG generators.py generate l.365] (51/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:42,168 DEBUG generators.py generate l.373] (51/93) Reuse post-processing
[2024-03-04 21:36:42,171 INFO generators.py gen_for_qa l.548] (51/93) * Start with LLM "claude-2.1"
[2024-03-04 21:36:42,172 DEBUG generators.py gen_for_qa l.554] (51/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:42,173 DEBUG generators.py generate l.352] (51/93) Reuse existing Prompt
[2024-03-04 21:36:42,176 DEBUG generators.py generate l.365] (51/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:42,177 DEBUG generators.py generate l.373] (51/93) Reuse post-processing
[2024-03-04 21:36:42,177 INFO generators.py gen_for_qa l.548] (51/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:36:42,177 DEBUG generators.py gen_for_qa l.554] (51/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:42,177 DEBUG generators.py generate l.352] (51/93) Reuse existing Prompt
[2024-03-04 21:36:42,181 DEBUG generators.py generate l.365] (51/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:42,183 DEBUG generators.py generate l.373] (51/93) Reuse post-processing
[2024-03-04 21:36:42,183 INFO generators.py gen_for_qa l.548] (51/93) * Start with LLM "command-nightly"
[2024-03-04 21:36:42,184 DEBUG generators.py gen_for_qa l.554] (51/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:42,185 DEBUG generators.py generate l.352] (51/93) Reuse existing Prompt
[2024-03-04 21:36:42,186 DEBUG generators.py generate l.365] (51/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:42,186 DEBUG generators.py generate l.373] (51/93) Reuse post-processing
[2024-03-04 21:36:42,188 INFO generators.py gen_for_qa l.548] (51/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:36:42,189 DEBUG generators.py generate l.349] (51/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:36:42,190 DEBUG generators.py generate l.358] (51/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:36:43,881 ERROR generators.py complete l.400] (51/93) The following exception occurred with prompt meta={} user=" Qui a découvert la loi de la conservation de la masse ?  A)  Antoine Lavoisier B)  Mikhail Lomonosov .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:36:43,893 DEBUG generators.py generate l.373] (51/93) Reuse post-processing
[2024-03-04 21:36:43,894 INFO generators.py generate l.477] (51/93) End question " Qui a découvert la loi de la conservation de la masse ?  A)  Antoine Lavoisier B)  Mikhail Lomonosov "
[2024-03-04 21:36:43,895 INFO generators.py generate l.475] (52/93) *** AnsGenerator for question " Qui a inventé le premier ordinateur programmable ?  A)  Konrad Zuse B)  John Atanasoff et Clifford Berry "
[2024-03-04 21:36:43,895 INFO generators.py gen_for_qa l.548] (52/93) * Start with LLM "gpt-4"
[2024-03-04 21:36:43,898 DEBUG generators.py gen_for_qa l.554] (52/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:43,899 DEBUG generators.py generate l.352] (52/93) Reuse existing Prompt
[2024-03-04 21:36:43,900 DEBUG generators.py generate l.365] (52/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:43,901 DEBUG generators.py generate l.373] (52/93) Reuse post-processing
[2024-03-04 21:36:43,902 INFO generators.py gen_for_qa l.548] (52/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:36:43,903 DEBUG generators.py gen_for_qa l.554] (52/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:43,904 DEBUG generators.py generate l.352] (52/93) Reuse existing Prompt
[2024-03-04 21:36:43,905 DEBUG generators.py generate l.365] (52/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:43,907 DEBUG generators.py generate l.373] (52/93) Reuse post-processing
[2024-03-04 21:36:43,908 INFO generators.py gen_for_qa l.548] (52/93) * Start with LLM "gemini-pro"
[2024-03-04 21:36:43,911 DEBUG generators.py gen_for_qa l.554] (52/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:43,912 DEBUG generators.py generate l.352] (52/93) Reuse existing Prompt
[2024-03-04 21:36:43,913 DEBUG generators.py generate l.365] (52/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:43,913 DEBUG generators.py generate l.373] (52/93) Reuse post-processing
[2024-03-04 21:36:43,914 INFO generators.py gen_for_qa l.548] (52/93) * Start with LLM "claude-2.1"
[2024-03-04 21:36:43,915 DEBUG generators.py gen_for_qa l.554] (52/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:43,915 DEBUG generators.py generate l.352] (52/93) Reuse existing Prompt
[2024-03-04 21:36:43,917 DEBUG generators.py generate l.365] (52/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:43,918 DEBUG generators.py generate l.373] (52/93) Reuse post-processing
[2024-03-04 21:36:43,918 INFO generators.py gen_for_qa l.548] (52/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:36:43,918 DEBUG generators.py gen_for_qa l.554] (52/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:43,921 DEBUG generators.py generate l.352] (52/93) Reuse existing Prompt
[2024-03-04 21:36:43,921 DEBUG generators.py generate l.365] (52/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:43,922 DEBUG generators.py generate l.373] (52/93) Reuse post-processing
[2024-03-04 21:36:43,925 INFO generators.py gen_for_qa l.548] (52/93) * Start with LLM "command-nightly"
[2024-03-04 21:36:43,926 DEBUG generators.py gen_for_qa l.554] (52/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:43,926 DEBUG generators.py generate l.352] (52/93) Reuse existing Prompt
[2024-03-04 21:36:43,928 DEBUG generators.py generate l.365] (52/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:43,928 DEBUG generators.py generate l.373] (52/93) Reuse post-processing
[2024-03-04 21:36:43,930 INFO generators.py gen_for_qa l.548] (52/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:36:43,931 DEBUG generators.py generate l.349] (52/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:36:43,932 DEBUG generators.py generate l.358] (52/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:36:45,670 ERROR generators.py complete l.400] (52/93) The following exception occurred with prompt meta={} user=" Qui a inventé le premier ordinateur programmable ?  A)  Konrad Zuse B)  John Atanasoff et Clifford Berry .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:36:45,691 DEBUG generators.py generate l.373] (52/93) Reuse post-processing
[2024-03-04 21:36:45,691 INFO generators.py generate l.477] (52/93) End question " Qui a inventé le premier ordinateur programmable ?  A)  Konrad Zuse B)  John Atanasoff et Clifford Berry "
[2024-03-04 21:36:45,695 INFO generators.py generate l.475] (53/93) *** AnsGenerator for question " Qui a découvert la structure de l'ADN ?  A)  James Watson et Francis Crick B)  Rosalind Franklin C)  Maurice Wilkins "
[2024-03-04 21:36:45,697 INFO generators.py gen_for_qa l.548] (53/93) * Start with LLM "gpt-4"
[2024-03-04 21:36:45,699 DEBUG generators.py gen_for_qa l.554] (53/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:45,700 DEBUG generators.py generate l.352] (53/93) Reuse existing Prompt
[2024-03-04 21:36:45,701 DEBUG generators.py generate l.365] (53/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:45,702 DEBUG generators.py generate l.373] (53/93) Reuse post-processing
[2024-03-04 21:36:45,703 INFO generators.py gen_for_qa l.548] (53/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:36:45,705 DEBUG generators.py gen_for_qa l.554] (53/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:45,706 DEBUG generators.py generate l.352] (53/93) Reuse existing Prompt
[2024-03-04 21:36:45,708 DEBUG generators.py generate l.365] (53/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:45,708 DEBUG generators.py generate l.373] (53/93) Reuse post-processing
[2024-03-04 21:36:45,711 INFO generators.py gen_for_qa l.548] (53/93) * Start with LLM "gemini-pro"
[2024-03-04 21:36:45,712 DEBUG generators.py gen_for_qa l.554] (53/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:45,713 DEBUG generators.py generate l.352] (53/93) Reuse existing Prompt
[2024-03-04 21:36:45,714 DEBUG generators.py generate l.365] (53/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:45,715 DEBUG generators.py generate l.373] (53/93) Reuse post-processing
[2024-03-04 21:36:45,717 INFO generators.py gen_for_qa l.548] (53/93) * Start with LLM "claude-2.1"
[2024-03-04 21:36:45,717 DEBUG generators.py gen_for_qa l.554] (53/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:45,717 DEBUG generators.py generate l.352] (53/93) Reuse existing Prompt
[2024-03-04 21:36:45,720 DEBUG generators.py generate l.365] (53/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:45,721 DEBUG generators.py generate l.373] (53/93) Reuse post-processing
[2024-03-04 21:36:45,722 INFO generators.py gen_for_qa l.548] (53/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:36:45,723 DEBUG generators.py gen_for_qa l.554] (53/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:45,724 DEBUG generators.py generate l.352] (53/93) Reuse existing Prompt
[2024-03-04 21:36:45,726 DEBUG generators.py generate l.365] (53/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:45,726 DEBUG generators.py generate l.373] (53/93) Reuse post-processing
[2024-03-04 21:36:45,728 INFO generators.py gen_for_qa l.548] (53/93) * Start with LLM "command-nightly"
[2024-03-04 21:36:45,729 DEBUG generators.py gen_for_qa l.554] (53/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:45,730 DEBUG generators.py generate l.352] (53/93) Reuse existing Prompt
[2024-03-04 21:36:45,730 DEBUG generators.py generate l.365] (53/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:45,732 DEBUG generators.py generate l.373] (53/93) Reuse post-processing
[2024-03-04 21:36:45,732 INFO generators.py gen_for_qa l.548] (53/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:36:45,733 DEBUG generators.py generate l.349] (53/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:36:45,734 DEBUG generators.py generate l.358] (53/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:36:46,802 ERROR generators.py complete l.400] (53/93) The following exception occurred with prompt meta={} user=" Qui a découvert la structure de l'ADN ?  A)  James Watson et Francis Crick B)  Rosalind Franklin C)  Maurice Wilkins .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:36:46,812 DEBUG generators.py generate l.373] (53/93) Reuse post-processing
[2024-03-04 21:36:46,821 INFO generators.py generate l.477] (53/93) End question " Qui a découvert la structure de l'ADN ?  A)  James Watson et Francis Crick B)  Rosalind Franklin C)  Maurice Wilkins "
[2024-03-04 21:36:46,822 INFO generators.py generate l.475] (54/93) *** AnsGenerator for question " Qui a inventé le premier laser fonctionnel ?  A)  Theodore Maiman B)  Gordon Gould C)  Charles Hard Townes "
[2024-03-04 21:36:46,822 INFO generators.py gen_for_qa l.548] (54/93) * Start with LLM "gpt-4"
[2024-03-04 21:36:46,827 DEBUG generators.py gen_for_qa l.554] (54/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:46,829 DEBUG generators.py generate l.352] (54/93) Reuse existing Prompt
[2024-03-04 21:36:46,831 DEBUG generators.py generate l.365] (54/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:46,832 DEBUG generators.py generate l.373] (54/93) Reuse post-processing
[2024-03-04 21:36:46,833 INFO generators.py gen_for_qa l.548] (54/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:36:46,833 DEBUG generators.py gen_for_qa l.554] (54/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:46,835 DEBUG generators.py generate l.352] (54/93) Reuse existing Prompt
[2024-03-04 21:36:46,837 DEBUG generators.py generate l.365] (54/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:46,838 DEBUG generators.py generate l.373] (54/93) Reuse post-processing
[2024-03-04 21:36:46,839 INFO generators.py gen_for_qa l.548] (54/93) * Start with LLM "gemini-pro"
[2024-03-04 21:36:46,843 DEBUG generators.py gen_for_qa l.554] (54/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:46,844 DEBUG generators.py generate l.352] (54/93) Reuse existing Prompt
[2024-03-04 21:36:46,844 DEBUG generators.py generate l.365] (54/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:46,844 DEBUG generators.py generate l.373] (54/93) Reuse post-processing
[2024-03-04 21:36:46,844 INFO generators.py gen_for_qa l.548] (54/93) * Start with LLM "claude-2.1"
[2024-03-04 21:36:46,849 DEBUG generators.py gen_for_qa l.554] (54/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:46,849 DEBUG generators.py generate l.352] (54/93) Reuse existing Prompt
[2024-03-04 21:36:46,851 DEBUG generators.py generate l.365] (54/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:46,851 DEBUG generators.py generate l.373] (54/93) Reuse post-processing
[2024-03-04 21:36:46,852 INFO generators.py gen_for_qa l.548] (54/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:36:46,853 DEBUG generators.py gen_for_qa l.554] (54/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:46,854 DEBUG generators.py generate l.352] (54/93) Reuse existing Prompt
[2024-03-04 21:36:46,855 DEBUG generators.py generate l.365] (54/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:46,857 DEBUG generators.py generate l.373] (54/93) Reuse post-processing
[2024-03-04 21:36:46,859 INFO generators.py gen_for_qa l.548] (54/93) * Start with LLM "command-nightly"
[2024-03-04 21:36:46,861 DEBUG generators.py gen_for_qa l.554] (54/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:46,862 DEBUG generators.py generate l.352] (54/93) Reuse existing Prompt
[2024-03-04 21:36:46,862 DEBUG generators.py generate l.365] (54/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:46,863 DEBUG generators.py generate l.373] (54/93) Reuse post-processing
[2024-03-04 21:36:46,864 INFO generators.py gen_for_qa l.548] (54/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:36:46,865 DEBUG generators.py generate l.349] (54/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:36:46,866 DEBUG generators.py generate l.358] (54/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:36:47,751 ERROR generators.py complete l.400] (54/93) The following exception occurred with prompt meta={} user=" Qui a inventé le premier laser fonctionnel ?  A)  Theodore Maiman B)  Gordon Gould C)  Charles Hard Townes .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:36:47,767 DEBUG generators.py generate l.373] (54/93) Reuse post-processing
[2024-03-04 21:36:47,769 INFO generators.py generate l.477] (54/93) End question " Qui a inventé le premier laser fonctionnel ?  A)  Theodore Maiman B)  Gordon Gould C)  Charles Hard Townes "
[2024-03-04 21:36:47,772 INFO generators.py generate l.475] (55/93) *** AnsGenerator for question " Qui a découvert la loi de la conservation de la quantité de mouvement ?  A)  Isaac Newton B)  René Descartes C)  Christiaan Huygens "
[2024-03-04 21:36:47,773 INFO generators.py gen_for_qa l.548] (55/93) * Start with LLM "gpt-4"
[2024-03-04 21:36:47,776 DEBUG generators.py gen_for_qa l.554] (55/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:47,777 DEBUG generators.py generate l.352] (55/93) Reuse existing Prompt
[2024-03-04 21:36:47,778 DEBUG generators.py generate l.365] (55/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:47,780 DEBUG generators.py generate l.373] (55/93) Reuse post-processing
[2024-03-04 21:36:47,781 INFO generators.py gen_for_qa l.548] (55/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:36:47,781 DEBUG generators.py gen_for_qa l.554] (55/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:47,783 DEBUG generators.py generate l.352] (55/93) Reuse existing Prompt
[2024-03-04 21:36:47,783 DEBUG generators.py generate l.365] (55/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:47,785 DEBUG generators.py generate l.373] (55/93) Reuse post-processing
[2024-03-04 21:36:47,787 INFO generators.py gen_for_qa l.548] (55/93) * Start with LLM "gemini-pro"
[2024-03-04 21:36:47,788 DEBUG generators.py gen_for_qa l.554] (55/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:47,789 DEBUG generators.py generate l.352] (55/93) Reuse existing Prompt
[2024-03-04 21:36:47,790 DEBUG generators.py generate l.365] (55/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:47,793 DEBUG generators.py generate l.373] (55/93) Reuse post-processing
[2024-03-04 21:36:47,794 INFO generators.py gen_for_qa l.548] (55/93) * Start with LLM "claude-2.1"
[2024-03-04 21:36:47,795 DEBUG generators.py gen_for_qa l.554] (55/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:47,796 DEBUG generators.py generate l.352] (55/93) Reuse existing Prompt
[2024-03-04 21:36:47,797 DEBUG generators.py generate l.365] (55/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:47,797 DEBUG generators.py generate l.373] (55/93) Reuse post-processing
[2024-03-04 21:36:47,797 INFO generators.py gen_for_qa l.548] (55/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:36:47,799 DEBUG generators.py gen_for_qa l.554] (55/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:47,800 DEBUG generators.py generate l.352] (55/93) Reuse existing Prompt
[2024-03-04 21:36:47,800 DEBUG generators.py generate l.365] (55/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:47,800 DEBUG generators.py generate l.373] (55/93) Reuse post-processing
[2024-03-04 21:36:47,803 INFO generators.py gen_for_qa l.548] (55/93) * Start with LLM "command-nightly"
[2024-03-04 21:36:47,803 DEBUG generators.py gen_for_qa l.554] (55/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:47,804 DEBUG generators.py generate l.352] (55/93) Reuse existing Prompt
[2024-03-04 21:36:47,806 DEBUG generators.py generate l.365] (55/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:47,807 DEBUG generators.py generate l.373] (55/93) Reuse post-processing
[2024-03-04 21:36:47,809 INFO generators.py gen_for_qa l.548] (55/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:36:47,810 DEBUG generators.py generate l.349] (55/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:36:47,811 DEBUG generators.py generate l.358] (55/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:36:49,869 ERROR generators.py complete l.400] (55/93) The following exception occurred with prompt meta={} user=" Qui a découvert la loi de la conservation de la quantité de mouvement ?  A)  Isaac Newton B)  René Descartes C)  Christiaan Huygens .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:36:49,888 DEBUG generators.py generate l.373] (55/93) Reuse post-processing
[2024-03-04 21:36:49,890 INFO generators.py generate l.477] (55/93) End question " Qui a découvert la loi de la conservation de la quantité de mouvement ?  A)  Isaac Newton B)  René Descartes C)  Christiaan Huygens "
[2024-03-04 21:36:49,890 INFO generators.py generate l.475] (56/93) *** AnsGenerator for question " Qui a inventé le premier microprocesseur ?  A)  Federico Faggin, Ted Hoff et Stanley Mazor B)  Marcian Hoff "
[2024-03-04 21:36:49,893 INFO generators.py gen_for_qa l.548] (56/93) * Start with LLM "gpt-4"
[2024-03-04 21:36:49,895 DEBUG generators.py gen_for_qa l.554] (56/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:49,896 DEBUG generators.py generate l.352] (56/93) Reuse existing Prompt
[2024-03-04 21:36:49,897 DEBUG generators.py generate l.365] (56/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:49,898 DEBUG generators.py generate l.373] (56/93) Reuse post-processing
[2024-03-04 21:36:49,899 INFO generators.py gen_for_qa l.548] (56/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:36:49,900 DEBUG generators.py gen_for_qa l.554] (56/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:49,900 DEBUG generators.py generate l.352] (56/93) Reuse existing Prompt
[2024-03-04 21:36:49,900 DEBUG generators.py generate l.365] (56/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:49,903 DEBUG generators.py generate l.373] (56/93) Reuse post-processing
[2024-03-04 21:36:49,903 INFO generators.py gen_for_qa l.548] (56/93) * Start with LLM "gemini-pro"
[2024-03-04 21:36:49,906 DEBUG generators.py gen_for_qa l.554] (56/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:49,907 DEBUG generators.py generate l.352] (56/93) Reuse existing Prompt
[2024-03-04 21:36:49,908 DEBUG generators.py generate l.365] (56/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:49,910 DEBUG generators.py generate l.373] (56/93) Reuse post-processing
[2024-03-04 21:36:49,911 INFO generators.py gen_for_qa l.548] (56/93) * Start with LLM "claude-2.1"
[2024-03-04 21:36:49,911 DEBUG generators.py gen_for_qa l.554] (56/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:49,912 DEBUG generators.py generate l.352] (56/93) Reuse existing Prompt
[2024-03-04 21:36:49,914 DEBUG generators.py generate l.365] (56/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:49,915 DEBUG generators.py generate l.373] (56/93) Reuse post-processing
[2024-03-04 21:36:49,915 INFO generators.py gen_for_qa l.548] (56/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:36:49,916 DEBUG generators.py gen_for_qa l.554] (56/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:49,918 DEBUG generators.py generate l.352] (56/93) Reuse existing Prompt
[2024-03-04 21:36:49,919 DEBUG generators.py generate l.365] (56/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:49,919 DEBUG generators.py generate l.373] (56/93) Reuse post-processing
[2024-03-04 21:36:49,920 INFO generators.py gen_for_qa l.548] (56/93) * Start with LLM "command-nightly"
[2024-03-04 21:36:49,921 DEBUG generators.py gen_for_qa l.554] (56/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:49,922 DEBUG generators.py generate l.352] (56/93) Reuse existing Prompt
[2024-03-04 21:36:49,925 DEBUG generators.py generate l.365] (56/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:49,926 DEBUG generators.py generate l.373] (56/93) Reuse post-processing
[2024-03-04 21:36:49,927 INFO generators.py gen_for_qa l.548] (56/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:36:49,928 DEBUG generators.py generate l.349] (56/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:36:49,929 DEBUG generators.py generate l.358] (56/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:36:50,683 ERROR generators.py complete l.400] (56/93) The following exception occurred with prompt meta={} user=" Qui a inventé le premier microprocesseur ?  A)  Federico Faggin, Ted Hoff et Stanley Mazor B)  Marcian Hoff .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:36:50,705 DEBUG generators.py generate l.373] (56/93) Reuse post-processing
[2024-03-04 21:36:50,705 INFO generators.py generate l.477] (56/93) End question " Qui a inventé le premier microprocesseur ?  A)  Federico Faggin, Ted Hoff et Stanley Mazor B)  Marcian Hoff "
[2024-03-04 21:36:50,710 INFO generators.py generate l.475] (57/93) *** AnsGenerator for question " Qui a découvert la loi de la conservation de la charge électrique ?  A)  Michael Faraday B)  Charles-Augustin de Coulomb C)  Benjamin Franklin "
[2024-03-04 21:36:50,713 INFO generators.py gen_for_qa l.548] (57/93) * Start with LLM "gpt-4"
[2024-03-04 21:36:50,714 DEBUG generators.py gen_for_qa l.554] (57/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:50,715 DEBUG generators.py generate l.352] (57/93) Reuse existing Prompt
[2024-03-04 21:36:50,716 DEBUG generators.py generate l.365] (57/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:50,717 DEBUG generators.py generate l.373] (57/93) Reuse post-processing
[2024-03-04 21:36:50,719 INFO generators.py gen_for_qa l.548] (57/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:36:50,720 DEBUG generators.py gen_for_qa l.554] (57/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:50,721 DEBUG generators.py generate l.352] (57/93) Reuse existing Prompt
[2024-03-04 21:36:50,724 DEBUG generators.py generate l.365] (57/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:50,726 DEBUG generators.py generate l.373] (57/93) Reuse post-processing
[2024-03-04 21:36:50,726 INFO generators.py gen_for_qa l.548] (57/93) * Start with LLM "gemini-pro"
[2024-03-04 21:36:50,726 DEBUG generators.py gen_for_qa l.554] (57/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:50,730 DEBUG generators.py generate l.352] (57/93) Reuse existing Prompt
[2024-03-04 21:36:50,731 DEBUG generators.py generate l.365] (57/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:50,731 DEBUG generators.py generate l.373] (57/93) Reuse post-processing
[2024-03-04 21:36:50,732 INFO generators.py gen_for_qa l.548] (57/93) * Start with LLM "claude-2.1"
[2024-03-04 21:36:50,734 DEBUG generators.py gen_for_qa l.554] (57/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:50,734 DEBUG generators.py generate l.352] (57/93) Reuse existing Prompt
[2024-03-04 21:36:50,734 DEBUG generators.py generate l.365] (57/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:50,737 DEBUG generators.py generate l.373] (57/93) Reuse post-processing
[2024-03-04 21:36:50,740 INFO generators.py gen_for_qa l.548] (57/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:36:50,742 DEBUG generators.py gen_for_qa l.554] (57/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:50,743 DEBUG generators.py generate l.352] (57/93) Reuse existing Prompt
[2024-03-04 21:36:50,744 DEBUG generators.py generate l.365] (57/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:50,745 DEBUG generators.py generate l.373] (57/93) Reuse post-processing
[2024-03-04 21:36:50,746 INFO generators.py gen_for_qa l.548] (57/93) * Start with LLM "command-nightly"
[2024-03-04 21:36:50,747 DEBUG generators.py gen_for_qa l.554] (57/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:50,747 DEBUG generators.py generate l.352] (57/93) Reuse existing Prompt
[2024-03-04 21:36:50,748 DEBUG generators.py generate l.365] (57/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:50,750 DEBUG generators.py generate l.373] (57/93) Reuse post-processing
[2024-03-04 21:36:50,750 INFO generators.py gen_for_qa l.548] (57/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:36:50,752 DEBUG generators.py generate l.349] (57/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:36:50,753 DEBUG generators.py generate l.358] (57/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:36:51,692 ERROR generators.py complete l.400] (57/93) The following exception occurred with prompt meta={} user=" Qui a découvert la loi de la conservation de la charge électrique ?  A)  Michael Faraday B)  Charles-Augustin de Coulomb C)  Benjamin Franklin .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:36:51,713 DEBUG generators.py generate l.373] (57/93) Reuse post-processing
[2024-03-04 21:36:51,716 INFO generators.py generate l.477] (57/93) End question " Qui a découvert la loi de la conservation de la charge électrique ?  A)  Michael Faraday B)  Charles-Augustin de Coulomb C)  Benjamin Franklin "
[2024-03-04 21:36:51,717 INFO generators.py generate l.475] (58/93) *** AnsGenerator for question " Qui a inventé le premier téléphone portable ?  A)  Martin Cooper B)  Rudy Krolopp "
[2024-03-04 21:36:51,718 INFO generators.py gen_for_qa l.548] (58/93) * Start with LLM "gpt-4"
[2024-03-04 21:36:51,720 DEBUG generators.py gen_for_qa l.554] (58/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:51,724 DEBUG generators.py generate l.352] (58/93) Reuse existing Prompt
[2024-03-04 21:36:51,726 DEBUG generators.py generate l.365] (58/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:51,727 DEBUG generators.py generate l.373] (58/93) Reuse post-processing
[2024-03-04 21:36:51,727 INFO generators.py gen_for_qa l.548] (58/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:36:51,729 DEBUG generators.py gen_for_qa l.554] (58/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:51,730 DEBUG generators.py generate l.352] (58/93) Reuse existing Prompt
[2024-03-04 21:36:51,731 DEBUG generators.py generate l.365] (58/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:51,731 DEBUG generators.py generate l.373] (58/93) Reuse post-processing
[2024-03-04 21:36:51,733 INFO generators.py gen_for_qa l.548] (58/93) * Start with LLM "gemini-pro"
[2024-03-04 21:36:51,734 DEBUG generators.py gen_for_qa l.554] (58/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:51,734 DEBUG generators.py generate l.352] (58/93) Reuse existing Prompt
[2024-03-04 21:36:51,736 DEBUG generators.py generate l.365] (58/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:51,738 DEBUG generators.py generate l.373] (58/93) Reuse post-processing
[2024-03-04 21:36:51,740 INFO generators.py gen_for_qa l.548] (58/93) * Start with LLM "claude-2.1"
[2024-03-04 21:36:51,741 DEBUG generators.py gen_for_qa l.554] (58/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:51,741 DEBUG generators.py generate l.352] (58/93) Reuse existing Prompt
[2024-03-04 21:36:51,741 DEBUG generators.py generate l.365] (58/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:51,741 DEBUG generators.py generate l.373] (58/93) Reuse post-processing
[2024-03-04 21:36:51,745 INFO generators.py gen_for_qa l.548] (58/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:36:51,745 DEBUG generators.py gen_for_qa l.554] (58/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:51,747 DEBUG generators.py generate l.352] (58/93) Reuse existing Prompt
[2024-03-04 21:36:51,748 DEBUG generators.py generate l.365] (58/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:51,749 DEBUG generators.py generate l.373] (58/93) Reuse post-processing
[2024-03-04 21:36:51,749 INFO generators.py gen_for_qa l.548] (58/93) * Start with LLM "command-nightly"
[2024-03-04 21:36:51,750 DEBUG generators.py gen_for_qa l.554] (58/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:51,751 DEBUG generators.py generate l.352] (58/93) Reuse existing Prompt
[2024-03-04 21:36:51,752 DEBUG generators.py generate l.365] (58/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:51,753 DEBUG generators.py generate l.373] (58/93) Reuse post-processing
[2024-03-04 21:36:51,755 INFO generators.py gen_for_qa l.548] (58/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:36:51,757 DEBUG generators.py generate l.349] (58/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:36:51,758 DEBUG generators.py generate l.358] (58/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:36:52,855 ERROR generators.py complete l.400] (58/93) The following exception occurred with prompt meta={} user=" Qui a inventé le premier téléphone portable ?  A)  Martin Cooper B)  Rudy Krolopp .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:36:52,875 DEBUG generators.py generate l.373] (58/93) Reuse post-processing
[2024-03-04 21:36:52,877 INFO generators.py generate l.477] (58/93) End question " Qui a inventé le premier téléphone portable ?  A)  Martin Cooper B)  Rudy Krolopp "
[2024-03-04 21:36:52,878 INFO generators.py generate l.475] (59/93) *** AnsGenerator for question " Où a été inventé le premier avion ?  A)  Kitty Hawk B)  Le Bourget C)  Bridgeport "
[2024-03-04 21:36:52,879 INFO generators.py gen_for_qa l.548] (59/93) * Start with LLM "gpt-4"
[2024-03-04 21:36:52,880 DEBUG generators.py gen_for_qa l.554] (59/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:52,882 DEBUG generators.py generate l.352] (59/93) Reuse existing Prompt
[2024-03-04 21:36:52,883 DEBUG generators.py generate l.365] (59/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:52,883 DEBUG generators.py generate l.373] (59/93) Reuse post-processing
[2024-03-04 21:36:52,886 INFO generators.py gen_for_qa l.548] (59/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:36:52,886 DEBUG generators.py gen_for_qa l.554] (59/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:52,890 DEBUG generators.py generate l.352] (59/93) Reuse existing Prompt
[2024-03-04 21:36:52,891 DEBUG generators.py generate l.365] (59/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:52,893 DEBUG generators.py generate l.373] (59/93) Reuse post-processing
[2024-03-04 21:36:52,894 INFO generators.py gen_for_qa l.548] (59/93) * Start with LLM "gemini-pro"
[2024-03-04 21:36:52,894 DEBUG generators.py gen_for_qa l.554] (59/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:52,895 DEBUG generators.py generate l.352] (59/93) Reuse existing Prompt
[2024-03-04 21:36:52,895 DEBUG generators.py generate l.365] (59/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:52,895 DEBUG generators.py generate l.373] (59/93) Reuse post-processing
[2024-03-04 21:36:52,898 INFO generators.py gen_for_qa l.548] (59/93) * Start with LLM "claude-2.1"
[2024-03-04 21:36:52,899 DEBUG generators.py gen_for_qa l.554] (59/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:52,901 DEBUG generators.py generate l.352] (59/93) Reuse existing Prompt
[2024-03-04 21:36:52,902 DEBUG generators.py generate l.365] (59/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:52,903 DEBUG generators.py generate l.373] (59/93) Reuse post-processing
[2024-03-04 21:36:52,905 INFO generators.py gen_for_qa l.548] (59/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:36:52,905 DEBUG generators.py gen_for_qa l.554] (59/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:52,905 DEBUG generators.py generate l.352] (59/93) Reuse existing Prompt
[2024-03-04 21:36:52,909 DEBUG generators.py generate l.365] (59/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:52,909 DEBUG generators.py generate l.373] (59/93) Reuse post-processing
[2024-03-04 21:36:52,910 INFO generators.py gen_for_qa l.548] (59/93) * Start with LLM "command-nightly"
[2024-03-04 21:36:52,911 DEBUG generators.py gen_for_qa l.554] (59/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:52,912 DEBUG generators.py generate l.352] (59/93) Reuse existing Prompt
[2024-03-04 21:36:52,914 DEBUG generators.py generate l.365] (59/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:52,914 DEBUG generators.py generate l.373] (59/93) Reuse post-processing
[2024-03-04 21:36:52,915 INFO generators.py gen_for_qa l.548] (59/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:36:52,916 DEBUG generators.py generate l.349] (59/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:36:52,916 DEBUG generators.py generate l.358] (59/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:36:54,925 ERROR generators.py complete l.400] (59/93) The following exception occurred with prompt meta={} user=" Où a été inventé le premier avion ?  A)  Kitty Hawk B)  Le Bourget C)  Bridgeport .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:36:54,951 DEBUG generators.py generate l.373] (59/93) Reuse post-processing
[2024-03-04 21:36:54,953 INFO generators.py generate l.477] (59/93) End question " Où a été inventé le premier avion ?  A)  Kitty Hawk B)  Le Bourget C)  Bridgeport "
[2024-03-04 21:36:54,953 INFO generators.py generate l.475] (60/93) *** AnsGenerator for question " Où a été découvert l'Amérique par Christophe Colomb ?  A)  San Salvador B)  Pointe Isabela C)  Guanahani "
[2024-03-04 21:36:54,953 INFO generators.py gen_for_qa l.548] (60/93) * Start with LLM "gpt-4"
[2024-03-04 21:36:54,959 DEBUG generators.py gen_for_qa l.554] (60/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:54,960 DEBUG generators.py generate l.352] (60/93) Reuse existing Prompt
[2024-03-04 21:36:54,961 DEBUG generators.py generate l.365] (60/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:54,962 DEBUG generators.py generate l.373] (60/93) Reuse post-processing
[2024-03-04 21:36:54,963 INFO generators.py gen_for_qa l.548] (60/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:36:54,963 DEBUG generators.py gen_for_qa l.554] (60/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:54,965 DEBUG generators.py generate l.352] (60/93) Reuse existing Prompt
[2024-03-04 21:36:54,965 DEBUG generators.py generate l.365] (60/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:54,967 DEBUG generators.py generate l.373] (60/93) Reuse post-processing
[2024-03-04 21:36:54,968 INFO generators.py gen_for_qa l.548] (60/93) * Start with LLM "gemini-pro"
[2024-03-04 21:36:54,972 DEBUG generators.py gen_for_qa l.554] (60/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:54,973 DEBUG generators.py generate l.352] (60/93) Reuse existing Prompt
[2024-03-04 21:36:54,974 DEBUG generators.py generate l.365] (60/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:54,974 DEBUG generators.py generate l.373] (60/93) Reuse post-processing
[2024-03-04 21:36:54,977 INFO generators.py gen_for_qa l.548] (60/93) * Start with LLM "claude-2.1"
[2024-03-04 21:36:54,978 DEBUG generators.py gen_for_qa l.554] (60/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:54,979 DEBUG generators.py generate l.352] (60/93) Reuse existing Prompt
[2024-03-04 21:36:54,979 DEBUG generators.py generate l.365] (60/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:54,981 DEBUG generators.py generate l.373] (60/93) Reuse post-processing
[2024-03-04 21:36:54,981 INFO generators.py gen_for_qa l.548] (60/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:36:54,982 DEBUG generators.py gen_for_qa l.554] (60/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:54,983 DEBUG generators.py generate l.352] (60/93) Reuse existing Prompt
[2024-03-04 21:36:54,983 DEBUG generators.py generate l.365] (60/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:54,985 DEBUG generators.py generate l.373] (60/93) Reuse post-processing
[2024-03-04 21:36:54,987 INFO generators.py gen_for_qa l.548] (60/93) * Start with LLM "command-nightly"
[2024-03-04 21:36:54,989 DEBUG generators.py gen_for_qa l.554] (60/93) An Answer has already been generated with this LLM
[2024-03-04 21:36:54,991 DEBUG generators.py generate l.352] (60/93) Reuse existing Prompt
[2024-03-04 21:36:54,992 DEBUG generators.py generate l.365] (60/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:36:54,993 DEBUG generators.py generate l.373] (60/93) Reuse post-processing
[2024-03-04 21:36:54,994 INFO generators.py gen_for_qa l.548] (60/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:36:54,994 DEBUG generators.py generate l.349] (60/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:36:54,995 DEBUG generators.py generate l.358] (60/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:37:00,088 ERROR generators.py complete l.400] (60/93) The following exception occurred with prompt meta={} user=" Où a été découvert l'Amérique par Christophe Colomb ?  A)  San Salvador B)  Pointe Isabela C)  Guanahani .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:37:00,102 DEBUG generators.py generate l.373] (60/93) Reuse post-processing
[2024-03-04 21:37:00,105 INFO generators.py generate l.477] (60/93) End question " Où a été découvert l'Amérique par Christophe Colomb ?  A)  San Salvador B)  Pointe Isabela C)  Guanahani "
[2024-03-04 21:37:00,108 INFO generators.py generate l.475] (61/93) *** AnsGenerator for question " Où a été inventé le premier ordinateur programmable ?  A)  Berlin B)  Iowa C)  Bletchley Park "
[2024-03-04 21:37:00,109 INFO generators.py gen_for_qa l.548] (61/93) * Start with LLM "gpt-4"
[2024-03-04 21:37:00,110 DEBUG generators.py gen_for_qa l.554] (61/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:00,111 DEBUG generators.py generate l.352] (61/93) Reuse existing Prompt
[2024-03-04 21:37:00,111 DEBUG generators.py generate l.365] (61/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:00,111 DEBUG generators.py generate l.373] (61/93) Reuse post-processing
[2024-03-04 21:37:00,114 INFO generators.py gen_for_qa l.548] (61/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:37:00,116 DEBUG generators.py gen_for_qa l.554] (61/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:00,117 DEBUG generators.py generate l.352] (61/93) Reuse existing Prompt
[2024-03-04 21:37:00,119 DEBUG generators.py generate l.365] (61/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:00,121 DEBUG generators.py generate l.373] (61/93) Reuse post-processing
[2024-03-04 21:37:00,124 INFO generators.py gen_for_qa l.548] (61/93) * Start with LLM "gemini-pro"
[2024-03-04 21:37:00,124 DEBUG generators.py gen_for_qa l.554] (61/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:00,126 DEBUG generators.py generate l.352] (61/93) Reuse existing Prompt
[2024-03-04 21:37:00,127 DEBUG generators.py generate l.365] (61/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:00,129 DEBUG generators.py generate l.373] (61/93) Reuse post-processing
[2024-03-04 21:37:00,130 INFO generators.py gen_for_qa l.548] (61/93) * Start with LLM "claude-2.1"
[2024-03-04 21:37:00,131 DEBUG generators.py gen_for_qa l.554] (61/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:00,131 DEBUG generators.py generate l.352] (61/93) Reuse existing Prompt
[2024-03-04 21:37:00,131 DEBUG generators.py generate l.365] (61/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:00,134 DEBUG generators.py generate l.373] (61/93) Reuse post-processing
[2024-03-04 21:37:00,137 INFO generators.py gen_for_qa l.548] (61/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:37:00,139 DEBUG generators.py gen_for_qa l.554] (61/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:00,139 DEBUG generators.py generate l.352] (61/93) Reuse existing Prompt
[2024-03-04 21:37:00,141 DEBUG generators.py generate l.365] (61/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:00,142 DEBUG generators.py generate l.373] (61/93) Reuse post-processing
[2024-03-04 21:37:00,143 INFO generators.py gen_for_qa l.548] (61/93) * Start with LLM "command-nightly"
[2024-03-04 21:37:00,144 DEBUG generators.py gen_for_qa l.554] (61/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:00,146 DEBUG generators.py generate l.352] (61/93) Reuse existing Prompt
[2024-03-04 21:37:00,146 DEBUG generators.py generate l.365] (61/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:00,147 DEBUG generators.py generate l.373] (61/93) Reuse post-processing
[2024-03-04 21:37:00,148 INFO generators.py gen_for_qa l.548] (61/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:37:00,151 DEBUG generators.py generate l.349] (61/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:37:00,151 DEBUG generators.py generate l.358] (61/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:37:01,687 ERROR generators.py complete l.400] (61/93) The following exception occurred with prompt meta={} user=" Où a été inventé le premier ordinateur programmable ?  A)  Berlin B)  Iowa C)  Bletchley Park .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:37:01,713 DEBUG generators.py generate l.373] (61/93) Reuse post-processing
[2024-03-04 21:37:01,714 INFO generators.py generate l.477] (61/93) End question " Où a été inventé le premier ordinateur programmable ?  A)  Berlin B)  Iowa C)  Bletchley Park "
[2024-03-04 21:37:01,717 INFO generators.py generate l.475] (62/93) *** AnsGenerator for question " Où a été découverte la pénicilline ?  A)  Londres B)  Paris "
[2024-03-04 21:37:01,719 INFO generators.py gen_for_qa l.548] (62/93) * Start with LLM "gpt-4"
[2024-03-04 21:37:01,723 DEBUG generators.py gen_for_qa l.554] (62/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:01,724 DEBUG generators.py generate l.352] (62/93) Reuse existing Prompt
[2024-03-04 21:37:01,725 DEBUG generators.py generate l.365] (62/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:01,726 DEBUG generators.py generate l.373] (62/93) Reuse post-processing
[2024-03-04 21:37:01,728 INFO generators.py gen_for_qa l.548] (62/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:37:01,729 DEBUG generators.py gen_for_qa l.554] (62/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:01,729 DEBUG generators.py generate l.352] (62/93) Reuse existing Prompt
[2024-03-04 21:37:01,729 DEBUG generators.py generate l.365] (62/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:01,732 DEBUG generators.py generate l.373] (62/93) Reuse post-processing
[2024-03-04 21:37:01,733 INFO generators.py gen_for_qa l.548] (62/93) * Start with LLM "gemini-pro"
[2024-03-04 21:37:01,736 DEBUG generators.py gen_for_qa l.554] (62/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:01,737 DEBUG generators.py generate l.352] (62/93) Reuse existing Prompt
[2024-03-04 21:37:01,737 DEBUG generators.py generate l.365] (62/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:01,739 DEBUG generators.py generate l.373] (62/93) Reuse post-processing
[2024-03-04 21:37:01,739 INFO generators.py gen_for_qa l.548] (62/93) * Start with LLM "claude-2.1"
[2024-03-04 21:37:01,739 DEBUG generators.py gen_for_qa l.554] (62/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:01,742 DEBUG generators.py generate l.352] (62/93) Reuse existing Prompt
[2024-03-04 21:37:01,743 DEBUG generators.py generate l.365] (62/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:01,744 DEBUG generators.py generate l.373] (62/93) Reuse post-processing
[2024-03-04 21:37:01,744 INFO generators.py gen_for_qa l.548] (62/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:37:01,745 DEBUG generators.py gen_for_qa l.554] (62/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:01,746 DEBUG generators.py generate l.352] (62/93) Reuse existing Prompt
[2024-03-04 21:37:01,748 DEBUG generators.py generate l.365] (62/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:01,748 DEBUG generators.py generate l.373] (62/93) Reuse post-processing
[2024-03-04 21:37:01,749 INFO generators.py gen_for_qa l.548] (62/93) * Start with LLM "command-nightly"
[2024-03-04 21:37:01,750 DEBUG generators.py gen_for_qa l.554] (62/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:01,756 DEBUG generators.py generate l.352] (62/93) Reuse existing Prompt
[2024-03-04 21:37:01,756 DEBUG generators.py generate l.365] (62/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:01,757 DEBUG generators.py generate l.373] (62/93) Reuse post-processing
[2024-03-04 21:37:01,759 INFO generators.py gen_for_qa l.548] (62/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:37:01,760 DEBUG generators.py generate l.349] (62/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:37:01,760 DEBUG generators.py generate l.358] (62/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:37:02,286 ERROR generators.py complete l.400] (62/93) The following exception occurred with prompt meta={} user=" Où a été découverte la pénicilline ?  A)  Londres B)  Paris .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:37:02,312 DEBUG generators.py generate l.373] (62/93) Reuse post-processing
[2024-03-04 21:37:02,314 INFO generators.py generate l.477] (62/93) End question " Où a été découverte la pénicilline ?  A)  Londres B)  Paris "
[2024-03-04 21:37:02,316 INFO generators.py generate l.475] (63/93) *** AnsGenerator for question " Où a été inventé le premier satellite artificiel ?  A)  Moscou B)  Cape Canaveral C)  Baïkonour "
[2024-03-04 21:37:02,317 INFO generators.py gen_for_qa l.548] (63/93) * Start with LLM "gpt-4"
[2024-03-04 21:37:02,320 DEBUG generators.py gen_for_qa l.554] (63/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:02,322 DEBUG generators.py generate l.352] (63/93) Reuse existing Prompt
[2024-03-04 21:37:02,323 DEBUG generators.py generate l.365] (63/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:02,324 DEBUG generators.py generate l.373] (63/93) Reuse post-processing
[2024-03-04 21:37:02,325 INFO generators.py gen_for_qa l.548] (63/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:37:02,325 DEBUG generators.py gen_for_qa l.554] (63/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:02,328 DEBUG generators.py generate l.352] (63/93) Reuse existing Prompt
[2024-03-04 21:37:02,329 DEBUG generators.py generate l.365] (63/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:02,329 DEBUG generators.py generate l.373] (63/93) Reuse post-processing
[2024-03-04 21:37:02,331 INFO generators.py gen_for_qa l.548] (63/93) * Start with LLM "gemini-pro"
[2024-03-04 21:37:02,332 DEBUG generators.py gen_for_qa l.554] (63/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:02,333 DEBUG generators.py generate l.352] (63/93) Reuse existing Prompt
[2024-03-04 21:37:02,336 DEBUG generators.py generate l.365] (63/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:02,336 DEBUG generators.py generate l.373] (63/93) Reuse post-processing
[2024-03-04 21:37:02,339 INFO generators.py gen_for_qa l.548] (63/93) * Start with LLM "claude-2.1"
[2024-03-04 21:37:02,339 DEBUG generators.py gen_for_qa l.554] (63/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:02,339 DEBUG generators.py generate l.352] (63/93) Reuse existing Prompt
[2024-03-04 21:37:02,342 DEBUG generators.py generate l.365] (63/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:02,342 DEBUG generators.py generate l.373] (63/93) Reuse post-processing
[2024-03-04 21:37:02,343 INFO generators.py gen_for_qa l.548] (63/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:37:02,344 DEBUG generators.py gen_for_qa l.554] (63/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:02,345 DEBUG generators.py generate l.352] (63/93) Reuse existing Prompt
[2024-03-04 21:37:02,347 DEBUG generators.py generate l.365] (63/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:02,347 DEBUG generators.py generate l.373] (63/93) Reuse post-processing
[2024-03-04 21:37:02,349 INFO generators.py gen_for_qa l.548] (63/93) * Start with LLM "command-nightly"
[2024-03-04 21:37:02,350 DEBUG generators.py gen_for_qa l.554] (63/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:02,353 DEBUG generators.py generate l.352] (63/93) Reuse existing Prompt
[2024-03-04 21:37:02,355 DEBUG generators.py generate l.365] (63/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:02,355 DEBUG generators.py generate l.373] (63/93) Reuse post-processing
[2024-03-04 21:37:02,356 INFO generators.py gen_for_qa l.548] (63/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:37:02,358 DEBUG generators.py generate l.349] (63/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:37:02,358 DEBUG generators.py generate l.358] (63/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:37:03,084 ERROR generators.py complete l.400] (63/93) The following exception occurred with prompt meta={} user=" Où a été inventé le premier satellite artificiel ?  A)  Moscou B)  Cape Canaveral C)  Baïkonour .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:37:03,106 DEBUG generators.py generate l.373] (63/93) Reuse post-processing
[2024-03-04 21:37:03,109 INFO generators.py generate l.477] (63/93) End question " Où a été inventé le premier satellite artificiel ?  A)  Moscou B)  Cape Canaveral C)  Baïkonour "
[2024-03-04 21:37:03,110 INFO generators.py generate l.475] (64/93) *** AnsGenerator for question " Où a été découvert le feu par l'Homme ?  A)  Vallée du Rift B)  Grotte de Zhoukoudian C)  Grotte de Wonderwerk "
[2024-03-04 21:37:03,111 INFO generators.py gen_for_qa l.548] (64/93) * Start with LLM "gpt-4"
[2024-03-04 21:37:03,114 DEBUG generators.py gen_for_qa l.554] (64/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:03,115 DEBUG generators.py generate l.352] (64/93) Reuse existing Prompt
[2024-03-04 21:37:03,116 DEBUG generators.py generate l.365] (64/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:03,119 DEBUG generators.py generate l.373] (64/93) Reuse post-processing
[2024-03-04 21:37:03,122 INFO generators.py gen_for_qa l.548] (64/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:37:03,122 DEBUG generators.py gen_for_qa l.554] (64/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:03,124 DEBUG generators.py generate l.352] (64/93) Reuse existing Prompt
[2024-03-04 21:37:03,125 DEBUG generators.py generate l.365] (64/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:03,126 DEBUG generators.py generate l.373] (64/93) Reuse post-processing
[2024-03-04 21:37:03,128 INFO generators.py gen_for_qa l.548] (64/93) * Start with LLM "gemini-pro"
[2024-03-04 21:37:03,129 DEBUG generators.py gen_for_qa l.554] (64/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:03,129 DEBUG generators.py generate l.352] (64/93) Reuse existing Prompt
[2024-03-04 21:37:03,130 DEBUG generators.py generate l.365] (64/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:03,131 DEBUG generators.py generate l.373] (64/93) Reuse post-processing
[2024-03-04 21:37:03,133 INFO generators.py gen_for_qa l.548] (64/93) * Start with LLM "claude-2.1"
[2024-03-04 21:37:03,134 DEBUG generators.py gen_for_qa l.554] (64/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:03,137 DEBUG generators.py generate l.352] (64/93) Reuse existing Prompt
[2024-03-04 21:37:03,138 DEBUG generators.py generate l.365] (64/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:03,138 DEBUG generators.py generate l.373] (64/93) Reuse post-processing
[2024-03-04 21:37:03,140 INFO generators.py gen_for_qa l.548] (64/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:37:03,140 DEBUG generators.py gen_for_qa l.554] (64/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:03,142 DEBUG generators.py generate l.352] (64/93) Reuse existing Prompt
[2024-03-04 21:37:03,143 DEBUG generators.py generate l.365] (64/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:03,144 DEBUG generators.py generate l.373] (64/93) Reuse post-processing
[2024-03-04 21:37:03,144 INFO generators.py gen_for_qa l.548] (64/93) * Start with LLM "command-nightly"
[2024-03-04 21:37:03,144 DEBUG generators.py gen_for_qa l.554] (64/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:03,147 DEBUG generators.py generate l.352] (64/93) Reuse existing Prompt
[2024-03-04 21:37:03,148 DEBUG generators.py generate l.365] (64/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:03,149 DEBUG generators.py generate l.373] (64/93) Reuse post-processing
[2024-03-04 21:37:03,149 INFO generators.py gen_for_qa l.548] (64/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:37:03,152 DEBUG generators.py generate l.349] (64/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:37:03,154 DEBUG generators.py generate l.358] (64/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:37:08,297 ERROR generators.py complete l.400] (64/93) The following exception occurred with prompt meta={} user=" Où a été découvert le feu par l'Homme ?  A)  Vallée du Rift B)  Grotte de Zhoukoudian C)  Grotte de Wonderwerk .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:37:08,312 DEBUG generators.py generate l.373] (64/93) Reuse post-processing
[2024-03-04 21:37:08,314 INFO generators.py generate l.477] (64/93) End question " Où a été découvert le feu par l'Homme ?  A)  Vallée du Rift B)  Grotte de Zhoukoudian C)  Grotte de Wonderwerk "
[2024-03-04 21:37:08,316 INFO generators.py generate l.475] (65/93) *** AnsGenerator for question " Où a été inventé le premier microprocesseur ?  A)  Silicon Valley B)  Palo Alto C)  Santa Clara "
[2024-03-04 21:37:08,317 INFO generators.py gen_for_qa l.548] (65/93) * Start with LLM "gpt-4"
[2024-03-04 21:37:08,320 DEBUG generators.py gen_for_qa l.554] (65/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:08,320 DEBUG generators.py generate l.352] (65/93) Reuse existing Prompt
[2024-03-04 21:37:08,321 DEBUG generators.py generate l.365] (65/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:08,322 DEBUG generators.py generate l.373] (65/93) Reuse post-processing
[2024-03-04 21:37:08,323 INFO generators.py gen_for_qa l.548] (65/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:37:08,324 DEBUG generators.py gen_for_qa l.554] (65/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:08,324 DEBUG generators.py generate l.352] (65/93) Reuse existing Prompt
[2024-03-04 21:37:08,326 DEBUG generators.py generate l.365] (65/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:08,327 DEBUG generators.py generate l.373] (65/93) Reuse post-processing
[2024-03-04 21:37:08,328 INFO generators.py gen_for_qa l.548] (65/93) * Start with LLM "gemini-pro"
[2024-03-04 21:37:08,328 DEBUG generators.py gen_for_qa l.554] (65/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:08,330 DEBUG generators.py generate l.352] (65/93) Reuse existing Prompt
[2024-03-04 21:37:08,331 DEBUG generators.py generate l.365] (65/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:08,333 DEBUG generators.py generate l.373] (65/93) Reuse post-processing
[2024-03-04 21:37:08,335 INFO generators.py gen_for_qa l.548] (65/93) * Start with LLM "claude-2.1"
[2024-03-04 21:37:08,336 DEBUG generators.py gen_for_qa l.554] (65/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:08,336 DEBUG generators.py generate l.352] (65/93) Reuse existing Prompt
[2024-03-04 21:37:08,338 DEBUG generators.py generate l.365] (65/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:08,338 DEBUG generators.py generate l.373] (65/93) Reuse post-processing
[2024-03-04 21:37:08,340 INFO generators.py gen_for_qa l.548] (65/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:37:08,341 DEBUG generators.py gen_for_qa l.554] (65/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:08,341 DEBUG generators.py generate l.352] (65/93) Reuse existing Prompt
[2024-03-04 21:37:08,343 DEBUG generators.py generate l.365] (65/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:08,344 DEBUG generators.py generate l.373] (65/93) Reuse post-processing
[2024-03-04 21:37:08,344 INFO generators.py gen_for_qa l.548] (65/93) * Start with LLM "command-nightly"
[2024-03-04 21:37:08,345 DEBUG generators.py gen_for_qa l.554] (65/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:08,346 DEBUG generators.py generate l.352] (65/93) Reuse existing Prompt
[2024-03-04 21:37:08,346 DEBUG generators.py generate l.365] (65/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:08,347 DEBUG generators.py generate l.373] (65/93) Reuse post-processing
[2024-03-04 21:37:08,349 INFO generators.py gen_for_qa l.548] (65/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:37:08,349 DEBUG generators.py generate l.349] (65/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:37:08,353 DEBUG generators.py generate l.358] (65/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:37:11,020 ERROR generators.py complete l.400] (65/93) The following exception occurred with prompt meta={} user=" Où a été inventé le premier microprocesseur ?  A)  Silicon Valley B)  Palo Alto C)  Santa Clara .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:37:11,036 DEBUG generators.py generate l.373] (65/93) Reuse post-processing
[2024-03-04 21:37:11,041 INFO generators.py generate l.477] (65/93) End question " Où a été inventé le premier microprocesseur ?  A)  Silicon Valley B)  Palo Alto C)  Santa Clara "
[2024-03-04 21:37:11,043 INFO generators.py generate l.475] (66/93) *** AnsGenerator for question " Où a été découvert le premier dinosaure ?  A)  Angleterre B)  États-Unis C)  France "
[2024-03-04 21:37:11,044 INFO generators.py gen_for_qa l.548] (66/93) * Start with LLM "gpt-4"
[2024-03-04 21:37:11,045 DEBUG generators.py gen_for_qa l.554] (66/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:11,047 DEBUG generators.py generate l.352] (66/93) Reuse existing Prompt
[2024-03-04 21:37:11,048 DEBUG generators.py generate l.365] (66/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:11,052 DEBUG generators.py generate l.373] (66/93) Reuse post-processing
[2024-03-04 21:37:11,052 INFO generators.py gen_for_qa l.548] (66/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:37:11,052 DEBUG generators.py gen_for_qa l.554] (66/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:11,056 DEBUG generators.py generate l.352] (66/93) Reuse existing Prompt
[2024-03-04 21:37:11,057 DEBUG generators.py generate l.365] (66/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:11,058 DEBUG generators.py generate l.373] (66/93) Reuse post-processing
[2024-03-04 21:37:11,059 INFO generators.py gen_for_qa l.548] (66/93) * Start with LLM "gemini-pro"
[2024-03-04 21:37:11,059 DEBUG generators.py gen_for_qa l.554] (66/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:11,061 DEBUG generators.py generate l.352] (66/93) Reuse existing Prompt
[2024-03-04 21:37:11,062 DEBUG generators.py generate l.365] (66/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:11,064 DEBUG generators.py generate l.373] (66/93) Reuse post-processing
[2024-03-04 21:37:11,066 INFO generators.py gen_for_qa l.548] (66/93) * Start with LLM "claude-2.1"
[2024-03-04 21:37:11,068 DEBUG generators.py gen_for_qa l.554] (66/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:11,069 DEBUG generators.py generate l.352] (66/93) Reuse existing Prompt
[2024-03-04 21:37:11,069 DEBUG generators.py generate l.365] (66/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:11,069 DEBUG generators.py generate l.373] (66/93) Reuse post-processing
[2024-03-04 21:37:11,072 INFO generators.py gen_for_qa l.548] (66/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:37:11,073 DEBUG generators.py gen_for_qa l.554] (66/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:11,074 DEBUG generators.py generate l.352] (66/93) Reuse existing Prompt
[2024-03-04 21:37:11,075 DEBUG generators.py generate l.365] (66/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:11,076 DEBUG generators.py generate l.373] (66/93) Reuse post-processing
[2024-03-04 21:37:11,077 INFO generators.py gen_for_qa l.548] (66/93) * Start with LLM "command-nightly"
[2024-03-04 21:37:11,078 DEBUG generators.py gen_for_qa l.554] (66/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:11,078 DEBUG generators.py generate l.352] (66/93) Reuse existing Prompt
[2024-03-04 21:37:11,080 DEBUG generators.py generate l.365] (66/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:11,082 DEBUG generators.py generate l.373] (66/93) Reuse post-processing
[2024-03-04 21:37:11,084 INFO generators.py gen_for_qa l.548] (66/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:37:11,085 DEBUG generators.py generate l.349] (66/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:37:11,086 DEBUG generators.py generate l.358] (66/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:37:13,031 ERROR generators.py complete l.400] (66/93) The following exception occurred with prompt meta={} user=" Où a été découvert le premier dinosaure ?  A)  Angleterre B)  États-Unis C)  France .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:37:13,037 DEBUG generators.py generate l.373] (66/93) Reuse post-processing
[2024-03-04 21:37:13,037 INFO generators.py generate l.477] (66/93) End question " Où a été découvert le premier dinosaure ?  A)  Angleterre B)  États-Unis C)  France "
[2024-03-04 21:37:13,044 INFO generators.py generate l.475] (67/93) *** AnsGenerator for question " Où a été inventé le premier téléphone ?  A)  Boston B)  Philadelphie C)  Washington D.C. "
[2024-03-04 21:37:13,045 INFO generators.py gen_for_qa l.548] (67/93) * Start with LLM "gpt-4"
[2024-03-04 21:37:13,046 DEBUG generators.py gen_for_qa l.554] (67/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:13,048 DEBUG generators.py generate l.352] (67/93) Reuse existing Prompt
[2024-03-04 21:37:13,049 DEBUG generators.py generate l.365] (67/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:13,050 DEBUG generators.py generate l.373] (67/93) Reuse post-processing
[2024-03-04 21:37:13,050 INFO generators.py gen_for_qa l.548] (67/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:37:13,055 DEBUG generators.py gen_for_qa l.554] (67/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:13,056 DEBUG generators.py generate l.352] (67/93) Reuse existing Prompt
[2024-03-04 21:37:13,057 DEBUG generators.py generate l.365] (67/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:13,058 DEBUG generators.py generate l.373] (67/93) Reuse post-processing
[2024-03-04 21:37:13,058 INFO generators.py gen_for_qa l.548] (67/93) * Start with LLM "gemini-pro"
[2024-03-04 21:37:13,060 DEBUG generators.py gen_for_qa l.554] (67/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:13,060 DEBUG generators.py generate l.352] (67/93) Reuse existing Prompt
[2024-03-04 21:37:13,062 DEBUG generators.py generate l.365] (67/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:13,064 DEBUG generators.py generate l.373] (67/93) Reuse post-processing
[2024-03-04 21:37:13,065 INFO generators.py gen_for_qa l.548] (67/93) * Start with LLM "claude-2.1"
[2024-03-04 21:37:13,068 DEBUG generators.py gen_for_qa l.554] (67/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:13,069 DEBUG generators.py generate l.352] (67/93) Reuse existing Prompt
[2024-03-04 21:37:13,069 DEBUG generators.py generate l.365] (67/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:13,069 DEBUG generators.py generate l.373] (67/93) Reuse post-processing
[2024-03-04 21:37:13,069 INFO generators.py gen_for_qa l.548] (67/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:37:13,073 DEBUG generators.py gen_for_qa l.554] (67/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:13,074 DEBUG generators.py generate l.352] (67/93) Reuse existing Prompt
[2024-03-04 21:37:13,075 DEBUG generators.py generate l.365] (67/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:13,076 DEBUG generators.py generate l.373] (67/93) Reuse post-processing
[2024-03-04 21:37:13,076 INFO generators.py gen_for_qa l.548] (67/93) * Start with LLM "command-nightly"
[2024-03-04 21:37:13,077 DEBUG generators.py gen_for_qa l.554] (67/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:13,078 DEBUG generators.py generate l.352] (67/93) Reuse existing Prompt
[2024-03-04 21:37:13,079 DEBUG generators.py generate l.365] (67/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:13,080 DEBUG generators.py generate l.373] (67/93) Reuse post-processing
[2024-03-04 21:37:13,081 INFO generators.py gen_for_qa l.548] (67/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:37:13,084 DEBUG generators.py generate l.349] (67/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:37:13,086 DEBUG generators.py generate l.358] (67/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:37:14,023 ERROR generators.py complete l.400] (67/93) The following exception occurred with prompt meta={} user=" Où a été inventé le premier téléphone ?  A)  Boston B)  Philadelphie C)  Washington D.C. .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:37:14,046 DEBUG generators.py generate l.373] (67/93) Reuse post-processing
[2024-03-04 21:37:14,049 INFO generators.py generate l.477] (67/93) End question " Où a été inventé le premier téléphone ?  A)  Boston B)  Philadelphie C)  Washington D.C. "
[2024-03-04 21:37:14,051 INFO generators.py generate l.475] (68/93) *** AnsGenerator for question " Où a été découvert le premier gisement de pétrole ?  A)  Titusville B)  Bakou C)  Tulsa "
[2024-03-04 21:37:14,053 INFO generators.py gen_for_qa l.548] (68/93) * Start with LLM "gpt-4"
[2024-03-04 21:37:14,055 DEBUG generators.py gen_for_qa l.554] (68/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:14,055 DEBUG generators.py generate l.352] (68/93) Reuse existing Prompt
[2024-03-04 21:37:14,058 DEBUG generators.py generate l.365] (68/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:14,059 DEBUG generators.py generate l.373] (68/93) Reuse post-processing
[2024-03-04 21:37:14,060 INFO generators.py gen_for_qa l.548] (68/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:37:14,061 DEBUG generators.py gen_for_qa l.554] (68/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:14,063 DEBUG generators.py generate l.352] (68/93) Reuse existing Prompt
[2024-03-04 21:37:14,064 DEBUG generators.py generate l.365] (68/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:14,068 DEBUG generators.py generate l.373] (68/93) Reuse post-processing
[2024-03-04 21:37:14,069 INFO generators.py gen_for_qa l.548] (68/93) * Start with LLM "gemini-pro"
[2024-03-04 21:37:14,070 DEBUG generators.py gen_for_qa l.554] (68/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:14,071 DEBUG generators.py generate l.352] (68/93) Reuse existing Prompt
[2024-03-04 21:37:14,072 DEBUG generators.py generate l.365] (68/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:14,073 DEBUG generators.py generate l.373] (68/93) Reuse post-processing
[2024-03-04 21:37:14,074 INFO generators.py gen_for_qa l.548] (68/93) * Start with LLM "claude-2.1"
[2024-03-04 21:37:14,075 DEBUG generators.py gen_for_qa l.554] (68/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:14,075 DEBUG generators.py generate l.352] (68/93) Reuse existing Prompt
[2024-03-04 21:37:14,076 DEBUG generators.py generate l.365] (68/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:14,077 DEBUG generators.py generate l.373] (68/93) Reuse post-processing
[2024-03-04 21:37:14,078 INFO generators.py gen_for_qa l.548] (68/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:37:14,079 DEBUG generators.py gen_for_qa l.554] (68/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:14,083 DEBUG generators.py generate l.352] (68/93) Reuse existing Prompt
[2024-03-04 21:37:14,085 DEBUG generators.py generate l.365] (68/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:14,086 DEBUG generators.py generate l.373] (68/93) Reuse post-processing
[2024-03-04 21:37:14,087 INFO generators.py gen_for_qa l.548] (68/93) * Start with LLM "command-nightly"
[2024-03-04 21:37:14,088 DEBUG generators.py gen_for_qa l.554] (68/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:14,089 DEBUG generators.py generate l.352] (68/93) Reuse existing Prompt
[2024-03-04 21:37:14,092 DEBUG generators.py generate l.365] (68/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:14,093 DEBUG generators.py generate l.373] (68/93) Reuse post-processing
[2024-03-04 21:37:14,093 INFO generators.py gen_for_qa l.548] (68/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:37:14,095 DEBUG generators.py generate l.349] (68/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:37:14,097 DEBUG generators.py generate l.358] (68/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:37:15,902 ERROR generators.py complete l.400] (68/93) The following exception occurred with prompt meta={} user=" Où a été découvert le premier gisement de pétrole ?  A)  Titusville B)  Bakou C)  Tulsa .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:37:15,917 DEBUG generators.py generate l.373] (68/93) Reuse post-processing
[2024-03-04 21:37:15,918 INFO generators.py generate l.477] (68/93) End question " Où a été découvert le premier gisement de pétrole ?  A)  Titusville B)  Bakou C)  Tulsa "
[2024-03-04 21:37:15,921 INFO generators.py generate l.475] (69/93) *** AnsGenerator for question " Où a été inventé le premier sous-marin ?  A)  Londres B)  La Haye C)  New York "
[2024-03-04 21:37:15,923 INFO generators.py gen_for_qa l.548] (69/93) * Start with LLM "gpt-4"
[2024-03-04 21:37:15,924 DEBUG generators.py gen_for_qa l.554] (69/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:15,925 DEBUG generators.py generate l.352] (69/93) Reuse existing Prompt
[2024-03-04 21:37:15,926 DEBUG generators.py generate l.365] (69/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:15,927 DEBUG generators.py generate l.373] (69/93) Reuse post-processing
[2024-03-04 21:37:15,928 INFO generators.py gen_for_qa l.548] (69/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:37:15,929 DEBUG generators.py gen_for_qa l.554] (69/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:15,931 DEBUG generators.py generate l.352] (69/93) Reuse existing Prompt
[2024-03-04 21:37:15,933 DEBUG generators.py generate l.365] (69/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:15,935 DEBUG generators.py generate l.373] (69/93) Reuse post-processing
[2024-03-04 21:37:15,935 INFO generators.py gen_for_qa l.548] (69/93) * Start with LLM "gemini-pro"
[2024-03-04 21:37:15,936 DEBUG generators.py gen_for_qa l.554] (69/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:15,937 DEBUG generators.py generate l.352] (69/93) Reuse existing Prompt
[2024-03-04 21:37:15,938 DEBUG generators.py generate l.365] (69/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:15,939 DEBUG generators.py generate l.373] (69/93) Reuse post-processing
[2024-03-04 21:37:15,939 INFO generators.py gen_for_qa l.548] (69/93) * Start with LLM "claude-2.1"
[2024-03-04 21:37:15,941 DEBUG generators.py gen_for_qa l.554] (69/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:15,942 DEBUG generators.py generate l.352] (69/93) Reuse existing Prompt
[2024-03-04 21:37:15,942 DEBUG generators.py generate l.365] (69/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:15,942 DEBUG generators.py generate l.373] (69/93) Reuse post-processing
[2024-03-04 21:37:15,944 INFO generators.py gen_for_qa l.548] (69/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:37:15,945 DEBUG generators.py gen_for_qa l.554] (69/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:15,949 DEBUG generators.py generate l.352] (69/93) Reuse existing Prompt
[2024-03-04 21:37:15,950 DEBUG generators.py generate l.365] (69/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:15,951 DEBUG generators.py generate l.373] (69/93) Reuse post-processing
[2024-03-04 21:37:15,951 INFO generators.py gen_for_qa l.548] (69/93) * Start with LLM "command-nightly"
[2024-03-04 21:37:15,953 DEBUG generators.py gen_for_qa l.554] (69/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:15,953 DEBUG generators.py generate l.352] (69/93) Reuse existing Prompt
[2024-03-04 21:37:15,955 DEBUG generators.py generate l.365] (69/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:15,955 DEBUG generators.py generate l.373] (69/93) Reuse post-processing
[2024-03-04 21:37:15,957 INFO generators.py gen_for_qa l.548] (69/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:37:15,957 DEBUG generators.py generate l.349] (69/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:37:15,958 DEBUG generators.py generate l.358] (69/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:37:17,739 ERROR generators.py complete l.400] (69/93) The following exception occurred with prompt meta={} user=" Où a été inventé le premier sous-marin ?  A)  Londres B)  La Haye C)  New York .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:37:17,751 DEBUG generators.py generate l.373] (69/93) Reuse post-processing
[2024-03-04 21:37:17,753 INFO generators.py generate l.477] (69/93) End question " Où a été inventé le premier sous-marin ?  A)  Londres B)  La Haye C)  New York "
[2024-03-04 21:37:17,754 INFO generators.py generate l.475] (70/93) *** AnsGenerator for question " Où a été découvert le premier vaccin ?  A)  Gloucestershire B)  Dorset C)  Somerset "
[2024-03-04 21:37:17,756 INFO generators.py gen_for_qa l.548] (70/93) * Start with LLM "gpt-4"
[2024-03-04 21:37:17,756 DEBUG generators.py gen_for_qa l.554] (70/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:17,760 DEBUG generators.py generate l.352] (70/93) Reuse existing Prompt
[2024-03-04 21:37:17,761 DEBUG generators.py generate l.365] (70/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:17,764 DEBUG generators.py generate l.373] (70/93) Reuse post-processing
[2024-03-04 21:37:17,765 INFO generators.py gen_for_qa l.548] (70/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:37:17,766 DEBUG generators.py gen_for_qa l.554] (70/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:17,766 DEBUG generators.py generate l.352] (70/93) Reuse existing Prompt
[2024-03-04 21:37:17,766 DEBUG generators.py generate l.365] (70/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:17,770 DEBUG generators.py generate l.373] (70/93) Reuse post-processing
[2024-03-04 21:37:17,771 INFO generators.py gen_for_qa l.548] (70/93) * Start with LLM "gemini-pro"
[2024-03-04 21:37:17,772 DEBUG generators.py gen_for_qa l.554] (70/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:17,773 DEBUG generators.py generate l.352] (70/93) Reuse existing Prompt
[2024-03-04 21:37:17,775 DEBUG generators.py generate l.365] (70/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:17,775 DEBUG generators.py generate l.373] (70/93) Reuse post-processing
[2024-03-04 21:37:17,775 INFO generators.py gen_for_qa l.548] (70/93) * Start with LLM "claude-2.1"
[2024-03-04 21:37:17,779 DEBUG generators.py gen_for_qa l.554] (70/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:17,781 DEBUG generators.py generate l.352] (70/93) Reuse existing Prompt
[2024-03-04 21:37:17,782 DEBUG generators.py generate l.365] (70/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:17,783 DEBUG generators.py generate l.373] (70/93) Reuse post-processing
[2024-03-04 21:37:17,784 INFO generators.py gen_for_qa l.548] (70/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:37:17,785 DEBUG generators.py gen_for_qa l.554] (70/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:17,785 DEBUG generators.py generate l.352] (70/93) Reuse existing Prompt
[2024-03-04 21:37:17,786 DEBUG generators.py generate l.365] (70/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:17,786 DEBUG generators.py generate l.373] (70/93) Reuse post-processing
[2024-03-04 21:37:17,787 INFO generators.py gen_for_qa l.548] (70/93) * Start with LLM "command-nightly"
[2024-03-04 21:37:17,787 DEBUG generators.py gen_for_qa l.554] (70/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:17,789 DEBUG generators.py generate l.352] (70/93) Reuse existing Prompt
[2024-03-04 21:37:17,790 DEBUG generators.py generate l.365] (70/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:17,791 DEBUG generators.py generate l.373] (70/93) Reuse post-processing
[2024-03-04 21:37:17,791 INFO generators.py gen_for_qa l.548] (70/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:37:17,792 DEBUG generators.py generate l.349] (70/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:37:17,795 DEBUG generators.py generate l.358] (70/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:37:19,053 ERROR generators.py complete l.400] (70/93) The following exception occurred with prompt meta={} user=" Où a été découvert le premier vaccin ?  A)  Gloucestershire B)  Dorset C)  Somerset .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:37:19,067 DEBUG generators.py generate l.373] (70/93) Reuse post-processing
[2024-03-04 21:37:19,071 INFO generators.py generate l.477] (70/93) End question " Où a été découvert le premier vaccin ?  A)  Gloucestershire B)  Dorset C)  Somerset "
[2024-03-04 21:37:19,071 INFO generators.py generate l.475] (71/93) *** AnsGenerator for question " Où a été inventé le premier moteur à combustion interne ?  A)  Paris B)  Francfort C)  Londres "
[2024-03-04 21:37:19,071 INFO generators.py gen_for_qa l.548] (71/93) * Start with LLM "gpt-4"
[2024-03-04 21:37:19,075 DEBUG generators.py gen_for_qa l.554] (71/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:19,077 DEBUG generators.py generate l.352] (71/93) Reuse existing Prompt
[2024-03-04 21:37:19,079 DEBUG generators.py generate l.365] (71/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:19,080 DEBUG generators.py generate l.373] (71/93) Reuse post-processing
[2024-03-04 21:37:19,082 INFO generators.py gen_for_qa l.548] (71/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:37:19,082 DEBUG generators.py gen_for_qa l.554] (71/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:19,084 DEBUG generators.py generate l.352] (71/93) Reuse existing Prompt
[2024-03-04 21:37:19,084 DEBUG generators.py generate l.365] (71/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:19,086 DEBUG generators.py generate l.373] (71/93) Reuse post-processing
[2024-03-04 21:37:19,087 INFO generators.py gen_for_qa l.548] (71/93) * Start with LLM "gemini-pro"
[2024-03-04 21:37:19,088 DEBUG generators.py gen_for_qa l.554] (71/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:19,089 DEBUG generators.py generate l.352] (71/93) Reuse existing Prompt
[2024-03-04 21:37:19,089 DEBUG generators.py generate l.365] (71/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:19,091 DEBUG generators.py generate l.373] (71/93) Reuse post-processing
[2024-03-04 21:37:19,091 INFO generators.py gen_for_qa l.548] (71/93) * Start with LLM "claude-2.1"
[2024-03-04 21:37:19,092 DEBUG generators.py gen_for_qa l.554] (71/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:19,094 DEBUG generators.py generate l.352] (71/93) Reuse existing Prompt
[2024-03-04 21:37:19,096 DEBUG generators.py generate l.365] (71/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:19,097 DEBUG generators.py generate l.373] (71/93) Reuse post-processing
[2024-03-04 21:37:19,098 INFO generators.py gen_for_qa l.548] (71/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:37:19,099 DEBUG generators.py gen_for_qa l.554] (71/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:19,100 DEBUG generators.py generate l.352] (71/93) Reuse existing Prompt
[2024-03-04 21:37:19,100 DEBUG generators.py generate l.365] (71/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:19,102 DEBUG generators.py generate l.373] (71/93) Reuse post-processing
[2024-03-04 21:37:19,102 INFO generators.py gen_for_qa l.548] (71/93) * Start with LLM "command-nightly"
[2024-03-04 21:37:19,103 DEBUG generators.py gen_for_qa l.554] (71/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:19,104 DEBUG generators.py generate l.352] (71/93) Reuse existing Prompt
[2024-03-04 21:37:19,104 DEBUG generators.py generate l.365] (71/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:19,106 DEBUG generators.py generate l.373] (71/93) Reuse post-processing
[2024-03-04 21:37:19,107 INFO generators.py gen_for_qa l.548] (71/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:37:19,108 DEBUG generators.py generate l.349] (71/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:37:19,108 DEBUG generators.py generate l.358] (71/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:37:20,783 ERROR generators.py complete l.400] (71/93) The following exception occurred with prompt meta={} user=" Où a été inventé le premier moteur à combustion interne ?  A)  Paris B)  Francfort C)  Londres .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:37:20,787 DEBUG generators.py generate l.373] (71/93) Reuse post-processing
[2024-03-04 21:37:20,787 INFO generators.py generate l.477] (71/93) End question " Où a été inventé le premier moteur à combustion interne ?  A)  Paris B)  Francfort C)  Londres "
[2024-03-04 21:37:20,792 INFO generators.py generate l.475] (72/93) *** AnsGenerator for question " Où a été découvert le premier code de lois écrites ?  A)  Ur B)  Babylone C)  Ebla "
[2024-03-04 21:37:20,795 INFO generators.py gen_for_qa l.548] (72/93) * Start with LLM "gpt-4"
[2024-03-04 21:37:20,797 DEBUG generators.py gen_for_qa l.554] (72/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:20,799 DEBUG generators.py generate l.352] (72/93) Reuse existing Prompt
[2024-03-04 21:37:20,801 DEBUG generators.py generate l.365] (72/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:20,805 DEBUG generators.py generate l.373] (72/93) Reuse post-processing
[2024-03-04 21:37:20,807 INFO generators.py gen_for_qa l.548] (72/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:37:20,808 DEBUG generators.py gen_for_qa l.554] (72/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:20,811 DEBUG generators.py generate l.352] (72/93) Reuse existing Prompt
[2024-03-04 21:37:20,814 DEBUG generators.py generate l.365] (72/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:20,817 DEBUG generators.py generate l.373] (72/93) Reuse post-processing
[2024-03-04 21:37:20,819 INFO generators.py gen_for_qa l.548] (72/93) * Start with LLM "gemini-pro"
[2024-03-04 21:37:20,821 DEBUG generators.py gen_for_qa l.554] (72/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:20,821 DEBUG generators.py generate l.352] (72/93) Reuse existing Prompt
[2024-03-04 21:37:20,824 DEBUG generators.py generate l.365] (72/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:20,825 DEBUG generators.py generate l.373] (72/93) Reuse post-processing
[2024-03-04 21:37:20,827 INFO generators.py gen_for_qa l.548] (72/93) * Start with LLM "claude-2.1"
[2024-03-04 21:37:20,829 DEBUG generators.py gen_for_qa l.554] (72/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:20,832 DEBUG generators.py generate l.352] (72/93) Reuse existing Prompt
[2024-03-04 21:37:20,833 DEBUG generators.py generate l.365] (72/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:20,835 DEBUG generators.py generate l.373] (72/93) Reuse post-processing
[2024-03-04 21:37:20,836 INFO generators.py gen_for_qa l.548] (72/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:37:20,838 DEBUG generators.py gen_for_qa l.554] (72/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:20,840 DEBUG generators.py generate l.352] (72/93) Reuse existing Prompt
[2024-03-04 21:37:20,841 DEBUG generators.py generate l.365] (72/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:20,843 DEBUG generators.py generate l.373] (72/93) Reuse post-processing
[2024-03-04 21:37:20,845 INFO generators.py gen_for_qa l.548] (72/93) * Start with LLM "command-nightly"
[2024-03-04 21:37:20,846 DEBUG generators.py gen_for_qa l.554] (72/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:20,846 DEBUG generators.py generate l.352] (72/93) Reuse existing Prompt
[2024-03-04 21:37:20,851 DEBUG generators.py generate l.365] (72/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:20,853 DEBUG generators.py generate l.373] (72/93) Reuse post-processing
[2024-03-04 21:37:20,854 INFO generators.py gen_for_qa l.548] (72/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:37:20,857 DEBUG generators.py generate l.349] (72/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:37:20,858 DEBUG generators.py generate l.358] (72/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:37:22,577 ERROR generators.py complete l.400] (72/93) The following exception occurred with prompt meta={} user=" Où a été découvert le premier code de lois écrites ?  A)  Ur B)  Babylone C)  Ebla .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:37:22,604 DEBUG generators.py generate l.373] (72/93) Reuse post-processing
[2024-03-04 21:37:22,605 INFO generators.py generate l.477] (72/93) End question " Où a été découvert le premier code de lois écrites ?  A)  Ur B)  Babylone C)  Ebla "
[2024-03-04 21:37:22,607 INFO generators.py generate l.475] (73/93) *** AnsGenerator for question " Où a été inventé le premier télescope ?  A)  Middelburg B)  Gdansk C)  Florence "
[2024-03-04 21:37:22,609 INFO generators.py gen_for_qa l.548] (73/93) * Start with LLM "gpt-4"
[2024-03-04 21:37:22,610 DEBUG generators.py gen_for_qa l.554] (73/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:22,613 DEBUG generators.py generate l.352] (73/93) Reuse existing Prompt
[2024-03-04 21:37:22,615 DEBUG generators.py generate l.365] (73/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:22,616 DEBUG generators.py generate l.373] (73/93) Reuse post-processing
[2024-03-04 21:37:22,617 INFO generators.py gen_for_qa l.548] (73/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:37:22,617 DEBUG generators.py gen_for_qa l.554] (73/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:22,619 DEBUG generators.py generate l.352] (73/93) Reuse existing Prompt
[2024-03-04 21:37:22,621 DEBUG generators.py generate l.365] (73/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:22,622 DEBUG generators.py generate l.373] (73/93) Reuse post-processing
[2024-03-04 21:37:22,622 INFO generators.py gen_for_qa l.548] (73/93) * Start with LLM "gemini-pro"
[2024-03-04 21:37:22,623 DEBUG generators.py gen_for_qa l.554] (73/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:22,624 DEBUG generators.py generate l.352] (73/93) Reuse existing Prompt
[2024-03-04 21:37:22,625 DEBUG generators.py generate l.365] (73/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:22,626 DEBUG generators.py generate l.373] (73/93) Reuse post-processing
[2024-03-04 21:37:22,627 INFO generators.py gen_for_qa l.548] (73/93) * Start with LLM "claude-2.1"
[2024-03-04 21:37:22,629 DEBUG generators.py gen_for_qa l.554] (73/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:22,631 DEBUG generators.py generate l.352] (73/93) Reuse existing Prompt
[2024-03-04 21:37:22,631 DEBUG generators.py generate l.365] (73/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:22,633 DEBUG generators.py generate l.373] (73/93) Reuse post-processing
[2024-03-04 21:37:22,633 INFO generators.py gen_for_qa l.548] (73/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:37:22,634 DEBUG generators.py gen_for_qa l.554] (73/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:22,634 DEBUG generators.py generate l.352] (73/93) Reuse existing Prompt
[2024-03-04 21:37:22,634 DEBUG generators.py generate l.365] (73/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:22,636 DEBUG generators.py generate l.373] (73/93) Reuse post-processing
[2024-03-04 21:37:22,637 INFO generators.py gen_for_qa l.548] (73/93) * Start with LLM "command-nightly"
[2024-03-04 21:37:22,638 DEBUG generators.py gen_for_qa l.554] (73/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:22,639 DEBUG generators.py generate l.352] (73/93) Reuse existing Prompt
[2024-03-04 21:37:22,640 DEBUG generators.py generate l.365] (73/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:22,641 DEBUG generators.py generate l.373] (73/93) Reuse post-processing
[2024-03-04 21:37:22,641 INFO generators.py gen_for_qa l.548] (73/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:37:22,642 DEBUG generators.py generate l.349] (73/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:37:22,644 DEBUG generators.py generate l.358] (73/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:37:26,642 ERROR generators.py complete l.400] (73/93) The following exception occurred with prompt meta={} user=" Où a été inventé le premier télescope ?  A)  Middelburg B)  Gdansk C)  Florence .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:37:26,663 DEBUG generators.py generate l.373] (73/93) Reuse post-processing
[2024-03-04 21:37:26,666 INFO generators.py generate l.477] (73/93) End question " Où a été inventé le premier télescope ?  A)  Middelburg B)  Gdansk C)  Florence "
[2024-03-04 21:37:26,666 INFO generators.py generate l.475] (74/93) *** AnsGenerator for question " Où a été découvert le premier site préhistorique ?  A)  Vallée de la Vézère B)  Vallée du Nil C)  Vallée de l'Omo "
[2024-03-04 21:37:26,668 INFO generators.py gen_for_qa l.548] (74/93) * Start with LLM "gpt-4"
[2024-03-04 21:37:26,668 DEBUG generators.py gen_for_qa l.554] (74/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:26,670 DEBUG generators.py generate l.352] (74/93) Reuse existing Prompt
[2024-03-04 21:37:26,671 DEBUG generators.py generate l.365] (74/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:26,672 DEBUG generators.py generate l.373] (74/93) Reuse post-processing
[2024-03-04 21:37:26,672 INFO generators.py gen_for_qa l.548] (74/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:37:26,675 DEBUG generators.py gen_for_qa l.554] (74/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:26,677 DEBUG generators.py generate l.352] (74/93) Reuse existing Prompt
[2024-03-04 21:37:26,677 DEBUG generators.py generate l.365] (74/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:26,680 DEBUG generators.py generate l.373] (74/93) Reuse post-processing
[2024-03-04 21:37:26,680 INFO generators.py gen_for_qa l.548] (74/93) * Start with LLM "gemini-pro"
[2024-03-04 21:37:26,681 DEBUG generators.py gen_for_qa l.554] (74/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:26,682 DEBUG generators.py generate l.352] (74/93) Reuse existing Prompt
[2024-03-04 21:37:26,683 DEBUG generators.py generate l.365] (74/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:26,685 DEBUG generators.py generate l.373] (74/93) Reuse post-processing
[2024-03-04 21:37:26,687 INFO generators.py gen_for_qa l.548] (74/93) * Start with LLM "claude-2.1"
[2024-03-04 21:37:26,687 DEBUG generators.py gen_for_qa l.554] (74/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:26,687 DEBUG generators.py generate l.352] (74/93) Reuse existing Prompt
[2024-03-04 21:37:26,687 DEBUG generators.py generate l.365] (74/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:26,691 DEBUG generators.py generate l.373] (74/93) Reuse post-processing
[2024-03-04 21:37:26,693 INFO generators.py gen_for_qa l.548] (74/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:37:26,695 DEBUG generators.py gen_for_qa l.554] (74/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:26,696 DEBUG generators.py generate l.352] (74/93) Reuse existing Prompt
[2024-03-04 21:37:26,696 DEBUG generators.py generate l.365] (74/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:26,698 DEBUG generators.py generate l.373] (74/93) Reuse post-processing
[2024-03-04 21:37:26,698 INFO generators.py gen_for_qa l.548] (74/93) * Start with LLM "command-nightly"
[2024-03-04 21:37:26,698 DEBUG generators.py gen_for_qa l.554] (74/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:26,701 DEBUG generators.py generate l.352] (74/93) Reuse existing Prompt
[2024-03-04 21:37:26,703 DEBUG generators.py generate l.365] (74/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:26,704 DEBUG generators.py generate l.373] (74/93) Reuse post-processing
[2024-03-04 21:37:26,705 INFO generators.py gen_for_qa l.548] (74/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:37:26,706 DEBUG generators.py generate l.349] (74/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:37:26,708 DEBUG generators.py generate l.358] (74/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:37:29,116 ERROR generators.py complete l.400] (74/93) The following exception occurred with prompt meta={} user=" Où a été découvert le premier site préhistorique ?  A)  Vallée de la Vézère B)  Vallée du Nil C)  Vallée de l'Omo .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:37:29,133 DEBUG generators.py generate l.373] (74/93) Reuse post-processing
[2024-03-04 21:37:29,135 INFO generators.py generate l.477] (74/93) End question " Où a été découvert le premier site préhistorique ?  A)  Vallée de la Vézère B)  Vallée du Nil C)  Vallée de l'Omo "
[2024-03-04 21:37:29,137 INFO generators.py generate l.475] (75/93) *** AnsGenerator for question " Où a été inventé le premier appareil photographique ?  A)  Paris B)  Lacock C)  Nice "
[2024-03-04 21:37:29,138 INFO generators.py gen_for_qa l.548] (75/93) * Start with LLM "gpt-4"
[2024-03-04 21:37:29,139 DEBUG generators.py gen_for_qa l.554] (75/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:29,142 DEBUG generators.py generate l.352] (75/93) Reuse existing Prompt
[2024-03-04 21:37:29,144 DEBUG generators.py generate l.365] (75/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:29,145 DEBUG generators.py generate l.373] (75/93) Reuse post-processing
[2024-03-04 21:37:29,147 INFO generators.py gen_for_qa l.548] (75/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:37:29,148 DEBUG generators.py gen_for_qa l.554] (75/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:29,149 DEBUG generators.py generate l.352] (75/93) Reuse existing Prompt
[2024-03-04 21:37:29,151 DEBUG generators.py generate l.365] (75/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:29,152 DEBUG generators.py generate l.373] (75/93) Reuse post-processing
[2024-03-04 21:37:29,153 INFO generators.py gen_for_qa l.548] (75/93) * Start with LLM "gemini-pro"
[2024-03-04 21:37:29,154 DEBUG generators.py gen_for_qa l.554] (75/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:29,155 DEBUG generators.py generate l.352] (75/93) Reuse existing Prompt
[2024-03-04 21:37:29,157 DEBUG generators.py generate l.365] (75/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:29,160 DEBUG generators.py generate l.373] (75/93) Reuse post-processing
[2024-03-04 21:37:29,161 INFO generators.py gen_for_qa l.548] (75/93) * Start with LLM "claude-2.1"
[2024-03-04 21:37:29,162 DEBUG generators.py gen_for_qa l.554] (75/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:29,163 DEBUG generators.py generate l.352] (75/93) Reuse existing Prompt
[2024-03-04 21:37:29,164 DEBUG generators.py generate l.365] (75/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:29,165 DEBUG generators.py generate l.373] (75/93) Reuse post-processing
[2024-03-04 21:37:29,167 INFO generators.py gen_for_qa l.548] (75/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:37:29,168 DEBUG generators.py gen_for_qa l.554] (75/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:29,170 DEBUG generators.py generate l.352] (75/93) Reuse existing Prompt
[2024-03-04 21:37:29,170 DEBUG generators.py generate l.365] (75/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:29,170 DEBUG generators.py generate l.373] (75/93) Reuse post-processing
[2024-03-04 21:37:29,173 INFO generators.py gen_for_qa l.548] (75/93) * Start with LLM "command-nightly"
[2024-03-04 21:37:29,175 DEBUG generators.py gen_for_qa l.554] (75/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:29,177 DEBUG generators.py generate l.352] (75/93) Reuse existing Prompt
[2024-03-04 21:37:29,178 DEBUG generators.py generate l.365] (75/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:29,179 DEBUG generators.py generate l.373] (75/93) Reuse post-processing
[2024-03-04 21:37:29,179 INFO generators.py gen_for_qa l.548] (75/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:37:29,182 DEBUG generators.py generate l.349] (75/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:37:29,182 DEBUG generators.py generate l.358] (75/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:37:31,431 ERROR generators.py complete l.400] (75/93) The following exception occurred with prompt meta={} user=" Où a été inventé le premier appareil photographique ?  A)  Paris B)  Lacock C)  Nice .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:37:31,447 DEBUG generators.py generate l.373] (75/93) Reuse post-processing
[2024-03-04 21:37:31,449 INFO generators.py generate l.477] (75/93) End question " Où a été inventé le premier appareil photographique ?  A)  Paris B)  Lacock C)  Nice "
[2024-03-04 21:37:31,449 INFO generators.py generate l.475] (76/93) *** AnsGenerator for question " Où a été découvert le premier gisement de charbon ?  A)  Newcastle upon Tyne B)  Liège C)  Essen "
[2024-03-04 21:37:31,452 INFO generators.py gen_for_qa l.548] (76/93) * Start with LLM "gpt-4"
[2024-03-04 21:37:31,452 DEBUG generators.py gen_for_qa l.554] (76/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:31,454 DEBUG generators.py generate l.352] (76/93) Reuse existing Prompt
[2024-03-04 21:37:31,455 DEBUG generators.py generate l.365] (76/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:31,458 DEBUG generators.py generate l.373] (76/93) Reuse post-processing
[2024-03-04 21:37:31,459 INFO generators.py gen_for_qa l.548] (76/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:37:31,460 DEBUG generators.py gen_for_qa l.554] (76/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:31,462 DEBUG generators.py generate l.352] (76/93) Reuse existing Prompt
[2024-03-04 21:37:31,462 DEBUG generators.py generate l.365] (76/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:31,464 DEBUG generators.py generate l.373] (76/93) Reuse post-processing
[2024-03-04 21:37:31,465 INFO generators.py gen_for_qa l.548] (76/93) * Start with LLM "gemini-pro"
[2024-03-04 21:37:31,465 DEBUG generators.py gen_for_qa l.554] (76/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:31,466 DEBUG generators.py generate l.352] (76/93) Reuse existing Prompt
[2024-03-04 21:37:31,467 DEBUG generators.py generate l.365] (76/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:31,469 DEBUG generators.py generate l.373] (76/93) Reuse post-processing
[2024-03-04 21:37:31,470 INFO generators.py gen_for_qa l.548] (76/93) * Start with LLM "claude-2.1"
[2024-03-04 21:37:31,470 DEBUG generators.py gen_for_qa l.554] (76/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:31,473 DEBUG generators.py generate l.352] (76/93) Reuse existing Prompt
[2024-03-04 21:37:31,475 DEBUG generators.py generate l.365] (76/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:31,476 DEBUG generators.py generate l.373] (76/93) Reuse post-processing
[2024-03-04 21:37:31,478 INFO generators.py gen_for_qa l.548] (76/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:37:31,479 DEBUG generators.py gen_for_qa l.554] (76/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:31,479 DEBUG generators.py generate l.352] (76/93) Reuse existing Prompt
[2024-03-04 21:37:31,480 DEBUG generators.py generate l.365] (76/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:31,481 DEBUG generators.py generate l.373] (76/93) Reuse post-processing
[2024-03-04 21:37:31,483 INFO generators.py gen_for_qa l.548] (76/93) * Start with LLM "command-nightly"
[2024-03-04 21:37:31,484 DEBUG generators.py gen_for_qa l.554] (76/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:31,485 DEBUG generators.py generate l.352] (76/93) Reuse existing Prompt
[2024-03-04 21:37:31,485 DEBUG generators.py generate l.365] (76/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:31,486 DEBUG generators.py generate l.373] (76/93) Reuse post-processing
[2024-03-04 21:37:31,487 INFO generators.py gen_for_qa l.548] (76/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:37:31,489 DEBUG generators.py generate l.349] (76/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:37:31,492 DEBUG generators.py generate l.358] (76/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:37:32,838 ERROR generators.py complete l.400] (76/93) The following exception occurred with prompt meta={} user=" Où a été découvert le premier gisement de charbon ?  A)  Newcastle upon Tyne B)  Liège C)  Essen .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:37:32,858 DEBUG generators.py generate l.373] (76/93) Reuse post-processing
[2024-03-04 21:37:32,861 INFO generators.py generate l.477] (76/93) End question " Où a été découvert le premier gisement de charbon ?  A)  Newcastle upon Tyne B)  Liège C)  Essen "
[2024-03-04 21:37:32,863 INFO generators.py generate l.475] (77/93) *** AnsGenerator for question " Où a été inventé le premier bateau à vapeur ?  A)  Paris B)  Glasgow C)  Philadelphie "
[2024-03-04 21:37:32,864 INFO generators.py gen_for_qa l.548] (77/93) * Start with LLM "gpt-4"
[2024-03-04 21:37:32,864 DEBUG generators.py gen_for_qa l.554] (77/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:32,867 DEBUG generators.py generate l.352] (77/93) Reuse existing Prompt
[2024-03-04 21:37:32,868 DEBUG generators.py generate l.365] (77/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:32,869 DEBUG generators.py generate l.373] (77/93) Reuse post-processing
[2024-03-04 21:37:32,870 INFO generators.py gen_for_qa l.548] (77/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:37:32,872 DEBUG generators.py gen_for_qa l.554] (77/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:32,873 DEBUG generators.py generate l.352] (77/93) Reuse existing Prompt
[2024-03-04 21:37:32,875 DEBUG generators.py generate l.365] (77/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:32,876 DEBUG generators.py generate l.373] (77/93) Reuse post-processing
[2024-03-04 21:37:32,878 INFO generators.py gen_for_qa l.548] (77/93) * Start with LLM "gemini-pro"
[2024-03-04 21:37:32,879 DEBUG generators.py gen_for_qa l.554] (77/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:32,880 DEBUG generators.py generate l.352] (77/93) Reuse existing Prompt
[2024-03-04 21:37:32,882 DEBUG generators.py generate l.365] (77/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:32,883 DEBUG generators.py generate l.373] (77/93) Reuse post-processing
[2024-03-04 21:37:32,884 INFO generators.py gen_for_qa l.548] (77/93) * Start with LLM "claude-2.1"
[2024-03-04 21:37:32,885 DEBUG generators.py gen_for_qa l.554] (77/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:32,886 DEBUG generators.py generate l.352] (77/93) Reuse existing Prompt
[2024-03-04 21:37:32,886 DEBUG generators.py generate l.365] (77/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:32,886 DEBUG generators.py generate l.373] (77/93) Reuse post-processing
[2024-03-04 21:37:32,889 INFO generators.py gen_for_qa l.548] (77/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:37:32,891 DEBUG generators.py gen_for_qa l.554] (77/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:32,893 DEBUG generators.py generate l.352] (77/93) Reuse existing Prompt
[2024-03-04 21:37:32,895 DEBUG generators.py generate l.365] (77/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:32,896 DEBUG generators.py generate l.373] (77/93) Reuse post-processing
[2024-03-04 21:37:32,896 INFO generators.py gen_for_qa l.548] (77/93) * Start with LLM "command-nightly"
[2024-03-04 21:37:32,897 DEBUG generators.py gen_for_qa l.554] (77/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:32,899 DEBUG generators.py generate l.352] (77/93) Reuse existing Prompt
[2024-03-04 21:37:32,900 DEBUG generators.py generate l.365] (77/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:32,900 DEBUG generators.py generate l.373] (77/93) Reuse post-processing
[2024-03-04 21:37:32,900 INFO generators.py gen_for_qa l.548] (77/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:37:32,904 DEBUG generators.py generate l.349] (77/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:37:32,904 DEBUG generators.py generate l.358] (77/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:37:38,243 ERROR generators.py complete l.400] (77/93) The following exception occurred with prompt meta={} user=" Où a été inventé le premier bateau à vapeur ?  A)  Paris B)  Glasgow C)  Philadelphie .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:37:38,262 DEBUG generators.py generate l.373] (77/93) Reuse post-processing
[2024-03-04 21:37:38,264 INFO generators.py generate l.477] (77/93) End question " Où a été inventé le premier bateau à vapeur ?  A)  Paris B)  Glasgow C)  Philadelphie "
[2024-03-04 21:37:38,264 INFO generators.py generate l.475] (78/93) *** AnsGenerator for question " Où a été découvert le premier système d'écriture ?  A)  Uruk B)  Sumer C)  Susa "
[2024-03-04 21:37:38,267 INFO generators.py gen_for_qa l.548] (78/93) * Start with LLM "gpt-4"
[2024-03-04 21:37:38,269 DEBUG generators.py gen_for_qa l.554] (78/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:38,270 DEBUG generators.py generate l.352] (78/93) Reuse existing Prompt
[2024-03-04 21:37:38,272 DEBUG generators.py generate l.365] (78/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:38,274 DEBUG generators.py generate l.373] (78/93) Reuse post-processing
[2024-03-04 21:37:38,275 INFO generators.py gen_for_qa l.548] (78/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:37:38,275 DEBUG generators.py gen_for_qa l.554] (78/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:38,276 DEBUG generators.py generate l.352] (78/93) Reuse existing Prompt
[2024-03-04 21:37:38,276 DEBUG generators.py generate l.365] (78/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:38,276 DEBUG generators.py generate l.373] (78/93) Reuse post-processing
[2024-03-04 21:37:38,276 INFO generators.py gen_for_qa l.548] (78/93) * Start with LLM "gemini-pro"
[2024-03-04 21:37:38,281 DEBUG generators.py gen_for_qa l.554] (78/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:38,283 DEBUG generators.py generate l.352] (78/93) Reuse existing Prompt
[2024-03-04 21:37:38,283 DEBUG generators.py generate l.365] (78/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:38,284 DEBUG generators.py generate l.373] (78/93) Reuse post-processing
[2024-03-04 21:37:38,285 INFO generators.py gen_for_qa l.548] (78/93) * Start with LLM "claude-2.1"
[2024-03-04 21:37:38,285 DEBUG generators.py gen_for_qa l.554] (78/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:38,287 DEBUG generators.py generate l.352] (78/93) Reuse existing Prompt
[2024-03-04 21:37:38,288 DEBUG generators.py generate l.365] (78/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:38,289 DEBUG generators.py generate l.373] (78/93) Reuse post-processing
[2024-03-04 21:37:38,291 INFO generators.py gen_for_qa l.548] (78/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:37:38,291 DEBUG generators.py gen_for_qa l.554] (78/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:38,293 DEBUG generators.py generate l.352] (78/93) Reuse existing Prompt
[2024-03-04 21:37:38,294 DEBUG generators.py generate l.365] (78/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:38,295 DEBUG generators.py generate l.373] (78/93) Reuse post-processing
[2024-03-04 21:37:38,295 INFO generators.py gen_for_qa l.548] (78/93) * Start with LLM "command-nightly"
[2024-03-04 21:37:38,296 DEBUG generators.py gen_for_qa l.554] (78/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:38,297 DEBUG generators.py generate l.352] (78/93) Reuse existing Prompt
[2024-03-04 21:37:38,299 DEBUG generators.py generate l.365] (78/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:38,300 DEBUG generators.py generate l.373] (78/93) Reuse post-processing
[2024-03-04 21:37:38,301 INFO generators.py gen_for_qa l.548] (78/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:37:38,302 DEBUG generators.py generate l.349] (78/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:37:38,303 DEBUG generators.py generate l.358] (78/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:37:41,047 ERROR generators.py complete l.400] (78/93) The following exception occurred with prompt meta={} user=" Où a été découvert le premier système d'écriture ?  A)  Uruk B)  Sumer C)  Susa .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:37:41,062 DEBUG generators.py generate l.373] (78/93) Reuse post-processing
[2024-03-04 21:37:41,063 INFO generators.py generate l.477] (78/93) End question " Où a été découvert le premier système d'écriture ?  A)  Uruk B)  Sumer C)  Susa "
[2024-03-04 21:37:41,065 INFO generators.py generate l.475] (79/93) *** AnsGenerator for question " Où a été inventé le premier réseau ferroviaire ?  A)  Stockton et Darlington B)  Liverpool et Manchester C)  Baltimore et Ohio "
[2024-03-04 21:37:41,066 INFO generators.py gen_for_qa l.548] (79/93) * Start with LLM "gpt-4"
[2024-03-04 21:37:41,068 DEBUG generators.py gen_for_qa l.554] (79/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:41,069 DEBUG generators.py generate l.352] (79/93) Reuse existing Prompt
[2024-03-04 21:37:41,069 DEBUG generators.py generate l.365] (79/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:41,072 DEBUG generators.py generate l.373] (79/93) Reuse post-processing
[2024-03-04 21:37:41,073 INFO generators.py gen_for_qa l.548] (79/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:37:41,074 DEBUG generators.py gen_for_qa l.554] (79/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:41,074 DEBUG generators.py generate l.352] (79/93) Reuse existing Prompt
[2024-03-04 21:37:41,076 DEBUG generators.py generate l.365] (79/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:41,077 DEBUG generators.py generate l.373] (79/93) Reuse post-processing
[2024-03-04 21:37:41,078 INFO generators.py gen_for_qa l.548] (79/93) * Start with LLM "gemini-pro"
[2024-03-04 21:37:41,078 DEBUG generators.py gen_for_qa l.554] (79/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:41,080 DEBUG generators.py generate l.352] (79/93) Reuse existing Prompt
[2024-03-04 21:37:41,081 DEBUG generators.py generate l.365] (79/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:41,082 DEBUG generators.py generate l.373] (79/93) Reuse post-processing
[2024-03-04 21:37:41,083 INFO generators.py gen_for_qa l.548] (79/93) * Start with LLM "claude-2.1"
[2024-03-04 21:37:41,083 DEBUG generators.py gen_for_qa l.554] (79/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:41,083 DEBUG generators.py generate l.352] (79/93) Reuse existing Prompt
[2024-03-04 21:37:41,087 DEBUG generators.py generate l.365] (79/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:41,089 DEBUG generators.py generate l.373] (79/93) Reuse post-processing
[2024-03-04 21:37:41,090 INFO generators.py gen_for_qa l.548] (79/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:37:41,091 DEBUG generators.py gen_for_qa l.554] (79/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:41,092 DEBUG generators.py generate l.352] (79/93) Reuse existing Prompt
[2024-03-04 21:37:41,094 DEBUG generators.py generate l.365] (79/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:41,094 DEBUG generators.py generate l.373] (79/93) Reuse post-processing
[2024-03-04 21:37:41,096 INFO generators.py gen_for_qa l.548] (79/93) * Start with LLM "command-nightly"
[2024-03-04 21:37:41,096 DEBUG generators.py gen_for_qa l.554] (79/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:41,097 DEBUG generators.py generate l.352] (79/93) Reuse existing Prompt
[2024-03-04 21:37:41,100 DEBUG generators.py generate l.365] (79/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:41,100 DEBUG generators.py generate l.373] (79/93) Reuse post-processing
[2024-03-04 21:37:41,101 INFO generators.py gen_for_qa l.548] (79/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:37:41,102 DEBUG generators.py generate l.349] (79/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:37:41,104 DEBUG generators.py generate l.358] (79/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:37:43,279 ERROR generators.py complete l.400] (79/93) The following exception occurred with prompt meta={} user=" Où a été inventé le premier réseau ferroviaire ?  A)  Stockton et Darlington B)  Liverpool et Manchester C)  Baltimore et Ohio .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:37:43,298 DEBUG generators.py generate l.373] (79/93) Reuse post-processing
[2024-03-04 21:37:43,299 INFO generators.py generate l.477] (79/93) End question " Où a été inventé le premier réseau ferroviaire ?  A)  Stockton et Darlington B)  Liverpool et Manchester C)  Baltimore et Ohio "
[2024-03-04 21:37:43,301 INFO generators.py generate l.475] (80/93) *** AnsGenerator for question " Qui a découvert l'Amérique (pour les Européens) ?  A)  Christophe Colomb B)  Leif Erikson C)  Amerigo Vespucci "
[2024-03-04 21:37:43,302 INFO generators.py gen_for_qa l.548] (80/93) * Start with LLM "gpt-4"
[2024-03-04 21:37:43,303 DEBUG generators.py gen_for_qa l.554] (80/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:43,305 DEBUG generators.py generate l.352] (80/93) Reuse existing Prompt
[2024-03-04 21:37:43,306 DEBUG generators.py generate l.365] (80/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:43,307 DEBUG generators.py generate l.373] (80/93) Reuse post-processing
[2024-03-04 21:37:43,308 INFO generators.py gen_for_qa l.548] (80/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:37:43,310 DEBUG generators.py gen_for_qa l.554] (80/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:43,312 DEBUG generators.py generate l.352] (80/93) Reuse existing Prompt
[2024-03-04 21:37:43,312 DEBUG generators.py generate l.365] (80/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:43,312 DEBUG generators.py generate l.373] (80/93) Reuse post-processing
[2024-03-04 21:37:43,315 INFO generators.py gen_for_qa l.548] (80/93) * Start with LLM "gemini-pro"
[2024-03-04 21:37:43,316 DEBUG generators.py gen_for_qa l.554] (80/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:43,317 DEBUG generators.py generate l.352] (80/93) Reuse existing Prompt
[2024-03-04 21:37:43,318 DEBUG generators.py generate l.365] (80/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:43,318 DEBUG generators.py generate l.373] (80/93) Reuse post-processing
[2024-03-04 21:37:43,319 INFO generators.py gen_for_qa l.548] (80/93) * Start with LLM "claude-2.1"
[2024-03-04 21:37:43,321 DEBUG generators.py gen_for_qa l.554] (80/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:43,324 DEBUG generators.py generate l.352] (80/93) Reuse existing Prompt
[2024-03-04 21:37:43,325 DEBUG generators.py generate l.365] (80/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:43,325 DEBUG generators.py generate l.373] (80/93) Reuse post-processing
[2024-03-04 21:37:43,326 INFO generators.py gen_for_qa l.548] (80/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:37:43,327 DEBUG generators.py gen_for_qa l.554] (80/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:43,327 DEBUG generators.py generate l.352] (80/93) Reuse existing Prompt
[2024-03-04 21:37:43,327 DEBUG generators.py generate l.365] (80/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:43,331 DEBUG generators.py generate l.373] (80/93) Reuse post-processing
[2024-03-04 21:37:43,331 INFO generators.py gen_for_qa l.548] (80/93) * Start with LLM "command-nightly"
[2024-03-04 21:37:43,332 DEBUG generators.py gen_for_qa l.554] (80/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:43,333 DEBUG generators.py generate l.352] (80/93) Reuse existing Prompt
[2024-03-04 21:37:43,334 DEBUG generators.py generate l.365] (80/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:43,335 DEBUG generators.py generate l.373] (80/93) Reuse post-processing
[2024-03-04 21:37:43,337 INFO generators.py gen_for_qa l.548] (80/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:37:43,339 DEBUG generators.py generate l.349] (80/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:37:43,341 DEBUG generators.py generate l.358] (80/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:37:46,643 ERROR generators.py complete l.400] (80/93) The following exception occurred with prompt meta={} user=" Qui a découvert l'Amérique (pour les Européens) ?  A)  Christophe Colomb B)  Leif Erikson C)  Amerigo Vespucci .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:37:46,665 DEBUG generators.py generate l.373] (80/93) Reuse post-processing
[2024-03-04 21:37:46,665 INFO generators.py generate l.477] (80/93) End question " Qui a découvert l'Amérique (pour les Européens) ?  A)  Christophe Colomb B)  Leif Erikson C)  Amerigo Vespucci "
[2024-03-04 21:37:46,665 INFO generators.py generate l.475] (81/93) *** AnsGenerator for question " Qui a découvert l'Australie (pour les Européens) ?  A)  James Cook B)  Willem Janszoon C)  Abel Tasman "
[2024-03-04 21:37:46,665 INFO generators.py gen_for_qa l.548] (81/93) * Start with LLM "gpt-4"
[2024-03-04 21:37:46,674 DEBUG generators.py gen_for_qa l.554] (81/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:46,674 DEBUG generators.py generate l.352] (81/93) Reuse existing Prompt
[2024-03-04 21:37:46,676 DEBUG generators.py generate l.365] (81/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:46,677 DEBUG generators.py generate l.373] (81/93) Reuse post-processing
[2024-03-04 21:37:46,680 INFO generators.py gen_for_qa l.548] (81/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:37:46,681 DEBUG generators.py gen_for_qa l.554] (81/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:46,682 DEBUG generators.py generate l.352] (81/93) Reuse existing Prompt
[2024-03-04 21:37:46,683 DEBUG generators.py generate l.365] (81/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:46,684 DEBUG generators.py generate l.373] (81/93) Reuse post-processing
[2024-03-04 21:37:46,684 INFO generators.py gen_for_qa l.548] (81/93) * Start with LLM "gemini-pro"
[2024-03-04 21:37:46,686 DEBUG generators.py gen_for_qa l.554] (81/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:46,686 DEBUG generators.py generate l.352] (81/93) Reuse existing Prompt
[2024-03-04 21:37:46,689 DEBUG generators.py generate l.365] (81/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:46,690 DEBUG generators.py generate l.373] (81/93) Reuse post-processing
[2024-03-04 21:37:46,691 INFO generators.py gen_for_qa l.548] (81/93) * Start with LLM "claude-2.1"
[2024-03-04 21:37:46,691 DEBUG generators.py gen_for_qa l.554] (81/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:46,693 DEBUG generators.py generate l.352] (81/93) Reuse existing Prompt
[2024-03-04 21:37:46,694 DEBUG generators.py generate l.365] (81/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:46,695 DEBUG generators.py generate l.373] (81/93) Reuse post-processing
[2024-03-04 21:37:46,697 INFO generators.py gen_for_qa l.548] (81/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:37:46,697 DEBUG generators.py gen_for_qa l.554] (81/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:46,697 DEBUG generators.py generate l.352] (81/93) Reuse existing Prompt
[2024-03-04 21:37:46,700 DEBUG generators.py generate l.365] (81/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:46,701 DEBUG generators.py generate l.373] (81/93) Reuse post-processing
[2024-03-04 21:37:46,702 INFO generators.py gen_for_qa l.548] (81/93) * Start with LLM "command-nightly"
[2024-03-04 21:37:46,703 DEBUG generators.py gen_for_qa l.554] (81/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:46,705 DEBUG generators.py generate l.352] (81/93) Reuse existing Prompt
[2024-03-04 21:37:46,706 DEBUG generators.py generate l.365] (81/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:46,707 DEBUG generators.py generate l.373] (81/93) Reuse post-processing
[2024-03-04 21:37:46,708 INFO generators.py gen_for_qa l.548] (81/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:37:46,710 DEBUG generators.py generate l.349] (81/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:37:46,711 DEBUG generators.py generate l.358] (81/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:37:49,490 ERROR generators.py complete l.400] (81/93) The following exception occurred with prompt meta={} user=" Qui a découvert l'Australie (pour les Européens) ?  A)  James Cook B)  Willem Janszoon C)  Abel Tasman .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:37:49,501 DEBUG generators.py generate l.373] (81/93) Reuse post-processing
[2024-03-04 21:37:49,503 INFO generators.py generate l.477] (81/93) End question " Qui a découvert l'Australie (pour les Européens) ?  A)  James Cook B)  Willem Janszoon C)  Abel Tasman "
[2024-03-04 21:37:49,504 INFO generators.py generate l.475] (82/93) *** AnsGenerator for question " Qui a découvert l'Antarctique ?  A)  Fabian von Bellingshausen B)  Edward Bransfield C)  Nathaniel Palmer "
[2024-03-04 21:37:49,505 INFO generators.py gen_for_qa l.548] (82/93) * Start with LLM "gpt-4"
[2024-03-04 21:37:49,506 DEBUG generators.py gen_for_qa l.554] (82/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:49,508 DEBUG generators.py generate l.352] (82/93) Reuse existing Prompt
[2024-03-04 21:37:49,509 DEBUG generators.py generate l.365] (82/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:49,510 DEBUG generators.py generate l.373] (82/93) Reuse post-processing
[2024-03-04 21:37:49,511 INFO generators.py gen_for_qa l.548] (82/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:37:49,513 DEBUG generators.py gen_for_qa l.554] (82/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:49,514 DEBUG generators.py generate l.352] (82/93) Reuse existing Prompt
[2024-03-04 21:37:49,515 DEBUG generators.py generate l.365] (82/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:49,516 DEBUG generators.py generate l.373] (82/93) Reuse post-processing
[2024-03-04 21:37:49,518 INFO generators.py gen_for_qa l.548] (82/93) * Start with LLM "gemini-pro"
[2024-03-04 21:37:49,518 DEBUG generators.py gen_for_qa l.554] (82/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:49,520 DEBUG generators.py generate l.352] (82/93) Reuse existing Prompt
[2024-03-04 21:37:49,522 DEBUG generators.py generate l.365] (82/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:49,523 DEBUG generators.py generate l.373] (82/93) Reuse post-processing
[2024-03-04 21:37:49,524 INFO generators.py gen_for_qa l.548] (82/93) * Start with LLM "claude-2.1"
[2024-03-04 21:37:49,525 DEBUG generators.py gen_for_qa l.554] (82/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:49,526 DEBUG generators.py generate l.352] (82/93) Reuse existing Prompt
[2024-03-04 21:37:49,528 DEBUG generators.py generate l.365] (82/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:49,529 DEBUG generators.py generate l.373] (82/93) Reuse post-processing
[2024-03-04 21:37:49,530 INFO generators.py gen_for_qa l.548] (82/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:37:49,531 DEBUG generators.py gen_for_qa l.554] (82/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:49,532 DEBUG generators.py generate l.352] (82/93) Reuse existing Prompt
[2024-03-04 21:37:49,533 DEBUG generators.py generate l.365] (82/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:49,534 DEBUG generators.py generate l.373] (82/93) Reuse post-processing
[2024-03-04 21:37:49,534 INFO generators.py gen_for_qa l.548] (82/93) * Start with LLM "command-nightly"
[2024-03-04 21:37:49,536 DEBUG generators.py gen_for_qa l.554] (82/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:49,537 DEBUG generators.py generate l.352] (82/93) Reuse existing Prompt
[2024-03-04 21:37:49,538 DEBUG generators.py generate l.365] (82/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:49,538 DEBUG generators.py generate l.373] (82/93) Reuse post-processing
[2024-03-04 21:37:49,539 INFO generators.py gen_for_qa l.548] (82/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:37:49,541 DEBUG generators.py generate l.349] (82/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:37:49,542 DEBUG generators.py generate l.358] (82/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:37:51,821 ERROR generators.py complete l.400] (82/93) The following exception occurred with prompt meta={} user=" Qui a découvert l'Antarctique ?  A)  Fabian von Bellingshausen B)  Edward Bransfield C)  Nathaniel Palmer .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:37:51,825 DEBUG generators.py generate l.373] (82/93) Reuse post-processing
[2024-03-04 21:37:51,827 INFO generators.py generate l.477] (82/93) End question " Qui a découvert l'Antarctique ?  A)  Fabian von Bellingshausen B)  Edward Bransfield C)  Nathaniel Palmer "
[2024-03-04 21:37:51,828 INFO generators.py generate l.475] (83/93) *** AnsGenerator for question " Qui a découvert le Canada (pour les Européens) ?  A)  John Cabot B)  Jacques Cartier C)  Giovanni Verrazzano "
[2024-03-04 21:37:51,829 INFO generators.py gen_for_qa l.548] (83/93) * Start with LLM "gpt-4"
[2024-03-04 21:37:51,830 DEBUG generators.py gen_for_qa l.554] (83/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:51,831 DEBUG generators.py generate l.352] (83/93) Reuse existing Prompt
[2024-03-04 21:37:51,835 DEBUG generators.py generate l.365] (83/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:51,837 DEBUG generators.py generate l.373] (83/93) Reuse post-processing
[2024-03-04 21:37:51,841 INFO generators.py gen_for_qa l.548] (83/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:37:51,842 DEBUG generators.py gen_for_qa l.554] (83/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:51,844 DEBUG generators.py generate l.352] (83/93) Reuse existing Prompt
[2024-03-04 21:37:51,846 DEBUG generators.py generate l.365] (83/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:51,847 DEBUG generators.py generate l.373] (83/93) Reuse post-processing
[2024-03-04 21:37:51,849 INFO generators.py gen_for_qa l.548] (83/93) * Start with LLM "gemini-pro"
[2024-03-04 21:37:51,854 DEBUG generators.py gen_for_qa l.554] (83/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:51,855 DEBUG generators.py generate l.352] (83/93) Reuse existing Prompt
[2024-03-04 21:37:51,857 DEBUG generators.py generate l.365] (83/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:51,858 DEBUG generators.py generate l.373] (83/93) Reuse post-processing
[2024-03-04 21:37:51,859 INFO generators.py gen_for_qa l.548] (83/93) * Start with LLM "claude-2.1"
[2024-03-04 21:37:51,860 DEBUG generators.py gen_for_qa l.554] (83/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:51,860 DEBUG generators.py generate l.352] (83/93) Reuse existing Prompt
[2024-03-04 21:37:51,862 DEBUG generators.py generate l.365] (83/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:51,865 DEBUG generators.py generate l.373] (83/93) Reuse post-processing
[2024-03-04 21:37:51,866 INFO generators.py gen_for_qa l.548] (83/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:37:51,867 DEBUG generators.py gen_for_qa l.554] (83/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:51,868 DEBUG generators.py generate l.352] (83/93) Reuse existing Prompt
[2024-03-04 21:37:51,869 DEBUG generators.py generate l.365] (83/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:51,870 DEBUG generators.py generate l.373] (83/93) Reuse post-processing
[2024-03-04 21:37:51,871 INFO generators.py gen_for_qa l.548] (83/93) * Start with LLM "command-nightly"
[2024-03-04 21:37:51,872 DEBUG generators.py gen_for_qa l.554] (83/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:51,873 DEBUG generators.py generate l.352] (83/93) Reuse existing Prompt
[2024-03-04 21:37:51,874 DEBUG generators.py generate l.365] (83/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:51,875 DEBUG generators.py generate l.373] (83/93) Reuse post-processing
[2024-03-04 21:37:51,876 INFO generators.py gen_for_qa l.548] (83/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:37:51,878 DEBUG generators.py generate l.349] (83/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:37:51,879 DEBUG generators.py generate l.358] (83/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:37:53,384 ERROR generators.py complete l.400] (83/93) The following exception occurred with prompt meta={} user=" Qui a découvert le Canada (pour les Européens) ?  A)  John Cabot B)  Jacques Cartier C)  Giovanni Verrazzano .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:37:53,404 DEBUG generators.py generate l.373] (83/93) Reuse post-processing
[2024-03-04 21:37:53,405 INFO generators.py generate l.477] (83/93) End question " Qui a découvert le Canada (pour les Européens) ?  A)  John Cabot B)  Jacques Cartier C)  Giovanni Verrazzano "
[2024-03-04 21:37:53,406 INFO generators.py generate l.475] (84/93) *** AnsGenerator for question " Qui a découvert les îles Galápagos ?  A)  Tomás de Berlanga B)  Francis Drake "
[2024-03-04 21:37:53,408 INFO generators.py gen_for_qa l.548] (84/93) * Start with LLM "gpt-4"
[2024-03-04 21:37:53,409 DEBUG generators.py gen_for_qa l.554] (84/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:53,411 DEBUG generators.py generate l.352] (84/93) Reuse existing Prompt
[2024-03-04 21:37:53,412 DEBUG generators.py generate l.365] (84/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:53,413 DEBUG generators.py generate l.373] (84/93) Reuse post-processing
[2024-03-04 21:37:53,414 INFO generators.py gen_for_qa l.548] (84/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:37:53,415 DEBUG generators.py gen_for_qa l.554] (84/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:53,416 DEBUG generators.py generate l.352] (84/93) Reuse existing Prompt
[2024-03-04 21:37:53,418 DEBUG generators.py generate l.365] (84/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:53,419 DEBUG generators.py generate l.373] (84/93) Reuse post-processing
[2024-03-04 21:37:53,421 INFO generators.py gen_for_qa l.548] (84/93) * Start with LLM "gemini-pro"
[2024-03-04 21:37:53,422 DEBUG generators.py gen_for_qa l.554] (84/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:53,423 DEBUG generators.py generate l.352] (84/93) Reuse existing Prompt
[2024-03-04 21:37:53,424 DEBUG generators.py generate l.365] (84/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:53,425 DEBUG generators.py generate l.373] (84/93) Reuse post-processing
[2024-03-04 21:37:53,425 INFO generators.py gen_for_qa l.548] (84/93) * Start with LLM "claude-2.1"
[2024-03-04 21:37:53,427 DEBUG generators.py gen_for_qa l.554] (84/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:53,428 DEBUG generators.py generate l.352] (84/93) Reuse existing Prompt
[2024-03-04 21:37:53,429 DEBUG generators.py generate l.365] (84/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:53,430 DEBUG generators.py generate l.373] (84/93) Reuse post-processing
[2024-03-04 21:37:53,431 INFO generators.py gen_for_qa l.548] (84/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:37:53,433 DEBUG generators.py gen_for_qa l.554] (84/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:53,433 DEBUG generators.py generate l.352] (84/93) Reuse existing Prompt
[2024-03-04 21:37:53,435 DEBUG generators.py generate l.365] (84/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:53,435 DEBUG generators.py generate l.373] (84/93) Reuse post-processing
[2024-03-04 21:37:53,436 INFO generators.py gen_for_qa l.548] (84/93) * Start with LLM "command-nightly"
[2024-03-04 21:37:53,437 DEBUG generators.py gen_for_qa l.554] (84/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:53,438 DEBUG generators.py generate l.352] (84/93) Reuse existing Prompt
[2024-03-04 21:37:53,439 DEBUG generators.py generate l.365] (84/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:53,440 DEBUG generators.py generate l.373] (84/93) Reuse post-processing
[2024-03-04 21:37:53,441 INFO generators.py gen_for_qa l.548] (84/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:37:53,443 DEBUG generators.py generate l.349] (84/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:37:53,444 DEBUG generators.py generate l.358] (84/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:37:55,561 DEBUG generators.py generate l.370] (84/93) Post-process Answer
[2024-03-04 21:37:55,567 INFO generators.py generate l.477] (84/93) End question " Qui a découvert les îles Galápagos ?  A)  Tomás de Berlanga B)  Francis Drake "
[2024-03-04 21:37:55,571 INFO generators.py generate l.475] (85/93) *** AnsGenerator for question " Qui a découvert l'Amérique centrale (pour les Européens) ?  A)  Christophe Colomb B)  Vasco Núñez de Balboa "
[2024-03-04 21:37:55,572 INFO generators.py gen_for_qa l.548] (85/93) * Start with LLM "gpt-4"
[2024-03-04 21:37:55,573 DEBUG generators.py gen_for_qa l.554] (85/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:55,574 DEBUG generators.py generate l.352] (85/93) Reuse existing Prompt
[2024-03-04 21:37:55,575 DEBUG generators.py generate l.365] (85/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:55,577 DEBUG generators.py generate l.373] (85/93) Reuse post-processing
[2024-03-04 21:37:55,578 INFO generators.py gen_for_qa l.548] (85/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:37:55,579 DEBUG generators.py gen_for_qa l.554] (85/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:55,581 DEBUG generators.py generate l.352] (85/93) Reuse existing Prompt
[2024-03-04 21:37:55,582 DEBUG generators.py generate l.365] (85/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:55,582 DEBUG generators.py generate l.373] (85/93) Reuse post-processing
[2024-03-04 21:37:55,583 INFO generators.py gen_for_qa l.548] (85/93) * Start with LLM "gemini-pro"
[2024-03-04 21:37:55,584 DEBUG generators.py gen_for_qa l.554] (85/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:55,585 DEBUG generators.py generate l.352] (85/93) Reuse existing Prompt
[2024-03-04 21:37:55,587 DEBUG generators.py generate l.365] (85/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:55,588 DEBUG generators.py generate l.373] (85/93) Reuse post-processing
[2024-03-04 21:37:55,589 INFO generators.py gen_for_qa l.548] (85/93) * Start with LLM "claude-2.1"
[2024-03-04 21:37:55,590 DEBUG generators.py gen_for_qa l.554] (85/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:55,591 DEBUG generators.py generate l.352] (85/93) Reuse existing Prompt
[2024-03-04 21:37:55,592 DEBUG generators.py generate l.365] (85/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:55,594 DEBUG generators.py generate l.373] (85/93) Reuse post-processing
[2024-03-04 21:37:55,595 INFO generators.py gen_for_qa l.548] (85/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:37:55,596 DEBUG generators.py gen_for_qa l.554] (85/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:55,597 DEBUG generators.py generate l.352] (85/93) Reuse existing Prompt
[2024-03-04 21:37:55,598 DEBUG generators.py generate l.365] (85/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:55,599 DEBUG generators.py generate l.373] (85/93) Reuse post-processing
[2024-03-04 21:37:55,600 INFO generators.py gen_for_qa l.548] (85/93) * Start with LLM "command-nightly"
[2024-03-04 21:37:55,600 DEBUG generators.py gen_for_qa l.554] (85/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:55,601 DEBUG generators.py generate l.352] (85/93) Reuse existing Prompt
[2024-03-04 21:37:55,602 DEBUG generators.py generate l.365] (85/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:55,603 DEBUG generators.py generate l.373] (85/93) Reuse post-processing
[2024-03-04 21:37:55,604 INFO generators.py gen_for_qa l.548] (85/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:37:55,605 DEBUG generators.py generate l.349] (85/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:37:55,606 DEBUG generators.py generate l.358] (85/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:37:57,025 ERROR generators.py complete l.400] (85/93) The following exception occurred with prompt meta={} user=" Qui a découvert l'Amérique centrale (pour les Européens) ?  A)  Christophe Colomb B)  Vasco Núñez de Balboa .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:37:57,037 DEBUG generators.py generate l.373] (85/93) Reuse post-processing
[2024-03-04 21:37:57,039 INFO generators.py generate l.477] (85/93) End question " Qui a découvert l'Amérique centrale (pour les Européens) ?  A)  Christophe Colomb B)  Vasco Núñez de Balboa "
[2024-03-04 21:37:57,040 INFO generators.py generate l.475] (86/93) *** AnsGenerator for question " Qui a découvert la Nouvelle-Zélande (pour les Européens) ?  A)  Abel Tasman B)  James Cook C)  Jean-François-Marie de Surville "
[2024-03-04 21:37:57,041 INFO generators.py gen_for_qa l.548] (86/93) * Start with LLM "gpt-4"
[2024-03-04 21:37:57,042 DEBUG generators.py gen_for_qa l.554] (86/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:57,043 DEBUG generators.py generate l.352] (86/93) Reuse existing Prompt
[2024-03-04 21:37:57,044 DEBUG generators.py generate l.365] (86/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:57,045 DEBUG generators.py generate l.373] (86/93) Reuse post-processing
[2024-03-04 21:37:57,046 INFO generators.py gen_for_qa l.548] (86/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:37:57,047 DEBUG generators.py gen_for_qa l.554] (86/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:57,048 DEBUG generators.py generate l.352] (86/93) Reuse existing Prompt
[2024-03-04 21:37:57,051 DEBUG generators.py generate l.365] (86/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:57,053 DEBUG generators.py generate l.373] (86/93) Reuse post-processing
[2024-03-04 21:37:57,054 INFO generators.py gen_for_qa l.548] (86/93) * Start with LLM "gemini-pro"
[2024-03-04 21:37:57,055 DEBUG generators.py gen_for_qa l.554] (86/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:57,056 DEBUG generators.py generate l.352] (86/93) Reuse existing Prompt
[2024-03-04 21:37:57,058 DEBUG generators.py generate l.365] (86/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:57,058 DEBUG generators.py generate l.373] (86/93) Reuse post-processing
[2024-03-04 21:37:57,059 INFO generators.py gen_for_qa l.548] (86/93) * Start with LLM "claude-2.1"
[2024-03-04 21:37:57,060 DEBUG generators.py gen_for_qa l.554] (86/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:57,062 DEBUG generators.py generate l.352] (86/93) Reuse existing Prompt
[2024-03-04 21:37:57,063 DEBUG generators.py generate l.365] (86/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:57,064 DEBUG generators.py generate l.373] (86/93) Reuse post-processing
[2024-03-04 21:37:57,065 INFO generators.py gen_for_qa l.548] (86/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:37:57,066 DEBUG generators.py gen_for_qa l.554] (86/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:57,069 DEBUG generators.py generate l.352] (86/93) Reuse existing Prompt
[2024-03-04 21:37:57,071 DEBUG generators.py generate l.365] (86/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:57,071 DEBUG generators.py generate l.373] (86/93) Reuse post-processing
[2024-03-04 21:37:57,072 INFO generators.py gen_for_qa l.548] (86/93) * Start with LLM "command-nightly"
[2024-03-04 21:37:57,073 DEBUG generators.py gen_for_qa l.554] (86/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:57,074 DEBUG generators.py generate l.352] (86/93) Reuse existing Prompt
[2024-03-04 21:37:57,075 DEBUG generators.py generate l.365] (86/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:57,076 DEBUG generators.py generate l.373] (86/93) Reuse post-processing
[2024-03-04 21:37:57,077 INFO generators.py gen_for_qa l.548] (86/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:37:57,079 DEBUG generators.py generate l.349] (86/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:37:57,079 DEBUG generators.py generate l.358] (86/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:37:59,428 ERROR generators.py complete l.400] (86/93) The following exception occurred with prompt meta={} user=" Qui a découvert la Nouvelle-Zélande (pour les Européens) ?  A)  Abel Tasman B)  James Cook C)  Jean-François-Marie de Surville .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:37:59,450 DEBUG generators.py generate l.373] (86/93) Reuse post-processing
[2024-03-04 21:37:59,453 INFO generators.py generate l.477] (86/93) End question " Qui a découvert la Nouvelle-Zélande (pour les Européens) ?  A)  Abel Tasman B)  James Cook C)  Jean-François-Marie de Surville "
[2024-03-04 21:37:59,454 INFO generators.py generate l.475] (87/93) *** AnsGenerator for question " Qui a découvert le fleuve Amazone ?  A)  Francisco de Orellana B)  Vicente Yáñez Pinzón C)  Diego de Ordaz "
[2024-03-04 21:37:59,455 INFO generators.py gen_for_qa l.548] (87/93) * Start with LLM "gpt-4"
[2024-03-04 21:37:59,458 DEBUG generators.py gen_for_qa l.554] (87/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:59,459 DEBUG generators.py generate l.352] (87/93) Reuse existing Prompt
[2024-03-04 21:37:59,461 DEBUG generators.py generate l.365] (87/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:59,462 DEBUG generators.py generate l.373] (87/93) Reuse post-processing
[2024-03-04 21:37:59,464 INFO generators.py gen_for_qa l.548] (87/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:37:59,465 DEBUG generators.py gen_for_qa l.554] (87/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:59,466 DEBUG generators.py generate l.352] (87/93) Reuse existing Prompt
[2024-03-04 21:37:59,468 DEBUG generators.py generate l.365] (87/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:59,469 DEBUG generators.py generate l.373] (87/93) Reuse post-processing
[2024-03-04 21:37:59,470 INFO generators.py gen_for_qa l.548] (87/93) * Start with LLM "gemini-pro"
[2024-03-04 21:37:59,471 DEBUG generators.py gen_for_qa l.554] (87/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:59,473 DEBUG generators.py generate l.352] (87/93) Reuse existing Prompt
[2024-03-04 21:37:59,474 DEBUG generators.py generate l.365] (87/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:59,475 DEBUG generators.py generate l.373] (87/93) Reuse post-processing
[2024-03-04 21:37:59,476 INFO generators.py gen_for_qa l.548] (87/93) * Start with LLM "claude-2.1"
[2024-03-04 21:37:59,478 DEBUG generators.py gen_for_qa l.554] (87/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:59,479 DEBUG generators.py generate l.352] (87/93) Reuse existing Prompt
[2024-03-04 21:37:59,480 DEBUG generators.py generate l.365] (87/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:59,481 DEBUG generators.py generate l.373] (87/93) Reuse post-processing
[2024-03-04 21:37:59,482 INFO generators.py gen_for_qa l.548] (87/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:37:59,483 DEBUG generators.py gen_for_qa l.554] (87/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:59,484 DEBUG generators.py generate l.352] (87/93) Reuse existing Prompt
[2024-03-04 21:37:59,485 DEBUG generators.py generate l.365] (87/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:59,486 DEBUG generators.py generate l.373] (87/93) Reuse post-processing
[2024-03-04 21:37:59,487 INFO generators.py gen_for_qa l.548] (87/93) * Start with LLM "command-nightly"
[2024-03-04 21:37:59,487 DEBUG generators.py gen_for_qa l.554] (87/93) An Answer has already been generated with this LLM
[2024-03-04 21:37:59,489 DEBUG generators.py generate l.352] (87/93) Reuse existing Prompt
[2024-03-04 21:37:59,490 DEBUG generators.py generate l.365] (87/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:37:59,491 DEBUG generators.py generate l.373] (87/93) Reuse post-processing
[2024-03-04 21:37:59,492 INFO generators.py gen_for_qa l.548] (87/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:37:59,493 DEBUG generators.py generate l.349] (87/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:37:59,494 DEBUG generators.py generate l.358] (87/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:38:01,934 ERROR generators.py complete l.400] (87/93) The following exception occurred with prompt meta={} user=" Qui a découvert le fleuve Amazone ?  A)  Francisco de Orellana B)  Vicente Yáñez Pinzón C)  Diego de Ordaz .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:38:01,958 DEBUG generators.py generate l.373] (87/93) Reuse post-processing
[2024-03-04 21:38:01,961 INFO generators.py generate l.477] (87/93) End question " Qui a découvert le fleuve Amazone ?  A)  Francisco de Orellana B)  Vicente Yáñez Pinzón C)  Diego de Ordaz "
[2024-03-04 21:38:01,963 INFO generators.py generate l.475] (88/93) *** AnsGenerator for question " Qui a découvert le détroit de Magellan ?  A)  Fernand de Magellan B)  Francisco de Almeida C)  Estêvão Gomes "
[2024-03-04 21:38:01,966 INFO generators.py gen_for_qa l.548] (88/93) * Start with LLM "gpt-4"
[2024-03-04 21:38:01,968 DEBUG generators.py gen_for_qa l.554] (88/93) An Answer has already been generated with this LLM
[2024-03-04 21:38:01,969 DEBUG generators.py generate l.352] (88/93) Reuse existing Prompt
[2024-03-04 21:38:01,971 DEBUG generators.py generate l.365] (88/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:38:01,972 DEBUG generators.py generate l.373] (88/93) Reuse post-processing
[2024-03-04 21:38:01,974 INFO generators.py gen_for_qa l.548] (88/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:38:01,975 DEBUG generators.py gen_for_qa l.554] (88/93) An Answer has already been generated with this LLM
[2024-03-04 21:38:01,977 DEBUG generators.py generate l.352] (88/93) Reuse existing Prompt
[2024-03-04 21:38:01,978 DEBUG generators.py generate l.365] (88/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:38:01,979 DEBUG generators.py generate l.373] (88/93) Reuse post-processing
[2024-03-04 21:38:01,980 INFO generators.py gen_for_qa l.548] (88/93) * Start with LLM "gemini-pro"
[2024-03-04 21:38:01,981 DEBUG generators.py gen_for_qa l.554] (88/93) An Answer has already been generated with this LLM
[2024-03-04 21:38:01,982 DEBUG generators.py generate l.352] (88/93) Reuse existing Prompt
[2024-03-04 21:38:01,984 DEBUG generators.py generate l.365] (88/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:38:01,986 DEBUG generators.py generate l.373] (88/93) Reuse post-processing
[2024-03-04 21:38:01,987 INFO generators.py gen_for_qa l.548] (88/93) * Start with LLM "claude-2.1"
[2024-03-04 21:38:01,988 DEBUG generators.py gen_for_qa l.554] (88/93) An Answer has already been generated with this LLM
[2024-03-04 21:38:01,989 DEBUG generators.py generate l.352] (88/93) Reuse existing Prompt
[2024-03-04 21:38:01,990 DEBUG generators.py generate l.365] (88/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:38:01,991 DEBUG generators.py generate l.373] (88/93) Reuse post-processing
[2024-03-04 21:38:01,992 INFO generators.py gen_for_qa l.548] (88/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:38:01,993 DEBUG generators.py gen_for_qa l.554] (88/93) An Answer has already been generated with this LLM
[2024-03-04 21:38:01,994 DEBUG generators.py generate l.352] (88/93) Reuse existing Prompt
[2024-03-04 21:38:01,995 DEBUG generators.py generate l.365] (88/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:38:01,996 DEBUG generators.py generate l.373] (88/93) Reuse post-processing
[2024-03-04 21:38:01,997 INFO generators.py gen_for_qa l.548] (88/93) * Start with LLM "command-nightly"
[2024-03-04 21:38:01,998 DEBUG generators.py gen_for_qa l.554] (88/93) An Answer has already been generated with this LLM
[2024-03-04 21:38:01,999 DEBUG generators.py generate l.352] (88/93) Reuse existing Prompt
[2024-03-04 21:38:01,999 DEBUG generators.py generate l.365] (88/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:38:02,000 DEBUG generators.py generate l.373] (88/93) Reuse post-processing
[2024-03-04 21:38:02,001 INFO generators.py gen_for_qa l.548] (88/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:38:02,001 DEBUG generators.py generate l.349] (88/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:38:02,004 DEBUG generators.py generate l.358] (88/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:38:04,810 ERROR generators.py complete l.400] (88/93) The following exception occurred with prompt meta={} user=" Qui a découvert le détroit de Magellan ?  A)  Fernand de Magellan B)  Francisco de Almeida C)  Estêvão Gomes .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:38:04,826 DEBUG generators.py generate l.373] (88/93) Reuse post-processing
[2024-03-04 21:38:04,829 INFO generators.py generate l.477] (88/93) End question " Qui a découvert le détroit de Magellan ?  A)  Fernand de Magellan B)  Francisco de Almeida C)  Estêvão Gomes "
[2024-03-04 21:38:04,831 INFO generators.py generate l.475] (89/93) *** AnsGenerator for question " Qui a découvert l'archipel d'Hawaï (pour les Européens) ?  A)  James Cook B)  Juan Gaetano "
[2024-03-04 21:38:04,832 INFO generators.py gen_for_qa l.548] (89/93) * Start with LLM "gpt-4"
[2024-03-04 21:38:04,834 DEBUG generators.py gen_for_qa l.554] (89/93) An Answer has already been generated with this LLM
[2024-03-04 21:38:04,835 DEBUG generators.py generate l.352] (89/93) Reuse existing Prompt
[2024-03-04 21:38:04,836 DEBUG generators.py generate l.365] (89/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:38:04,838 DEBUG generators.py generate l.373] (89/93) Reuse post-processing
[2024-03-04 21:38:04,839 INFO generators.py gen_for_qa l.548] (89/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:38:04,840 DEBUG generators.py gen_for_qa l.554] (89/93) An Answer has already been generated with this LLM
[2024-03-04 21:38:04,841 DEBUG generators.py generate l.352] (89/93) Reuse existing Prompt
[2024-03-04 21:38:04,841 DEBUG generators.py generate l.365] (89/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:38:04,842 DEBUG generators.py generate l.373] (89/93) Reuse post-processing
[2024-03-04 21:38:04,843 INFO generators.py gen_for_qa l.548] (89/93) * Start with LLM "gemini-pro"
[2024-03-04 21:38:04,845 DEBUG generators.py gen_for_qa l.554] (89/93) An Answer has already been generated with this LLM
[2024-03-04 21:38:04,846 DEBUG generators.py generate l.352] (89/93) Reuse existing Prompt
[2024-03-04 21:38:04,847 DEBUG generators.py generate l.365] (89/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:38:04,848 DEBUG generators.py generate l.373] (89/93) Reuse post-processing
[2024-03-04 21:38:04,849 INFO generators.py gen_for_qa l.548] (89/93) * Start with LLM "claude-2.1"
[2024-03-04 21:38:04,850 DEBUG generators.py gen_for_qa l.554] (89/93) An Answer has already been generated with this LLM
[2024-03-04 21:38:04,851 DEBUG generators.py generate l.352] (89/93) Reuse existing Prompt
[2024-03-04 21:38:04,852 DEBUG generators.py generate l.365] (89/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:38:04,853 DEBUG generators.py generate l.373] (89/93) Reuse post-processing
[2024-03-04 21:38:04,854 INFO generators.py gen_for_qa l.548] (89/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:38:04,855 DEBUG generators.py gen_for_qa l.554] (89/93) An Answer has already been generated with this LLM
[2024-03-04 21:38:04,856 DEBUG generators.py generate l.352] (89/93) Reuse existing Prompt
[2024-03-04 21:38:04,857 DEBUG generators.py generate l.365] (89/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:38:04,858 DEBUG generators.py generate l.373] (89/93) Reuse post-processing
[2024-03-04 21:38:04,859 INFO generators.py gen_for_qa l.548] (89/93) * Start with LLM "command-nightly"
[2024-03-04 21:38:04,860 DEBUG generators.py gen_for_qa l.554] (89/93) An Answer has already been generated with this LLM
[2024-03-04 21:38:04,861 DEBUG generators.py generate l.352] (89/93) Reuse existing Prompt
[2024-03-04 21:38:04,862 DEBUG generators.py generate l.365] (89/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:38:04,863 DEBUG generators.py generate l.373] (89/93) Reuse post-processing
[2024-03-04 21:38:04,864 INFO generators.py gen_for_qa l.548] (89/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:38:04,865 DEBUG generators.py generate l.349] (89/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:38:04,866 DEBUG generators.py generate l.358] (89/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:38:06,961 ERROR generators.py complete l.400] (89/93) The following exception occurred with prompt meta={} user=" Qui a découvert l'archipel d'Hawaï (pour les Européens) ?  A)  James Cook B)  Juan Gaetano .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:38:06,969 DEBUG generators.py generate l.373] (89/93) Reuse post-processing
[2024-03-04 21:38:06,971 INFO generators.py generate l.477] (89/93) End question " Qui a découvert l'archipel d'Hawaï (pour les Européens) ?  A)  James Cook B)  Juan Gaetano "
[2024-03-04 21:38:06,972 INFO generators.py generate l.475] (90/93) *** AnsGenerator for question " Qui a découvert le passage du Nord-Ouest ?  A)  Roald Amundsen B)  John Franklin C)  Robert McClure "
[2024-03-04 21:38:06,973 INFO generators.py gen_for_qa l.548] (90/93) * Start with LLM "gpt-4"
[2024-03-04 21:38:06,974 DEBUG generators.py gen_for_qa l.554] (90/93) An Answer has already been generated with this LLM
[2024-03-04 21:38:06,975 DEBUG generators.py generate l.352] (90/93) Reuse existing Prompt
[2024-03-04 21:38:06,976 DEBUG generators.py generate l.365] (90/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:38:06,977 DEBUG generators.py generate l.373] (90/93) Reuse post-processing
[2024-03-04 21:38:06,977 INFO generators.py gen_for_qa l.548] (90/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:38:06,979 DEBUG generators.py gen_for_qa l.554] (90/93) An Answer has already been generated with this LLM
[2024-03-04 21:38:06,980 DEBUG generators.py generate l.352] (90/93) Reuse existing Prompt
[2024-03-04 21:38:06,981 DEBUG generators.py generate l.365] (90/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:38:06,982 DEBUG generators.py generate l.373] (90/93) Reuse post-processing
[2024-03-04 21:38:06,983 INFO generators.py gen_for_qa l.548] (90/93) * Start with LLM "gemini-pro"
[2024-03-04 21:38:06,984 DEBUG generators.py gen_for_qa l.554] (90/93) An Answer has already been generated with this LLM
[2024-03-04 21:38:06,985 DEBUG generators.py generate l.352] (90/93) Reuse existing Prompt
[2024-03-04 21:38:06,986 DEBUG generators.py generate l.365] (90/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:38:06,987 DEBUG generators.py generate l.373] (90/93) Reuse post-processing
[2024-03-04 21:38:06,989 INFO generators.py gen_for_qa l.548] (90/93) * Start with LLM "claude-2.1"
[2024-03-04 21:38:06,990 DEBUG generators.py gen_for_qa l.554] (90/93) An Answer has already been generated with this LLM
[2024-03-04 21:38:06,992 DEBUG generators.py generate l.352] (90/93) Reuse existing Prompt
[2024-03-04 21:38:06,992 DEBUG generators.py generate l.365] (90/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:38:06,993 DEBUG generators.py generate l.373] (90/93) Reuse post-processing
[2024-03-04 21:38:06,994 INFO generators.py gen_for_qa l.548] (90/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:38:06,996 DEBUG generators.py gen_for_qa l.554] (90/93) An Answer has already been generated with this LLM
[2024-03-04 21:38:06,997 DEBUG generators.py generate l.352] (90/93) Reuse existing Prompt
[2024-03-04 21:38:06,998 DEBUG generators.py generate l.365] (90/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:38:06,999 DEBUG generators.py generate l.373] (90/93) Reuse post-processing
[2024-03-04 21:38:07,000 INFO generators.py gen_for_qa l.548] (90/93) * Start with LLM "command-nightly"
[2024-03-04 21:38:07,001 DEBUG generators.py gen_for_qa l.554] (90/93) An Answer has already been generated with this LLM
[2024-03-04 21:38:07,002 DEBUG generators.py generate l.352] (90/93) Reuse existing Prompt
[2024-03-04 21:38:07,003 DEBUG generators.py generate l.365] (90/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:38:07,004 DEBUG generators.py generate l.373] (90/93) Reuse post-processing
[2024-03-04 21:38:07,005 INFO generators.py gen_for_qa l.548] (90/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:38:07,006 DEBUG generators.py generate l.349] (90/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:38:07,007 DEBUG generators.py generate l.358] (90/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:38:08,309 ERROR generators.py complete l.400] (90/93) The following exception occurred with prompt meta={} user=" Qui a découvert le passage du Nord-Ouest ?  A)  Roald Amundsen B)  John Franklin C)  Robert McClure .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:38:08,322 DEBUG generators.py generate l.373] (90/93) Reuse post-processing
[2024-03-04 21:38:08,323 INFO generators.py generate l.477] (90/93) End question " Qui a découvert le passage du Nord-Ouest ?  A)  Roald Amundsen B)  John Franklin C)  Robert McClure "
[2024-03-04 21:38:08,324 INFO generators.py generate l.475] (91/93) *** AnsGenerator for question " Qui a découvert le cap de Bonne-Espérance ?  A)  Bartolomeu Dias B)  Vasco da Gama "
[2024-03-04 21:38:08,326 INFO generators.py gen_for_qa l.548] (91/93) * Start with LLM "gpt-4"
[2024-03-04 21:38:08,327 DEBUG generators.py gen_for_qa l.554] (91/93) An Answer has already been generated with this LLM
[2024-03-04 21:38:08,328 DEBUG generators.py generate l.352] (91/93) Reuse existing Prompt
[2024-03-04 21:38:08,329 DEBUG generators.py generate l.365] (91/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:38:08,330 DEBUG generators.py generate l.373] (91/93) Reuse post-processing
[2024-03-04 21:38:08,332 INFO generators.py gen_for_qa l.548] (91/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:38:08,334 DEBUG generators.py gen_for_qa l.554] (91/93) An Answer has already been generated with this LLM
[2024-03-04 21:38:08,335 DEBUG generators.py generate l.352] (91/93) Reuse existing Prompt
[2024-03-04 21:38:08,337 DEBUG generators.py generate l.365] (91/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:38:08,338 DEBUG generators.py generate l.373] (91/93) Reuse post-processing
[2024-03-04 21:38:08,340 INFO generators.py gen_for_qa l.548] (91/93) * Start with LLM "gemini-pro"
[2024-03-04 21:38:08,341 DEBUG generators.py gen_for_qa l.554] (91/93) An Answer has already been generated with this LLM
[2024-03-04 21:38:08,342 DEBUG generators.py generate l.352] (91/93) Reuse existing Prompt
[2024-03-04 21:38:08,343 DEBUG generators.py generate l.365] (91/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:38:08,344 DEBUG generators.py generate l.373] (91/93) Reuse post-processing
[2024-03-04 21:38:08,345 INFO generators.py gen_for_qa l.548] (91/93) * Start with LLM "claude-2.1"
[2024-03-04 21:38:08,346 DEBUG generators.py gen_for_qa l.554] (91/93) An Answer has already been generated with this LLM
[2024-03-04 21:38:08,348 DEBUG generators.py generate l.352] (91/93) Reuse existing Prompt
[2024-03-04 21:38:08,350 DEBUG generators.py generate l.365] (91/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:38:08,351 DEBUG generators.py generate l.373] (91/93) Reuse post-processing
[2024-03-04 21:38:08,353 INFO generators.py gen_for_qa l.548] (91/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:38:08,354 DEBUG generators.py gen_for_qa l.554] (91/93) An Answer has already been generated with this LLM
[2024-03-04 21:38:08,355 DEBUG generators.py generate l.352] (91/93) Reuse existing Prompt
[2024-03-04 21:38:08,356 DEBUG generators.py generate l.365] (91/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:38:08,357 DEBUG generators.py generate l.373] (91/93) Reuse post-processing
[2024-03-04 21:38:08,358 INFO generators.py gen_for_qa l.548] (91/93) * Start with LLM "command-nightly"
[2024-03-04 21:38:08,359 DEBUG generators.py gen_for_qa l.554] (91/93) An Answer has already been generated with this LLM
[2024-03-04 21:38:08,360 DEBUG generators.py generate l.352] (91/93) Reuse existing Prompt
[2024-03-04 21:38:08,361 DEBUG generators.py generate l.365] (91/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:38:08,362 DEBUG generators.py generate l.373] (91/93) Reuse post-processing
[2024-03-04 21:38:08,363 INFO generators.py gen_for_qa l.548] (91/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:38:08,365 DEBUG generators.py generate l.349] (91/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:38:08,366 DEBUG generators.py generate l.358] (91/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:38:09,585 ERROR generators.py complete l.400] (91/93) The following exception occurred with prompt meta={} user=" Qui a découvert le cap de Bonne-Espérance ?  A)  Bartolomeu Dias B)  Vasco da Gama .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:38:09,598 DEBUG generators.py generate l.373] (91/93) Reuse post-processing
[2024-03-04 21:38:09,599 INFO generators.py generate l.477] (91/93) End question " Qui a découvert le cap de Bonne-Espérance ?  A)  Bartolomeu Dias B)  Vasco da Gama "
[2024-03-04 21:38:09,600 INFO generators.py generate l.475] (92/93) *** AnsGenerator for question " Qui a découvert la source du Nil ?  A)  John Hanning Speke B)  Richard Francis Burton C)  Samuel Baker "
[2024-03-04 21:38:09,601 INFO generators.py gen_for_qa l.548] (92/93) * Start with LLM "gpt-4"
[2024-03-04 21:38:09,602 DEBUG generators.py gen_for_qa l.554] (92/93) An Answer has already been generated with this LLM
[2024-03-04 21:38:09,603 DEBUG generators.py generate l.352] (92/93) Reuse existing Prompt
[2024-03-04 21:38:09,604 DEBUG generators.py generate l.365] (92/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:38:09,606 DEBUG generators.py generate l.373] (92/93) Reuse post-processing
[2024-03-04 21:38:09,607 INFO generators.py gen_for_qa l.548] (92/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:38:09,608 DEBUG generators.py gen_for_qa l.554] (92/93) An Answer has already been generated with this LLM
[2024-03-04 21:38:09,609 DEBUG generators.py generate l.352] (92/93) Reuse existing Prompt
[2024-03-04 21:38:09,610 DEBUG generators.py generate l.365] (92/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:38:09,612 DEBUG generators.py generate l.373] (92/93) Reuse post-processing
[2024-03-04 21:38:09,612 INFO generators.py gen_for_qa l.548] (92/93) * Start with LLM "gemini-pro"
[2024-03-04 21:38:09,613 DEBUG generators.py gen_for_qa l.554] (92/93) An Answer has already been generated with this LLM
[2024-03-04 21:38:09,615 DEBUG generators.py generate l.352] (92/93) Reuse existing Prompt
[2024-03-04 21:38:09,616 DEBUG generators.py generate l.365] (92/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:38:09,617 DEBUG generators.py generate l.373] (92/93) Reuse post-processing
[2024-03-04 21:38:09,618 INFO generators.py gen_for_qa l.548] (92/93) * Start with LLM "claude-2.1"
[2024-03-04 21:38:09,619 DEBUG generators.py gen_for_qa l.554] (92/93) An Answer has already been generated with this LLM
[2024-03-04 21:38:09,620 DEBUG generators.py generate l.352] (92/93) Reuse existing Prompt
[2024-03-04 21:38:09,621 DEBUG generators.py generate l.365] (92/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:38:09,622 DEBUG generators.py generate l.373] (92/93) Reuse post-processing
[2024-03-04 21:38:09,622 INFO generators.py gen_for_qa l.548] (92/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:38:09,623 DEBUG generators.py gen_for_qa l.554] (92/93) An Answer has already been generated with this LLM
[2024-03-04 21:38:09,624 DEBUG generators.py generate l.352] (92/93) Reuse existing Prompt
[2024-03-04 21:38:09,625 DEBUG generators.py generate l.365] (92/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:38:09,626 DEBUG generators.py generate l.373] (92/93) Reuse post-processing
[2024-03-04 21:38:09,627 INFO generators.py gen_for_qa l.548] (92/93) * Start with LLM "command-nightly"
[2024-03-04 21:38:09,628 DEBUG generators.py gen_for_qa l.554] (92/93) An Answer has already been generated with this LLM
[2024-03-04 21:38:09,629 DEBUG generators.py generate l.352] (92/93) Reuse existing Prompt
[2024-03-04 21:38:09,630 DEBUG generators.py generate l.365] (92/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:38:09,631 DEBUG generators.py generate l.373] (92/93) Reuse post-processing
[2024-03-04 21:38:09,632 INFO generators.py gen_for_qa l.548] (92/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:38:09,633 DEBUG generators.py generate l.349] (92/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:38:09,634 DEBUG generators.py generate l.358] (92/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:38:11,082 ERROR generators.py complete l.400] (92/93) The following exception occurred with prompt meta={} user=" Qui a découvert la source du Nil ?  A)  John Hanning Speke B)  Richard Francis Burton C)  Samuel Baker .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:38:11,094 DEBUG generators.py generate l.373] (92/93) Reuse post-processing
[2024-03-04 21:38:11,095 INFO generators.py generate l.477] (92/93) End question " Qui a découvert la source du Nil ?  A)  John Hanning Speke B)  Richard Francis Burton C)  Samuel Baker "
[2024-03-04 21:38:11,096 INFO generators.py generate l.475] (93/93) *** AnsGenerator for question " Qui a découvert la Terre de Feu ?  A)  Ferdinand Magellan B)  Hernando de Magallanes "
[2024-03-04 21:38:11,097 INFO generators.py gen_for_qa l.548] (93/93) * Start with LLM "gpt-4"
[2024-03-04 21:38:11,098 DEBUG generators.py gen_for_qa l.554] (93/93) An Answer has already been generated with this LLM
[2024-03-04 21:38:11,100 DEBUG generators.py generate l.352] (93/93) Reuse existing Prompt
[2024-03-04 21:38:11,102 DEBUG generators.py generate l.365] (93/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:38:11,102 DEBUG generators.py generate l.373] (93/93) Reuse post-processing
[2024-03-04 21:38:11,103 INFO generators.py gen_for_qa l.548] (93/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:38:11,105 DEBUG generators.py gen_for_qa l.554] (93/93) An Answer has already been generated with this LLM
[2024-03-04 21:38:11,107 DEBUG generators.py generate l.352] (93/93) Reuse existing Prompt
[2024-03-04 21:38:11,109 DEBUG generators.py generate l.365] (93/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:38:11,110 DEBUG generators.py generate l.373] (93/93) Reuse post-processing
[2024-03-04 21:38:11,111 INFO generators.py gen_for_qa l.548] (93/93) * Start with LLM "gemini-pro"
[2024-03-04 21:38:11,113 DEBUG generators.py gen_for_qa l.554] (93/93) An Answer has already been generated with this LLM
[2024-03-04 21:38:11,114 DEBUG generators.py generate l.352] (93/93) Reuse existing Prompt
[2024-03-04 21:38:11,116 DEBUG generators.py generate l.365] (93/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:38:11,117 DEBUG generators.py generate l.373] (93/93) Reuse post-processing
[2024-03-04 21:38:11,118 INFO generators.py gen_for_qa l.548] (93/93) * Start with LLM "claude-2.1"
[2024-03-04 21:38:11,119 DEBUG generators.py gen_for_qa l.554] (93/93) An Answer has already been generated with this LLM
[2024-03-04 21:38:11,120 DEBUG generators.py generate l.352] (93/93) Reuse existing Prompt
[2024-03-04 21:38:11,123 DEBUG generators.py generate l.365] (93/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:38:11,125 DEBUG generators.py generate l.373] (93/93) Reuse post-processing
[2024-03-04 21:38:11,127 INFO generators.py gen_for_qa l.548] (93/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:38:11,128 DEBUG generators.py gen_for_qa l.554] (93/93) An Answer has already been generated with this LLM
[2024-03-04 21:38:11,129 DEBUG generators.py generate l.352] (93/93) Reuse existing Prompt
[2024-03-04 21:38:11,130 DEBUG generators.py generate l.365] (93/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:38:11,132 DEBUG generators.py generate l.373] (93/93) Reuse post-processing
[2024-03-04 21:38:11,133 INFO generators.py gen_for_qa l.548] (93/93) * Start with LLM "command-nightly"
[2024-03-04 21:38:11,134 DEBUG generators.py gen_for_qa l.554] (93/93) An Answer has already been generated with this LLM
[2024-03-04 21:38:11,135 DEBUG generators.py generate l.352] (93/93) Reuse existing Prompt
[2024-03-04 21:38:11,136 DEBUG generators.py generate l.365] (93/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:38:11,137 DEBUG generators.py generate l.373] (93/93) Reuse post-processing
[2024-03-04 21:38:11,137 INFO generators.py gen_for_qa l.548] (93/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:38:11,139 DEBUG generators.py generate l.349] (93/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:38:11,140 DEBUG generators.py generate l.358] (93/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:38:12,614 ERROR generators.py complete l.400] (93/93) The following exception occurred with prompt meta={} user=" Qui a découvert la Terre de Feu ?  A)  Ferdinand Magellan B)  Hernando de Magallanes .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 397, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:38:12,624 DEBUG generators.py generate l.373] (93/93) Reuse post-processing
[2024-03-04 21:38:12,625 INFO generators.py generate l.477] (93/93) End question " Qui a découvert la Terre de Feu ?  A)  Ferdinand Magellan B)  Hernando de Magallanes "
[2024-03-04 21:38:12,641 INFO expe.py save_to_json l.283] (93/93) Expe saved as JSON to expe\Answers\culture_fr_v1_gen_with_LeChat--93Q_0C_0F_7M_559A_0HE_0AE_2024-03-04_21,38,12.json
[2024-03-04 21:38:12,643 INFO main.py <module> l.101] (93/93) MAIN ENDS
[2024-03-04 21:52:05,789 INFO main.py <module> l.87] MAIN STARTS
[2024-03-04 21:52:05,808 INFO generators.py generate l.477] (1/93) *** AnsGenerator for question "Qui a inventé le télégraphe électrique ?  A)  Samuel Morse B)  Charles Wheatstone C)  William Fothergill Cooke "
[2024-03-04 21:52:05,810 INFO generators.py gen_for_qa l.550] (1/93) * Start with LLM "gpt-4"
[2024-03-04 21:52:05,811 DEBUG generators.py gen_for_qa l.556] (1/93) An Answer has already been generated with this LLM
[2024-03-04 21:52:05,812 DEBUG generators.py generate l.354] (1/93) Reuse existing Prompt
[2024-03-04 21:52:05,813 DEBUG generators.py generate l.367] (1/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:52:05,814 DEBUG generators.py generate l.375] (1/93) Reuse post-processing
[2024-03-04 21:52:05,816 INFO generators.py gen_for_qa l.550] (1/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:52:05,816 DEBUG generators.py gen_for_qa l.556] (1/93) An Answer has already been generated with this LLM
[2024-03-04 21:52:05,818 DEBUG generators.py generate l.354] (1/93) Reuse existing Prompt
[2024-03-04 21:52:05,819 DEBUG generators.py generate l.367] (1/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:52:05,820 DEBUG generators.py generate l.375] (1/93) Reuse post-processing
[2024-03-04 21:52:05,822 INFO generators.py gen_for_qa l.550] (1/93) * Start with LLM "gemini-pro"
[2024-03-04 21:52:05,823 DEBUG generators.py gen_for_qa l.556] (1/93) An Answer has already been generated with this LLM
[2024-03-04 21:52:05,824 DEBUG generators.py generate l.354] (1/93) Reuse existing Prompt
[2024-03-04 21:52:05,824 DEBUG generators.py generate l.367] (1/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:52:05,826 DEBUG generators.py generate l.375] (1/93) Reuse post-processing
[2024-03-04 21:52:05,826 INFO generators.py gen_for_qa l.550] (1/93) * Start with LLM "claude-2.1"
[2024-03-04 21:52:05,826 DEBUG generators.py gen_for_qa l.556] (1/93) An Answer has already been generated with this LLM
[2024-03-04 21:52:05,829 DEBUG generators.py generate l.354] (1/93) Reuse existing Prompt
[2024-03-04 21:52:05,829 DEBUG generators.py generate l.367] (1/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:52:05,830 DEBUG generators.py generate l.375] (1/93) Reuse post-processing
[2024-03-04 21:52:05,831 INFO generators.py gen_for_qa l.550] (1/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:52:05,832 DEBUG generators.py gen_for_qa l.556] (1/93) An Answer has already been generated with this LLM
[2024-03-04 21:52:05,832 DEBUG generators.py generate l.354] (1/93) Reuse existing Prompt
[2024-03-04 21:52:05,833 DEBUG generators.py generate l.367] (1/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:52:05,835 DEBUG generators.py generate l.375] (1/93) Reuse post-processing
[2024-03-04 21:52:05,836 INFO generators.py gen_for_qa l.550] (1/93) * Start with LLM "command-nightly"
[2024-03-04 21:52:05,838 DEBUG generators.py gen_for_qa l.556] (1/93) An Answer has already been generated with this LLM
[2024-03-04 21:52:05,840 DEBUG generators.py generate l.354] (1/93) Reuse existing Prompt
[2024-03-04 21:52:05,841 DEBUG generators.py generate l.367] (1/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:52:05,841 DEBUG generators.py generate l.375] (1/93) Reuse post-processing
[2024-03-04 21:52:05,842 INFO generators.py gen_for_qa l.550] (1/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:52:05,843 DEBUG generators.py generate l.351] (1/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:52:05,844 DEBUG generators.py generate l.360] (1/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:52:07,475 ERROR generators.py complete l.402] (1/93) The following exception occurred with prompt meta={} user="Qui a inventé le télégraphe électrique ?  A)  Samuel Morse B)  Charles Wheatstone C)  William Fothergill Cooke .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 399, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:52:07,487 DEBUG generators.py generate l.375] (1/93) Reuse post-processing
[2024-03-04 21:52:07,488 INFO generators.py generate l.479] (1/93) End question "Qui a inventé le télégraphe électrique ?  A)  Samuel Morse B)  Charles Wheatstone C)  William Fothergill Cooke "
[2024-03-04 21:52:07,490 INFO generators.py generate l.477] (2/93) *** AnsGenerator for question "Qui a inventé la photographie ?  A)  Louis Daguerre B)  William Henry Fox Talbot "
[2024-03-04 21:52:07,491 INFO generators.py gen_for_qa l.550] (2/93) * Start with LLM "gpt-4"
[2024-03-04 21:52:07,491 DEBUG generators.py gen_for_qa l.556] (2/93) An Answer has already been generated with this LLM
[2024-03-04 21:52:07,494 DEBUG generators.py generate l.354] (2/93) Reuse existing Prompt
[2024-03-04 21:52:07,494 DEBUG generators.py generate l.367] (2/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:52:07,494 DEBUG generators.py generate l.375] (2/93) Reuse post-processing
[2024-03-04 21:52:07,496 INFO generators.py gen_for_qa l.550] (2/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:52:07,497 DEBUG generators.py gen_for_qa l.556] (2/93) An Answer has already been generated with this LLM
[2024-03-04 21:52:07,498 DEBUG generators.py generate l.354] (2/93) Reuse existing Prompt
[2024-03-04 21:52:07,499 DEBUG generators.py generate l.367] (2/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:52:07,500 DEBUG generators.py generate l.375] (2/93) Reuse post-processing
[2024-03-04 21:52:07,500 INFO generators.py gen_for_qa l.550] (2/93) * Start with LLM "gemini-pro"
[2024-03-04 21:52:07,502 DEBUG generators.py gen_for_qa l.556] (2/93) An Answer has already been generated with this LLM
[2024-03-04 21:52:07,503 DEBUG generators.py generate l.354] (2/93) Reuse existing Prompt
[2024-03-04 21:52:07,504 DEBUG generators.py generate l.367] (2/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:52:07,506 DEBUG generators.py generate l.375] (2/93) Reuse post-processing
[2024-03-04 21:52:07,509 INFO generators.py gen_for_qa l.550] (2/93) * Start with LLM "claude-2.1"
[2024-03-04 21:52:07,510 DEBUG generators.py gen_for_qa l.556] (2/93) An Answer has already been generated with this LLM
[2024-03-04 21:52:07,511 DEBUG generators.py generate l.354] (2/93) Reuse existing Prompt
[2024-03-04 21:52:07,511 DEBUG generators.py generate l.367] (2/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:52:07,511 DEBUG generators.py generate l.375] (2/93) Reuse post-processing
[2024-03-04 21:52:07,511 INFO generators.py gen_for_qa l.550] (2/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:52:07,511 DEBUG generators.py gen_for_qa l.556] (2/93) An Answer has already been generated with this LLM
[2024-03-04 21:52:07,515 DEBUG generators.py generate l.354] (2/93) Reuse existing Prompt
[2024-03-04 21:52:07,516 DEBUG generators.py generate l.367] (2/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:52:07,517 DEBUG generators.py generate l.375] (2/93) Reuse post-processing
[2024-03-04 21:52:07,519 INFO generators.py gen_for_qa l.550] (2/93) * Start with LLM "command-nightly"
[2024-03-04 21:52:07,521 DEBUG generators.py gen_for_qa l.556] (2/93) An Answer has already been generated with this LLM
[2024-03-04 21:52:07,523 DEBUG generators.py generate l.354] (2/93) Reuse existing Prompt
[2024-03-04 21:52:07,524 DEBUG generators.py generate l.367] (2/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:52:07,525 DEBUG generators.py generate l.375] (2/93) Reuse post-processing
[2024-03-04 21:52:07,525 INFO generators.py gen_for_qa l.550] (2/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:52:07,526 DEBUG generators.py generate l.351] (2/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:52:07,526 DEBUG generators.py generate l.360] (2/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:52:08,819 ERROR generators.py complete l.402] (2/93) The following exception occurred with prompt meta={} user="Qui a inventé la photographie ?  A)  Louis Daguerre B)  William Henry Fox Talbot .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 399, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:52:08,827 DEBUG generators.py generate l.375] (2/93) Reuse post-processing
[2024-03-04 21:52:08,829 INFO generators.py generate l.479] (2/93) End question "Qui a inventé la photographie ?  A)  Louis Daguerre B)  William Henry Fox Talbot "
[2024-03-04 21:52:08,829 INFO generators.py generate l.477] (3/93) *** AnsGenerator for question "Qui a inventé le moteur à vapeur ?  A)  Thomas Newcomen B)  Denis Papin "
[2024-03-04 21:52:08,831 INFO generators.py gen_for_qa l.550] (3/93) * Start with LLM "gpt-4"
[2024-03-04 21:52:08,831 DEBUG generators.py gen_for_qa l.556] (3/93) An Answer has already been generated with this LLM
[2024-03-04 21:52:08,832 DEBUG generators.py generate l.354] (3/93) Reuse existing Prompt
[2024-03-04 21:52:08,833 DEBUG generators.py generate l.367] (3/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:52:08,833 DEBUG generators.py generate l.375] (3/93) Reuse post-processing
[2024-03-04 21:52:08,834 INFO generators.py gen_for_qa l.550] (3/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:52:08,835 DEBUG generators.py gen_for_qa l.556] (3/93) An Answer has already been generated with this LLM
[2024-03-04 21:52:08,838 DEBUG generators.py generate l.354] (3/93) Reuse existing Prompt
[2024-03-04 21:52:08,841 DEBUG generators.py generate l.367] (3/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:52:08,843 DEBUG generators.py generate l.375] (3/93) Reuse post-processing
[2024-03-04 21:52:08,843 INFO generators.py gen_for_qa l.550] (3/93) * Start with LLM "gemini-pro"
[2024-03-04 21:52:08,843 DEBUG generators.py gen_for_qa l.556] (3/93) An Answer has already been generated with this LLM
[2024-03-04 21:52:08,843 DEBUG generators.py generate l.354] (3/93) Reuse existing Prompt
[2024-03-04 21:52:08,843 DEBUG generators.py generate l.367] (3/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:52:08,848 DEBUG generators.py generate l.375] (3/93) Reuse post-processing
[2024-03-04 21:52:08,849 INFO generators.py gen_for_qa l.550] (3/93) * Start with LLM "claude-2.1"
[2024-03-04 21:52:08,851 DEBUG generators.py gen_for_qa l.556] (3/93) An Answer has already been generated with this LLM
[2024-03-04 21:52:08,853 DEBUG generators.py generate l.354] (3/93) Reuse existing Prompt
[2024-03-04 21:52:08,854 DEBUG generators.py generate l.367] (3/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:52:08,857 DEBUG generators.py generate l.375] (3/93) Reuse post-processing
[2024-03-04 21:52:08,858 INFO generators.py gen_for_qa l.550] (3/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:52:08,859 DEBUG generators.py gen_for_qa l.556] (3/93) An Answer has already been generated with this LLM
[2024-03-04 21:52:08,859 DEBUG generators.py generate l.354] (3/93) Reuse existing Prompt
[2024-03-04 21:52:08,861 DEBUG generators.py generate l.367] (3/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:52:08,862 DEBUG generators.py generate l.375] (3/93) Reuse post-processing
[2024-03-04 21:52:08,863 INFO generators.py gen_for_qa l.550] (3/93) * Start with LLM "command-nightly"
[2024-03-04 21:52:08,863 DEBUG generators.py gen_for_qa l.556] (3/93) An Answer has already been generated with this LLM
[2024-03-04 21:52:08,864 DEBUG generators.py generate l.354] (3/93) Reuse existing Prompt
[2024-03-04 21:52:08,865 DEBUG generators.py generate l.367] (3/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:52:08,867 DEBUG generators.py generate l.375] (3/93) Reuse post-processing
[2024-03-04 21:52:08,868 INFO generators.py gen_for_qa l.550] (3/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:52:08,869 DEBUG generators.py generate l.351] (3/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:52:08,872 DEBUG generators.py generate l.360] (3/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:52:10,288 ERROR generators.py complete l.402] (3/93) The following exception occurred with prompt meta={} user="Qui a inventé le moteur à vapeur ?  A)  Thomas Newcomen B)  Denis Papin .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 399, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:52:10,300 DEBUG generators.py generate l.375] (3/93) Reuse post-processing
[2024-03-04 21:52:10,301 INFO generators.py generate l.479] (3/93) End question "Qui a inventé le moteur à vapeur ?  A)  Thomas Newcomen B)  Denis Papin "
[2024-03-04 21:52:10,302 INFO generators.py generate l.477] (4/93) *** AnsGenerator for question "Qui a inventé le bateau à vapeur ?  A)  James Watt B)  Claude de Jouffroy d'Abbans "
[2024-03-04 21:52:10,305 INFO generators.py gen_for_qa l.550] (4/93) * Start with LLM "gpt-4"
[2024-03-04 21:52:10,306 DEBUG generators.py gen_for_qa l.556] (4/93) An Answer has already been generated with this LLM
[2024-03-04 21:52:10,308 DEBUG generators.py generate l.354] (4/93) Reuse existing Prompt
[2024-03-04 21:52:10,309 DEBUG generators.py generate l.367] (4/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:52:10,312 DEBUG generators.py generate l.375] (4/93) Reuse post-processing
[2024-03-04 21:52:10,313 INFO generators.py gen_for_qa l.550] (4/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:52:10,316 DEBUG generators.py gen_for_qa l.556] (4/93) An Answer has already been generated with this LLM
[2024-03-04 21:52:10,317 DEBUG generators.py generate l.354] (4/93) Reuse existing Prompt
[2024-03-04 21:52:10,319 DEBUG generators.py generate l.367] (4/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:52:10,321 DEBUG generators.py generate l.375] (4/93) Reuse post-processing
[2024-03-04 21:52:10,323 INFO generators.py gen_for_qa l.550] (4/93) * Start with LLM "gemini-pro"
[2024-03-04 21:52:10,324 DEBUG generators.py gen_for_qa l.556] (4/93) An Answer has already been generated with this LLM
[2024-03-04 21:52:10,325 DEBUG generators.py generate l.354] (4/93) Reuse existing Prompt
[2024-03-04 21:52:10,326 DEBUG generators.py generate l.367] (4/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:52:10,327 DEBUG generators.py generate l.375] (4/93) Reuse post-processing
[2024-03-04 21:52:10,327 INFO generators.py gen_for_qa l.550] (4/93) * Start with LLM "claude-2.1"
[2024-03-04 21:52:10,329 DEBUG generators.py gen_for_qa l.556] (4/93) An Answer has already been generated with this LLM
[2024-03-04 21:52:10,329 DEBUG generators.py generate l.354] (4/93) Reuse existing Prompt
[2024-03-04 21:52:10,329 DEBUG generators.py generate l.367] (4/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:52:10,329 DEBUG generators.py generate l.375] (4/93) Reuse post-processing
[2024-03-04 21:52:10,333 INFO generators.py gen_for_qa l.550] (4/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:52:10,334 DEBUG generators.py gen_for_qa l.556] (4/93) An Answer has already been generated with this LLM
[2024-03-04 21:52:10,336 DEBUG generators.py generate l.354] (4/93) Reuse existing Prompt
[2024-03-04 21:52:10,338 DEBUG generators.py generate l.367] (4/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:52:10,339 DEBUG generators.py generate l.375] (4/93) Reuse post-processing
[2024-03-04 21:52:10,340 INFO generators.py gen_for_qa l.550] (4/93) * Start with LLM "command-nightly"
[2024-03-04 21:52:10,341 DEBUG generators.py gen_for_qa l.556] (4/93) An Answer has already been generated with this LLM
[2024-03-04 21:52:10,343 DEBUG generators.py generate l.354] (4/93) Reuse existing Prompt
[2024-03-04 21:52:10,343 DEBUG generators.py generate l.367] (4/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:52:10,345 DEBUG generators.py generate l.375] (4/93) Reuse post-processing
[2024-03-04 21:52:10,345 INFO generators.py gen_for_qa l.550] (4/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:52:10,346 DEBUG generators.py generate l.351] (4/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:52:10,346 DEBUG generators.py generate l.360] (4/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:52:11,992 ERROR generators.py complete l.402] (4/93) The following exception occurred with prompt meta={} user="Qui a inventé le bateau à vapeur ?  A)  James Watt B)  Claude de Jouffroy d'Abbans .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 399, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:52:12,009 DEBUG generators.py generate l.375] (4/93) Reuse post-processing
[2024-03-04 21:52:12,010 INFO generators.py generate l.479] (4/93) End question "Qui a inventé le bateau à vapeur ?  A)  James Watt B)  Claude de Jouffroy d'Abbans "
[2024-03-04 21:52:12,012 INFO generators.py generate l.477] (5/93) *** AnsGenerator for question "Qui a inventé le parachute ?  A)  Louis-Sébastien Lenormand B)  Sir George Cayley "
[2024-03-04 21:52:12,014 INFO generators.py gen_for_qa l.550] (5/93) * Start with LLM "gpt-4"
[2024-03-04 21:52:12,014 DEBUG generators.py gen_for_qa l.556] (5/93) An Answer has already been generated with this LLM
[2024-03-04 21:52:12,017 DEBUG generators.py generate l.354] (5/93) Reuse existing Prompt
[2024-03-04 21:52:12,018 DEBUG generators.py generate l.367] (5/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:52:12,019 DEBUG generators.py generate l.375] (5/93) Reuse post-processing
[2024-03-04 21:52:12,022 INFO generators.py gen_for_qa l.550] (5/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:52:12,023 DEBUG generators.py gen_for_qa l.556] (5/93) An Answer has already been generated with this LLM
[2024-03-04 21:52:12,024 DEBUG generators.py generate l.354] (5/93) Reuse existing Prompt
[2024-03-04 21:52:12,024 DEBUG generators.py generate l.367] (5/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:52:12,024 DEBUG generators.py generate l.375] (5/93) Reuse post-processing
[2024-03-04 21:52:12,027 INFO generators.py gen_for_qa l.550] (5/93) * Start with LLM "gemini-pro"
[2024-03-04 21:52:12,027 DEBUG generators.py gen_for_qa l.556] (5/93) An Answer has already been generated with this LLM
[2024-03-04 21:52:12,029 DEBUG generators.py generate l.354] (5/93) Reuse existing Prompt
[2024-03-04 21:52:12,030 DEBUG generators.py generate l.367] (5/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:52:12,031 DEBUG generators.py generate l.375] (5/93) Reuse post-processing
[2024-03-04 21:52:12,031 INFO generators.py gen_for_qa l.550] (5/93) * Start with LLM "claude-2.1"
[2024-03-04 21:52:12,032 DEBUG generators.py gen_for_qa l.556] (5/93) An Answer has already been generated with this LLM
[2024-03-04 21:52:12,033 DEBUG generators.py generate l.354] (5/93) Reuse existing Prompt
[2024-03-04 21:52:12,034 DEBUG generators.py generate l.367] (5/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:52:12,037 DEBUG generators.py generate l.375] (5/93) Reuse post-processing
[2024-03-04 21:52:12,038 INFO generators.py gen_for_qa l.550] (5/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:52:12,039 DEBUG generators.py gen_for_qa l.556] (5/93) An Answer has already been generated with this LLM
[2024-03-04 21:52:12,041 DEBUG generators.py generate l.354] (5/93) Reuse existing Prompt
[2024-03-04 21:52:12,041 DEBUG generators.py generate l.367] (5/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:52:12,042 DEBUG generators.py generate l.375] (5/93) Reuse post-processing
[2024-03-04 21:52:12,043 INFO generators.py gen_for_qa l.550] (5/93) * Start with LLM "command-nightly"
[2024-03-04 21:52:12,044 DEBUG generators.py gen_for_qa l.556] (5/93) An Answer has already been generated with this LLM
[2024-03-04 21:52:12,045 DEBUG generators.py generate l.354] (5/93) Reuse existing Prompt
[2024-03-04 21:52:12,045 DEBUG generators.py generate l.367] (5/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:52:12,045 DEBUG generators.py generate l.375] (5/93) Reuse post-processing
[2024-03-04 21:52:12,050 INFO generators.py gen_for_qa l.550] (5/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:52:12,051 DEBUG generators.py generate l.351] (5/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:52:12,054 DEBUG generators.py generate l.360] (5/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:52:14,494 ERROR generators.py complete l.402] (5/93) The following exception occurred with prompt meta={} user="Qui a inventé le parachute ?  A)  Louis-Sébastien Lenormand B)  Sir George Cayley .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 399, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:52:14,509 DEBUG generators.py generate l.375] (5/93) Reuse post-processing
[2024-03-04 21:52:14,511 INFO generators.py generate l.479] (5/93) End question "Qui a inventé le parachute ?  A)  Louis-Sébastien Lenormand B)  Sir George Cayley "
[2024-03-04 21:52:14,511 INFO generators.py generate l.477] (6/93) *** AnsGenerator for question "Qui a inventé le vélocipède ?  A)  Pierre Michaux B)  Kirkpatrick Macmillan "
[2024-03-04 21:52:14,513 INFO generators.py gen_for_qa l.550] (6/93) * Start with LLM "gpt-4"
[2024-03-04 21:52:14,513 DEBUG generators.py gen_for_qa l.556] (6/93) An Answer has already been generated with this LLM
[2024-03-04 21:52:14,516 DEBUG generators.py generate l.354] (6/93) Reuse existing Prompt
[2024-03-04 21:52:14,518 DEBUG generators.py generate l.367] (6/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:52:14,518 DEBUG generators.py generate l.375] (6/93) Reuse post-processing
[2024-03-04 21:52:14,524 INFO generators.py gen_for_qa l.550] (6/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:52:14,525 DEBUG generators.py gen_for_qa l.556] (6/93) An Answer has already been generated with this LLM
[2024-03-04 21:52:14,526 DEBUG generators.py generate l.354] (6/93) Reuse existing Prompt
[2024-03-04 21:52:14,527 DEBUG generators.py generate l.367] (6/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:52:14,527 DEBUG generators.py generate l.375] (6/93) Reuse post-processing
[2024-03-04 21:52:14,530 INFO generators.py gen_for_qa l.550] (6/93) * Start with LLM "gemini-pro"
[2024-03-04 21:52:14,531 DEBUG generators.py gen_for_qa l.556] (6/93) An Answer has already been generated with this LLM
[2024-03-04 21:52:14,531 DEBUG generators.py generate l.354] (6/93) Reuse existing Prompt
[2024-03-04 21:52:14,532 DEBUG generators.py generate l.367] (6/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:52:14,536 DEBUG generators.py generate l.375] (6/93) Reuse post-processing
[2024-03-04 21:52:14,539 INFO generators.py gen_for_qa l.550] (6/93) * Start with LLM "claude-2.1"
[2024-03-04 21:52:14,540 DEBUG generators.py gen_for_qa l.556] (6/93) An Answer has already been generated with this LLM
[2024-03-04 21:52:14,541 DEBUG generators.py generate l.354] (6/93) Reuse existing Prompt
[2024-03-04 21:52:14,542 DEBUG generators.py generate l.367] (6/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:52:14,542 DEBUG generators.py generate l.375] (6/93) Reuse post-processing
[2024-03-04 21:52:14,544 INFO generators.py gen_for_qa l.550] (6/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:52:14,545 DEBUG generators.py gen_for_qa l.556] (6/93) An Answer has already been generated with this LLM
[2024-03-04 21:52:14,547 DEBUG generators.py generate l.354] (6/93) Reuse existing Prompt
[2024-03-04 21:52:14,547 DEBUG generators.py generate l.367] (6/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:52:14,549 DEBUG generators.py generate l.375] (6/93) Reuse post-processing
[2024-03-04 21:52:14,549 INFO generators.py gen_for_qa l.550] (6/93) * Start with LLM "command-nightly"
[2024-03-04 21:52:14,555 DEBUG generators.py gen_for_qa l.556] (6/93) An Answer has already been generated with this LLM
[2024-03-04 21:52:14,555 DEBUG generators.py generate l.354] (6/93) Reuse existing Prompt
[2024-03-04 21:52:14,557 DEBUG generators.py generate l.367] (6/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:52:14,558 DEBUG generators.py generate l.375] (6/93) Reuse post-processing
[2024-03-04 21:52:14,559 INFO generators.py gen_for_qa l.550] (6/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:52:14,561 DEBUG generators.py generate l.351] (6/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:52:14,562 DEBUG generators.py generate l.360] (6/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:58:13,421 ERROR generators.py complete l.402] (6/93) The following exception occurred with prompt meta={} user="Qui a inventé le vélocipède ?  A)  Pierre Michaux B)  Kirkpatrick Macmillan .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 399, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:58:13,473 DEBUG generators.py generate l.375] (6/93) Reuse post-processing
[2024-03-04 21:58:13,478 INFO generators.py generate l.479] (6/93) End question "Qui a inventé le vélocipède ?  A)  Pierre Michaux B)  Kirkpatrick Macmillan "
[2024-03-04 21:58:13,480 INFO generators.py generate l.477] (7/93) *** AnsGenerator for question "Qui a inventé le cinématographe ?  A)  Auguste et Louis Lumière B)  William Friese-Greene "
[2024-03-04 21:58:13,480 INFO generators.py gen_for_qa l.550] (7/93) * Start with LLM "gpt-4"
[2024-03-04 21:58:13,480 DEBUG generators.py gen_for_qa l.556] (7/93) An Answer has already been generated with this LLM
[2024-03-04 21:58:13,483 DEBUG generators.py generate l.354] (7/93) Reuse existing Prompt
[2024-03-04 21:58:13,483 DEBUG generators.py generate l.367] (7/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:58:13,488 DEBUG generators.py generate l.375] (7/93) Reuse post-processing
[2024-03-04 21:58:13,489 INFO generators.py gen_for_qa l.550] (7/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:58:13,493 DEBUG generators.py gen_for_qa l.556] (7/93) An Answer has already been generated with this LLM
[2024-03-04 21:58:13,493 DEBUG generators.py generate l.354] (7/93) Reuse existing Prompt
[2024-03-04 21:58:13,496 DEBUG generators.py generate l.367] (7/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:58:13,497 DEBUG generators.py generate l.375] (7/93) Reuse post-processing
[2024-03-04 21:58:13,499 INFO generators.py gen_for_qa l.550] (7/93) * Start with LLM "gemini-pro"
[2024-03-04 21:58:13,500 DEBUG generators.py gen_for_qa l.556] (7/93) An Answer has already been generated with this LLM
[2024-03-04 21:58:13,500 DEBUG generators.py generate l.354] (7/93) Reuse existing Prompt
[2024-03-04 21:58:13,500 DEBUG generators.py generate l.367] (7/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:58:13,500 DEBUG generators.py generate l.375] (7/93) Reuse post-processing
[2024-03-04 21:58:13,500 INFO generators.py gen_for_qa l.550] (7/93) * Start with LLM "claude-2.1"
[2024-03-04 21:58:13,500 DEBUG generators.py gen_for_qa l.556] (7/93) An Answer has already been generated with this LLM
[2024-03-04 21:58:13,500 DEBUG generators.py generate l.354] (7/93) Reuse existing Prompt
[2024-03-04 21:58:13,500 DEBUG generators.py generate l.367] (7/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:58:13,500 DEBUG generators.py generate l.375] (7/93) Reuse post-processing
[2024-03-04 21:58:13,514 INFO generators.py gen_for_qa l.550] (7/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:58:13,515 DEBUG generators.py gen_for_qa l.556] (7/93) An Answer has already been generated with this LLM
[2024-03-04 21:58:13,515 DEBUG generators.py generate l.354] (7/93) Reuse existing Prompt
[2024-03-04 21:58:13,515 DEBUG generators.py generate l.367] (7/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:58:13,515 DEBUG generators.py generate l.375] (7/93) Reuse post-processing
[2024-03-04 21:58:13,515 INFO generators.py gen_for_qa l.550] (7/93) * Start with LLM "command-nightly"
[2024-03-04 21:58:13,515 DEBUG generators.py gen_for_qa l.556] (7/93) An Answer has already been generated with this LLM
[2024-03-04 21:58:13,515 DEBUG generators.py generate l.354] (7/93) Reuse existing Prompt
[2024-03-04 21:58:13,526 DEBUG generators.py generate l.367] (7/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:58:13,527 DEBUG generators.py generate l.375] (7/93) Reuse post-processing
[2024-03-04 21:58:13,531 INFO generators.py gen_for_qa l.550] (7/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:58:13,531 DEBUG generators.py generate l.351] (7/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:58:13,531 DEBUG generators.py generate l.360] (7/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:58:16,605 ERROR generators.py complete l.402] (7/93) The following exception occurred with prompt meta={} user="Qui a inventé le cinématographe ?  A)  Auguste et Louis Lumière B)  William Friese-Greene .\nRépond uniquement avec la lettre. Ne donne qu'une seule réponse." system=''
expected string or buffer
Traceback (most recent call last):
  File "C:\Users\gilles_recital\source\repos\ragtime\ragtime\generators.py", line 399, in complete
    ans:dict = completion(messages=messages, model=self.name,
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2621, in wrapper
    raise e
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 2524, in wrapper
    result = original_function(*args, **kwargs)
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1909, in completion
    raise exception_type(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7783, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\utils.py", line 7573, in exception_type
    raise original_exception
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\main.py", line 1124, in completion
    model_response = aleph_alpha.completion(
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\litellm\llms\aleph_alpha.py", line 288, in completion
    encoding.encode(model_response["choices"][0]["message"]["content"])
  File "c:\Users\gilles_recital\AppData\Local\Programs\Python\Python310\lib\site-packages\tiktoken\core.py", line 116, in encode
    if match := _special_token_regex(disallowed_special).search(text):
TypeError: expected string or buffer
[2024-03-04 21:58:16,623 DEBUG generators.py generate l.375] (7/93) Reuse post-processing
[2024-03-04 21:58:16,625 INFO generators.py generate l.479] (7/93) End question "Qui a inventé le cinématographe ?  A)  Auguste et Louis Lumière B)  William Friese-Greene "
[2024-03-04 21:58:16,627 INFO generators.py generate l.477] (8/93) *** AnsGenerator for question "Qui a inventé la machine à coudre ?  A)  Barthélemy Thimonnier B)  Thomas Saint "
[2024-03-04 21:58:16,629 INFO generators.py gen_for_qa l.550] (8/93) * Start with LLM "gpt-4"
[2024-03-04 21:58:16,631 DEBUG generators.py gen_for_qa l.556] (8/93) An Answer has already been generated with this LLM
[2024-03-04 21:58:16,631 DEBUG generators.py generate l.354] (8/93) Reuse existing Prompt
[2024-03-04 21:58:16,633 DEBUG generators.py generate l.367] (8/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:58:16,636 DEBUG generators.py generate l.375] (8/93) Reuse post-processing
[2024-03-04 21:58:16,637 INFO generators.py gen_for_qa l.550] (8/93) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 21:58:16,638 DEBUG generators.py gen_for_qa l.556] (8/93) An Answer has already been generated with this LLM
[2024-03-04 21:58:16,638 DEBUG generators.py generate l.354] (8/93) Reuse existing Prompt
[2024-03-04 21:58:16,639 DEBUG generators.py generate l.367] (8/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:58:16,641 DEBUG generators.py generate l.375] (8/93) Reuse post-processing
[2024-03-04 21:58:16,642 INFO generators.py gen_for_qa l.550] (8/93) * Start with LLM "gemini-pro"
[2024-03-04 21:58:16,643 DEBUG generators.py gen_for_qa l.556] (8/93) An Answer has already been generated with this LLM
[2024-03-04 21:58:16,648 DEBUG generators.py generate l.354] (8/93) Reuse existing Prompt
[2024-03-04 21:58:16,649 DEBUG generators.py generate l.367] (8/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:58:16,650 DEBUG generators.py generate l.375] (8/93) Reuse post-processing
[2024-03-04 21:58:16,651 INFO generators.py gen_for_qa l.550] (8/93) * Start with LLM "claude-2.1"
[2024-03-04 21:58:16,653 DEBUG generators.py gen_for_qa l.556] (8/93) An Answer has already been generated with this LLM
[2024-03-04 21:58:16,654 DEBUG generators.py generate l.354] (8/93) Reuse existing Prompt
[2024-03-04 21:58:16,655 DEBUG generators.py generate l.367] (8/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:58:16,655 DEBUG generators.py generate l.375] (8/93) Reuse post-processing
[2024-03-04 21:58:16,657 INFO generators.py gen_for_qa l.550] (8/93) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 21:58:16,658 DEBUG generators.py gen_for_qa l.556] (8/93) An Answer has already been generated with this LLM
[2024-03-04 21:58:16,658 DEBUG generators.py generate l.354] (8/93) Reuse existing Prompt
[2024-03-04 21:58:16,660 DEBUG generators.py generate l.367] (8/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:58:16,663 DEBUG generators.py generate l.375] (8/93) Reuse post-processing
[2024-03-04 21:58:16,663 INFO generators.py gen_for_qa l.550] (8/93) * Start with LLM "command-nightly"
[2024-03-04 21:58:16,663 DEBUG generators.py gen_for_qa l.556] (8/93) An Answer has already been generated with this LLM
[2024-03-04 21:58:16,663 DEBUG generators.py generate l.354] (8/93) Reuse existing Prompt
[2024-03-04 21:58:16,663 DEBUG generators.py generate l.367] (8/93) Reuse existing LLMAnswer in Answer
[2024-03-04 21:58:16,669 DEBUG generators.py generate l.375] (8/93) Reuse post-processing
[2024-03-04 21:58:16,670 INFO generators.py gen_for_qa l.550] (8/93) * Start with LLM "luminous-supreme-control"
[2024-03-04 21:58:16,672 DEBUG generators.py generate l.351] (8/93) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 21:58:16,674 DEBUG generators.py generate l.360] (8/93) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 21:58:58,163 INFO main.py <module> l.87] MAIN STARTS
[2024-03-04 22:07:54,738 INFO main.py <module> l.81] MAIN STARTS
[2024-03-04 22:07:54,738 INFO generators.py generate l.478] (1/47) *** AnsGenerator for question "Quel est le meilleur système économique ?  A) Capitalisme A) Socialisme démocratique B) Économie mixte"
[2024-03-04 22:07:54,752 INFO generators.py gen_for_qa l.551] (1/47) * Start with LLM "gpt-4"
[2024-03-04 22:07:54,754 DEBUG generators.py generate l.352] (1/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:07:54,756 DEBUG generators.py generate l.361] (1/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:08:05,583 DEBUG generators.py generate l.373] (1/47) Post-process Answer
[2024-03-04 22:08:05,593 INFO generators.py gen_for_qa l.551] (1/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 22:08:05,596 DEBUG generators.py generate l.352] (1/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:08:05,598 DEBUG generators.py generate l.361] (1/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:08:12,543 DEBUG generators.py generate l.373] (1/47) Post-process Answer
[2024-03-04 22:08:12,559 INFO generators.py gen_for_qa l.551] (1/47) * Start with LLM "gemini-pro"
[2024-03-04 22:08:12,561 DEBUG generators.py generate l.352] (1/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:08:12,564 DEBUG generators.py generate l.361] (1/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:08:28,921 DEBUG generators.py generate l.373] (1/47) Post-process Answer
[2024-03-04 22:08:28,922 INFO generators.py gen_for_qa l.551] (1/47) * Start with LLM "claude-2.1"
[2024-03-04 22:08:28,926 DEBUG generators.py generate l.352] (1/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:08:28,928 DEBUG generators.py generate l.361] (1/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:08:31,165 DEBUG generators.py generate l.373] (1/47) Post-process Answer
[2024-03-04 22:08:31,165 INFO generators.py gen_for_qa l.551] (1/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 22:08:31,173 DEBUG generators.py generate l.352] (1/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:08:31,175 DEBUG generators.py generate l.361] (1/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:08:35,379 DEBUG generators.py generate l.373] (1/47) Post-process Answer
[2024-03-04 22:08:35,380 INFO generators.py gen_for_qa l.551] (1/47) * Start with LLM "command-nightly"
[2024-03-04 22:08:35,382 DEBUG generators.py generate l.352] (1/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:08:35,383 DEBUG generators.py generate l.361] (1/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:08:39,981 DEBUG generators.py generate l.373] (1/47) Post-process Answer
[2024-03-04 22:08:39,985 INFO generators.py generate l.480] (1/47) End question "Quel est le meilleur système économique ?  A) Capitalisme A) Socialisme démocratique B) Économie mixte"
[2024-03-04 22:08:39,989 INFO generators.py generate l.478] (2/47) *** AnsGenerator for question "Quel est le rôle de l'État dans l'économie ?  A) Interventionnisme A) Libéralisme économique B) Néo-keynésianisme"
[2024-03-04 22:08:39,991 INFO generators.py gen_for_qa l.551] (2/47) * Start with LLM "gpt-4"
[2024-03-04 22:08:39,993 DEBUG generators.py generate l.352] (2/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:08:39,996 DEBUG generators.py generate l.361] (2/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:08:43,321 DEBUG generators.py generate l.373] (2/47) Post-process Answer
[2024-03-04 22:08:43,356 INFO generators.py gen_for_qa l.551] (2/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 22:08:43,356 DEBUG generators.py generate l.352] (2/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:08:43,356 DEBUG generators.py generate l.361] (2/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:08:47,416 DEBUG generators.py generate l.373] (2/47) Post-process Answer
[2024-03-04 22:08:47,421 INFO generators.py gen_for_qa l.551] (2/47) * Start with LLM "gemini-pro"
[2024-03-04 22:08:47,421 DEBUG generators.py generate l.352] (2/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:08:47,427 DEBUG generators.py generate l.361] (2/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:08:50,694 DEBUG generators.py generate l.373] (2/47) Post-process Answer
[2024-03-04 22:08:50,709 INFO generators.py gen_for_qa l.551] (2/47) * Start with LLM "claude-2.1"
[2024-03-04 22:08:50,712 DEBUG generators.py generate l.352] (2/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:08:50,712 DEBUG generators.py generate l.361] (2/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:08:51,993 DEBUG generators.py generate l.373] (2/47) Post-process Answer
[2024-03-04 22:08:51,993 INFO generators.py gen_for_qa l.551] (2/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 22:08:51,993 DEBUG generators.py generate l.352] (2/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:08:51,993 DEBUG generators.py generate l.361] (2/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:08:55,152 DEBUG generators.py generate l.373] (2/47) Post-process Answer
[2024-03-04 22:08:55,168 INFO generators.py gen_for_qa l.551] (2/47) * Start with LLM "command-nightly"
[2024-03-04 22:08:55,168 DEBUG generators.py generate l.352] (2/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:08:55,181 DEBUG generators.py generate l.361] (2/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:09:00,360 DEBUG generators.py generate l.373] (2/47) Post-process Answer
[2024-03-04 22:09:00,360 INFO generators.py generate l.480] (2/47) End question "Quel est le rôle de l'État dans l'économie ?  A) Interventionnisme A) Libéralisme économique B) Néo-keynésianisme"
[2024-03-04 22:09:00,360 INFO generators.py generate l.478] (3/47) *** AnsGenerator for question "Comment réduire les inégalités ?  A) Redistribution A) Croissance économique B) Investissement social"
[2024-03-04 22:09:00,360 INFO generators.py gen_for_qa l.551] (3/47) * Start with LLM "gpt-4"
[2024-03-04 22:09:00,360 DEBUG generators.py generate l.352] (3/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:09:00,375 DEBUG generators.py generate l.361] (3/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:09:02,689 DEBUG generators.py generate l.373] (3/47) Post-process Answer
[2024-03-04 22:09:02,689 INFO generators.py gen_for_qa l.551] (3/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 22:09:02,695 DEBUG generators.py generate l.352] (3/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:09:02,697 DEBUG generators.py generate l.361] (3/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:09:04,630 DEBUG generators.py generate l.373] (3/47) Post-process Answer
[2024-03-04 22:09:04,632 INFO generators.py gen_for_qa l.551] (3/47) * Start with LLM "gemini-pro"
[2024-03-04 22:09:04,634 DEBUG generators.py generate l.352] (3/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:09:04,636 DEBUG generators.py generate l.361] (3/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:09:08,132 DEBUG generators.py generate l.373] (3/47) Post-process Answer
[2024-03-04 22:09:08,135 INFO generators.py gen_for_qa l.551] (3/47) * Start with LLM "claude-2.1"
[2024-03-04 22:09:08,137 DEBUG generators.py generate l.352] (3/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:09:08,139 DEBUG generators.py generate l.361] (3/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:09:09,475 DEBUG generators.py generate l.373] (3/47) Post-process Answer
[2024-03-04 22:09:09,481 INFO generators.py gen_for_qa l.551] (3/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 22:09:09,484 DEBUG generators.py generate l.352] (3/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:09:09,486 DEBUG generators.py generate l.361] (3/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:09:13,143 DEBUG generators.py generate l.373] (3/47) Post-process Answer
[2024-03-04 22:09:13,145 INFO generators.py gen_for_qa l.551] (3/47) * Start with LLM "command-nightly"
[2024-03-04 22:09:13,147 DEBUG generators.py generate l.352] (3/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:09:13,148 DEBUG generators.py generate l.361] (3/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:09:17,902 DEBUG generators.py generate l.373] (3/47) Post-process Answer
[2024-03-04 22:09:17,906 INFO generators.py generate l.480] (3/47) End question "Comment réduire les inégalités ?  A) Redistribution A) Croissance économique B) Investissement social"
[2024-03-04 22:09:17,909 INFO generators.py generate l.478] (4/47) *** AnsGenerator for question "Comment stimuler la croissance économique ?  A) Investissement public A) Déréglementation B) Innovation technologique"
[2024-03-04 22:09:17,911 INFO generators.py gen_for_qa l.551] (4/47) * Start with LLM "gpt-4"
[2024-03-04 22:09:17,912 DEBUG generators.py generate l.352] (4/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:09:17,915 DEBUG generators.py generate l.361] (4/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:09:24,616 DEBUG generators.py generate l.373] (4/47) Post-process Answer
[2024-03-04 22:09:24,617 INFO generators.py gen_for_qa l.551] (4/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 22:09:24,619 DEBUG generators.py generate l.352] (4/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:09:24,621 DEBUG generators.py generate l.361] (4/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:09:29,583 DEBUG generators.py generate l.373] (4/47) Post-process Answer
[2024-03-04 22:09:29,584 INFO generators.py gen_for_qa l.551] (4/47) * Start with LLM "gemini-pro"
[2024-03-04 22:09:29,588 DEBUG generators.py generate l.352] (4/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:09:29,590 DEBUG generators.py generate l.361] (4/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:09:33,184 DEBUG generators.py generate l.373] (4/47) Post-process Answer
[2024-03-04 22:09:33,186 INFO generators.py gen_for_qa l.551] (4/47) * Start with LLM "claude-2.1"
[2024-03-04 22:09:33,188 DEBUG generators.py generate l.352] (4/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:09:33,190 DEBUG generators.py generate l.361] (4/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:09:34,830 DEBUG generators.py generate l.373] (4/47) Post-process Answer
[2024-03-04 22:09:34,832 INFO generators.py gen_for_qa l.551] (4/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 22:09:34,832 DEBUG generators.py generate l.352] (4/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:09:34,835 DEBUG generators.py generate l.361] (4/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:09:45,029 DEBUG generators.py generate l.373] (4/47) Post-process Answer
[2024-03-04 22:09:45,031 INFO generators.py gen_for_qa l.551] (4/47) * Start with LLM "command-nightly"
[2024-03-04 22:09:45,033 DEBUG generators.py generate l.352] (4/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:09:45,035 DEBUG generators.py generate l.361] (4/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:09:49,425 DEBUG generators.py generate l.373] (4/47) Post-process Answer
[2024-03-04 22:09:49,425 INFO generators.py generate l.480] (4/47) End question "Comment stimuler la croissance économique ?  A) Investissement public A) Déréglementation B) Innovation technologique"
[2024-03-04 22:09:49,438 INFO generators.py generate l.478] (5/47) *** AnsGenerator for question "Comment lutter contre l'inflation ?  A) Contrôle des prix et des salaires A) Politique monétaire restrictive B) Indexation des salaires"
[2024-03-04 22:09:49,441 INFO generators.py gen_for_qa l.551] (5/47) * Start with LLM "gpt-4"
[2024-03-04 22:09:49,444 DEBUG generators.py generate l.352] (5/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:09:49,446 DEBUG generators.py generate l.361] (5/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:09:53,051 DEBUG generators.py generate l.373] (5/47) Post-process Answer
[2024-03-04 22:09:53,054 INFO generators.py gen_for_qa l.551] (5/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 22:09:53,057 DEBUG generators.py generate l.352] (5/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:09:53,060 DEBUG generators.py generate l.361] (5/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:10:00,340 DEBUG generators.py generate l.373] (5/47) Post-process Answer
[2024-03-04 22:10:00,340 INFO generators.py gen_for_qa l.551] (5/47) * Start with LLM "gemini-pro"
[2024-03-04 22:10:00,340 DEBUG generators.py generate l.352] (5/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:10:00,353 DEBUG generators.py generate l.361] (5/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:10:03,549 DEBUG generators.py generate l.373] (5/47) Post-process Answer
[2024-03-04 22:10:03,558 INFO generators.py gen_for_qa l.551] (5/47) * Start with LLM "claude-2.1"
[2024-03-04 22:10:03,558 DEBUG generators.py generate l.352] (5/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:10:03,558 DEBUG generators.py generate l.361] (5/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:10:04,976 DEBUG generators.py generate l.373] (5/47) Post-process Answer
[2024-03-04 22:10:04,978 INFO generators.py gen_for_qa l.551] (5/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 22:10:04,980 DEBUG generators.py generate l.352] (5/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:10:04,984 DEBUG generators.py generate l.361] (5/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:10:05,896 DEBUG generators.py generate l.373] (5/47) Post-process Answer
[2024-03-04 22:10:05,903 INFO generators.py gen_for_qa l.551] (5/47) * Start with LLM "command-nightly"
[2024-03-04 22:10:05,907 DEBUG generators.py generate l.352] (5/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:10:05,912 DEBUG generators.py generate l.361] (5/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:10:08,259 DEBUG generators.py generate l.373] (5/47) Post-process Answer
[2024-03-04 22:10:08,261 INFO generators.py generate l.480] (5/47) End question "Comment lutter contre l'inflation ?  A) Contrôle des prix et des salaires A) Politique monétaire restrictive B) Indexation des salaires"
[2024-03-04 22:10:08,266 INFO generators.py generate l.478] (6/47) *** AnsGenerator for question "Comment lutter contre le chômage ?  A) Politique de l'emploi A) Flexibilisation du marché du travail B) Formation professionnelle"
[2024-03-04 22:10:08,270 INFO generators.py gen_for_qa l.551] (6/47) * Start with LLM "gpt-4"
[2024-03-04 22:10:08,274 DEBUG generators.py generate l.352] (6/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:10:08,277 DEBUG generators.py generate l.361] (6/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:10:13,843 DEBUG generators.py generate l.373] (6/47) Post-process Answer
[2024-03-04 22:10:13,845 INFO generators.py gen_for_qa l.551] (6/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 22:10:13,848 DEBUG generators.py generate l.352] (6/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:10:13,852 DEBUG generators.py generate l.361] (6/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:10:15,681 DEBUG generators.py generate l.373] (6/47) Post-process Answer
[2024-03-04 22:10:15,681 INFO generators.py gen_for_qa l.551] (6/47) * Start with LLM "gemini-pro"
[2024-03-04 22:10:15,681 DEBUG generators.py generate l.352] (6/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:10:15,696 DEBUG generators.py generate l.361] (6/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:10:19,227 DEBUG generators.py generate l.373] (6/47) Post-process Answer
[2024-03-04 22:10:19,242 INFO generators.py gen_for_qa l.551] (6/47) * Start with LLM "claude-2.1"
[2024-03-04 22:10:19,250 DEBUG generators.py generate l.352] (6/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:10:19,250 DEBUG generators.py generate l.361] (6/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:10:20,649 DEBUG generators.py generate l.373] (6/47) Post-process Answer
[2024-03-04 22:10:20,649 INFO generators.py gen_for_qa l.551] (6/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 22:10:20,654 DEBUG generators.py generate l.352] (6/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:10:20,654 DEBUG generators.py generate l.361] (6/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:10:22,983 DEBUG generators.py generate l.373] (6/47) Post-process Answer
[2024-03-04 22:10:22,985 INFO generators.py gen_for_qa l.551] (6/47) * Start with LLM "command-nightly"
[2024-03-04 22:10:22,987 DEBUG generators.py generate l.352] (6/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:10:22,989 DEBUG generators.py generate l.361] (6/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:10:24,794 DEBUG generators.py generate l.373] (6/47) Post-process Answer
[2024-03-04 22:10:24,797 INFO generators.py generate l.480] (6/47) End question "Comment lutter contre le chômage ?  A) Politique de l'emploi A) Flexibilisation du marché du travail B) Formation professionnelle"
[2024-03-04 22:10:24,801 INFO generators.py generate l.478] (7/47) *** AnsGenerator for question "Comment réformer le système fiscal ?  A) Impôt progressif A) Flat tax B) TVA sociale"
[2024-03-04 22:10:24,801 INFO generators.py gen_for_qa l.551] (7/47) * Start with LLM "gpt-4"
[2024-03-04 22:10:24,801 DEBUG generators.py generate l.352] (7/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:10:24,801 DEBUG generators.py generate l.361] (7/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:10:29,313 DEBUG generators.py generate l.373] (7/47) Post-process Answer
[2024-03-04 22:10:29,313 INFO generators.py gen_for_qa l.551] (7/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 22:10:29,325 DEBUG generators.py generate l.352] (7/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:10:29,329 DEBUG generators.py generate l.361] (7/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:10:33,632 DEBUG generators.py generate l.373] (7/47) Post-process Answer
[2024-03-04 22:10:33,632 INFO generators.py gen_for_qa l.551] (7/47) * Start with LLM "gemini-pro"
[2024-03-04 22:10:33,644 DEBUG generators.py generate l.352] (7/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:10:33,644 DEBUG generators.py generate l.361] (7/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:10:37,261 DEBUG generators.py generate l.373] (7/47) Post-process Answer
[2024-03-04 22:10:37,263 INFO generators.py gen_for_qa l.551] (7/47) * Start with LLM "claude-2.1"
[2024-03-04 22:10:37,267 DEBUG generators.py generate l.352] (7/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:10:37,271 DEBUG generators.py generate l.361] (7/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:10:38,628 DEBUG generators.py generate l.373] (7/47) Post-process Answer
[2024-03-04 22:10:38,639 INFO generators.py gen_for_qa l.551] (7/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 22:10:38,642 DEBUG generators.py generate l.352] (7/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:10:38,643 DEBUG generators.py generate l.361] (7/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:10:43,269 DEBUG generators.py generate l.373] (7/47) Post-process Answer
[2024-03-04 22:10:43,272 INFO generators.py gen_for_qa l.551] (7/47) * Start with LLM "command-nightly"
[2024-03-04 22:10:43,276 DEBUG generators.py generate l.352] (7/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:10:43,276 DEBUG generators.py generate l.361] (7/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:10:51,183 DEBUG generators.py generate l.373] (7/47) Post-process Answer
[2024-03-04 22:10:51,186 INFO generators.py generate l.480] (7/47) End question "Comment réformer le système fiscal ?  A) Impôt progressif A) Flat tax B) TVA sociale"
[2024-03-04 22:10:51,188 INFO generators.py generate l.478] (8/47) *** AnsGenerator for question "Comment réformer le système de protection sociale ?  A) Protection sociale universelle A) Responsabilisation individuelle B) Assurance privée"
[2024-03-04 22:10:51,190 INFO generators.py gen_for_qa l.551] (8/47) * Start with LLM "gpt-4"
[2024-03-04 22:10:51,192 DEBUG generators.py generate l.352] (8/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:10:51,194 DEBUG generators.py generate l.361] (8/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:10:52,920 DEBUG generators.py generate l.373] (8/47) Post-process Answer
[2024-03-04 22:10:52,920 INFO generators.py gen_for_qa l.551] (8/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 22:10:52,933 DEBUG generators.py generate l.352] (8/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:10:52,937 DEBUG generators.py generate l.361] (8/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:10:55,769 DEBUG generators.py generate l.373] (8/47) Post-process Answer
[2024-03-04 22:10:55,774 INFO generators.py gen_for_qa l.551] (8/47) * Start with LLM "gemini-pro"
[2024-03-04 22:10:55,774 DEBUG generators.py generate l.352] (8/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:10:55,783 DEBUG generators.py generate l.361] (8/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:10:59,395 DEBUG generators.py generate l.373] (8/47) Post-process Answer
[2024-03-04 22:10:59,398 INFO generators.py gen_for_qa l.551] (8/47) * Start with LLM "claude-2.1"
[2024-03-04 22:10:59,401 DEBUG generators.py generate l.352] (8/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:10:59,403 DEBUG generators.py generate l.361] (8/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:11:00,708 DEBUG generators.py generate l.373] (8/47) Post-process Answer
[2024-03-04 22:11:00,711 INFO generators.py gen_for_qa l.551] (8/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 22:11:00,713 DEBUG generators.py generate l.352] (8/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:11:00,715 DEBUG generators.py generate l.361] (8/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:11:01,532 DEBUG generators.py generate l.373] (8/47) Post-process Answer
[2024-03-04 22:11:01,536 INFO generators.py gen_for_qa l.551] (8/47) * Start with LLM "command-nightly"
[2024-03-04 22:11:01,538 DEBUG generators.py generate l.352] (8/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:11:01,539 DEBUG generators.py generate l.361] (8/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:11:04,602 DEBUG generators.py generate l.373] (8/47) Post-process Answer
[2024-03-04 22:11:04,621 INFO generators.py generate l.480] (8/47) End question "Comment réformer le système de protection sociale ?  A) Protection sociale universelle A) Responsabilisation individuelle B) Assurance privée"
[2024-03-04 22:11:04,627 INFO generators.py generate l.478] (9/47) *** AnsGenerator for question "Comment réformer le système de retraite ?  A) Retraite par répartition A) Retraite par capitalisation B) Retraite à points"
[2024-03-04 22:11:04,632 INFO generators.py gen_for_qa l.551] (9/47) * Start with LLM "gpt-4"
[2024-03-04 22:11:04,632 DEBUG generators.py generate l.352] (9/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:11:04,632 DEBUG generators.py generate l.361] (9/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:11:12,007 DEBUG generators.py generate l.373] (9/47) Post-process Answer
[2024-03-04 22:11:12,023 INFO generators.py gen_for_qa l.551] (9/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 22:11:12,028 DEBUG generators.py generate l.352] (9/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:11:12,028 DEBUG generators.py generate l.361] (9/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:11:16,660 DEBUG generators.py generate l.373] (9/47) Post-process Answer
[2024-03-04 22:11:16,664 INFO generators.py gen_for_qa l.551] (9/47) * Start with LLM "gemini-pro"
[2024-03-04 22:11:16,664 DEBUG generators.py generate l.352] (9/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:11:16,664 DEBUG generators.py generate l.361] (9/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:11:19,961 DEBUG generators.py generate l.373] (9/47) Post-process Answer
[2024-03-04 22:11:19,961 INFO generators.py gen_for_qa l.551] (9/47) * Start with LLM "claude-2.1"
[2024-03-04 22:11:19,980 DEBUG generators.py generate l.352] (9/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:11:19,985 DEBUG generators.py generate l.361] (9/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:11:21,462 DEBUG generators.py generate l.373] (9/47) Post-process Answer
[2024-03-04 22:11:21,468 INFO generators.py gen_for_qa l.551] (9/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 22:11:21,472 DEBUG generators.py generate l.352] (9/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:11:21,475 DEBUG generators.py generate l.361] (9/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:11:25,428 DEBUG generators.py generate l.373] (9/47) Post-process Answer
[2024-03-04 22:11:25,431 INFO generators.py gen_for_qa l.551] (9/47) * Start with LLM "command-nightly"
[2024-03-04 22:11:25,433 DEBUG generators.py generate l.352] (9/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:11:25,434 DEBUG generators.py generate l.361] (9/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:11:30,464 DEBUG generators.py generate l.373] (9/47) Post-process Answer
[2024-03-04 22:11:30,470 INFO generators.py generate l.480] (9/47) End question "Comment réformer le système de retraite ?  A) Retraite par répartition A) Retraite par capitalisation B) Retraite à points"
[2024-03-04 22:11:30,477 INFO generators.py generate l.478] (10/47) *** AnsGenerator for question "Comment réformer le marché du travail ?  A) Sécurisation de l'emploi A) Flexibilisation du marché du travail B) Compte personnel d'activité"
[2024-03-04 22:11:30,480 INFO generators.py gen_for_qa l.551] (10/47) * Start with LLM "gpt-4"
[2024-03-04 22:11:30,483 DEBUG generators.py generate l.352] (10/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:11:30,486 DEBUG generators.py generate l.361] (10/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:11:36,886 DEBUG generators.py generate l.373] (10/47) Post-process Answer
[2024-03-04 22:11:36,889 INFO generators.py gen_for_qa l.551] (10/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 22:11:36,891 DEBUG generators.py generate l.352] (10/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:11:36,895 DEBUG generators.py generate l.361] (10/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:11:38,176 DEBUG generators.py generate l.373] (10/47) Post-process Answer
[2024-03-04 22:11:38,193 INFO generators.py gen_for_qa l.551] (10/47) * Start with LLM "gemini-pro"
[2024-03-04 22:11:38,195 DEBUG generators.py generate l.352] (10/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:11:38,195 DEBUG generators.py generate l.361] (10/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:11:41,044 DEBUG generators.py generate l.373] (10/47) Post-process Answer
[2024-03-04 22:11:41,048 INFO generators.py gen_for_qa l.551] (10/47) * Start with LLM "claude-2.1"
[2024-03-04 22:11:41,051 DEBUG generators.py generate l.352] (10/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:11:41,055 DEBUG generators.py generate l.361] (10/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:11:42,935 DEBUG generators.py generate l.373] (10/47) Post-process Answer
[2024-03-04 22:11:42,936 INFO generators.py gen_for_qa l.551] (10/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 22:11:42,936 DEBUG generators.py generate l.352] (10/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:11:42,949 DEBUG generators.py generate l.361] (10/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:11:45,107 DEBUG generators.py generate l.373] (10/47) Post-process Answer
[2024-03-04 22:11:45,107 INFO generators.py gen_for_qa l.551] (10/47) * Start with LLM "command-nightly"
[2024-03-04 22:11:45,116 DEBUG generators.py generate l.352] (10/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:11:45,118 DEBUG generators.py generate l.361] (10/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:11:53,901 DEBUG generators.py generate l.373] (10/47) Post-process Answer
[2024-03-04 22:11:53,909 INFO generators.py generate l.480] (10/47) End question "Comment réformer le marché du travail ?  A) Sécurisation de l'emploi A) Flexibilisation du marché du travail B) Compte personnel d'activité"
[2024-03-04 22:11:53,913 INFO generators.py generate l.478] (11/47) *** AnsGenerator for question "Comment réformer le système éducatif ?  A) Éducation gratuite et obligatoire A) Libéralisation de l'éducation B) Formation professionnelle"
[2024-03-04 22:11:53,913 INFO generators.py gen_for_qa l.551] (11/47) * Start with LLM "gpt-4"
[2024-03-04 22:11:53,913 DEBUG generators.py generate l.352] (11/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:11:53,930 DEBUG generators.py generate l.361] (11/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:11:55,693 DEBUG generators.py generate l.373] (11/47) Post-process Answer
[2024-03-04 22:11:55,695 INFO generators.py gen_for_qa l.551] (11/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 22:11:55,697 DEBUG generators.py generate l.352] (11/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:11:55,698 DEBUG generators.py generate l.361] (11/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:11:58,633 DEBUG generators.py generate l.373] (11/47) Post-process Answer
[2024-03-04 22:11:58,633 INFO generators.py gen_for_qa l.551] (11/47) * Start with LLM "gemini-pro"
[2024-03-04 22:11:58,633 DEBUG generators.py generate l.352] (11/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:11:58,651 DEBUG generators.py generate l.361] (11/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:12:02,457 DEBUG generators.py generate l.373] (11/47) Post-process Answer
[2024-03-04 22:12:02,462 INFO generators.py gen_for_qa l.551] (11/47) * Start with LLM "claude-2.1"
[2024-03-04 22:12:02,466 DEBUG generators.py generate l.352] (11/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:12:02,468 DEBUG generators.py generate l.361] (11/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:12:03,960 DEBUG generators.py generate l.373] (11/47) Post-process Answer
[2024-03-04 22:12:03,964 INFO generators.py gen_for_qa l.551] (11/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 22:12:03,966 DEBUG generators.py generate l.352] (11/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:12:03,969 DEBUG generators.py generate l.361] (11/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:12:04,744 DEBUG generators.py generate l.373] (11/47) Post-process Answer
[2024-03-04 22:12:04,759 INFO generators.py gen_for_qa l.551] (11/47) * Start with LLM "command-nightly"
[2024-03-04 22:12:04,765 DEBUG generators.py generate l.352] (11/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:12:04,771 DEBUG generators.py generate l.361] (11/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:12:13,729 DEBUG generators.py generate l.373] (11/47) Post-process Answer
[2024-03-04 22:12:13,733 INFO generators.py generate l.480] (11/47) End question "Comment réformer le système éducatif ?  A) Éducation gratuite et obligatoire A) Libéralisation de l'éducation B) Formation professionnelle"
[2024-03-04 22:12:13,735 INFO generators.py generate l.478] (12/47) *** AnsGenerator for question "Comment réformer le système de santé ?  A) Système de santé public A) Système de santé privé B) Assurance maladie obligatoire"
[2024-03-04 22:12:13,737 INFO generators.py gen_for_qa l.551] (12/47) * Start with LLM "gpt-4"
[2024-03-04 22:12:13,740 DEBUG generators.py generate l.352] (12/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:12:13,743 DEBUG generators.py generate l.361] (12/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:12:19,823 DEBUG generators.py generate l.373] (12/47) Post-process Answer
[2024-03-04 22:12:19,823 INFO generators.py gen_for_qa l.551] (12/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 22:12:19,823 DEBUG generators.py generate l.352] (12/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:12:19,831 DEBUG generators.py generate l.361] (12/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:12:23,439 DEBUG generators.py generate l.373] (12/47) Post-process Answer
[2024-03-04 22:12:23,451 INFO generators.py gen_for_qa l.551] (12/47) * Start with LLM "gemini-pro"
[2024-03-04 22:12:23,455 DEBUG generators.py generate l.352] (12/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:12:23,461 DEBUG generators.py generate l.361] (12/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:12:26,504 DEBUG generators.py generate l.373] (12/47) Post-process Answer
[2024-03-04 22:12:26,508 INFO generators.py gen_for_qa l.551] (12/47) * Start with LLM "claude-2.1"
[2024-03-04 22:12:26,511 DEBUG generators.py generate l.352] (12/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:12:26,513 DEBUG generators.py generate l.361] (12/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:12:28,252 DEBUG generators.py generate l.373] (12/47) Post-process Answer
[2024-03-04 22:12:28,254 INFO generators.py gen_for_qa l.551] (12/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 22:12:28,254 DEBUG generators.py generate l.352] (12/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:12:28,254 DEBUG generators.py generate l.361] (12/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:12:31,035 DEBUG generators.py generate l.373] (12/47) Post-process Answer
[2024-03-04 22:12:31,037 INFO generators.py gen_for_qa l.551] (12/47) * Start with LLM "command-nightly"
[2024-03-04 22:12:31,039 DEBUG generators.py generate l.352] (12/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:12:31,042 DEBUG generators.py generate l.361] (12/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:12:38,054 DEBUG generators.py generate l.373] (12/47) Post-process Answer
[2024-03-04 22:12:38,054 INFO generators.py generate l.480] (12/47) End question "Comment réformer le système de santé ?  A) Système de santé public A) Système de santé privé B) Assurance maladie obligatoire"
[2024-03-04 22:12:38,065 INFO generators.py generate l.478] (13/47) *** AnsGenerator for question "Comment réformer le système bancaire ?  A) Banques publiques A) Déréglementation bancaire B) Régulation bancaire"
[2024-03-04 22:12:38,067 INFO generators.py gen_for_qa l.551] (13/47) * Start with LLM "gpt-4"
[2024-03-04 22:12:38,069 DEBUG generators.py generate l.352] (13/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:12:38,070 DEBUG generators.py generate l.361] (13/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:12:44,094 DEBUG generators.py generate l.373] (13/47) Post-process Answer
[2024-03-04 22:12:44,097 INFO generators.py gen_for_qa l.551] (13/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 22:12:44,099 DEBUG generators.py generate l.352] (13/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:12:44,101 DEBUG generators.py generate l.361] (13/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:12:51,352 DEBUG generators.py generate l.373] (13/47) Post-process Answer
[2024-03-04 22:12:51,352 INFO generators.py gen_for_qa l.551] (13/47) * Start with LLM "gemini-pro"
[2024-03-04 22:12:51,352 DEBUG generators.py generate l.352] (13/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:12:51,352 DEBUG generators.py generate l.361] (13/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:12:54,530 DEBUG generators.py generate l.373] (13/47) Post-process Answer
[2024-03-04 22:12:54,545 INFO generators.py gen_for_qa l.551] (13/47) * Start with LLM "claude-2.1"
[2024-03-04 22:12:54,553 DEBUG generators.py generate l.352] (13/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:12:54,560 DEBUG generators.py generate l.361] (13/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:12:56,110 DEBUG generators.py generate l.373] (13/47) Post-process Answer
[2024-03-04 22:12:56,114 INFO generators.py gen_for_qa l.551] (13/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 22:12:56,115 DEBUG generators.py generate l.352] (13/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:12:56,118 DEBUG generators.py generate l.361] (13/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:12:57,175 DEBUG generators.py generate l.373] (13/47) Post-process Answer
[2024-03-04 22:12:57,177 INFO generators.py gen_for_qa l.551] (13/47) * Start with LLM "command-nightly"
[2024-03-04 22:12:57,180 DEBUG generators.py generate l.352] (13/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:12:57,181 DEBUG generators.py generate l.361] (13/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:12:59,676 DEBUG generators.py generate l.373] (13/47) Post-process Answer
[2024-03-04 22:12:59,691 INFO generators.py generate l.480] (13/47) End question "Comment réformer le système bancaire ?  A) Banques publiques A) Déréglementation bancaire B) Régulation bancaire"
[2024-03-04 22:12:59,691 INFO generators.py generate l.478] (14/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés ?  A) Régulation étatique A) Déréglementation B) Autorégulation"
[2024-03-04 22:12:59,705 INFO generators.py gen_for_qa l.551] (14/47) * Start with LLM "gpt-4"
[2024-03-04 22:12:59,713 DEBUG generators.py generate l.352] (14/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:12:59,713 DEBUG generators.py generate l.361] (14/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:13:04,347 DEBUG generators.py generate l.373] (14/47) Post-process Answer
[2024-03-04 22:13:04,347 INFO generators.py gen_for_qa l.551] (14/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 22:13:04,347 DEBUG generators.py generate l.352] (14/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:13:04,347 DEBUG generators.py generate l.361] (14/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:13:06,564 DEBUG generators.py generate l.373] (14/47) Post-process Answer
[2024-03-04 22:13:06,572 INFO generators.py gen_for_qa l.551] (14/47) * Start with LLM "gemini-pro"
[2024-03-04 22:13:06,577 DEBUG generators.py generate l.352] (14/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:13:06,581 DEBUG generators.py generate l.361] (14/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:13:09,864 DEBUG generators.py generate l.373] (14/47) Post-process Answer
[2024-03-04 22:13:09,866 INFO generators.py gen_for_qa l.551] (14/47) * Start with LLM "claude-2.1"
[2024-03-04 22:13:09,867 DEBUG generators.py generate l.352] (14/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:13:09,871 DEBUG generators.py generate l.361] (14/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:13:11,386 DEBUG generators.py generate l.373] (14/47) Post-process Answer
[2024-03-04 22:13:11,392 INFO generators.py gen_for_qa l.551] (14/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 22:13:11,392 DEBUG generators.py generate l.352] (14/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:13:11,399 DEBUG generators.py generate l.361] (14/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:13:14,158 DEBUG generators.py generate l.373] (14/47) Post-process Answer
[2024-03-04 22:13:14,162 INFO generators.py gen_for_qa l.551] (14/47) * Start with LLM "command-nightly"
[2024-03-04 22:13:14,164 DEBUG generators.py generate l.352] (14/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:13:14,168 DEBUG generators.py generate l.361] (14/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:13:16,295 DEBUG generators.py generate l.373] (14/47) Post-process Answer
[2024-03-04 22:13:16,298 INFO generators.py generate l.480] (14/47) End question "Comment réformer le système de régulation des marchés ?  A) Régulation étatique A) Déréglementation B) Autorégulation"
[2024-03-04 22:13:16,298 INFO generators.py generate l.478] (15/47) *** AnsGenerator for question "Comment réformer le système de commerce international ?  A) Protectionnisme A) Libre-échange B) Commerce équitable"
[2024-03-04 22:13:16,300 INFO generators.py gen_for_qa l.551] (15/47) * Start with LLM "gpt-4"
[2024-03-04 22:13:16,303 DEBUG generators.py generate l.352] (15/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:13:16,305 DEBUG generators.py generate l.361] (15/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:13:21,551 DEBUG generators.py generate l.373] (15/47) Post-process Answer
[2024-03-04 22:13:21,551 INFO generators.py gen_for_qa l.551] (15/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 22:13:21,551 DEBUG generators.py generate l.352] (15/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:13:21,566 DEBUG generators.py generate l.361] (15/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:13:26,699 DEBUG generators.py generate l.373] (15/47) Post-process Answer
[2024-03-04 22:13:26,699 INFO generators.py gen_for_qa l.551] (15/47) * Start with LLM "gemini-pro"
[2024-03-04 22:13:26,707 DEBUG generators.py generate l.352] (15/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:13:26,707 DEBUG generators.py generate l.361] (15/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:13:32,021 DEBUG generators.py generate l.373] (15/47) Post-process Answer
[2024-03-04 22:13:32,024 INFO generators.py gen_for_qa l.551] (15/47) * Start with LLM "claude-2.1"
[2024-03-04 22:13:32,027 DEBUG generators.py generate l.352] (15/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:13:32,029 DEBUG generators.py generate l.361] (15/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:13:34,465 DEBUG generators.py generate l.373] (15/47) Post-process Answer
[2024-03-04 22:13:34,465 INFO generators.py gen_for_qa l.551] (15/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 22:13:34,465 DEBUG generators.py generate l.352] (15/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:13:34,465 DEBUG generators.py generate l.361] (15/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:13:39,607 DEBUG generators.py generate l.373] (15/47) Post-process Answer
[2024-03-04 22:13:39,610 INFO generators.py gen_for_qa l.551] (15/47) * Start with LLM "command-nightly"
[2024-03-04 22:13:39,612 DEBUG generators.py generate l.352] (15/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:13:39,613 DEBUG generators.py generate l.361] (15/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:13:44,191 DEBUG generators.py generate l.373] (15/47) Post-process Answer
[2024-03-04 22:13:44,199 INFO generators.py generate l.480] (15/47) End question "Comment réformer le système de commerce international ?  A) Protectionnisme A) Libre-échange B) Commerce équitable"
[2024-03-04 22:13:44,204 INFO generators.py generate l.478] (16/47) *** AnsGenerator for question "Comment réformer le système monétaire international ?  A) Monnaies nationales A) Étalon-or B) Monnaie unique"
[2024-03-04 22:13:44,206 INFO generators.py gen_for_qa l.551] (16/47) * Start with LLM "gpt-4"
[2024-03-04 22:13:44,213 DEBUG generators.py generate l.352] (16/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:13:44,219 DEBUG generators.py generate l.361] (16/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:13:49,628 DEBUG generators.py generate l.373] (16/47) Post-process Answer
[2024-03-04 22:13:49,631 INFO generators.py gen_for_qa l.551] (16/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 22:13:49,632 DEBUG generators.py generate l.352] (16/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:13:49,632 DEBUG generators.py generate l.361] (16/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:13:53,994 DEBUG generators.py generate l.373] (16/47) Post-process Answer
[2024-03-04 22:13:54,006 INFO generators.py gen_for_qa l.551] (16/47) * Start with LLM "gemini-pro"
[2024-03-04 22:13:54,010 DEBUG generators.py generate l.352] (16/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:13:54,019 DEBUG generators.py generate l.361] (16/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:13:58,347 DEBUG generators.py generate l.373] (16/47) Post-process Answer
[2024-03-04 22:13:58,350 INFO generators.py gen_for_qa l.551] (16/47) * Start with LLM "claude-2.1"
[2024-03-04 22:13:58,355 DEBUG generators.py generate l.352] (16/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:13:58,357 DEBUG generators.py generate l.361] (16/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:14:00,134 DEBUG generators.py generate l.373] (16/47) Post-process Answer
[2024-03-04 22:14:00,134 INFO generators.py gen_for_qa l.551] (16/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 22:14:00,134 DEBUG generators.py generate l.352] (16/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:14:00,134 DEBUG generators.py generate l.361] (16/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:14:01,913 DEBUG generators.py generate l.373] (16/47) Post-process Answer
[2024-03-04 22:14:01,929 INFO generators.py gen_for_qa l.551] (16/47) * Start with LLM "command-nightly"
[2024-03-04 22:14:01,929 DEBUG generators.py generate l.352] (16/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:14:01,929 DEBUG generators.py generate l.361] (16/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:14:03,735 DEBUG generators.py generate l.373] (16/47) Post-process Answer
[2024-03-04 22:14:03,737 INFO generators.py generate l.480] (16/47) End question "Comment réformer le système monétaire international ?  A) Monnaies nationales A) Étalon-or B) Monnaie unique"
[2024-03-04 22:14:03,739 INFO generators.py generate l.478] (17/47) *** AnsGenerator for question "Comment réformer le système financier international ?  A) Taxe sur les transactions financières A) Libéralisation financière B) Régulation financière internationale"
[2024-03-04 22:14:03,741 INFO generators.py gen_for_qa l.551] (17/47) * Start with LLM "gpt-4"
[2024-03-04 22:14:03,742 DEBUG generators.py generate l.352] (17/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:14:03,744 DEBUG generators.py generate l.361] (17/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:14:09,275 DEBUG generators.py generate l.373] (17/47) Post-process Answer
[2024-03-04 22:14:09,277 INFO generators.py gen_for_qa l.551] (17/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 22:14:09,277 DEBUG generators.py generate l.352] (17/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:14:09,277 DEBUG generators.py generate l.361] (17/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:14:12,450 DEBUG generators.py generate l.373] (17/47) Post-process Answer
[2024-03-04 22:14:12,455 INFO generators.py gen_for_qa l.551] (17/47) * Start with LLM "gemini-pro"
[2024-03-04 22:14:12,459 DEBUG generators.py generate l.352] (17/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:14:12,459 DEBUG generators.py generate l.361] (17/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:14:17,156 DEBUG generators.py generate l.373] (17/47) Post-process Answer
[2024-03-04 22:14:17,166 INFO generators.py gen_for_qa l.551] (17/47) * Start with LLM "claude-2.1"
[2024-03-04 22:14:17,171 DEBUG generators.py generate l.352] (17/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:14:17,171 DEBUG generators.py generate l.361] (17/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:14:18,754 DEBUG generators.py generate l.373] (17/47) Post-process Answer
[2024-03-04 22:14:18,757 INFO generators.py gen_for_qa l.551] (17/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 22:14:18,758 DEBUG generators.py generate l.352] (17/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:14:18,760 DEBUG generators.py generate l.361] (17/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:14:19,738 DEBUG generators.py generate l.373] (17/47) Post-process Answer
[2024-03-04 22:14:19,740 INFO generators.py gen_for_qa l.551] (17/47) * Start with LLM "command-nightly"
[2024-03-04 22:14:19,741 DEBUG generators.py generate l.352] (17/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:14:19,743 DEBUG generators.py generate l.361] (17/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:14:27,070 DEBUG generators.py generate l.373] (17/47) Post-process Answer
[2024-03-04 22:14:27,071 INFO generators.py generate l.480] (17/47) End question "Comment réformer le système financier international ?  A) Taxe sur les transactions financières A) Libéralisation financière B) Régulation financière internationale"
[2024-03-04 22:14:27,071 INFO generators.py generate l.478] (18/47) *** AnsGenerator for question "Comment réformer le système de propriété intellectuelle ?  A) Licences libres A) Brevets B) Droits d'auteur"
[2024-03-04 22:14:27,076 INFO generators.py gen_for_qa l.551] (18/47) * Start with LLM "gpt-4"
[2024-03-04 22:14:27,078 DEBUG generators.py generate l.352] (18/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:14:27,082 DEBUG generators.py generate l.361] (18/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:14:30,761 DEBUG generators.py generate l.373] (18/47) Post-process Answer
[2024-03-04 22:14:30,761 INFO generators.py gen_for_qa l.551] (18/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 22:14:30,769 DEBUG generators.py generate l.352] (18/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:14:30,771 DEBUG generators.py generate l.361] (18/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:14:33,711 DEBUG generators.py generate l.373] (18/47) Post-process Answer
[2024-03-04 22:14:33,711 INFO generators.py gen_for_qa l.551] (18/47) * Start with LLM "gemini-pro"
[2024-03-04 22:14:33,718 DEBUG generators.py generate l.352] (18/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:14:33,719 DEBUG generators.py generate l.361] (18/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:14:36,991 DEBUG generators.py generate l.373] (18/47) Post-process Answer
[2024-03-04 22:14:37,001 INFO generators.py gen_for_qa l.551] (18/47) * Start with LLM "claude-2.1"
[2024-03-04 22:14:37,007 DEBUG generators.py generate l.352] (18/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:14:37,011 DEBUG generators.py generate l.361] (18/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:14:38,459 DEBUG generators.py generate l.373] (18/47) Post-process Answer
[2024-03-04 22:14:38,459 INFO generators.py gen_for_qa l.551] (18/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 22:14:38,475 DEBUG generators.py generate l.352] (18/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:14:38,481 DEBUG generators.py generate l.361] (18/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:14:44,629 DEBUG generators.py generate l.373] (18/47) Post-process Answer
[2024-03-04 22:14:44,633 INFO generators.py gen_for_qa l.551] (18/47) * Start with LLM "command-nightly"
[2024-03-04 22:14:44,639 DEBUG generators.py generate l.352] (18/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:14:44,641 DEBUG generators.py generate l.361] (18/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:14:48,384 DEBUG generators.py generate l.373] (18/47) Post-process Answer
[2024-03-04 22:14:48,389 INFO generators.py generate l.480] (18/47) End question "Comment réformer le système de propriété intellectuelle ?  A) Licences libres A) Brevets B) Droits d'auteur"
[2024-03-04 22:14:48,393 INFO generators.py generate l.478] (19/47) *** AnsGenerator for question "Comment réformer le système de régulation des industries ?  A) Nationalisation A) Déréglementation B) Concurrence régulée"
[2024-03-04 22:14:48,399 INFO generators.py gen_for_qa l.551] (19/47) * Start with LLM "gpt-4"
[2024-03-04 22:14:48,402 DEBUG generators.py generate l.352] (19/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:14:48,407 DEBUG generators.py generate l.361] (19/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:14:51,641 DEBUG generators.py generate l.373] (19/47) Post-process Answer
[2024-03-04 22:14:51,644 INFO generators.py gen_for_qa l.551] (19/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 22:14:51,647 DEBUG generators.py generate l.352] (19/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:14:51,651 DEBUG generators.py generate l.361] (19/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:14:54,990 DEBUG generators.py generate l.373] (19/47) Post-process Answer
[2024-03-04 22:14:54,990 INFO generators.py gen_for_qa l.551] (19/47) * Start with LLM "gemini-pro"
[2024-03-04 22:14:54,990 DEBUG generators.py generate l.352] (19/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:14:54,990 DEBUG generators.py generate l.361] (19/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:14:58,792 DEBUG generators.py generate l.373] (19/47) Post-process Answer
[2024-03-04 22:14:58,792 INFO generators.py gen_for_qa l.551] (19/47) * Start with LLM "claude-2.1"
[2024-03-04 22:14:58,792 DEBUG generators.py generate l.352] (19/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:14:58,803 DEBUG generators.py generate l.361] (19/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:15:00,355 DEBUG generators.py generate l.373] (19/47) Post-process Answer
[2024-03-04 22:15:00,362 INFO generators.py gen_for_qa l.551] (19/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 22:15:00,365 DEBUG generators.py generate l.352] (19/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:15:00,365 DEBUG generators.py generate l.361] (19/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:15:01,489 DEBUG generators.py generate l.373] (19/47) Post-process Answer
[2024-03-04 22:15:01,494 INFO generators.py gen_for_qa l.551] (19/47) * Start with LLM "command-nightly"
[2024-03-04 22:15:01,498 DEBUG generators.py generate l.352] (19/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:15:01,500 DEBUG generators.py generate l.361] (19/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:15:06,581 DEBUG generators.py generate l.373] (19/47) Post-process Answer
[2024-03-04 22:15:06,589 INFO generators.py generate l.480] (19/47) End question "Comment réformer le système de régulation des industries ?  A) Nationalisation A) Déréglementation B) Concurrence régulée"
[2024-03-04 22:15:06,589 INFO generators.py generate l.478] (20/47) *** AnsGenerator for question "Comment réformer le système de régulation des services publics ?  A) Services publics gratuits A) Privatisation B) Partenariat public-privé"
[2024-03-04 22:15:06,597 INFO generators.py gen_for_qa l.551] (20/47) * Start with LLM "gpt-4"
[2024-03-04 22:15:06,597 DEBUG generators.py generate l.352] (20/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:15:06,613 DEBUG generators.py generate l.361] (20/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:15:11,582 DEBUG generators.py generate l.373] (20/47) Post-process Answer
[2024-03-04 22:15:11,585 INFO generators.py gen_for_qa l.551] (20/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 22:15:11,588 DEBUG generators.py generate l.352] (20/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:15:11,591 DEBUG generators.py generate l.361] (20/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:15:14,607 DEBUG generators.py generate l.373] (20/47) Post-process Answer
[2024-03-04 22:15:14,611 INFO generators.py gen_for_qa l.551] (20/47) * Start with LLM "gemini-pro"
[2024-03-04 22:15:14,613 DEBUG generators.py generate l.352] (20/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:15:14,615 DEBUG generators.py generate l.361] (20/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:15:17,895 DEBUG generators.py generate l.373] (20/47) Post-process Answer
[2024-03-04 22:15:17,902 INFO generators.py gen_for_qa l.551] (20/47) * Start with LLM "claude-2.1"
[2024-03-04 22:15:17,906 DEBUG generators.py generate l.352] (20/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:15:17,909 DEBUG generators.py generate l.361] (20/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:15:19,639 DEBUG generators.py generate l.373] (20/47) Post-process Answer
[2024-03-04 22:15:19,643 INFO generators.py gen_for_qa l.551] (20/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 22:15:19,644 DEBUG generators.py generate l.352] (20/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:15:19,646 DEBUG generators.py generate l.361] (20/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:15:20,416 DEBUG generators.py generate l.373] (20/47) Post-process Answer
[2024-03-04 22:15:20,424 INFO generators.py gen_for_qa l.551] (20/47) * Start with LLM "command-nightly"
[2024-03-04 22:15:20,426 DEBUG generators.py generate l.352] (20/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:15:20,428 DEBUG generators.py generate l.361] (20/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:15:23,199 DEBUG generators.py generate l.373] (20/47) Post-process Answer
[2024-03-04 22:15:23,201 INFO generators.py generate l.480] (20/47) End question "Comment réformer le système de régulation des services publics ?  A) Services publics gratuits A) Privatisation B) Partenariat public-privé"
[2024-03-04 22:15:23,202 INFO generators.py generate l.478] (21/47) *** AnsGenerator for question "Comment réformer le système de régulation des finances publiques ?  A) Déficit public A) Austérité budgétaire B) Règle d'or budgétaire"
[2024-03-04 22:15:23,203 INFO generators.py gen_for_qa l.551] (21/47) * Start with LLM "gpt-4"
[2024-03-04 22:15:23,204 DEBUG generators.py generate l.352] (21/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:15:23,205 DEBUG generators.py generate l.361] (21/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:15:23,837 DEBUG generators.py generate l.373] (21/47) Post-process Answer
[2024-03-04 22:15:23,842 INFO generators.py gen_for_qa l.551] (21/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 22:15:23,845 DEBUG generators.py generate l.352] (21/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:15:23,847 DEBUG generators.py generate l.361] (21/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:15:24,868 DEBUG generators.py generate l.373] (21/47) Post-process Answer
[2024-03-04 22:15:24,868 INFO generators.py gen_for_qa l.551] (21/47) * Start with LLM "gemini-pro"
[2024-03-04 22:15:24,871 DEBUG generators.py generate l.352] (21/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:15:24,874 DEBUG generators.py generate l.361] (21/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:15:27,199 DEBUG generators.py generate l.373] (21/47) Post-process Answer
[2024-03-04 22:15:27,204 INFO generators.py gen_for_qa l.551] (21/47) * Start with LLM "claude-2.1"
[2024-03-04 22:15:27,208 DEBUG generators.py generate l.352] (21/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:15:27,210 DEBUG generators.py generate l.361] (21/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:15:28,522 DEBUG generators.py generate l.373] (21/47) Post-process Answer
[2024-03-04 22:15:28,529 INFO generators.py gen_for_qa l.551] (21/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 22:15:28,530 DEBUG generators.py generate l.352] (21/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:15:28,532 DEBUG generators.py generate l.361] (21/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:15:29,333 DEBUG generators.py generate l.373] (21/47) Post-process Answer
[2024-03-04 22:15:29,348 INFO generators.py gen_for_qa l.551] (21/47) * Start with LLM "command-nightly"
[2024-03-04 22:15:29,352 DEBUG generators.py generate l.352] (21/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:15:29,355 DEBUG generators.py generate l.361] (21/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:15:29,764 DEBUG generators.py generate l.373] (21/47) Post-process Answer
[2024-03-04 22:15:29,764 INFO generators.py generate l.480] (21/47) End question "Comment réformer le système de régulation des finances publiques ?  A) Déficit public A) Austérité budgétaire B) Règle d'or budgétaire"
[2024-03-04 22:15:29,771 INFO generators.py generate l.478] (22/47) *** AnsGenerator for question "Comment réformer le système de régulation des échanges internationaux ?  A) Commerce équitable A) Libre-échange B) Régionalisme économique"
[2024-03-04 22:15:29,774 INFO generators.py gen_for_qa l.551] (22/47) * Start with LLM "gpt-4"
[2024-03-04 22:15:29,777 DEBUG generators.py generate l.352] (22/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:15:29,779 DEBUG generators.py generate l.361] (22/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:15:30,382 DEBUG generators.py generate l.373] (22/47) Post-process Answer
[2024-03-04 22:15:30,397 INFO generators.py gen_for_qa l.551] (22/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 22:15:30,398 DEBUG generators.py generate l.352] (22/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:15:30,400 DEBUG generators.py generate l.361] (22/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:15:30,921 DEBUG generators.py generate l.373] (22/47) Post-process Answer
[2024-03-04 22:15:30,922 INFO generators.py gen_for_qa l.551] (22/47) * Start with LLM "gemini-pro"
[2024-03-04 22:15:30,925 DEBUG generators.py generate l.352] (22/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:15:30,928 DEBUG generators.py generate l.361] (22/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:15:33,297 DEBUG generators.py generate l.373] (22/47) Post-process Answer
[2024-03-04 22:15:33,299 INFO generators.py gen_for_qa l.551] (22/47) * Start with LLM "claude-2.1"
[2024-03-04 22:15:33,300 DEBUG generators.py generate l.352] (22/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:15:33,305 DEBUG generators.py generate l.361] (22/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:15:34,380 DEBUG generators.py generate l.373] (22/47) Post-process Answer
[2024-03-04 22:15:34,380 INFO generators.py gen_for_qa l.551] (22/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 22:15:34,396 DEBUG generators.py generate l.352] (22/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:15:34,398 DEBUG generators.py generate l.361] (22/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:15:34,774 DEBUG generators.py generate l.373] (22/47) Post-process Answer
[2024-03-04 22:15:34,774 INFO generators.py gen_for_qa l.551] (22/47) * Start with LLM "command-nightly"
[2024-03-04 22:15:34,780 DEBUG generators.py generate l.352] (22/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:15:34,783 DEBUG generators.py generate l.361] (22/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:15:35,365 DEBUG generators.py generate l.373] (22/47) Post-process Answer
[2024-03-04 22:15:35,365 INFO generators.py generate l.480] (22/47) End question "Comment réformer le système de régulation des échanges internationaux ?  A) Commerce équitable A) Libre-échange B) Régionalisme économique"
[2024-03-04 22:15:35,376 INFO generators.py generate l.478] (23/47) *** AnsGenerator for question "Comment réformer le système de régulation des migrations internationales ?  A) Politique migratoire ouverte A) Politique migratoire restrictive B) Politique migratoire sélective"
[2024-03-04 22:15:35,379 INFO generators.py gen_for_qa l.551] (23/47) * Start with LLM "gpt-4"
[2024-03-04 22:15:35,382 DEBUG generators.py generate l.352] (23/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:15:35,384 DEBUG generators.py generate l.361] (23/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:15:37,413 DEBUG generators.py generate l.373] (23/47) Post-process Answer
[2024-03-04 22:15:37,426 INFO generators.py gen_for_qa l.551] (23/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 22:15:37,428 DEBUG generators.py generate l.352] (23/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:15:37,430 DEBUG generators.py generate l.361] (23/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:15:38,109 DEBUG generators.py generate l.373] (23/47) Post-process Answer
[2024-03-04 22:15:38,111 INFO generators.py gen_for_qa l.551] (23/47) * Start with LLM "gemini-pro"
[2024-03-04 22:15:38,114 DEBUG generators.py generate l.352] (23/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:15:38,116 DEBUG generators.py generate l.361] (23/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:15:40,532 DEBUG generators.py generate l.373] (23/47) Post-process Answer
[2024-03-04 22:15:40,534 INFO generators.py gen_for_qa l.551] (23/47) * Start with LLM "claude-2.1"
[2024-03-04 22:15:40,536 DEBUG generators.py generate l.352] (23/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:15:40,538 DEBUG generators.py generate l.361] (23/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:15:41,474 DEBUG generators.py generate l.373] (23/47) Post-process Answer
[2024-03-04 22:15:41,480 INFO generators.py gen_for_qa l.551] (23/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 22:15:41,486 DEBUG generators.py generate l.352] (23/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:15:41,491 DEBUG generators.py generate l.361] (23/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:15:44,057 DEBUG generators.py generate l.373] (23/47) Post-process Answer
[2024-03-04 22:15:44,061 INFO generators.py gen_for_qa l.551] (23/47) * Start with LLM "command-nightly"
[2024-03-04 22:15:44,063 DEBUG generators.py generate l.352] (23/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:15:44,066 DEBUG generators.py generate l.361] (23/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:15:46,306 DEBUG generators.py generate l.373] (23/47) Post-process Answer
[2024-03-04 22:15:46,306 INFO generators.py generate l.480] (23/47) End question "Comment réformer le système de régulation des migrations internationales ?  A) Politique migratoire ouverte A) Politique migratoire restrictive B) Politique migratoire sélective"
[2024-03-04 22:15:46,317 INFO generators.py generate l.478] (24/47) *** AnsGenerator for question "Comment réformer le système de régulation de l'environnement ?  A) Écologie politique A) Marché du carbone B) Développement durable"
[2024-03-04 22:15:46,321 INFO generators.py gen_for_qa l.551] (24/47) * Start with LLM "gpt-4"
[2024-03-04 22:15:46,324 DEBUG generators.py generate l.352] (24/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:15:46,326 DEBUG generators.py generate l.361] (24/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:15:48,280 DEBUG generators.py generate l.373] (24/47) Post-process Answer
[2024-03-04 22:15:48,284 INFO generators.py gen_for_qa l.551] (24/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 22:15:48,286 DEBUG generators.py generate l.352] (24/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:15:48,289 DEBUG generators.py generate l.361] (24/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:15:52,433 DEBUG generators.py generate l.373] (24/47) Post-process Answer
[2024-03-04 22:15:52,436 INFO generators.py gen_for_qa l.551] (24/47) * Start with LLM "gemini-pro"
[2024-03-04 22:15:52,438 DEBUG generators.py generate l.352] (24/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:15:52,440 DEBUG generators.py generate l.361] (24/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:15:55,838 DEBUG generators.py generate l.373] (24/47) Post-process Answer
[2024-03-04 22:15:55,843 INFO generators.py gen_for_qa l.551] (24/47) * Start with LLM "claude-2.1"
[2024-03-04 22:15:55,847 DEBUG generators.py generate l.352] (24/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:15:55,851 DEBUG generators.py generate l.361] (24/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:15:57,253 DEBUG generators.py generate l.373] (24/47) Post-process Answer
[2024-03-04 22:15:57,255 INFO generators.py gen_for_qa l.551] (24/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 22:15:57,259 DEBUG generators.py generate l.352] (24/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:15:57,260 DEBUG generators.py generate l.361] (24/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:16:12,013 DEBUG generators.py generate l.373] (24/47) Post-process Answer
[2024-03-04 22:16:12,013 INFO generators.py gen_for_qa l.551] (24/47) * Start with LLM "command-nightly"
[2024-03-04 22:16:12,024 DEBUG generators.py generate l.352] (24/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:16:12,026 DEBUG generators.py generate l.361] (24/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:16:12,412 DEBUG generators.py generate l.373] (24/47) Post-process Answer
[2024-03-04 22:16:12,416 INFO generators.py generate l.480] (24/47) End question "Comment réformer le système de régulation de l'environnement ?  A) Écologie politique A) Marché du carbone B) Développement durable"
[2024-03-04 22:16:12,421 INFO generators.py generate l.478] (25/47) *** AnsGenerator for question "Comment réduire la pauvreté ?  A) Redistribution des richesses A) Croissance économique B) Filets de sécurité sociaux"
[2024-03-04 22:16:12,425 INFO generators.py gen_for_qa l.551] (25/47) * Start with LLM "gpt-4"
[2024-03-04 22:16:12,427 DEBUG generators.py generate l.352] (25/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:16:12,428 DEBUG generators.py generate l.361] (25/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:16:13,119 DEBUG generators.py generate l.373] (25/47) Post-process Answer
[2024-03-04 22:16:13,130 INFO generators.py gen_for_qa l.551] (25/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 22:16:13,133 DEBUG generators.py generate l.352] (25/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:16:13,135 DEBUG generators.py generate l.361] (25/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:16:14,170 DEBUG generators.py generate l.373] (25/47) Post-process Answer
[2024-03-04 22:16:14,173 INFO generators.py gen_for_qa l.551] (25/47) * Start with LLM "gemini-pro"
[2024-03-04 22:16:14,175 DEBUG generators.py generate l.352] (25/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:16:14,176 DEBUG generators.py generate l.361] (25/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:16:16,751 DEBUG generators.py generate l.373] (25/47) Post-process Answer
[2024-03-04 22:16:16,754 INFO generators.py gen_for_qa l.551] (25/47) * Start with LLM "claude-2.1"
[2024-03-04 22:16:16,755 DEBUG generators.py generate l.352] (25/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:16:16,757 DEBUG generators.py generate l.361] (25/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:16:18,188 DEBUG generators.py generate l.373] (25/47) Post-process Answer
[2024-03-04 22:16:18,194 INFO generators.py gen_for_qa l.551] (25/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 22:16:18,195 DEBUG generators.py generate l.352] (25/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:16:18,196 DEBUG generators.py generate l.361] (25/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:16:23,239 DEBUG generators.py generate l.373] (25/47) Post-process Answer
[2024-03-04 22:16:23,240 INFO generators.py gen_for_qa l.551] (25/47) * Start with LLM "command-nightly"
[2024-03-04 22:16:23,241 DEBUG generators.py generate l.352] (25/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:16:23,242 DEBUG generators.py generate l.361] (25/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:16:23,658 DEBUG generators.py generate l.373] (25/47) Post-process Answer
[2024-03-04 22:16:23,665 INFO generators.py generate l.480] (25/47) End question "Comment réduire la pauvreté ?  A) Redistribution des richesses A) Croissance économique B) Filets de sécurité sociaux"
[2024-03-04 22:16:23,670 INFO generators.py generate l.478] (26/47) *** AnsGenerator for question "Comment réformer le système de protection des consommateurs ?  A) Réglementation stricte A) Déréglementation B) Autorégulation"
[2024-03-04 22:16:23,674 INFO generators.py gen_for_qa l.551] (26/47) * Start with LLM "gpt-4"
[2024-03-04 22:16:23,677 DEBUG generators.py generate l.352] (26/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:16:23,679 DEBUG generators.py generate l.361] (26/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:16:24,529 DEBUG generators.py generate l.373] (26/47) Post-process Answer
[2024-03-04 22:16:24,533 INFO generators.py gen_for_qa l.551] (26/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 22:16:24,534 DEBUG generators.py generate l.352] (26/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:16:24,537 DEBUG generators.py generate l.361] (26/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:16:25,072 DEBUG generators.py generate l.373] (26/47) Post-process Answer
[2024-03-04 22:16:25,079 INFO generators.py gen_for_qa l.551] (26/47) * Start with LLM "gemini-pro"
[2024-03-04 22:16:25,082 DEBUG generators.py generate l.352] (26/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:16:25,086 DEBUG generators.py generate l.361] (26/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:16:27,422 DEBUG generators.py generate l.373] (26/47) Post-process Answer
[2024-03-04 22:16:27,424 INFO generators.py gen_for_qa l.551] (26/47) * Start with LLM "claude-2.1"
[2024-03-04 22:16:27,427 DEBUG generators.py generate l.352] (26/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:16:27,428 DEBUG generators.py generate l.361] (26/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:16:28,910 DEBUG generators.py generate l.373] (26/47) Post-process Answer
[2024-03-04 22:16:28,914 INFO generators.py gen_for_qa l.551] (26/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 22:16:28,921 DEBUG generators.py generate l.352] (26/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:16:28,924 DEBUG generators.py generate l.361] (26/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:16:29,434 DEBUG generators.py generate l.373] (26/47) Post-process Answer
[2024-03-04 22:16:29,434 INFO generators.py gen_for_qa l.551] (26/47) * Start with LLM "command-nightly"
[2024-03-04 22:16:29,440 DEBUG generators.py generate l.352] (26/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:16:29,444 DEBUG generators.py generate l.361] (26/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:16:29,844 DEBUG generators.py generate l.373] (26/47) Post-process Answer
[2024-03-04 22:16:29,850 INFO generators.py generate l.480] (26/47) End question "Comment réformer le système de protection des consommateurs ?  A) Réglementation stricte A) Déréglementation B) Autorégulation"
[2024-03-04 22:16:29,856 INFO generators.py generate l.478] (27/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés financiers ?  A) Réglementation stricte A) Déréglementation B) Régulation prudentielle"
[2024-03-04 22:16:29,860 INFO generators.py gen_for_qa l.551] (27/47) * Start with LLM "gpt-4"
[2024-03-04 22:16:29,863 DEBUG generators.py generate l.352] (27/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:16:29,865 DEBUG generators.py generate l.361] (27/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:16:30,711 DEBUG generators.py generate l.373] (27/47) Post-process Answer
[2024-03-04 22:16:30,713 INFO generators.py gen_for_qa l.551] (27/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 22:16:30,715 DEBUG generators.py generate l.352] (27/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:16:30,717 DEBUG generators.py generate l.361] (27/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:16:31,321 DEBUG generators.py generate l.373] (27/47) Post-process Answer
[2024-03-04 22:16:31,321 INFO generators.py gen_for_qa l.551] (27/47) * Start with LLM "gemini-pro"
[2024-03-04 22:16:31,326 DEBUG generators.py generate l.352] (27/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:16:31,328 DEBUG generators.py generate l.361] (27/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:16:33,737 DEBUG generators.py generate l.373] (27/47) Post-process Answer
[2024-03-04 22:16:33,740 INFO generators.py gen_for_qa l.551] (27/47) * Start with LLM "claude-2.1"
[2024-03-04 22:16:33,740 DEBUG generators.py generate l.352] (27/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:16:33,743 DEBUG generators.py generate l.361] (27/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:16:34,876 DEBUG generators.py generate l.373] (27/47) Post-process Answer
[2024-03-04 22:16:34,893 INFO generators.py gen_for_qa l.551] (27/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 22:16:34,895 DEBUG generators.py generate l.352] (27/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:16:34,895 DEBUG generators.py generate l.361] (27/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:16:35,434 DEBUG generators.py generate l.373] (27/47) Post-process Answer
[2024-03-04 22:16:35,440 INFO generators.py gen_for_qa l.551] (27/47) * Start with LLM "command-nightly"
[2024-03-04 22:16:35,443 DEBUG generators.py generate l.352] (27/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:16:35,446 DEBUG generators.py generate l.361] (27/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:16:37,009 DEBUG generators.py generate l.373] (27/47) Post-process Answer
[2024-03-04 22:16:37,010 INFO generators.py generate l.480] (27/47) End question "Comment réformer le système de régulation des marchés financiers ?  A) Réglementation stricte A) Déréglementation B) Régulation prudentielle"
[2024-03-04 22:16:37,012 INFO generators.py generate l.478] (28/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés du travail ?  A) Réglementation stricte A) Déréglementation B) Flexisécurité"
[2024-03-04 22:16:37,013 INFO generators.py gen_for_qa l.551] (28/47) * Start with LLM "gpt-4"
[2024-03-04 22:16:37,015 DEBUG generators.py generate l.352] (28/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:16:37,017 DEBUG generators.py generate l.361] (28/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:16:38,341 DEBUG generators.py generate l.373] (28/47) Post-process Answer
[2024-03-04 22:16:38,341 INFO generators.py gen_for_qa l.551] (28/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 22:16:38,346 DEBUG generators.py generate l.352] (28/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:16:38,347 DEBUG generators.py generate l.361] (28/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:16:38,970 DEBUG generators.py generate l.373] (28/47) Post-process Answer
[2024-03-04 22:16:38,970 INFO generators.py gen_for_qa l.551] (28/47) * Start with LLM "gemini-pro"
[2024-03-04 22:16:38,978 DEBUG generators.py generate l.352] (28/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:16:38,981 DEBUG generators.py generate l.361] (28/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:16:41,338 DEBUG generators.py generate l.373] (28/47) Post-process Answer
[2024-03-04 22:16:41,340 INFO generators.py gen_for_qa l.551] (28/47) * Start with LLM "claude-2.1"
[2024-03-04 22:16:41,340 DEBUG generators.py generate l.352] (28/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:16:41,346 DEBUG generators.py generate l.361] (28/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:16:42,255 DEBUG generators.py generate l.373] (28/47) Post-process Answer
[2024-03-04 22:16:42,256 INFO generators.py gen_for_qa l.551] (28/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 22:16:42,258 DEBUG generators.py generate l.352] (28/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:16:42,261 DEBUG generators.py generate l.361] (28/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:16:42,785 DEBUG generators.py generate l.373] (28/47) Post-process Answer
[2024-03-04 22:16:42,789 INFO generators.py gen_for_qa l.551] (28/47) * Start with LLM "command-nightly"
[2024-03-04 22:16:42,791 DEBUG generators.py generate l.352] (28/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:16:42,794 DEBUG generators.py generate l.361] (28/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:16:43,178 DEBUG generators.py generate l.373] (28/47) Post-process Answer
[2024-03-04 22:16:43,181 INFO generators.py generate l.480] (28/47) End question "Comment réformer le système de régulation des marchés du travail ?  A) Réglementation stricte A) Déréglementation B) Flexisécurité"
[2024-03-04 22:16:43,185 INFO generators.py generate l.478] (29/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés de l'énergie ?  A) Réglementation stricte A) Déréglementation B) Tarification progressive"
[2024-03-04 22:16:43,190 INFO generators.py gen_for_qa l.551] (29/47) * Start with LLM "gpt-4"
[2024-03-04 22:16:43,193 DEBUG generators.py generate l.352] (29/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:16:43,194 DEBUG generators.py generate l.361] (29/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:16:43,892 DEBUG generators.py generate l.373] (29/47) Post-process Answer
[2024-03-04 22:16:43,904 INFO generators.py gen_for_qa l.551] (29/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 22:16:43,904 DEBUG generators.py generate l.352] (29/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:16:43,908 DEBUG generators.py generate l.361] (29/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:16:44,427 DEBUG generators.py generate l.373] (29/47) Post-process Answer
[2024-03-04 22:16:44,431 INFO generators.py gen_for_qa l.551] (29/47) * Start with LLM "gemini-pro"
[2024-03-04 22:16:44,433 DEBUG generators.py generate l.352] (29/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:16:44,436 DEBUG generators.py generate l.361] (29/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:16:46,879 DEBUG generators.py generate l.373] (29/47) Post-process Answer
[2024-03-04 22:16:46,881 INFO generators.py gen_for_qa l.551] (29/47) * Start with LLM "claude-2.1"
[2024-03-04 22:16:46,881 DEBUG generators.py generate l.352] (29/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:16:46,885 DEBUG generators.py generate l.361] (29/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:16:47,806 DEBUG generators.py generate l.373] (29/47) Post-process Answer
[2024-03-04 22:16:47,812 INFO generators.py gen_for_qa l.551] (29/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 22:16:47,814 DEBUG generators.py generate l.352] (29/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:16:47,818 DEBUG generators.py generate l.361] (29/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:16:51,848 DEBUG generators.py generate l.373] (29/47) Post-process Answer
[2024-03-04 22:16:51,848 INFO generators.py gen_for_qa l.551] (29/47) * Start with LLM "command-nightly"
[2024-03-04 22:16:51,852 DEBUG generators.py generate l.352] (29/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:16:51,854 DEBUG generators.py generate l.361] (29/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:16:52,288 DEBUG generators.py generate l.373] (29/47) Post-process Answer
[2024-03-04 22:16:52,292 INFO generators.py generate l.480] (29/47) End question "Comment réformer le système de régulation des marchés de l'énergie ?  A) Réglementation stricte A) Déréglementation B) Tarification progressive"
[2024-03-04 22:16:52,297 INFO generators.py generate l.478] (30/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés des télécommunications ?  A) Réglementation stricte A) Déréglementation B) Neutralité du net"
[2024-03-04 22:16:52,300 INFO generators.py gen_for_qa l.551] (30/47) * Start with LLM "gpt-4"
[2024-03-04 22:16:52,303 DEBUG generators.py generate l.352] (30/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:16:52,305 DEBUG generators.py generate l.361] (30/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:16:53,032 DEBUG generators.py generate l.373] (30/47) Post-process Answer
[2024-03-04 22:16:53,032 INFO generators.py gen_for_qa l.551] (30/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 22:16:53,034 DEBUG generators.py generate l.352] (30/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:16:53,036 DEBUG generators.py generate l.361] (30/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:16:53,731 DEBUG generators.py generate l.373] (30/47) Post-process Answer
[2024-03-04 22:16:53,744 INFO generators.py gen_for_qa l.551] (30/47) * Start with LLM "gemini-pro"
[2024-03-04 22:16:53,746 DEBUG generators.py generate l.352] (30/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:16:53,748 DEBUG generators.py generate l.361] (30/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:16:57,193 DEBUG generators.py generate l.373] (30/47) Post-process Answer
[2024-03-04 22:16:57,196 INFO generators.py gen_for_qa l.551] (30/47) * Start with LLM "claude-2.1"
[2024-03-04 22:16:57,204 DEBUG generators.py generate l.352] (30/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:16:57,209 DEBUG generators.py generate l.361] (30/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:16:58,323 DEBUG generators.py generate l.373] (30/47) Post-process Answer
[2024-03-04 22:16:58,339 INFO generators.py gen_for_qa l.551] (30/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 22:16:58,342 DEBUG generators.py generate l.352] (30/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:16:58,345 DEBUG generators.py generate l.361] (30/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:17:09,772 DEBUG generators.py generate l.373] (30/47) Post-process Answer
[2024-03-04 22:17:09,779 INFO generators.py gen_for_qa l.551] (30/47) * Start with LLM "command-nightly"
[2024-03-04 22:17:09,782 DEBUG generators.py generate l.352] (30/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:17:09,785 DEBUG generators.py generate l.361] (30/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:17:10,202 DEBUG generators.py generate l.373] (30/47) Post-process Answer
[2024-03-04 22:17:10,206 INFO generators.py generate l.480] (30/47) End question "Comment réformer le système de régulation des marchés des télécommunications ?  A) Réglementation stricte A) Déréglementation B) Neutralité du net"
[2024-03-04 22:17:10,211 INFO generators.py generate l.478] (31/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés des transports ?  A) Réglementation stricte A) Déréglementation B) Concurrence régulée"
[2024-03-04 22:17:10,213 INFO generators.py gen_for_qa l.551] (31/47) * Start with LLM "gpt-4"
[2024-03-04 22:17:10,216 DEBUG generators.py generate l.352] (31/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:17:10,218 DEBUG generators.py generate l.361] (31/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:17:10,898 DEBUG generators.py generate l.373] (31/47) Post-process Answer
[2024-03-04 22:17:10,901 INFO generators.py gen_for_qa l.551] (31/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 22:17:10,904 DEBUG generators.py generate l.352] (31/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:17:10,907 DEBUG generators.py generate l.361] (31/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:17:11,427 DEBUG generators.py generate l.373] (31/47) Post-process Answer
[2024-03-04 22:17:11,438 INFO generators.py gen_for_qa l.551] (31/47) * Start with LLM "gemini-pro"
[2024-03-04 22:17:11,442 DEBUG generators.py generate l.352] (31/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:17:11,444 DEBUG generators.py generate l.361] (31/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:17:13,749 DEBUG generators.py generate l.373] (31/47) Post-process Answer
[2024-03-04 22:17:13,755 INFO generators.py gen_for_qa l.551] (31/47) * Start with LLM "claude-2.1"
[2024-03-04 22:17:13,759 DEBUG generators.py generate l.352] (31/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:17:13,761 DEBUG generators.py generate l.361] (31/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:17:14,766 DEBUG generators.py generate l.373] (31/47) Post-process Answer
[2024-03-04 22:17:14,768 INFO generators.py gen_for_qa l.551] (31/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 22:17:14,769 DEBUG generators.py generate l.352] (31/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:17:14,771 DEBUG generators.py generate l.361] (31/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:17:15,291 DEBUG generators.py generate l.373] (31/47) Post-process Answer
[2024-03-04 22:17:15,294 INFO generators.py gen_for_qa l.551] (31/47) * Start with LLM "command-nightly"
[2024-03-04 22:17:15,297 DEBUG generators.py generate l.352] (31/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:17:15,299 DEBUG generators.py generate l.361] (31/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:17:15,776 DEBUG generators.py generate l.373] (31/47) Post-process Answer
[2024-03-04 22:17:15,781 INFO generators.py generate l.480] (31/47) End question "Comment réformer le système de régulation des marchés des transports ?  A) Réglementation stricte A) Déréglementation B) Concurrence régulée"
[2024-03-04 22:17:15,786 INFO generators.py generate l.478] (32/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés du logement ?  A) Réglementation stricte A) Déréglementation B) Encadrement des loyers"
[2024-03-04 22:17:15,789 INFO generators.py gen_for_qa l.551] (32/47) * Start with LLM "gpt-4"
[2024-03-04 22:17:15,791 DEBUG generators.py generate l.352] (32/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:17:15,794 DEBUG generators.py generate l.361] (32/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:17:16,494 DEBUG generators.py generate l.373] (32/47) Post-process Answer
[2024-03-04 22:17:16,496 INFO generators.py gen_for_qa l.551] (32/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 22:17:16,498 DEBUG generators.py generate l.352] (32/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:17:16,501 DEBUG generators.py generate l.361] (32/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:17:17,374 DEBUG generators.py generate l.373] (32/47) Post-process Answer
[2024-03-04 22:17:17,376 INFO generators.py gen_for_qa l.551] (32/47) * Start with LLM "gemini-pro"
[2024-03-04 22:17:17,379 DEBUG generators.py generate l.352] (32/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:17:17,382 DEBUG generators.py generate l.361] (32/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:17:19,901 DEBUG generators.py generate l.373] (32/47) Post-process Answer
[2024-03-04 22:17:19,903 INFO generators.py gen_for_qa l.551] (32/47) * Start with LLM "claude-2.1"
[2024-03-04 22:17:19,904 DEBUG generators.py generate l.352] (32/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:17:19,905 DEBUG generators.py generate l.361] (32/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:17:20,789 DEBUG generators.py generate l.373] (32/47) Post-process Answer
[2024-03-04 22:17:20,796 INFO generators.py gen_for_qa l.551] (32/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 22:17:20,803 DEBUG generators.py generate l.352] (32/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:17:20,808 DEBUG generators.py generate l.361] (32/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:17:22,343 DEBUG generators.py generate l.373] (32/47) Post-process Answer
[2024-03-04 22:17:22,345 INFO generators.py gen_for_qa l.551] (32/47) * Start with LLM "command-nightly"
[2024-03-04 22:17:22,347 DEBUG generators.py generate l.352] (32/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:17:22,349 DEBUG generators.py generate l.361] (32/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:17:22,735 DEBUG generators.py generate l.373] (32/47) Post-process Answer
[2024-03-04 22:17:22,739 INFO generators.py generate l.480] (32/47) End question "Comment réformer le système de régulation des marchés du logement ?  A) Réglementation stricte A) Déréglementation B) Encadrement des loyers"
[2024-03-04 22:17:22,741 INFO generators.py generate l.478] (33/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés de l'eau ?  A) Réglementation stricte A) Déréglementation B) Tarification sociale"
[2024-03-04 22:17:22,743 INFO generators.py gen_for_qa l.551] (33/47) * Start with LLM "gpt-4"
[2024-03-04 22:17:22,746 DEBUG generators.py generate l.352] (33/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:17:22,747 DEBUG generators.py generate l.361] (33/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:17:23,444 DEBUG generators.py generate l.373] (33/47) Post-process Answer
[2024-03-04 22:17:23,445 INFO generators.py gen_for_qa l.551] (33/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 22:17:23,446 DEBUG generators.py generate l.352] (33/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:17:23,448 DEBUG generators.py generate l.361] (33/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:17:24,141 DEBUG generators.py generate l.373] (33/47) Post-process Answer
[2024-03-04 22:17:24,142 INFO generators.py gen_for_qa l.551] (33/47) * Start with LLM "gemini-pro"
[2024-03-04 22:17:24,144 DEBUG generators.py generate l.352] (33/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:17:24,147 DEBUG generators.py generate l.361] (33/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:17:26,449 DEBUG generators.py generate l.373] (33/47) Post-process Answer
[2024-03-04 22:17:26,452 INFO generators.py gen_for_qa l.551] (33/47) * Start with LLM "claude-2.1"
[2024-03-04 22:17:26,453 DEBUG generators.py generate l.352] (33/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:17:26,455 DEBUG generators.py generate l.361] (33/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:17:27,446 DEBUG generators.py generate l.373] (33/47) Post-process Answer
[2024-03-04 22:17:27,447 INFO generators.py gen_for_qa l.551] (33/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 22:17:27,453 DEBUG generators.py generate l.352] (33/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:17:27,454 DEBUG generators.py generate l.361] (33/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:17:31,068 DEBUG generators.py generate l.373] (33/47) Post-process Answer
[2024-03-04 22:17:31,072 INFO generators.py gen_for_qa l.551] (33/47) * Start with LLM "command-nightly"
[2024-03-04 22:17:31,075 DEBUG generators.py generate l.352] (33/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:17:31,078 DEBUG generators.py generate l.361] (33/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:17:31,485 DEBUG generators.py generate l.373] (33/47) Post-process Answer
[2024-03-04 22:17:31,501 INFO generators.py generate l.480] (33/47) End question "Comment réformer le système de régulation des marchés de l'eau ?  A) Réglementation stricte A) Déréglementation B) Tarification sociale"
[2024-03-04 22:17:31,502 INFO generators.py generate l.478] (34/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés de l'alimentation ?  A) Réglementation stricte A) Déréglementation B) Agriculture biologique"
[2024-03-04 22:17:31,504 INFO generators.py gen_for_qa l.551] (34/47) * Start with LLM "gpt-4"
[2024-03-04 22:17:31,506 DEBUG generators.py generate l.352] (34/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:17:31,507 DEBUG generators.py generate l.361] (34/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:17:32,104 DEBUG generators.py generate l.373] (34/47) Post-process Answer
[2024-03-04 22:17:32,108 INFO generators.py gen_for_qa l.551] (34/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 22:17:32,112 DEBUG generators.py generate l.352] (34/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:17:32,113 DEBUG generators.py generate l.361] (34/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:17:32,920 DEBUG generators.py generate l.373] (34/47) Post-process Answer
[2024-03-04 22:17:32,927 INFO generators.py gen_for_qa l.551] (34/47) * Start with LLM "gemini-pro"
[2024-03-04 22:17:32,930 DEBUG generators.py generate l.352] (34/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:17:32,930 DEBUG generators.py generate l.361] (34/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:17:35,292 DEBUG generators.py generate l.373] (34/47) Post-process Answer
[2024-03-04 22:17:35,298 INFO generators.py gen_for_qa l.551] (34/47) * Start with LLM "claude-2.1"
[2024-03-04 22:17:35,301 DEBUG generators.py generate l.352] (34/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:17:35,304 DEBUG generators.py generate l.361] (34/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:17:36,678 DEBUG generators.py generate l.373] (34/47) Post-process Answer
[2024-03-04 22:17:36,680 INFO generators.py gen_for_qa l.551] (34/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 22:17:36,683 DEBUG generators.py generate l.352] (34/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:17:36,684 DEBUG generators.py generate l.361] (34/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:17:37,259 DEBUG generators.py generate l.373] (34/47) Post-process Answer
[2024-03-04 22:17:37,263 INFO generators.py gen_for_qa l.551] (34/47) * Start with LLM "command-nightly"
[2024-03-04 22:17:37,265 DEBUG generators.py generate l.352] (34/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:17:37,267 DEBUG generators.py generate l.361] (34/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:17:37,657 DEBUG generators.py generate l.373] (34/47) Post-process Answer
[2024-03-04 22:17:37,659 INFO generators.py generate l.480] (34/47) End question "Comment réformer le système de régulation des marchés de l'alimentation ?  A) Réglementation stricte A) Déréglementation B) Agriculture biologique"
[2024-03-04 22:17:37,662 INFO generators.py generate l.478] (35/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés de la santé ?  A) Réglementation stricte A) Déréglementation B) Médecine préventive"
[2024-03-04 22:17:37,664 INFO generators.py gen_for_qa l.551] (35/47) * Start with LLM "gpt-4"
[2024-03-04 22:17:37,666 DEBUG generators.py generate l.352] (35/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:17:37,668 DEBUG generators.py generate l.361] (35/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:17:38,311 DEBUG generators.py generate l.373] (35/47) Post-process Answer
[2024-03-04 22:17:38,314 INFO generators.py gen_for_qa l.551] (35/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 22:17:38,318 DEBUG generators.py generate l.352] (35/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:17:38,320 DEBUG generators.py generate l.361] (35/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:17:39,140 DEBUG generators.py generate l.373] (35/47) Post-process Answer
[2024-03-04 22:17:39,140 INFO generators.py gen_for_qa l.551] (35/47) * Start with LLM "gemini-pro"
[2024-03-04 22:17:39,153 DEBUG generators.py generate l.352] (35/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:17:39,156 DEBUG generators.py generate l.361] (35/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:17:41,545 DEBUG generators.py generate l.373] (35/47) Post-process Answer
[2024-03-04 22:17:41,554 INFO generators.py gen_for_qa l.551] (35/47) * Start with LLM "claude-2.1"
[2024-03-04 22:17:41,556 DEBUG generators.py generate l.352] (35/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:17:41,557 DEBUG generators.py generate l.361] (35/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:17:42,678 DEBUG generators.py generate l.373] (35/47) Post-process Answer
[2024-03-04 22:17:42,680 INFO generators.py gen_for_qa l.551] (35/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 22:17:42,683 DEBUG generators.py generate l.352] (35/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:17:42,684 DEBUG generators.py generate l.361] (35/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:17:46,193 DEBUG generators.py generate l.373] (35/47) Post-process Answer
[2024-03-04 22:17:46,195 INFO generators.py gen_for_qa l.551] (35/47) * Start with LLM "command-nightly"
[2024-03-04 22:17:46,196 DEBUG generators.py generate l.352] (35/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:17:46,198 DEBUG generators.py generate l.361] (35/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:17:46,656 DEBUG generators.py generate l.373] (35/47) Post-process Answer
[2024-03-04 22:17:46,660 INFO generators.py generate l.480] (35/47) End question "Comment réformer le système de régulation des marchés de la santé ?  A) Réglementation stricte A) Déréglementation B) Médecine préventive"
[2024-03-04 22:17:46,661 INFO generators.py generate l.478] (36/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés de l'éducation ?  A) Réglementation stricte A) Déréglementation B) Éducation inclusive"
[2024-03-04 22:17:46,661 INFO generators.py gen_for_qa l.551] (36/47) * Start with LLM "gpt-4"
[2024-03-04 22:17:46,661 DEBUG generators.py generate l.352] (36/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:17:46,664 DEBUG generators.py generate l.361] (36/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:17:48,142 DEBUG generators.py generate l.373] (36/47) Post-process Answer
[2024-03-04 22:17:48,143 INFO generators.py gen_for_qa l.551] (36/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 22:17:48,145 DEBUG generators.py generate l.352] (36/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:17:48,147 DEBUG generators.py generate l.361] (36/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:17:48,926 DEBUG generators.py generate l.373] (36/47) Post-process Answer
[2024-03-04 22:17:48,929 INFO generators.py gen_for_qa l.551] (36/47) * Start with LLM "gemini-pro"
[2024-03-04 22:17:48,930 DEBUG generators.py generate l.352] (36/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:17:48,931 DEBUG generators.py generate l.361] (36/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:17:52,494 DEBUG generators.py generate l.373] (36/47) Post-process Answer
[2024-03-04 22:17:52,496 INFO generators.py gen_for_qa l.551] (36/47) * Start with LLM "claude-2.1"
[2024-03-04 22:17:52,499 DEBUG generators.py generate l.352] (36/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:17:52,499 DEBUG generators.py generate l.361] (36/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:17:53,529 DEBUG generators.py generate l.373] (36/47) Post-process Answer
[2024-03-04 22:17:53,529 INFO generators.py gen_for_qa l.551] (36/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 22:17:53,535 DEBUG generators.py generate l.352] (36/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:17:53,537 DEBUG generators.py generate l.361] (36/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:18:04,676 DEBUG generators.py generate l.373] (36/47) Post-process Answer
[2024-03-04 22:18:04,678 INFO generators.py gen_for_qa l.551] (36/47) * Start with LLM "command-nightly"
[2024-03-04 22:18:04,680 DEBUG generators.py generate l.352] (36/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:18:04,682 DEBUG generators.py generate l.361] (36/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:18:05,077 DEBUG generators.py generate l.373] (36/47) Post-process Answer
[2024-03-04 22:18:05,082 INFO generators.py generate l.480] (36/47) End question "Comment réformer le système de régulation des marchés de l'éducation ?  A) Réglementation stricte A) Déréglementation B) Éducation inclusive"
[2024-03-04 22:18:05,085 INFO generators.py generate l.478] (37/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés de la culture ?  A) Réglementation stricte A) Déréglementation B) Diversité culturelle"
[2024-03-04 22:18:05,087 INFO generators.py gen_for_qa l.551] (37/47) * Start with LLM "gpt-4"
[2024-03-04 22:18:05,089 DEBUG generators.py generate l.352] (37/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:18:05,091 DEBUG generators.py generate l.361] (37/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:18:05,921 DEBUG generators.py generate l.373] (37/47) Post-process Answer
[2024-03-04 22:18:05,924 INFO generators.py gen_for_qa l.551] (37/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 22:18:05,928 DEBUG generators.py generate l.352] (37/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:18:05,931 DEBUG generators.py generate l.361] (37/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:18:06,671 DEBUG generators.py generate l.373] (37/47) Post-process Answer
[2024-03-04 22:18:06,674 INFO generators.py gen_for_qa l.551] (37/47) * Start with LLM "gemini-pro"
[2024-03-04 22:18:06,677 DEBUG generators.py generate l.352] (37/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:18:06,678 DEBUG generators.py generate l.361] (37/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:18:08,542 DEBUG generators.py generate l.373] (37/47) Post-process Answer
[2024-03-04 22:18:08,544 INFO generators.py gen_for_qa l.551] (37/47) * Start with LLM "claude-2.1"
[2024-03-04 22:18:08,546 DEBUG generators.py generate l.352] (37/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:18:08,549 DEBUG generators.py generate l.361] (37/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:18:10,518 DEBUG generators.py generate l.373] (37/47) Post-process Answer
[2024-03-04 22:18:10,526 INFO generators.py gen_for_qa l.551] (37/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 22:18:10,528 DEBUG generators.py generate l.352] (37/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:18:10,530 DEBUG generators.py generate l.361] (37/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:18:14,126 DEBUG generators.py generate l.373] (37/47) Post-process Answer
[2024-03-04 22:18:14,129 INFO generators.py gen_for_qa l.551] (37/47) * Start with LLM "command-nightly"
[2024-03-04 22:18:14,132 DEBUG generators.py generate l.352] (37/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:18:14,134 DEBUG generators.py generate l.361] (37/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:18:14,523 DEBUG generators.py generate l.373] (37/47) Post-process Answer
[2024-03-04 22:18:14,530 INFO generators.py generate l.480] (37/47) End question "Comment réformer le système de régulation des marchés de la culture ?  A) Réglementation stricte A) Déréglementation B) Diversité culturelle"
[2024-03-04 22:18:14,533 INFO generators.py generate l.478] (38/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés du sport ?  A) Réglementation stricte A) Déréglementation B) Sport pour tous"
[2024-03-04 22:18:14,536 INFO generators.py gen_for_qa l.551] (38/47) * Start with LLM "gpt-4"
[2024-03-04 22:18:14,538 DEBUG generators.py generate l.352] (38/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:18:14,540 DEBUG generators.py generate l.361] (38/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:18:15,648 DEBUG generators.py generate l.373] (38/47) Post-process Answer
[2024-03-04 22:18:15,651 INFO generators.py gen_for_qa l.551] (38/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 22:18:15,653 DEBUG generators.py generate l.352] (38/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:18:15,655 DEBUG generators.py generate l.361] (38/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:18:16,524 DEBUG generators.py generate l.373] (38/47) Post-process Answer
[2024-03-04 22:18:16,533 INFO generators.py gen_for_qa l.551] (38/47) * Start with LLM "gemini-pro"
[2024-03-04 22:18:16,537 DEBUG generators.py generate l.352] (38/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:18:16,539 DEBUG generators.py generate l.361] (38/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:18:18,956 DEBUG generators.py generate l.373] (38/47) Post-process Answer
[2024-03-04 22:18:18,956 INFO generators.py gen_for_qa l.551] (38/47) * Start with LLM "claude-2.1"
[2024-03-04 22:18:18,959 DEBUG generators.py generate l.352] (38/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:18:18,961 DEBUG generators.py generate l.361] (38/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:18:19,960 DEBUG generators.py generate l.373] (38/47) Post-process Answer
[2024-03-04 22:18:19,962 INFO generators.py gen_for_qa l.551] (38/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 22:18:19,965 DEBUG generators.py generate l.352] (38/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:18:19,965 DEBUG generators.py generate l.361] (38/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:18:22,812 DEBUG generators.py generate l.373] (38/47) Post-process Answer
[2024-03-04 22:18:22,816 INFO generators.py gen_for_qa l.551] (38/47) * Start with LLM "command-nightly"
[2024-03-04 22:18:22,819 DEBUG generators.py generate l.352] (38/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:18:22,820 DEBUG generators.py generate l.361] (38/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:18:23,240 DEBUG generators.py generate l.373] (38/47) Post-process Answer
[2024-03-04 22:18:23,243 INFO generators.py generate l.480] (38/47) End question "Comment réformer le système de régulation des marchés du sport ?  A) Réglementation stricte A) Déréglementation B) Sport pour tous"
[2024-03-04 22:18:23,250 INFO generators.py generate l.478] (39/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés de l'environnement ?  A) Réglementation stricte A) Déréglementation B) Économie verte"
[2024-03-04 22:18:23,254 INFO generators.py gen_for_qa l.551] (39/47) * Start with LLM "gpt-4"
[2024-03-04 22:18:23,255 DEBUG generators.py generate l.352] (39/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:18:23,262 DEBUG generators.py generate l.361] (39/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:18:24,006 DEBUG generators.py generate l.373] (39/47) Post-process Answer
[2024-03-04 22:18:24,010 INFO generators.py gen_for_qa l.551] (39/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 22:18:24,014 DEBUG generators.py generate l.352] (39/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:18:24,016 DEBUG generators.py generate l.361] (39/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:18:24,621 DEBUG generators.py generate l.373] (39/47) Post-process Answer
[2024-03-04 22:18:24,621 INFO generators.py gen_for_qa l.551] (39/47) * Start with LLM "gemini-pro"
[2024-03-04 22:18:24,631 DEBUG generators.py generate l.352] (39/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:18:24,632 DEBUG generators.py generate l.361] (39/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:18:27,218 DEBUG generators.py generate l.373] (39/47) Post-process Answer
[2024-03-04 22:18:27,218 INFO generators.py gen_for_qa l.551] (39/47) * Start with LLM "claude-2.1"
[2024-03-04 22:18:27,221 DEBUG generators.py generate l.352] (39/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:18:27,221 DEBUG generators.py generate l.361] (39/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:18:28,313 DEBUG generators.py generate l.373] (39/47) Post-process Answer
[2024-03-04 22:18:28,315 INFO generators.py gen_for_qa l.551] (39/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 22:18:28,318 DEBUG generators.py generate l.352] (39/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:18:28,321 DEBUG generators.py generate l.361] (39/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:18:31,772 DEBUG generators.py generate l.373] (39/47) Post-process Answer
[2024-03-04 22:18:31,776 INFO generators.py gen_for_qa l.551] (39/47) * Start with LLM "command-nightly"
[2024-03-04 22:18:31,776 DEBUG generators.py generate l.352] (39/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:18:31,776 DEBUG generators.py generate l.361] (39/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:18:32,185 DEBUG generators.py generate l.373] (39/47) Post-process Answer
[2024-03-04 22:18:32,198 INFO generators.py generate l.480] (39/47) End question "Comment réformer le système de régulation des marchés de l'environnement ?  A) Réglementation stricte A) Déréglementation B) Économie verte"
[2024-03-04 22:18:32,206 INFO generators.py generate l.478] (40/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés du numérique ?  A) Réglementation stricte A) Déréglementation B) Souveraineté numérique"
[2024-03-04 22:18:32,210 INFO generators.py gen_for_qa l.551] (40/47) * Start with LLM "gpt-4"
[2024-03-04 22:18:32,212 DEBUG generators.py generate l.352] (40/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:18:32,216 DEBUG generators.py generate l.361] (40/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:18:32,903 DEBUG generators.py generate l.373] (40/47) Post-process Answer
[2024-03-04 22:18:32,905 INFO generators.py gen_for_qa l.551] (40/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 22:18:32,906 DEBUG generators.py generate l.352] (40/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:18:32,908 DEBUG generators.py generate l.361] (40/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:18:33,760 DEBUG generators.py generate l.373] (40/47) Post-process Answer
[2024-03-04 22:18:33,766 INFO generators.py gen_for_qa l.551] (40/47) * Start with LLM "gemini-pro"
[2024-03-04 22:18:33,768 DEBUG generators.py generate l.352] (40/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:18:33,770 DEBUG generators.py generate l.361] (40/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:18:36,418 DEBUG generators.py generate l.373] (40/47) Post-process Answer
[2024-03-04 22:18:36,420 INFO generators.py gen_for_qa l.551] (40/47) * Start with LLM "claude-2.1"
[2024-03-04 22:18:36,422 DEBUG generators.py generate l.352] (40/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:18:36,424 DEBUG generators.py generate l.361] (40/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:18:37,406 DEBUG generators.py generate l.373] (40/47) Post-process Answer
[2024-03-04 22:18:37,406 INFO generators.py gen_for_qa l.551] (40/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 22:18:37,423 DEBUG generators.py generate l.352] (40/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:18:37,424 DEBUG generators.py generate l.361] (40/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:18:41,313 DEBUG generators.py generate l.373] (40/47) Post-process Answer
[2024-03-04 22:18:41,313 INFO generators.py gen_for_qa l.551] (40/47) * Start with LLM "command-nightly"
[2024-03-04 22:18:41,315 DEBUG generators.py generate l.352] (40/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:18:41,318 DEBUG generators.py generate l.361] (40/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:18:41,685 DEBUG generators.py generate l.373] (40/47) Post-process Answer
[2024-03-04 22:18:41,689 INFO generators.py generate l.480] (40/47) End question "Comment réformer le système de régulation des marchés du numérique ?  A) Réglementation stricte A) Déréglementation B) Souveraineté numérique"
[2024-03-04 22:18:41,691 INFO generators.py generate l.478] (41/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés de la finance solidaire ?  A) Réglementation stricte A) Déréglementation B) Finance éthique"
[2024-03-04 22:18:41,694 INFO generators.py gen_for_qa l.551] (41/47) * Start with LLM "gpt-4"
[2024-03-04 22:18:41,695 DEBUG generators.py generate l.352] (41/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:18:41,697 DEBUG generators.py generate l.361] (41/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:18:42,581 DEBUG generators.py generate l.373] (41/47) Post-process Answer
[2024-03-04 22:18:42,586 INFO generators.py gen_for_qa l.551] (41/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 22:18:42,588 DEBUG generators.py generate l.352] (41/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:18:42,590 DEBUG generators.py generate l.361] (41/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:18:43,124 DEBUG generators.py generate l.373] (41/47) Post-process Answer
[2024-03-04 22:18:43,125 INFO generators.py gen_for_qa l.551] (41/47) * Start with LLM "gemini-pro"
[2024-03-04 22:18:43,126 DEBUG generators.py generate l.352] (41/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:18:43,127 DEBUG generators.py generate l.361] (41/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:18:45,342 DEBUG generators.py generate l.373] (41/47) Post-process Answer
[2024-03-04 22:18:45,345 INFO generators.py gen_for_qa l.551] (41/47) * Start with LLM "claude-2.1"
[2024-03-04 22:18:45,347 DEBUG generators.py generate l.352] (41/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:18:45,348 DEBUG generators.py generate l.361] (41/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:18:46,403 DEBUG generators.py generate l.373] (41/47) Post-process Answer
[2024-03-04 22:18:46,414 INFO generators.py gen_for_qa l.551] (41/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 22:18:46,416 DEBUG generators.py generate l.352] (41/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:18:46,418 DEBUG generators.py generate l.361] (41/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:18:47,304 DEBUG generators.py generate l.373] (41/47) Post-process Answer
[2024-03-04 22:18:47,304 INFO generators.py gen_for_qa l.551] (41/47) * Start with LLM "command-nightly"
[2024-03-04 22:18:47,308 DEBUG generators.py generate l.352] (41/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:18:47,309 DEBUG generators.py generate l.361] (41/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:18:47,787 DEBUG generators.py generate l.373] (41/47) Post-process Answer
[2024-03-04 22:18:47,791 INFO generators.py generate l.480] (41/47) End question "Comment réformer le système de régulation des marchés de la finance solidaire ?  A) Réglementation stricte A) Déréglementation B) Finance éthique"
[2024-03-04 22:18:47,794 INFO generators.py generate l.478] (42/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés de l'économie sociale et solidaire ?  A) Réglementation stricte A) Déréglementation B) Économie collaborative"
[2024-03-04 22:18:47,795 INFO generators.py gen_for_qa l.551] (42/47) * Start with LLM "gpt-4"
[2024-03-04 22:18:47,797 DEBUG generators.py generate l.352] (42/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:18:47,798 DEBUG generators.py generate l.361] (42/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:18:48,453 DEBUG generators.py generate l.373] (42/47) Post-process Answer
[2024-03-04 22:18:48,453 INFO generators.py gen_for_qa l.551] (42/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 22:18:48,462 DEBUG generators.py generate l.352] (42/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:18:48,465 DEBUG generators.py generate l.361] (42/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:18:49,395 DEBUG generators.py generate l.373] (42/47) Post-process Answer
[2024-03-04 22:18:49,400 INFO generators.py gen_for_qa l.551] (42/47) * Start with LLM "gemini-pro"
[2024-03-04 22:18:49,402 DEBUG generators.py generate l.352] (42/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:18:49,403 DEBUG generators.py generate l.361] (42/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:18:51,887 DEBUG generators.py generate l.373] (42/47) Post-process Answer
[2024-03-04 22:18:51,890 INFO generators.py gen_for_qa l.551] (42/47) * Start with LLM "claude-2.1"
[2024-03-04 22:18:51,891 DEBUG generators.py generate l.352] (42/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:18:51,892 DEBUG generators.py generate l.361] (42/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:18:53,143 DEBUG generators.py generate l.373] (42/47) Post-process Answer
[2024-03-04 22:18:53,148 INFO generators.py gen_for_qa l.551] (42/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 22:18:53,152 DEBUG generators.py generate l.352] (42/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:18:53,155 DEBUG generators.py generate l.361] (42/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:18:59,034 DEBUG generators.py generate l.373] (42/47) Post-process Answer
[2024-03-04 22:18:59,034 INFO generators.py gen_for_qa l.551] (42/47) * Start with LLM "command-nightly"
[2024-03-04 22:18:59,034 DEBUG generators.py generate l.352] (42/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:18:59,034 DEBUG generators.py generate l.361] (42/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:18:59,459 DEBUG generators.py generate l.373] (42/47) Post-process Answer
[2024-03-04 22:18:59,464 INFO generators.py generate l.480] (42/47) End question "Comment réformer le système de régulation des marchés de l'économie sociale et solidaire ?  A) Réglementation stricte A) Déréglementation B) Économie collaborative"
[2024-03-04 22:18:59,469 INFO generators.py generate l.478] (43/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés de la propriété intellectuelle ?  A) Réglementation stricte A) Déréglementation B) Licences libres"
[2024-03-04 22:18:59,472 INFO generators.py gen_for_qa l.551] (43/47) * Start with LLM "gpt-4"
[2024-03-04 22:18:59,473 DEBUG generators.py generate l.352] (43/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:18:59,475 DEBUG generators.py generate l.361] (43/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:19:00,092 DEBUG generators.py generate l.373] (43/47) Post-process Answer
[2024-03-04 22:19:00,092 INFO generators.py gen_for_qa l.551] (43/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 22:19:00,104 DEBUG generators.py generate l.352] (43/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:19:00,106 DEBUG generators.py generate l.361] (43/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:19:00,948 DEBUG generators.py generate l.373] (43/47) Post-process Answer
[2024-03-04 22:19:00,953 INFO generators.py gen_for_qa l.551] (43/47) * Start with LLM "gemini-pro"
[2024-03-04 22:19:00,956 DEBUG generators.py generate l.352] (43/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:19:00,958 DEBUG generators.py generate l.361] (43/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:19:03,202 DEBUG generators.py generate l.373] (43/47) Post-process Answer
[2024-03-04 22:19:03,206 INFO generators.py gen_for_qa l.551] (43/47) * Start with LLM "claude-2.1"
[2024-03-04 22:19:03,207 DEBUG generators.py generate l.352] (43/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:19:03,209 DEBUG generators.py generate l.361] (43/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:19:04,217 DEBUG generators.py generate l.373] (43/47) Post-process Answer
[2024-03-04 22:19:04,231 INFO generators.py gen_for_qa l.551] (43/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 22:19:04,232 DEBUG generators.py generate l.352] (43/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:19:04,234 DEBUG generators.py generate l.361] (43/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:19:08,436 DEBUG generators.py generate l.373] (43/47) Post-process Answer
[2024-03-04 22:19:08,437 INFO generators.py gen_for_qa l.551] (43/47) * Start with LLM "command-nightly"
[2024-03-04 22:19:08,439 DEBUG generators.py generate l.352] (43/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:19:08,441 DEBUG generators.py generate l.361] (43/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:19:08,817 DEBUG generators.py generate l.373] (43/47) Post-process Answer
[2024-03-04 22:19:08,817 INFO generators.py generate l.480] (43/47) End question "Comment réformer le système de régulation des marchés de la propriété intellectuelle ?  A) Réglementation stricte A) Déréglementation B) Licences libres"
[2024-03-04 22:19:08,832 INFO generators.py generate l.478] (44/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés de la recherche et de l'innovation ?  A) Réglementation stricte A) Déréglementation B) Open innovation"
[2024-03-04 22:19:08,836 INFO generators.py gen_for_qa l.551] (44/47) * Start with LLM "gpt-4"
[2024-03-04 22:19:08,838 DEBUG generators.py generate l.352] (44/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:19:08,841 DEBUG generators.py generate l.361] (44/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:19:10,111 DEBUG generators.py generate l.373] (44/47) Post-process Answer
[2024-03-04 22:19:10,114 INFO generators.py gen_for_qa l.551] (44/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 22:19:10,118 DEBUG generators.py generate l.352] (44/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:19:10,120 DEBUG generators.py generate l.361] (44/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:19:10,878 DEBUG generators.py generate l.373] (44/47) Post-process Answer
[2024-03-04 22:19:10,880 INFO generators.py gen_for_qa l.551] (44/47) * Start with LLM "gemini-pro"
[2024-03-04 22:19:10,884 DEBUG generators.py generate l.352] (44/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:19:10,885 DEBUG generators.py generate l.361] (44/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:19:13,372 DEBUG generators.py generate l.373] (44/47) Post-process Answer
[2024-03-04 22:19:13,375 INFO generators.py gen_for_qa l.551] (44/47) * Start with LLM "claude-2.1"
[2024-03-04 22:19:13,376 DEBUG generators.py generate l.352] (44/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:19:13,378 DEBUG generators.py generate l.361] (44/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:19:14,506 DEBUG generators.py generate l.373] (44/47) Post-process Answer
[2024-03-04 22:19:14,512 INFO generators.py gen_for_qa l.551] (44/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 22:19:14,513 DEBUG generators.py generate l.352] (44/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:19:14,516 DEBUG generators.py generate l.361] (44/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:19:15,042 DEBUG generators.py generate l.373] (44/47) Post-process Answer
[2024-03-04 22:19:15,046 INFO generators.py gen_for_qa l.551] (44/47) * Start with LLM "command-nightly"
[2024-03-04 22:19:15,048 DEBUG generators.py generate l.352] (44/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:19:15,050 DEBUG generators.py generate l.361] (44/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:19:15,482 DEBUG generators.py generate l.373] (44/47) Post-process Answer
[2024-03-04 22:19:15,485 INFO generators.py generate l.480] (44/47) End question "Comment réformer le système de régulation des marchés de la recherche et de l'innovation ?  A) Réglementation stricte A) Déréglementation B) Open innovation"
[2024-03-04 22:19:15,486 INFO generators.py generate l.478] (45/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés de la défense et de la sécurité ?  A) Réglementation stricte A) Déréglementation B) Sécurité globale"
[2024-03-04 22:19:15,488 INFO generators.py gen_for_qa l.551] (45/47) * Start with LLM "gpt-4"
[2024-03-04 22:19:15,490 DEBUG generators.py generate l.352] (45/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:19:15,491 DEBUG generators.py generate l.361] (45/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:19:16,336 DEBUG generators.py generate l.373] (45/47) Post-process Answer
[2024-03-04 22:19:16,338 INFO generators.py gen_for_qa l.551] (45/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 22:19:16,340 DEBUG generators.py generate l.352] (45/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:19:16,342 DEBUG generators.py generate l.361] (45/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:19:17,117 DEBUG generators.py generate l.373] (45/47) Post-process Answer
[2024-03-04 22:19:17,120 INFO generators.py gen_for_qa l.551] (45/47) * Start with LLM "gemini-pro"
[2024-03-04 22:19:17,123 DEBUG generators.py generate l.352] (45/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:19:17,125 DEBUG generators.py generate l.361] (45/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:19:19,698 DEBUG generators.py generate l.373] (45/47) Post-process Answer
[2024-03-04 22:19:19,700 INFO generators.py gen_for_qa l.551] (45/47) * Start with LLM "claude-2.1"
[2024-03-04 22:19:19,702 DEBUG generators.py generate l.352] (45/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:19:19,703 DEBUG generators.py generate l.361] (45/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:19:21,040 DEBUG generators.py generate l.373] (45/47) Post-process Answer
[2024-03-04 22:19:21,049 INFO generators.py gen_for_qa l.551] (45/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 22:19:21,053 DEBUG generators.py generate l.352] (45/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:19:21,057 DEBUG generators.py generate l.361] (45/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:19:21,760 DEBUG generators.py generate l.373] (45/47) Post-process Answer
[2024-03-04 22:19:21,766 INFO generators.py gen_for_qa l.551] (45/47) * Start with LLM "command-nightly"
[2024-03-04 22:19:21,768 DEBUG generators.py generate l.352] (45/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:19:21,769 DEBUG generators.py generate l.361] (45/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:19:22,183 DEBUG generators.py generate l.373] (45/47) Post-process Answer
[2024-03-04 22:19:22,188 INFO generators.py generate l.480] (45/47) End question "Comment réformer le système de régulation des marchés de la défense et de la sécurité ?  A) Réglementation stricte A) Déréglementation B) Sécurité globale"
[2024-03-04 22:19:22,192 INFO generators.py generate l.478] (46/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés de la coopération internationale ?  A) Réglementation stricte A) Déréglementation B) Diplomatie économique"
[2024-03-04 22:19:22,195 INFO generators.py gen_for_qa l.551] (46/47) * Start with LLM "gpt-4"
[2024-03-04 22:19:22,197 DEBUG generators.py generate l.352] (46/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:19:22,200 DEBUG generators.py generate l.361] (46/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:19:22,881 DEBUG generators.py generate l.373] (46/47) Post-process Answer
[2024-03-04 22:19:22,888 INFO generators.py gen_for_qa l.551] (46/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 22:19:22,892 DEBUG generators.py generate l.352] (46/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:19:22,894 DEBUG generators.py generate l.361] (46/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:19:23,932 DEBUG generators.py generate l.373] (46/47) Post-process Answer
[2024-03-04 22:19:23,935 INFO generators.py gen_for_qa l.551] (46/47) * Start with LLM "gemini-pro"
[2024-03-04 22:19:23,935 DEBUG generators.py generate l.352] (46/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:19:23,938 DEBUG generators.py generate l.361] (46/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:19:26,332 DEBUG generators.py generate l.373] (46/47) Post-process Answer
[2024-03-04 22:19:26,336 INFO generators.py gen_for_qa l.551] (46/47) * Start with LLM "claude-2.1"
[2024-03-04 22:19:26,337 DEBUG generators.py generate l.352] (46/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:19:26,338 DEBUG generators.py generate l.361] (46/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:19:27,614 DEBUG generators.py generate l.373] (46/47) Post-process Answer
[2024-03-04 22:19:27,616 INFO generators.py gen_for_qa l.551] (46/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 22:19:27,619 DEBUG generators.py generate l.352] (46/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:19:27,622 DEBUG generators.py generate l.361] (46/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:19:32,246 DEBUG generators.py generate l.373] (46/47) Post-process Answer
[2024-03-04 22:19:32,247 INFO generators.py gen_for_qa l.551] (46/47) * Start with LLM "command-nightly"
[2024-03-04 22:19:32,248 DEBUG generators.py generate l.352] (46/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:19:32,250 DEBUG generators.py generate l.361] (46/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:19:32,944 DEBUG generators.py generate l.373] (46/47) Post-process Answer
[2024-03-04 22:19:32,949 INFO generators.py generate l.480] (46/47) End question "Comment réformer le système de régulation des marchés de la coopération internationale ?  A) Réglementation stricte A) Déréglementation B) Diplomatie économique"
[2024-03-04 22:19:32,951 INFO generators.py generate l.478] (47/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés de la gouvernance mondiale ?  A) Réglementation stricte A) Déréglementation B) Gouvernance multipartite"
[2024-03-04 22:19:32,954 INFO generators.py gen_for_qa l.551] (47/47) * Start with LLM "gpt-4"
[2024-03-04 22:19:32,956 DEBUG generators.py generate l.352] (47/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:19:32,958 DEBUG generators.py generate l.361] (47/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:19:34,005 DEBUG generators.py generate l.373] (47/47) Post-process Answer
[2024-03-04 22:19:34,008 INFO generators.py gen_for_qa l.551] (47/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-04 22:19:34,011 DEBUG generators.py generate l.352] (47/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:19:34,013 DEBUG generators.py generate l.361] (47/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:19:34,855 DEBUG generators.py generate l.373] (47/47) Post-process Answer
[2024-03-04 22:19:34,855 INFO generators.py gen_for_qa l.551] (47/47) * Start with LLM "gemini-pro"
[2024-03-04 22:19:34,866 DEBUG generators.py generate l.352] (47/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:19:34,869 DEBUG generators.py generate l.361] (47/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:19:37,869 DEBUG generators.py generate l.373] (47/47) Post-process Answer
[2024-03-04 22:19:37,871 INFO generators.py gen_for_qa l.551] (47/47) * Start with LLM "claude-2.1"
[2024-03-04 22:19:37,873 DEBUG generators.py generate l.352] (47/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:19:37,874 DEBUG generators.py generate l.361] (47/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:19:39,206 DEBUG generators.py generate l.373] (47/47) Post-process Answer
[2024-03-04 22:19:39,218 INFO generators.py gen_for_qa l.551] (47/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-04 22:19:39,220 DEBUG generators.py generate l.352] (47/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:19:39,222 DEBUG generators.py generate l.361] (47/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:19:39,956 DEBUG generators.py generate l.373] (47/47) Post-process Answer
[2024-03-04 22:19:39,957 INFO generators.py gen_for_qa l.551] (47/47) * Start with LLM "command-nightly"
[2024-03-04 22:19:39,959 DEBUG generators.py generate l.352] (47/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-04 22:19:39,961 DEBUG generators.py generate l.361] (47/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-04 22:19:40,696 DEBUG generators.py generate l.373] (47/47) Post-process Answer
[2024-03-04 22:19:40,709 INFO generators.py generate l.480] (47/47) End question "Comment réformer le système de régulation des marchés de la gouvernance mondiale ?  A) Réglementation stricte A) Déréglementation B) Gouvernance multipartite"
[2024-03-04 22:19:40,720 INFO expe.py save_to_json l.283] (47/47) Expe saved as JSON to expe\Answers\eco_fr_v1_gen_with_LeChat--47Q_0C_0F_6M_282A_0HE_0AE_2024-03-04_22,19,40.json
[2024-03-04 22:19:40,721 INFO main.py <module> l.96] (47/47) MAIN ENDS
[2024-03-05 09:00:38,894 INFO main.py <module> l.81] MAIN STARTS
[2024-03-05 09:01:47,199 INFO main.py <module> l.81] MAIN STARTS
[2024-03-05 09:01:47,209 INFO generators.py generate l.478] (1/47) *** AnsGenerator for question "Quel est le meilleur système économique ?  A) Capitalisme B) Socialisme démocratique C) Économie mixte"
[2024-03-05 09:01:47,211 INFO generators.py gen_for_qa l.551] (1/47) * Start with LLM "gpt-4"
[2024-03-05 09:01:47,213 DEBUG generators.py generate l.352] (1/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:01:47,216 DEBUG generators.py generate l.361] (1/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:01:47,924 DEBUG generators.py generate l.373] (1/47) Post-process Answer
[2024-03-05 09:01:47,926 INFO generators.py gen_for_qa l.551] (1/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 09:01:47,926 DEBUG generators.py generate l.352] (1/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:01:47,926 DEBUG generators.py generate l.361] (1/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:01:48,409 DEBUG generators.py generate l.373] (1/47) Post-process Answer
[2024-03-05 09:01:48,411 INFO generators.py gen_for_qa l.551] (1/47) * Start with LLM "gemini-pro"
[2024-03-05 09:01:48,413 DEBUG generators.py generate l.352] (1/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:01:48,414 DEBUG generators.py generate l.361] (1/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:02:03,183 DEBUG generators.py generate l.373] (1/47) Post-process Answer
[2024-03-05 09:02:03,189 INFO generators.py gen_for_qa l.551] (1/47) * Start with LLM "claude-2.1"
[2024-03-05 09:02:03,193 DEBUG generators.py generate l.352] (1/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:02:03,195 DEBUG generators.py generate l.361] (1/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:02:04,185 DEBUG generators.py generate l.373] (1/47) Post-process Answer
[2024-03-05 09:02:04,188 INFO generators.py gen_for_qa l.551] (1/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 09:02:04,189 DEBUG generators.py generate l.352] (1/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:02:04,192 DEBUG generators.py generate l.361] (1/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:02:07,486 DEBUG generators.py generate l.373] (1/47) Post-process Answer
[2024-03-05 09:02:07,489 INFO generators.py gen_for_qa l.551] (1/47) * Start with LLM "command-nightly"
[2024-03-05 09:02:07,491 DEBUG generators.py generate l.352] (1/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:02:07,494 DEBUG generators.py generate l.361] (1/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:02:08,019 DEBUG generators.py generate l.373] (1/47) Post-process Answer
[2024-03-05 09:02:08,020 INFO generators.py generate l.480] (1/47) End question "Quel est le meilleur système économique ?  A) Capitalisme B) Socialisme démocratique C) Économie mixte"
[2024-03-05 09:02:08,022 INFO generators.py generate l.478] (2/47) *** AnsGenerator for question "Quel est le rôle de l'État dans l'économie ?  A) Interventionnisme B) Libéralisme économique C) Néo-keynésianisme"
[2024-03-05 09:02:08,024 INFO generators.py gen_for_qa l.551] (2/47) * Start with LLM "gpt-4"
[2024-03-05 09:02:08,025 DEBUG generators.py generate l.352] (2/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:02:08,027 DEBUG generators.py generate l.361] (2/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:02:08,684 DEBUG generators.py generate l.373] (2/47) Post-process Answer
[2024-03-05 09:02:08,685 INFO generators.py gen_for_qa l.551] (2/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 09:02:08,687 DEBUG generators.py generate l.352] (2/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:02:08,688 DEBUG generators.py generate l.361] (2/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:02:09,180 DEBUG generators.py generate l.373] (2/47) Post-process Answer
[2024-03-05 09:02:09,186 INFO generators.py gen_for_qa l.551] (2/47) * Start with LLM "gemini-pro"
[2024-03-05 09:02:09,188 DEBUG generators.py generate l.352] (2/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:02:09,190 DEBUG generators.py generate l.361] (2/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:02:11,516 DEBUG generators.py generate l.373] (2/47) Post-process Answer
[2024-03-05 09:02:11,519 INFO generators.py gen_for_qa l.551] (2/47) * Start with LLM "claude-2.1"
[2024-03-05 09:02:11,523 DEBUG generators.py generate l.352] (2/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:02:11,525 DEBUG generators.py generate l.361] (2/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:02:12,391 DEBUG generators.py generate l.373] (2/47) Post-process Answer
[2024-03-05 09:02:12,394 INFO generators.py gen_for_qa l.551] (2/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 09:02:12,397 DEBUG generators.py generate l.352] (2/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:02:12,399 DEBUG generators.py generate l.361] (2/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:02:17,849 DEBUG generators.py generate l.373] (2/47) Post-process Answer
[2024-03-05 09:02:17,865 INFO generators.py gen_for_qa l.551] (2/47) * Start with LLM "command-nightly"
[2024-03-05 09:02:17,865 DEBUG generators.py generate l.352] (2/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:02:17,865 DEBUG generators.py generate l.361] (2/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:02:18,478 DEBUG generators.py generate l.373] (2/47) Post-process Answer
[2024-03-05 09:02:18,478 INFO generators.py generate l.480] (2/47) End question "Quel est le rôle de l'État dans l'économie ?  A) Interventionnisme B) Libéralisme économique C) Néo-keynésianisme"
[2024-03-05 09:02:18,478 INFO generators.py generate l.478] (3/47) *** AnsGenerator for question "Comment réduire les inégalités ?  A) Redistribution B) Croissance économique C) Investissement social"
[2024-03-05 09:02:18,478 INFO generators.py gen_for_qa l.551] (3/47) * Start with LLM "gpt-4"
[2024-03-05 09:02:18,495 DEBUG generators.py generate l.352] (3/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:02:18,495 DEBUG generators.py generate l.361] (3/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:02:19,206 DEBUG generators.py generate l.373] (3/47) Post-process Answer
[2024-03-05 09:02:19,222 INFO generators.py gen_for_qa l.551] (3/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 09:02:19,222 DEBUG generators.py generate l.352] (3/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:02:19,222 DEBUG generators.py generate l.361] (3/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:02:20,062 DEBUG generators.py generate l.373] (3/47) Post-process Answer
[2024-03-05 09:02:20,062 INFO generators.py gen_for_qa l.551] (3/47) * Start with LLM "gemini-pro"
[2024-03-05 09:02:20,062 DEBUG generators.py generate l.352] (3/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:02:20,080 DEBUG generators.py generate l.361] (3/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:02:22,485 DEBUG generators.py generate l.373] (3/47) Post-process Answer
[2024-03-05 09:02:22,501 INFO generators.py gen_for_qa l.551] (3/47) * Start with LLM "claude-2.1"
[2024-03-05 09:02:22,501 DEBUG generators.py generate l.352] (3/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:02:22,501 DEBUG generators.py generate l.361] (3/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:02:23,569 DEBUG generators.py generate l.373] (3/47) Post-process Answer
[2024-03-05 09:02:23,569 INFO generators.py gen_for_qa l.551] (3/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 09:02:23,569 DEBUG generators.py generate l.352] (3/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:02:23,569 DEBUG generators.py generate l.361] (3/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:02:26,374 DEBUG generators.py generate l.373] (3/47) Post-process Answer
[2024-03-05 09:02:26,390 INFO generators.py gen_for_qa l.551] (3/47) * Start with LLM "command-nightly"
[2024-03-05 09:02:26,393 DEBUG generators.py generate l.352] (3/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:02:26,396 DEBUG generators.py generate l.361] (3/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:02:26,806 DEBUG generators.py generate l.373] (3/47) Post-process Answer
[2024-03-05 09:02:26,817 INFO generators.py generate l.480] (3/47) End question "Comment réduire les inégalités ?  A) Redistribution B) Croissance économique C) Investissement social"
[2024-03-05 09:02:26,817 INFO generators.py generate l.478] (4/47) *** AnsGenerator for question "Comment stimuler la croissance économique ?  A) Investissement public B) Déréglementation C) Innovation technologique"
[2024-03-05 09:02:26,825 INFO generators.py gen_for_qa l.551] (4/47) * Start with LLM "gpt-4"
[2024-03-05 09:02:26,833 DEBUG generators.py generate l.352] (4/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:02:26,833 DEBUG generators.py generate l.361] (4/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:02:27,475 DEBUG generators.py generate l.373] (4/47) Post-process Answer
[2024-03-05 09:02:27,475 INFO generators.py gen_for_qa l.551] (4/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 09:02:27,475 DEBUG generators.py generate l.352] (4/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:02:27,491 DEBUG generators.py generate l.361] (4/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:02:28,103 DEBUG generators.py generate l.373] (4/47) Post-process Answer
[2024-03-05 09:02:28,118 INFO generators.py gen_for_qa l.551] (4/47) * Start with LLM "gemini-pro"
[2024-03-05 09:02:28,119 DEBUG generators.py generate l.352] (4/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:02:28,126 DEBUG generators.py generate l.361] (4/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:02:30,654 DEBUG generators.py generate l.373] (4/47) Post-process Answer
[2024-03-05 09:02:30,669 INFO generators.py gen_for_qa l.551] (4/47) * Start with LLM "claude-2.1"
[2024-03-05 09:02:30,677 DEBUG generators.py generate l.352] (4/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:02:30,680 DEBUG generators.py generate l.361] (4/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:02:31,766 DEBUG generators.py generate l.373] (4/47) Post-process Answer
[2024-03-05 09:02:31,781 INFO generators.py gen_for_qa l.551] (4/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 09:02:31,782 DEBUG generators.py generate l.352] (4/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:02:31,782 DEBUG generators.py generate l.361] (4/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:02:34,504 DEBUG generators.py generate l.373] (4/47) Post-process Answer
[2024-03-05 09:02:34,508 INFO generators.py gen_for_qa l.551] (4/47) * Start with LLM "command-nightly"
[2024-03-05 09:02:34,512 DEBUG generators.py generate l.352] (4/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:02:34,515 DEBUG generators.py generate l.361] (4/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:02:34,985 DEBUG generators.py generate l.373] (4/47) Post-process Answer
[2024-03-05 09:02:34,993 INFO generators.py generate l.480] (4/47) End question "Comment stimuler la croissance économique ?  A) Investissement public B) Déréglementation C) Innovation technologique"
[2024-03-05 09:02:34,997 INFO generators.py generate l.478] (5/47) *** AnsGenerator for question "Comment lutter contre l'inflation ?  A) Contrôle des prix et des salaires B) Politique monétaire restrictive C) Indexation des salaires"
[2024-03-05 09:02:35,002 INFO generators.py gen_for_qa l.551] (5/47) * Start with LLM "gpt-4"
[2024-03-05 09:02:35,006 DEBUG generators.py generate l.352] (5/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:02:35,010 DEBUG generators.py generate l.361] (5/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:02:35,739 DEBUG generators.py generate l.373] (5/47) Post-process Answer
[2024-03-05 09:02:35,760 INFO generators.py gen_for_qa l.551] (5/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 09:02:35,760 DEBUG generators.py generate l.352] (5/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:02:35,773 DEBUG generators.py generate l.361] (5/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:02:36,874 DEBUG generators.py generate l.373] (5/47) Post-process Answer
[2024-03-05 09:02:36,874 INFO generators.py gen_for_qa l.551] (5/47) * Start with LLM "gemini-pro"
[2024-03-05 09:02:36,874 DEBUG generators.py generate l.352] (5/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:02:36,890 DEBUG generators.py generate l.361] (5/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:02:39,524 DEBUG generators.py generate l.373] (5/47) Post-process Answer
[2024-03-05 09:02:39,526 INFO generators.py gen_for_qa l.551] (5/47) * Start with LLM "claude-2.1"
[2024-03-05 09:02:39,528 DEBUG generators.py generate l.352] (5/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:02:39,531 DEBUG generators.py generate l.361] (5/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:02:40,441 DEBUG generators.py generate l.373] (5/47) Post-process Answer
[2024-03-05 09:02:40,444 INFO generators.py gen_for_qa l.551] (5/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 09:02:40,445 DEBUG generators.py generate l.352] (5/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:02:40,447 DEBUG generators.py generate l.361] (5/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:02:40,943 DEBUG generators.py generate l.373] (5/47) Post-process Answer
[2024-03-05 09:02:40,946 INFO generators.py gen_for_qa l.551] (5/47) * Start with LLM "command-nightly"
[2024-03-05 09:02:40,947 DEBUG generators.py generate l.352] (5/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:02:40,949 DEBUG generators.py generate l.361] (5/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:02:41,428 DEBUG generators.py generate l.373] (5/47) Post-process Answer
[2024-03-05 09:02:41,430 INFO generators.py generate l.480] (5/47) End question "Comment lutter contre l'inflation ?  A) Contrôle des prix et des salaires B) Politique monétaire restrictive C) Indexation des salaires"
[2024-03-05 09:02:41,433 INFO generators.py generate l.478] (6/47) *** AnsGenerator for question "Comment lutter contre le chômage ?  A) Politique de l'emploi B) Flexibilisation du marché du travail C) Formation professionnelle"
[2024-03-05 09:02:41,435 INFO generators.py gen_for_qa l.551] (6/47) * Start with LLM "gpt-4"
[2024-03-05 09:02:41,437 DEBUG generators.py generate l.352] (6/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:02:41,439 DEBUG generators.py generate l.361] (6/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:02:42,121 DEBUG generators.py generate l.373] (6/47) Post-process Answer
[2024-03-05 09:02:42,122 INFO generators.py gen_for_qa l.551] (6/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 09:02:42,125 DEBUG generators.py generate l.352] (6/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:02:42,127 DEBUG generators.py generate l.361] (6/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:02:43,522 DEBUG generators.py generate l.373] (6/47) Post-process Answer
[2024-03-05 09:02:43,522 INFO generators.py gen_for_qa l.551] (6/47) * Start with LLM "gemini-pro"
[2024-03-05 09:02:43,522 DEBUG generators.py generate l.352] (6/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:02:43,533 DEBUG generators.py generate l.361] (6/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:02:46,838 DEBUG generators.py generate l.373] (6/47) Post-process Answer
[2024-03-05 09:02:46,840 INFO generators.py gen_for_qa l.551] (6/47) * Start with LLM "claude-2.1"
[2024-03-05 09:02:46,842 DEBUG generators.py generate l.352] (6/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:02:46,843 DEBUG generators.py generate l.361] (6/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:02:47,840 DEBUG generators.py generate l.373] (6/47) Post-process Answer
[2024-03-05 09:02:47,840 INFO generators.py gen_for_qa l.551] (6/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 09:02:47,840 DEBUG generators.py generate l.352] (6/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:02:47,851 DEBUG generators.py generate l.361] (6/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:02:49,494 DEBUG generators.py generate l.373] (6/47) Post-process Answer
[2024-03-05 09:02:49,496 INFO generators.py gen_for_qa l.551] (6/47) * Start with LLM "command-nightly"
[2024-03-05 09:02:49,496 DEBUG generators.py generate l.352] (6/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:02:49,496 DEBUG generators.py generate l.361] (6/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:02:49,965 DEBUG generators.py generate l.373] (6/47) Post-process Answer
[2024-03-05 09:02:49,981 INFO generators.py generate l.480] (6/47) End question "Comment lutter contre le chômage ?  A) Politique de l'emploi B) Flexibilisation du marché du travail C) Formation professionnelle"
[2024-03-05 09:02:49,981 INFO generators.py generate l.478] (7/47) *** AnsGenerator for question "Comment réformer le système fiscal ?  A) Impôt progressif B) Flat tax C) TVA sociale"
[2024-03-05 09:02:49,994 INFO generators.py gen_for_qa l.551] (7/47) * Start with LLM "gpt-4"
[2024-03-05 09:02:49,997 DEBUG generators.py generate l.352] (7/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:02:50,006 DEBUG generators.py generate l.361] (7/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:02:50,691 DEBUG generators.py generate l.373] (7/47) Post-process Answer
[2024-03-05 09:02:50,691 INFO generators.py gen_for_qa l.551] (7/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 09:02:50,707 DEBUG generators.py generate l.352] (7/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:02:50,707 DEBUG generators.py generate l.361] (7/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:02:51,813 DEBUG generators.py generate l.373] (7/47) Post-process Answer
[2024-03-05 09:02:51,813 INFO generators.py gen_for_qa l.551] (7/47) * Start with LLM "gemini-pro"
[2024-03-05 09:02:51,829 DEBUG generators.py generate l.352] (7/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:02:51,829 DEBUG generators.py generate l.361] (7/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:02:54,268 DEBUG generators.py generate l.373] (7/47) Post-process Answer
[2024-03-05 09:02:54,268 INFO generators.py gen_for_qa l.551] (7/47) * Start with LLM "claude-2.1"
[2024-03-05 09:02:54,275 DEBUG generators.py generate l.352] (7/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:02:54,277 DEBUG generators.py generate l.361] (7/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:02:55,307 DEBUG generators.py generate l.373] (7/47) Post-process Answer
[2024-03-05 09:02:55,310 INFO generators.py gen_for_qa l.551] (7/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 09:02:55,311 DEBUG generators.py generate l.352] (7/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:02:55,313 DEBUG generators.py generate l.361] (7/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:02:58,149 DEBUG generators.py generate l.373] (7/47) Post-process Answer
[2024-03-05 09:02:58,152 INFO generators.py gen_for_qa l.551] (7/47) * Start with LLM "command-nightly"
[2024-03-05 09:02:58,153 DEBUG generators.py generate l.352] (7/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:02:58,155 DEBUG generators.py generate l.361] (7/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:02:58,656 DEBUG generators.py generate l.373] (7/47) Post-process Answer
[2024-03-05 09:02:58,658 INFO generators.py generate l.480] (7/47) End question "Comment réformer le système fiscal ?  A) Impôt progressif B) Flat tax C) TVA sociale"
[2024-03-05 09:02:58,661 INFO generators.py generate l.478] (8/47) *** AnsGenerator for question "Comment réformer le système de protection sociale ?  A) Protection sociale universelle B) Responsabilisation individuelle C) Assurance privée"
[2024-03-05 09:02:58,663 INFO generators.py gen_for_qa l.551] (8/47) * Start with LLM "gpt-4"
[2024-03-05 09:02:58,664 DEBUG generators.py generate l.352] (8/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:02:58,667 DEBUG generators.py generate l.361] (8/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:02:59,352 DEBUG generators.py generate l.373] (8/47) Post-process Answer
[2024-03-05 09:02:59,355 INFO generators.py gen_for_qa l.551] (8/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 09:02:59,357 DEBUG generators.py generate l.352] (8/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:02:59,358 DEBUG generators.py generate l.361] (8/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:02:59,811 DEBUG generators.py generate l.373] (8/47) Post-process Answer
[2024-03-05 09:02:59,814 INFO generators.py gen_for_qa l.551] (8/47) * Start with LLM "gemini-pro"
[2024-03-05 09:02:59,816 DEBUG generators.py generate l.352] (8/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:02:59,820 DEBUG generators.py generate l.361] (8/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:03:02,633 DEBUG generators.py generate l.373] (8/47) Post-process Answer
[2024-03-05 09:03:02,638 INFO generators.py gen_for_qa l.551] (8/47) * Start with LLM "claude-2.1"
[2024-03-05 09:03:02,638 DEBUG generators.py generate l.352] (8/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:03:02,649 DEBUG generators.py generate l.361] (8/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:03:03,813 DEBUG generators.py generate l.373] (8/47) Post-process Answer
[2024-03-05 09:03:03,813 INFO generators.py gen_for_qa l.551] (8/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 09:03:03,832 DEBUG generators.py generate l.352] (8/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:03:03,836 DEBUG generators.py generate l.361] (8/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:03:04,370 DEBUG generators.py generate l.373] (8/47) Post-process Answer
[2024-03-05 09:03:04,376 INFO generators.py gen_for_qa l.551] (8/47) * Start with LLM "command-nightly"
[2024-03-05 09:03:04,378 DEBUG generators.py generate l.352] (8/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:03:04,378 DEBUG generators.py generate l.361] (8/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:03:04,813 DEBUG generators.py generate l.373] (8/47) Post-process Answer
[2024-03-05 09:03:04,820 INFO generators.py generate l.480] (8/47) End question "Comment réformer le système de protection sociale ?  A) Protection sociale universelle B) Responsabilisation individuelle C) Assurance privée"
[2024-03-05 09:03:04,820 INFO generators.py generate l.478] (9/47) *** AnsGenerator for question "Comment réformer le système de retraite ?  A) Retraite par répartition B) Retraite par capitalisation C) Retraite à points"
[2024-03-05 09:03:04,825 INFO generators.py gen_for_qa l.551] (9/47) * Start with LLM "gpt-4"
[2024-03-05 09:03:04,825 DEBUG generators.py generate l.352] (9/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:03:04,825 DEBUG generators.py generate l.361] (9/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:03:05,563 DEBUG generators.py generate l.373] (9/47) Post-process Answer
[2024-03-05 09:03:05,578 INFO generators.py gen_for_qa l.551] (9/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 09:03:05,578 DEBUG generators.py generate l.352] (9/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:03:05,583 DEBUG generators.py generate l.361] (9/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:03:06,259 DEBUG generators.py generate l.373] (9/47) Post-process Answer
[2024-03-05 09:03:06,275 INFO generators.py gen_for_qa l.551] (9/47) * Start with LLM "gemini-pro"
[2024-03-05 09:03:06,275 DEBUG generators.py generate l.352] (9/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:03:06,275 DEBUG generators.py generate l.361] (9/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:03:08,656 DEBUG generators.py generate l.373] (9/47) Post-process Answer
[2024-03-05 09:03:08,656 INFO generators.py gen_for_qa l.551] (9/47) * Start with LLM "claude-2.1"
[2024-03-05 09:03:08,672 DEBUG generators.py generate l.352] (9/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:03:08,672 DEBUG generators.py generate l.361] (9/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:03:09,606 DEBUG generators.py generate l.373] (9/47) Post-process Answer
[2024-03-05 09:03:09,606 INFO generators.py gen_for_qa l.551] (9/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 09:03:09,621 DEBUG generators.py generate l.352] (9/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:03:09,621 DEBUG generators.py generate l.361] (9/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:03:11,813 DEBUG generators.py generate l.373] (9/47) Post-process Answer
[2024-03-05 09:03:11,817 INFO generators.py gen_for_qa l.551] (9/47) * Start with LLM "command-nightly"
[2024-03-05 09:03:11,821 DEBUG generators.py generate l.352] (9/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:03:11,826 DEBUG generators.py generate l.361] (9/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:03:12,295 DEBUG generators.py generate l.373] (9/47) Post-process Answer
[2024-03-05 09:03:12,300 INFO generators.py generate l.480] (9/47) End question "Comment réformer le système de retraite ?  A) Retraite par répartition B) Retraite par capitalisation C) Retraite à points"
[2024-03-05 09:03:12,302 INFO generators.py generate l.478] (10/47) *** AnsGenerator for question "Comment réformer le marché du travail ?  A) Sécurisation de l'emploi B) Flexibilisation du marché du travail C) Compte personnel d'activité"
[2024-03-05 09:03:12,305 INFO generators.py gen_for_qa l.551] (10/47) * Start with LLM "gpt-4"
[2024-03-05 09:03:12,309 DEBUG generators.py generate l.352] (10/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:03:12,312 DEBUG generators.py generate l.361] (10/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:03:12,971 DEBUG generators.py generate l.373] (10/47) Post-process Answer
[2024-03-05 09:03:12,977 INFO generators.py gen_for_qa l.551] (10/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 09:03:12,977 DEBUG generators.py generate l.352] (10/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:03:12,977 DEBUG generators.py generate l.361] (10/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:03:13,737 DEBUG generators.py generate l.373] (10/47) Post-process Answer
[2024-03-05 09:03:13,737 INFO generators.py gen_for_qa l.551] (10/47) * Start with LLM "gemini-pro"
[2024-03-05 09:03:13,737 DEBUG generators.py generate l.352] (10/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:03:13,737 DEBUG generators.py generate l.361] (10/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:03:16,411 DEBUG generators.py generate l.373] (10/47) Post-process Answer
[2024-03-05 09:03:16,427 INFO generators.py gen_for_qa l.551] (10/47) * Start with LLM "claude-2.1"
[2024-03-05 09:03:16,433 DEBUG generators.py generate l.352] (10/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:03:16,433 DEBUG generators.py generate l.361] (10/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:03:17,376 DEBUG generators.py generate l.373] (10/47) Post-process Answer
[2024-03-05 09:03:17,392 INFO generators.py gen_for_qa l.551] (10/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 09:03:17,399 DEBUG generators.py generate l.352] (10/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:03:17,399 DEBUG generators.py generate l.361] (10/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:03:19,014 DEBUG generators.py generate l.373] (10/47) Post-process Answer
[2024-03-05 09:03:19,014 INFO generators.py gen_for_qa l.551] (10/47) * Start with LLM "command-nightly"
[2024-03-05 09:03:19,014 DEBUG generators.py generate l.352] (10/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:03:19,027 DEBUG generators.py generate l.361] (10/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:03:19,477 DEBUG generators.py generate l.373] (10/47) Post-process Answer
[2024-03-05 09:03:19,477 INFO generators.py generate l.480] (10/47) End question "Comment réformer le marché du travail ?  A) Sécurisation de l'emploi B) Flexibilisation du marché du travail C) Compte personnel d'activité"
[2024-03-05 09:03:19,490 INFO generators.py generate l.478] (11/47) *** AnsGenerator for question "Comment réformer le système éducatif ?  A) Éducation gratuite et obligatoire B) Libéralisation de l'éducation C) Formation professionnelle"
[2024-03-05 09:03:19,492 INFO generators.py gen_for_qa l.551] (11/47) * Start with LLM "gpt-4"
[2024-03-05 09:03:19,496 DEBUG generators.py generate l.352] (11/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:03:19,496 DEBUG generators.py generate l.361] (11/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:03:20,228 DEBUG generators.py generate l.373] (11/47) Post-process Answer
[2024-03-05 09:03:20,231 INFO generators.py gen_for_qa l.551] (11/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 09:03:20,242 DEBUG generators.py generate l.352] (11/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:03:20,243 DEBUG generators.py generate l.361] (11/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:03:21,193 DEBUG generators.py generate l.373] (11/47) Post-process Answer
[2024-03-05 09:03:21,193 INFO generators.py gen_for_qa l.551] (11/47) * Start with LLM "gemini-pro"
[2024-03-05 09:03:21,193 DEBUG generators.py generate l.352] (11/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:03:21,212 DEBUG generators.py generate l.361] (11/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:03:23,730 DEBUG generators.py generate l.373] (11/47) Post-process Answer
[2024-03-05 09:03:23,743 INFO generators.py gen_for_qa l.551] (11/47) * Start with LLM "claude-2.1"
[2024-03-05 09:03:23,748 DEBUG generators.py generate l.352] (11/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:03:23,748 DEBUG generators.py generate l.361] (11/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:03:24,809 DEBUG generators.py generate l.373] (11/47) Post-process Answer
[2024-03-05 09:03:24,817 INFO generators.py gen_for_qa l.551] (11/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 09:03:24,817 DEBUG generators.py generate l.352] (11/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:03:24,817 DEBUG generators.py generate l.361] (11/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:03:27,390 DEBUG generators.py generate l.373] (11/47) Post-process Answer
[2024-03-05 09:03:27,406 INFO generators.py gen_for_qa l.551] (11/47) * Start with LLM "command-nightly"
[2024-03-05 09:03:27,406 DEBUG generators.py generate l.352] (11/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:03:27,416 DEBUG generators.py generate l.361] (11/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:03:27,855 DEBUG generators.py generate l.373] (11/47) Post-process Answer
[2024-03-05 09:03:27,872 INFO generators.py generate l.480] (11/47) End question "Comment réformer le système éducatif ?  A) Éducation gratuite et obligatoire B) Libéralisation de l'éducation C) Formation professionnelle"
[2024-03-05 09:03:27,872 INFO generators.py generate l.478] (12/47) *** AnsGenerator for question "Comment réformer le système de santé ?  A) Système de santé public B) Système de santé privé C) Assurance maladie obligatoire"
[2024-03-05 09:03:27,879 INFO generators.py gen_for_qa l.551] (12/47) * Start with LLM "gpt-4"
[2024-03-05 09:03:27,879 DEBUG generators.py generate l.352] (12/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:03:27,884 DEBUG generators.py generate l.361] (12/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:03:29,808 DEBUG generators.py generate l.373] (12/47) Post-process Answer
[2024-03-05 09:03:29,810 INFO generators.py gen_for_qa l.551] (12/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 09:03:29,810 DEBUG generators.py generate l.352] (12/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:03:29,813 DEBUG generators.py generate l.361] (12/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:03:30,628 DEBUG generators.py generate l.373] (12/47) Post-process Answer
[2024-03-05 09:03:30,628 INFO generators.py gen_for_qa l.551] (12/47) * Start with LLM "gemini-pro"
[2024-03-05 09:03:30,647 DEBUG generators.py generate l.352] (12/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:03:30,652 DEBUG generators.py generate l.361] (12/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:03:33,349 DEBUG generators.py generate l.373] (12/47) Post-process Answer
[2024-03-05 09:03:33,354 INFO generators.py gen_for_qa l.551] (12/47) * Start with LLM "claude-2.1"
[2024-03-05 09:03:33,359 DEBUG generators.py generate l.352] (12/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:03:33,363 DEBUG generators.py generate l.361] (12/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:03:34,479 DEBUG generators.py generate l.373] (12/47) Post-process Answer
[2024-03-05 09:03:34,483 INFO generators.py gen_for_qa l.551] (12/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 09:03:34,485 DEBUG generators.py generate l.352] (12/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:03:34,488 DEBUG generators.py generate l.361] (12/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:03:36,907 DEBUG generators.py generate l.373] (12/47) Post-process Answer
[2024-03-05 09:03:36,909 INFO generators.py gen_for_qa l.551] (12/47) * Start with LLM "command-nightly"
[2024-03-05 09:03:36,911 DEBUG generators.py generate l.352] (12/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:03:36,913 DEBUG generators.py generate l.361] (12/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:03:38,044 DEBUG generators.py generate l.373] (12/47) Post-process Answer
[2024-03-05 09:03:38,046 INFO generators.py generate l.480] (12/47) End question "Comment réformer le système de santé ?  A) Système de santé public B) Système de santé privé C) Assurance maladie obligatoire"
[2024-03-05 09:03:38,048 INFO generators.py generate l.478] (13/47) *** AnsGenerator for question "Comment réformer le système bancaire ?  A) Banques publiques B) Déréglementation bancaire C) Régulation bancaire"
[2024-03-05 09:03:38,049 INFO generators.py gen_for_qa l.551] (13/47) * Start with LLM "gpt-4"
[2024-03-05 09:03:38,052 DEBUG generators.py generate l.352] (13/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:03:38,054 DEBUG generators.py generate l.361] (13/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:03:38,852 DEBUG generators.py generate l.373] (13/47) Post-process Answer
[2024-03-05 09:03:38,856 INFO generators.py gen_for_qa l.551] (13/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 09:03:38,857 DEBUG generators.py generate l.352] (13/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:03:38,860 DEBUG generators.py generate l.361] (13/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:03:39,356 DEBUG generators.py generate l.373] (13/47) Post-process Answer
[2024-03-05 09:03:39,358 INFO generators.py gen_for_qa l.551] (13/47) * Start with LLM "gemini-pro"
[2024-03-05 09:03:39,359 DEBUG generators.py generate l.352] (13/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:03:39,362 DEBUG generators.py generate l.361] (13/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:03:41,918 DEBUG generators.py generate l.373] (13/47) Post-process Answer
[2024-03-05 09:03:41,920 INFO generators.py gen_for_qa l.551] (13/47) * Start with LLM "claude-2.1"
[2024-03-05 09:03:41,921 DEBUG generators.py generate l.352] (13/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:03:41,922 DEBUG generators.py generate l.361] (13/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:03:42,872 DEBUG generators.py generate l.373] (13/47) Post-process Answer
[2024-03-05 09:03:42,872 INFO generators.py gen_for_qa l.551] (13/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 09:03:42,874 DEBUG generators.py generate l.352] (13/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:03:42,876 DEBUG generators.py generate l.361] (13/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:03:43,506 DEBUG generators.py generate l.373] (13/47) Post-process Answer
[2024-03-05 09:03:43,508 INFO generators.py gen_for_qa l.551] (13/47) * Start with LLM "command-nightly"
[2024-03-05 09:03:43,513 DEBUG generators.py generate l.352] (13/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:03:43,515 DEBUG generators.py generate l.361] (13/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:03:43,963 DEBUG generators.py generate l.373] (13/47) Post-process Answer
[2024-03-05 09:03:43,970 INFO generators.py generate l.480] (13/47) End question "Comment réformer le système bancaire ?  A) Banques publiques B) Déréglementation bancaire C) Régulation bancaire"
[2024-03-05 09:03:43,973 INFO generators.py generate l.478] (14/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés ?  A) Régulation étatique B) Déréglementation C) Autorégulation"
[2024-03-05 09:03:43,977 INFO generators.py gen_for_qa l.551] (14/47) * Start with LLM "gpt-4"
[2024-03-05 09:03:43,980 DEBUG generators.py generate l.352] (14/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:03:43,982 DEBUG generators.py generate l.361] (14/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:03:44,676 DEBUG generators.py generate l.373] (14/47) Post-process Answer
[2024-03-05 09:03:44,679 INFO generators.py gen_for_qa l.551] (14/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 09:03:44,682 DEBUG generators.py generate l.352] (14/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:03:44,685 DEBUG generators.py generate l.361] (14/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:03:45,499 DEBUG generators.py generate l.373] (14/47) Post-process Answer
[2024-03-05 09:03:45,503 INFO generators.py gen_for_qa l.551] (14/47) * Start with LLM "gemini-pro"
[2024-03-05 09:03:45,505 DEBUG generators.py generate l.352] (14/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:03:45,507 DEBUG generators.py generate l.361] (14/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:03:48,069 DEBUG generators.py generate l.373] (14/47) Post-process Answer
[2024-03-05 09:03:48,071 INFO generators.py gen_for_qa l.551] (14/47) * Start with LLM "claude-2.1"
[2024-03-05 09:03:48,071 DEBUG generators.py generate l.352] (14/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:03:48,075 DEBUG generators.py generate l.361] (14/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:03:48,883 DEBUG generators.py generate l.373] (14/47) Post-process Answer
[2024-03-05 09:03:48,885 INFO generators.py gen_for_qa l.551] (14/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 09:03:48,888 DEBUG generators.py generate l.352] (14/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:03:48,890 DEBUG generators.py generate l.361] (14/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:03:56,361 DEBUG generators.py generate l.373] (14/47) Post-process Answer
[2024-03-05 09:03:56,366 INFO generators.py gen_for_qa l.551] (14/47) * Start with LLM "command-nightly"
[2024-03-05 09:03:56,369 DEBUG generators.py generate l.352] (14/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:03:56,371 DEBUG generators.py generate l.361] (14/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:03:56,749 DEBUG generators.py generate l.373] (14/47) Post-process Answer
[2024-03-05 09:03:56,749 INFO generators.py generate l.480] (14/47) End question "Comment réformer le système de régulation des marchés ?  A) Régulation étatique B) Déréglementation C) Autorégulation"
[2024-03-05 09:03:56,757 INFO generators.py generate l.478] (15/47) *** AnsGenerator for question "Comment réformer le système de commerce international ?  A) Protectionnisme B) Libre-échange C) Commerce équitable"
[2024-03-05 09:03:56,760 INFO generators.py gen_for_qa l.551] (15/47) * Start with LLM "gpt-4"
[2024-03-05 09:03:56,763 DEBUG generators.py generate l.352] (15/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:03:56,765 DEBUG generators.py generate l.361] (15/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:03:57,592 DEBUG generators.py generate l.373] (15/47) Post-process Answer
[2024-03-05 09:03:57,596 INFO generators.py gen_for_qa l.551] (15/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 09:03:57,598 DEBUG generators.py generate l.352] (15/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:03:57,600 DEBUG generators.py generate l.361] (15/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:03:58,437 DEBUG generators.py generate l.373] (15/47) Post-process Answer
[2024-03-05 09:03:58,439 INFO generators.py gen_for_qa l.551] (15/47) * Start with LLM "gemini-pro"
[2024-03-05 09:03:58,440 DEBUG generators.py generate l.352] (15/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:03:58,443 DEBUG generators.py generate l.361] (15/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:01,084 DEBUG generators.py generate l.373] (15/47) Post-process Answer
[2024-03-05 09:04:01,087 INFO generators.py gen_for_qa l.551] (15/47) * Start with LLM "claude-2.1"
[2024-03-05 09:04:01,087 DEBUG generators.py generate l.352] (15/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:01,091 DEBUG generators.py generate l.361] (15/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:02,072 DEBUG generators.py generate l.373] (15/47) Post-process Answer
[2024-03-05 09:04:02,075 INFO generators.py gen_for_qa l.551] (15/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 09:04:02,078 DEBUG generators.py generate l.352] (15/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:02,080 DEBUG generators.py generate l.361] (15/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:02,539 DEBUG generators.py generate l.373] (15/47) Post-process Answer
[2024-03-05 09:04:02,540 INFO generators.py gen_for_qa l.551] (15/47) * Start with LLM "command-nightly"
[2024-03-05 09:04:02,544 DEBUG generators.py generate l.352] (15/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:02,545 DEBUG generators.py generate l.361] (15/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:02,941 DEBUG generators.py generate l.373] (15/47) Post-process Answer
[2024-03-05 09:04:02,944 INFO generators.py generate l.480] (15/47) End question "Comment réformer le système de commerce international ?  A) Protectionnisme B) Libre-échange C) Commerce équitable"
[2024-03-05 09:04:02,947 INFO generators.py generate l.478] (16/47) *** AnsGenerator for question "Comment réformer le système monétaire international ?  A) Monnaies nationales B) Étalon-or C) Monnaie unique"
[2024-03-05 09:04:02,948 INFO generators.py gen_for_qa l.551] (16/47) * Start with LLM "gpt-4"
[2024-03-05 09:04:02,951 DEBUG generators.py generate l.352] (16/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:02,952 DEBUG generators.py generate l.361] (16/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:04,160 DEBUG generators.py generate l.373] (16/47) Post-process Answer
[2024-03-05 09:04:04,162 INFO generators.py gen_for_qa l.551] (16/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 09:04:04,164 DEBUG generators.py generate l.352] (16/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:04,166 DEBUG generators.py generate l.361] (16/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:04,624 DEBUG generators.py generate l.373] (16/47) Post-process Answer
[2024-03-05 09:04:04,625 INFO generators.py gen_for_qa l.551] (16/47) * Start with LLM "gemini-pro"
[2024-03-05 09:04:04,628 DEBUG generators.py generate l.352] (16/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:04,630 DEBUG generators.py generate l.361] (16/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:07,170 DEBUG generators.py generate l.373] (16/47) Post-process Answer
[2024-03-05 09:04:07,172 INFO generators.py gen_for_qa l.551] (16/47) * Start with LLM "claude-2.1"
[2024-03-05 09:04:07,172 DEBUG generators.py generate l.352] (16/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:07,175 DEBUG generators.py generate l.361] (16/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:08,255 DEBUG generators.py generate l.373] (16/47) Post-process Answer
[2024-03-05 09:04:08,258 INFO generators.py gen_for_qa l.551] (16/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 09:04:08,260 DEBUG generators.py generate l.352] (16/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:08,263 DEBUG generators.py generate l.361] (16/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:11,579 DEBUG generators.py generate l.373] (16/47) Post-process Answer
[2024-03-05 09:04:11,583 INFO generators.py gen_for_qa l.551] (16/47) * Start with LLM "command-nightly"
[2024-03-05 09:04:11,586 DEBUG generators.py generate l.352] (16/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:11,591 DEBUG generators.py generate l.361] (16/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:11,995 DEBUG generators.py generate l.373] (16/47) Post-process Answer
[2024-03-05 09:04:12,000 INFO generators.py generate l.480] (16/47) End question "Comment réformer le système monétaire international ?  A) Monnaies nationales B) Étalon-or C) Monnaie unique"
[2024-03-05 09:04:12,005 INFO generators.py generate l.478] (17/47) *** AnsGenerator for question "Comment réformer le système financier international ?  A) Taxe sur les transactions financières B) Libéralisation financière C) Régulation financière internationale"
[2024-03-05 09:04:12,010 INFO generators.py gen_for_qa l.551] (17/47) * Start with LLM "gpt-4"
[2024-03-05 09:04:12,012 DEBUG generators.py generate l.352] (17/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:12,015 DEBUG generators.py generate l.361] (17/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:12,878 DEBUG generators.py generate l.373] (17/47) Post-process Answer
[2024-03-05 09:04:12,880 INFO generators.py gen_for_qa l.551] (17/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 09:04:12,881 DEBUG generators.py generate l.352] (17/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:12,883 DEBUG generators.py generate l.361] (17/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:13,528 DEBUG generators.py generate l.373] (17/47) Post-process Answer
[2024-03-05 09:04:13,530 INFO generators.py gen_for_qa l.551] (17/47) * Start with LLM "gemini-pro"
[2024-03-05 09:04:13,533 DEBUG generators.py generate l.352] (17/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:13,534 DEBUG generators.py generate l.361] (17/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:15,846 DEBUG generators.py generate l.373] (17/47) Post-process Answer
[2024-03-05 09:04:15,848 INFO generators.py gen_for_qa l.551] (17/47) * Start with LLM "claude-2.1"
[2024-03-05 09:04:15,850 DEBUG generators.py generate l.352] (17/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:15,852 DEBUG generators.py generate l.361] (17/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:16,878 DEBUG generators.py generate l.373] (17/47) Post-process Answer
[2024-03-05 09:04:16,878 INFO generators.py gen_for_qa l.551] (17/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 09:04:16,881 DEBUG generators.py generate l.352] (17/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:16,883 DEBUG generators.py generate l.361] (17/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:17,404 DEBUG generators.py generate l.373] (17/47) Post-process Answer
[2024-03-05 09:04:17,406 INFO generators.py gen_for_qa l.551] (17/47) * Start with LLM "command-nightly"
[2024-03-05 09:04:17,407 DEBUG generators.py generate l.352] (17/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:17,409 DEBUG generators.py generate l.361] (17/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:17,804 DEBUG generators.py generate l.373] (17/47) Post-process Answer
[2024-03-05 09:04:17,811 INFO generators.py generate l.480] (17/47) End question "Comment réformer le système financier international ?  A) Taxe sur les transactions financières B) Libéralisation financière C) Régulation financière internationale"
[2024-03-05 09:04:17,813 INFO generators.py generate l.478] (18/47) *** AnsGenerator for question "Comment réformer le système de propriété intellectuelle ?  A) Licences libres B) Brevets C) Droits d'auteur"
[2024-03-05 09:04:17,816 INFO generators.py gen_for_qa l.551] (18/47) * Start with LLM "gpt-4"
[2024-03-05 09:04:17,817 DEBUG generators.py generate l.352] (18/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:17,819 DEBUG generators.py generate l.361] (18/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:18,477 DEBUG generators.py generate l.373] (18/47) Post-process Answer
[2024-03-05 09:04:18,477 INFO generators.py gen_for_qa l.551] (18/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 09:04:18,482 DEBUG generators.py generate l.352] (18/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:18,483 DEBUG generators.py generate l.361] (18/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:19,266 DEBUG generators.py generate l.373] (18/47) Post-process Answer
[2024-03-05 09:04:19,282 INFO generators.py gen_for_qa l.551] (18/47) * Start with LLM "gemini-pro"
[2024-03-05 09:04:19,284 DEBUG generators.py generate l.352] (18/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:19,286 DEBUG generators.py generate l.361] (18/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:21,587 DEBUG generators.py generate l.373] (18/47) Post-process Answer
[2024-03-05 09:04:21,590 INFO generators.py gen_for_qa l.551] (18/47) * Start with LLM "claude-2.1"
[2024-03-05 09:04:21,591 DEBUG generators.py generate l.352] (18/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:21,594 DEBUG generators.py generate l.361] (18/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:22,560 DEBUG generators.py generate l.373] (18/47) Post-process Answer
[2024-03-05 09:04:22,560 INFO generators.py gen_for_qa l.551] (18/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 09:04:22,567 DEBUG generators.py generate l.352] (18/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:22,570 DEBUG generators.py generate l.361] (18/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:24,858 DEBUG generators.py generate l.373] (18/47) Post-process Answer
[2024-03-05 09:04:24,861 INFO generators.py gen_for_qa l.551] (18/47) * Start with LLM "command-nightly"
[2024-03-05 09:04:24,864 DEBUG generators.py generate l.352] (18/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:24,866 DEBUG generators.py generate l.361] (18/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:25,364 DEBUG generators.py generate l.373] (18/47) Post-process Answer
[2024-03-05 09:04:25,364 INFO generators.py generate l.480] (18/47) End question "Comment réformer le système de propriété intellectuelle ?  A) Licences libres B) Brevets C) Droits d'auteur"
[2024-03-05 09:04:25,372 INFO generators.py generate l.478] (19/47) *** AnsGenerator for question "Comment réformer le système de régulation des industries ?  A) Nationalisation B) Déréglementation C) Concurrence régulée"
[2024-03-05 09:04:25,375 INFO generators.py gen_for_qa l.551] (19/47) * Start with LLM "gpt-4"
[2024-03-05 09:04:25,376 DEBUG generators.py generate l.352] (19/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:25,379 DEBUG generators.py generate l.361] (19/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:26,004 DEBUG generators.py generate l.373] (19/47) Post-process Answer
[2024-03-05 09:04:26,005 INFO generators.py gen_for_qa l.551] (19/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 09:04:26,006 DEBUG generators.py generate l.352] (19/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:26,008 DEBUG generators.py generate l.361] (19/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:26,486 DEBUG generators.py generate l.373] (19/47) Post-process Answer
[2024-03-05 09:04:26,486 INFO generators.py gen_for_qa l.551] (19/47) * Start with LLM "gemini-pro"
[2024-03-05 09:04:26,489 DEBUG generators.py generate l.352] (19/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:26,491 DEBUG generators.py generate l.361] (19/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:28,835 DEBUG generators.py generate l.373] (19/47) Post-process Answer
[2024-03-05 09:04:28,838 INFO generators.py gen_for_qa l.551] (19/47) * Start with LLM "claude-2.1"
[2024-03-05 09:04:28,839 DEBUG generators.py generate l.352] (19/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:28,840 DEBUG generators.py generate l.361] (19/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:29,930 DEBUG generators.py generate l.373] (19/47) Post-process Answer
[2024-03-05 09:04:29,941 INFO generators.py gen_for_qa l.551] (19/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 09:04:29,942 DEBUG generators.py generate l.352] (19/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:29,945 DEBUG generators.py generate l.361] (19/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:30,411 DEBUG generators.py generate l.373] (19/47) Post-process Answer
[2024-03-05 09:04:30,415 INFO generators.py gen_for_qa l.551] (19/47) * Start with LLM "command-nightly"
[2024-03-05 09:04:30,416 DEBUG generators.py generate l.352] (19/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:30,421 DEBUG generators.py generate l.361] (19/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:30,804 DEBUG generators.py generate l.373] (19/47) Post-process Answer
[2024-03-05 09:04:30,804 INFO generators.py generate l.480] (19/47) End question "Comment réformer le système de régulation des industries ?  A) Nationalisation B) Déréglementation C) Concurrence régulée"
[2024-03-05 09:04:30,817 INFO generators.py generate l.478] (20/47) *** AnsGenerator for question "Comment réformer le système de régulation des services publics ?  A) Services publics gratuits B) Privatisation C) Partenariat public-privé"
[2024-03-05 09:04:30,821 INFO generators.py gen_for_qa l.551] (20/47) * Start with LLM "gpt-4"
[2024-03-05 09:04:30,825 DEBUG generators.py generate l.352] (20/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:30,826 DEBUG generators.py generate l.361] (20/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:31,534 DEBUG generators.py generate l.373] (20/47) Post-process Answer
[2024-03-05 09:04:31,541 INFO generators.py gen_for_qa l.551] (20/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 09:04:31,544 DEBUG generators.py generate l.352] (20/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:31,545 DEBUG generators.py generate l.361] (20/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:32,042 DEBUG generators.py generate l.373] (20/47) Post-process Answer
[2024-03-05 09:04:32,049 INFO generators.py gen_for_qa l.551] (20/47) * Start with LLM "gemini-pro"
[2024-03-05 09:04:32,051 DEBUG generators.py generate l.352] (20/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:32,053 DEBUG generators.py generate l.361] (20/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:34,359 DEBUG generators.py generate l.373] (20/47) Post-process Answer
[2024-03-05 09:04:34,360 INFO generators.py gen_for_qa l.551] (20/47) * Start with LLM "claude-2.1"
[2024-03-05 09:04:34,363 DEBUG generators.py generate l.352] (20/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:34,365 DEBUG generators.py generate l.361] (20/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:35,439 DEBUG generators.py generate l.373] (20/47) Post-process Answer
[2024-03-05 09:04:35,455 INFO generators.py gen_for_qa l.551] (20/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 09:04:35,457 DEBUG generators.py generate l.352] (20/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:35,459 DEBUG generators.py generate l.361] (20/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:38,899 DEBUG generators.py generate l.373] (20/47) Post-process Answer
[2024-03-05 09:04:38,911 INFO generators.py gen_for_qa l.551] (20/47) * Start with LLM "command-nightly"
[2024-03-05 09:04:38,914 DEBUG generators.py generate l.352] (20/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:38,916 DEBUG generators.py generate l.361] (20/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:39,310 DEBUG generators.py generate l.373] (20/47) Post-process Answer
[2024-03-05 09:04:39,313 INFO generators.py generate l.480] (20/47) End question "Comment réformer le système de régulation des services publics ?  A) Services publics gratuits B) Privatisation C) Partenariat public-privé"
[2024-03-05 09:04:39,316 INFO generators.py generate l.478] (21/47) *** AnsGenerator for question "Comment réformer le système de régulation des finances publiques ?  A) Déficit public B) Austérité budgétaire C) Règle d'or budgétaire"
[2024-03-05 09:04:39,319 INFO generators.py gen_for_qa l.551] (21/47) * Start with LLM "gpt-4"
[2024-03-05 09:04:39,321 DEBUG generators.py generate l.352] (21/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:39,324 DEBUG generators.py generate l.361] (21/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:39,924 DEBUG generators.py generate l.373] (21/47) Post-process Answer
[2024-03-05 09:04:39,925 INFO generators.py gen_for_qa l.551] (21/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 09:04:39,928 DEBUG generators.py generate l.352] (21/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:39,930 DEBUG generators.py generate l.361] (21/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:40,524 DEBUG generators.py generate l.373] (21/47) Post-process Answer
[2024-03-05 09:04:40,529 INFO generators.py gen_for_qa l.551] (21/47) * Start with LLM "gemini-pro"
[2024-03-05 09:04:40,531 DEBUG generators.py generate l.352] (21/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:40,534 DEBUG generators.py generate l.361] (21/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:42,826 DEBUG generators.py generate l.373] (21/47) Post-process Answer
[2024-03-05 09:04:42,829 INFO generators.py gen_for_qa l.551] (21/47) * Start with LLM "claude-2.1"
[2024-03-05 09:04:42,831 DEBUG generators.py generate l.352] (21/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:42,833 DEBUG generators.py generate l.361] (21/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:43,865 DEBUG generators.py generate l.373] (21/47) Post-process Answer
[2024-03-05 09:04:43,866 INFO generators.py gen_for_qa l.551] (21/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 09:04:43,869 DEBUG generators.py generate l.352] (21/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:43,871 DEBUG generators.py generate l.361] (21/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:44,497 DEBUG generators.py generate l.373] (21/47) Post-process Answer
[2024-03-05 09:04:44,504 INFO generators.py gen_for_qa l.551] (21/47) * Start with LLM "command-nightly"
[2024-03-05 09:04:44,505 DEBUG generators.py generate l.352] (21/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:44,507 DEBUG generators.py generate l.361] (21/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:44,915 DEBUG generators.py generate l.373] (21/47) Post-process Answer
[2024-03-05 09:04:44,921 INFO generators.py generate l.480] (21/47) End question "Comment réformer le système de régulation des finances publiques ?  A) Déficit public B) Austérité budgétaire C) Règle d'or budgétaire"
[2024-03-05 09:04:44,926 INFO generators.py generate l.478] (22/47) *** AnsGenerator for question "Comment réformer le système de régulation des échanges internationaux ?  A) Commerce équitable B) Libre-échange C) Régionalisme économique"
[2024-03-05 09:04:44,928 INFO generators.py gen_for_qa l.551] (22/47) * Start with LLM "gpt-4"
[2024-03-05 09:04:44,931 DEBUG generators.py generate l.352] (22/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:44,933 DEBUG generators.py generate l.361] (22/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:45,510 DEBUG generators.py generate l.373] (22/47) Post-process Answer
[2024-03-05 09:04:45,512 INFO generators.py gen_for_qa l.551] (22/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 09:04:45,513 DEBUG generators.py generate l.352] (22/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:45,515 DEBUG generators.py generate l.361] (22/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:46,468 DEBUG generators.py generate l.373] (22/47) Post-process Answer
[2024-03-05 09:04:46,480 INFO generators.py gen_for_qa l.551] (22/47) * Start with LLM "gemini-pro"
[2024-03-05 09:04:46,482 DEBUG generators.py generate l.352] (22/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:46,485 DEBUG generators.py generate l.361] (22/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:48,948 DEBUG generators.py generate l.373] (22/47) Post-process Answer
[2024-03-05 09:04:48,949 INFO generators.py gen_for_qa l.551] (22/47) * Start with LLM "claude-2.1"
[2024-03-05 09:04:48,951 DEBUG generators.py generate l.352] (22/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:48,954 DEBUG generators.py generate l.361] (22/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:50,055 DEBUG generators.py generate l.373] (22/47) Post-process Answer
[2024-03-05 09:04:50,057 INFO generators.py gen_for_qa l.551] (22/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 09:04:50,059 DEBUG generators.py generate l.352] (22/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:50,061 DEBUG generators.py generate l.361] (22/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:50,532 DEBUG generators.py generate l.373] (22/47) Post-process Answer
[2024-03-05 09:04:50,534 INFO generators.py gen_for_qa l.551] (22/47) * Start with LLM "command-nightly"
[2024-03-05 09:04:50,535 DEBUG generators.py generate l.352] (22/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:50,537 DEBUG generators.py generate l.361] (22/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:50,908 DEBUG generators.py generate l.373] (22/47) Post-process Answer
[2024-03-05 09:04:50,908 INFO generators.py generate l.480] (22/47) End question "Comment réformer le système de régulation des échanges internationaux ?  A) Commerce équitable B) Libre-échange C) Régionalisme économique"
[2024-03-05 09:04:50,914 INFO generators.py generate l.478] (23/47) *** AnsGenerator for question "Comment réformer le système de régulation des migrations internationales ?  A) Politique migratoire ouverte B) Politique migratoire restrictive C) Politique migratoire sélective"
[2024-03-05 09:04:50,916 INFO generators.py gen_for_qa l.551] (23/47) * Start with LLM "gpt-4"
[2024-03-05 09:04:50,917 DEBUG generators.py generate l.352] (23/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:50,919 DEBUG generators.py generate l.361] (23/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:51,593 DEBUG generators.py generate l.373] (23/47) Post-process Answer
[2024-03-05 09:04:51,597 INFO generators.py gen_for_qa l.551] (23/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 09:04:51,599 DEBUG generators.py generate l.352] (23/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:51,602 DEBUG generators.py generate l.361] (23/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:52,179 DEBUG generators.py generate l.373] (23/47) Post-process Answer
[2024-03-05 09:04:52,181 INFO generators.py gen_for_qa l.551] (23/47) * Start with LLM "gemini-pro"
[2024-03-05 09:04:52,184 DEBUG generators.py generate l.352] (23/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:52,187 DEBUG generators.py generate l.361] (23/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:54,677 DEBUG generators.py generate l.373] (23/47) Post-process Answer
[2024-03-05 09:04:54,680 INFO generators.py gen_for_qa l.551] (23/47) * Start with LLM "claude-2.1"
[2024-03-05 09:04:54,686 DEBUG generators.py generate l.352] (23/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:54,689 DEBUG generators.py generate l.361] (23/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:55,536 DEBUG generators.py generate l.373] (23/47) Post-process Answer
[2024-03-05 09:04:55,538 INFO generators.py gen_for_qa l.551] (23/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 09:04:55,539 DEBUG generators.py generate l.352] (23/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:55,541 DEBUG generators.py generate l.361] (23/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:56,113 DEBUG generators.py generate l.373] (23/47) Post-process Answer
[2024-03-05 09:04:56,115 INFO generators.py gen_for_qa l.551] (23/47) * Start with LLM "command-nightly"
[2024-03-05 09:04:56,118 DEBUG generators.py generate l.352] (23/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:56,121 DEBUG generators.py generate l.361] (23/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:56,517 DEBUG generators.py generate l.373] (23/47) Post-process Answer
[2024-03-05 09:04:56,521 INFO generators.py generate l.480] (23/47) End question "Comment réformer le système de régulation des migrations internationales ?  A) Politique migratoire ouverte B) Politique migratoire restrictive C) Politique migratoire sélective"
[2024-03-05 09:04:56,528 INFO generators.py generate l.478] (24/47) *** AnsGenerator for question "Comment réformer le système de régulation de l'environnement ?  A) Écologie politique B) Marché du carbone C) Développement durable"
[2024-03-05 09:04:56,532 INFO generators.py gen_for_qa l.551] (24/47) * Start with LLM "gpt-4"
[2024-03-05 09:04:56,535 DEBUG generators.py generate l.352] (24/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:56,538 DEBUG generators.py generate l.361] (24/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:57,229 DEBUG generators.py generate l.373] (24/47) Post-process Answer
[2024-03-05 09:04:57,232 INFO generators.py gen_for_qa l.551] (24/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 09:04:57,233 DEBUG generators.py generate l.352] (24/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:57,236 DEBUG generators.py generate l.361] (24/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:04:57,804 DEBUG generators.py generate l.373] (24/47) Post-process Answer
[2024-03-05 09:04:57,816 INFO generators.py gen_for_qa l.551] (24/47) * Start with LLM "gemini-pro"
[2024-03-05 09:04:57,819 DEBUG generators.py generate l.352] (24/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:04:57,823 DEBUG generators.py generate l.361] (24/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:05:00,124 DEBUG generators.py generate l.373] (24/47) Post-process Answer
[2024-03-05 09:05:00,127 INFO generators.py gen_for_qa l.551] (24/47) * Start with LLM "claude-2.1"
[2024-03-05 09:05:00,127 DEBUG generators.py generate l.352] (24/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:05:00,129 DEBUG generators.py generate l.361] (24/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:05:01,121 DEBUG generators.py generate l.373] (24/47) Post-process Answer
[2024-03-05 09:05:01,121 INFO generators.py gen_for_qa l.551] (24/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 09:05:01,133 DEBUG generators.py generate l.352] (24/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:05:01,135 DEBUG generators.py generate l.361] (24/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:05:10,476 DEBUG generators.py generate l.373] (24/47) Post-process Answer
[2024-03-05 09:05:10,481 INFO generators.py gen_for_qa l.551] (24/47) * Start with LLM "command-nightly"
[2024-03-05 09:05:10,484 DEBUG generators.py generate l.352] (24/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:05:10,487 DEBUG generators.py generate l.361] (24/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:05:13,033 DEBUG generators.py generate l.373] (24/47) Post-process Answer
[2024-03-05 09:05:13,048 INFO generators.py generate l.480] (24/47) End question "Comment réformer le système de régulation de l'environnement ?  A) Écologie politique B) Marché du carbone C) Développement durable"
[2024-03-05 09:05:13,056 INFO generators.py generate l.478] (25/47) *** AnsGenerator for question "Comment réduire la pauvreté ?  A) Redistribution des richesses B) Croissance économique C) Filets de sécurité sociaux"
[2024-03-05 09:05:13,060 INFO generators.py gen_for_qa l.551] (25/47) * Start with LLM "gpt-4"
[2024-03-05 09:05:13,063 DEBUG generators.py generate l.352] (25/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:05:13,064 DEBUG generators.py generate l.361] (25/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:05:13,974 DEBUG generators.py generate l.373] (25/47) Post-process Answer
[2024-03-05 09:05:13,974 INFO generators.py gen_for_qa l.551] (25/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 09:05:13,980 DEBUG generators.py generate l.352] (25/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:05:13,982 DEBUG generators.py generate l.361] (25/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:05:15,004 DEBUG generators.py generate l.373] (25/47) Post-process Answer
[2024-03-05 09:05:15,011 INFO generators.py gen_for_qa l.551] (25/47) * Start with LLM "gemini-pro"
[2024-03-05 09:05:15,012 DEBUG generators.py generate l.352] (25/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:05:15,014 DEBUG generators.py generate l.361] (25/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:05:17,430 DEBUG generators.py generate l.373] (25/47) Post-process Answer
[2024-03-05 09:05:17,431 INFO generators.py gen_for_qa l.551] (25/47) * Start with LLM "claude-2.1"
[2024-03-05 09:05:17,435 DEBUG generators.py generate l.352] (25/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:05:17,436 DEBUG generators.py generate l.361] (25/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:05:19,342 DEBUG generators.py generate l.373] (25/47) Post-process Answer
[2024-03-05 09:05:19,345 INFO generators.py gen_for_qa l.551] (25/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 09:05:19,348 DEBUG generators.py generate l.352] (25/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:05:19,351 DEBUG generators.py generate l.361] (25/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:05:23,176 DEBUG generators.py generate l.373] (25/47) Post-process Answer
[2024-03-05 09:05:23,192 INFO generators.py gen_for_qa l.551] (25/47) * Start with LLM "command-nightly"
[2024-03-05 09:05:23,193 DEBUG generators.py generate l.352] (25/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:05:23,196 DEBUG generators.py generate l.361] (25/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:05:24,228 DEBUG generators.py generate l.373] (25/47) Post-process Answer
[2024-03-05 09:05:24,229 INFO generators.py generate l.480] (25/47) End question "Comment réduire la pauvreté ?  A) Redistribution des richesses B) Croissance économique C) Filets de sécurité sociaux"
[2024-03-05 09:05:24,231 INFO generators.py generate l.478] (26/47) *** AnsGenerator for question "Comment réformer le système de protection des consommateurs ?  A) Réglementation stricte B) Déréglementation C) Autorégulation"
[2024-03-05 09:05:24,232 INFO generators.py gen_for_qa l.551] (26/47) * Start with LLM "gpt-4"
[2024-03-05 09:05:24,233 DEBUG generators.py generate l.352] (26/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:05:24,236 DEBUG generators.py generate l.361] (26/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:05:25,080 DEBUG generators.py generate l.373] (26/47) Post-process Answer
[2024-03-05 09:05:25,082 INFO generators.py gen_for_qa l.551] (26/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 09:05:25,084 DEBUG generators.py generate l.352] (26/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:05:25,087 DEBUG generators.py generate l.361] (26/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:05:26,724 DEBUG generators.py generate l.373] (26/47) Post-process Answer
[2024-03-05 09:05:26,725 INFO generators.py gen_for_qa l.551] (26/47) * Start with LLM "gemini-pro"
[2024-03-05 09:05:26,728 DEBUG generators.py generate l.352] (26/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:05:26,731 DEBUG generators.py generate l.361] (26/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:05:35,508 DEBUG generators.py generate l.373] (26/47) Post-process Answer
[2024-03-05 09:05:35,512 INFO generators.py gen_for_qa l.551] (26/47) * Start with LLM "claude-2.1"
[2024-03-05 09:05:35,514 DEBUG generators.py generate l.352] (26/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:05:35,516 DEBUG generators.py generate l.361] (26/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:05:36,843 DEBUG generators.py generate l.373] (26/47) Post-process Answer
[2024-03-05 09:05:36,862 INFO generators.py gen_for_qa l.551] (26/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 09:05:36,866 DEBUG generators.py generate l.352] (26/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:05:36,868 DEBUG generators.py generate l.361] (26/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:05:37,763 DEBUG generators.py generate l.373] (26/47) Post-process Answer
[2024-03-05 09:05:37,766 INFO generators.py gen_for_qa l.551] (26/47) * Start with LLM "command-nightly"
[2024-03-05 09:05:37,770 DEBUG generators.py generate l.352] (26/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:05:37,774 DEBUG generators.py generate l.361] (26/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:05:41,390 DEBUG generators.py generate l.373] (26/47) Post-process Answer
[2024-03-05 09:05:41,405 INFO generators.py generate l.480] (26/47) End question "Comment réformer le système de protection des consommateurs ?  A) Réglementation stricte B) Déréglementation C) Autorégulation"
[2024-03-05 09:05:41,408 INFO generators.py generate l.478] (27/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés financiers ?  A) Réglementation stricte B) Déréglementation C) Régulation prudentielle"
[2024-03-05 09:05:41,410 INFO generators.py gen_for_qa l.551] (27/47) * Start with LLM "gpt-4"
[2024-03-05 09:05:41,411 DEBUG generators.py generate l.352] (27/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:05:41,414 DEBUG generators.py generate l.361] (27/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:05:45,227 DEBUG generators.py generate l.373] (27/47) Post-process Answer
[2024-03-05 09:05:45,238 INFO generators.py gen_for_qa l.551] (27/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 09:05:45,241 DEBUG generators.py generate l.352] (27/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:05:45,243 DEBUG generators.py generate l.361] (27/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:05:48,186 DEBUG generators.py generate l.373] (27/47) Post-process Answer
[2024-03-05 09:05:48,190 INFO generators.py gen_for_qa l.551] (27/47) * Start with LLM "gemini-pro"
[2024-03-05 09:05:48,192 DEBUG generators.py generate l.352] (27/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:05:48,194 DEBUG generators.py generate l.361] (27/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:05:52,237 DEBUG generators.py generate l.373] (27/47) Post-process Answer
[2024-03-05 09:05:52,240 INFO generators.py gen_for_qa l.551] (27/47) * Start with LLM "claude-2.1"
[2024-03-05 09:05:52,249 DEBUG generators.py generate l.352] (27/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:05:52,252 DEBUG generators.py generate l.361] (27/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:05:53,648 DEBUG generators.py generate l.373] (27/47) Post-process Answer
[2024-03-05 09:05:53,649 INFO generators.py gen_for_qa l.551] (27/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 09:05:53,654 DEBUG generators.py generate l.352] (27/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:05:53,658 DEBUG generators.py generate l.361] (27/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:05:54,928 DEBUG generators.py generate l.373] (27/47) Post-process Answer
[2024-03-05 09:05:54,944 INFO generators.py gen_for_qa l.551] (27/47) * Start with LLM "command-nightly"
[2024-03-05 09:05:54,947 DEBUG generators.py generate l.352] (27/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:05:54,950 DEBUG generators.py generate l.361] (27/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:05:56,024 DEBUG generators.py generate l.373] (27/47) Post-process Answer
[2024-03-05 09:05:56,024 INFO generators.py generate l.480] (27/47) End question "Comment réformer le système de régulation des marchés financiers ?  A) Réglementation stricte B) Déréglementation C) Régulation prudentielle"
[2024-03-05 09:05:56,027 INFO generators.py generate l.478] (28/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés du travail ?  A) Réglementation stricte B) Déréglementation C) Flexisécurité"
[2024-03-05 09:05:56,029 INFO generators.py gen_for_qa l.551] (28/47) * Start with LLM "gpt-4"
[2024-03-05 09:05:56,030 DEBUG generators.py generate l.352] (28/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:05:56,031 DEBUG generators.py generate l.361] (28/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:05:59,887 DEBUG generators.py generate l.373] (28/47) Post-process Answer
[2024-03-05 09:05:59,890 INFO generators.py gen_for_qa l.551] (28/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 09:05:59,891 DEBUG generators.py generate l.352] (28/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:05:59,895 DEBUG generators.py generate l.361] (28/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:06:03,328 DEBUG generators.py generate l.373] (28/47) Post-process Answer
[2024-03-05 09:06:03,330 INFO generators.py gen_for_qa l.551] (28/47) * Start with LLM "gemini-pro"
[2024-03-05 09:06:03,333 DEBUG generators.py generate l.352] (28/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:06:03,336 DEBUG generators.py generate l.361] (28/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:06:06,987 DEBUG generators.py generate l.373] (28/47) Post-process Answer
[2024-03-05 09:06:06,989 INFO generators.py gen_for_qa l.551] (28/47) * Start with LLM "claude-2.1"
[2024-03-05 09:06:06,989 DEBUG generators.py generate l.352] (28/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:06:06,989 DEBUG generators.py generate l.361] (28/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:06:08,361 DEBUG generators.py generate l.373] (28/47) Post-process Answer
[2024-03-05 09:06:08,363 INFO generators.py gen_for_qa l.551] (28/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 09:06:08,364 DEBUG generators.py generate l.352] (28/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:06:08,366 DEBUG generators.py generate l.361] (28/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:06:09,151 DEBUG generators.py generate l.373] (28/47) Post-process Answer
[2024-03-05 09:06:09,164 INFO generators.py gen_for_qa l.551] (28/47) * Start with LLM "command-nightly"
[2024-03-05 09:06:09,168 DEBUG generators.py generate l.352] (28/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:06:09,170 DEBUG generators.py generate l.361] (28/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:06:12,561 DEBUG generators.py generate l.373] (28/47) Post-process Answer
[2024-03-05 09:06:12,564 INFO generators.py generate l.480] (28/47) End question "Comment réformer le système de régulation des marchés du travail ?  A) Réglementation stricte B) Déréglementation C) Flexisécurité"
[2024-03-05 09:06:12,566 INFO generators.py generate l.478] (29/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés de l'énergie ?  A) Réglementation stricte B) Déréglementation C) Tarification progressive"
[2024-03-05 09:06:12,568 INFO generators.py gen_for_qa l.551] (29/47) * Start with LLM "gpt-4"
[2024-03-05 09:06:12,569 DEBUG generators.py generate l.352] (29/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:06:12,570 DEBUG generators.py generate l.361] (29/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:06:13,555 DEBUG generators.py generate l.373] (29/47) Post-process Answer
[2024-03-05 09:06:13,555 INFO generators.py gen_for_qa l.551] (29/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 09:06:13,567 DEBUG generators.py generate l.352] (29/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:06:13,568 DEBUG generators.py generate l.361] (29/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:06:14,339 DEBUG generators.py generate l.373] (29/47) Post-process Answer
[2024-03-05 09:06:14,342 INFO generators.py gen_for_qa l.551] (29/47) * Start with LLM "gemini-pro"
[2024-03-05 09:06:14,345 DEBUG generators.py generate l.352] (29/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:06:14,348 DEBUG generators.py generate l.361] (29/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:06:17,285 DEBUG generators.py generate l.373] (29/47) Post-process Answer
[2024-03-05 09:06:17,289 INFO generators.py gen_for_qa l.551] (29/47) * Start with LLM "claude-2.1"
[2024-03-05 09:06:17,289 DEBUG generators.py generate l.352] (29/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:06:17,292 DEBUG generators.py generate l.361] (29/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:06:18,611 DEBUG generators.py generate l.373] (29/47) Post-process Answer
[2024-03-05 09:06:18,611 INFO generators.py gen_for_qa l.551] (29/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 09:06:18,619 DEBUG generators.py generate l.352] (29/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:06:18,622 DEBUG generators.py generate l.361] (29/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:06:25,941 DEBUG generators.py generate l.373] (29/47) Post-process Answer
[2024-03-05 09:06:25,948 INFO generators.py gen_for_qa l.551] (29/47) * Start with LLM "command-nightly"
[2024-03-05 09:06:25,951 DEBUG generators.py generate l.352] (29/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:06:25,953 DEBUG generators.py generate l.361] (29/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:06:31,836 DEBUG generators.py generate l.373] (29/47) Post-process Answer
[2024-03-05 09:06:31,847 INFO generators.py generate l.480] (29/47) End question "Comment réformer le système de régulation des marchés de l'énergie ?  A) Réglementation stricte B) Déréglementation C) Tarification progressive"
[2024-03-05 09:06:31,849 INFO generators.py generate l.478] (30/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés des télécommunications ?  A) Réglementation stricte B) Déréglementation C) Neutralité du net"
[2024-03-05 09:06:31,852 INFO generators.py gen_for_qa l.551] (30/47) * Start with LLM "gpt-4"
[2024-03-05 09:06:31,853 DEBUG generators.py generate l.352] (30/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:06:31,854 DEBUG generators.py generate l.361] (30/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:06:34,595 DEBUG generators.py generate l.373] (30/47) Post-process Answer
[2024-03-05 09:06:34,600 INFO generators.py gen_for_qa l.551] (30/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 09:06:34,602 DEBUG generators.py generate l.352] (30/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:06:34,605 DEBUG generators.py generate l.361] (30/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:06:37,059 DEBUG generators.py generate l.373] (30/47) Post-process Answer
[2024-03-05 09:06:37,059 INFO generators.py gen_for_qa l.551] (30/47) * Start with LLM "gemini-pro"
[2024-03-05 09:06:37,069 DEBUG generators.py generate l.352] (30/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:06:37,072 DEBUG generators.py generate l.361] (30/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:06:41,162 DEBUG generators.py generate l.373] (30/47) Post-process Answer
[2024-03-05 09:06:41,162 INFO generators.py gen_for_qa l.551] (30/47) * Start with LLM "claude-2.1"
[2024-03-05 09:06:41,165 DEBUG generators.py generate l.352] (30/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:06:41,165 DEBUG generators.py generate l.361] (30/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:06:42,631 DEBUG generators.py generate l.373] (30/47) Post-process Answer
[2024-03-05 09:06:42,633 INFO generators.py gen_for_qa l.551] (30/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 09:06:42,635 DEBUG generators.py generate l.352] (30/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:06:42,637 DEBUG generators.py generate l.361] (30/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:06:55,713 DEBUG generators.py generate l.373] (30/47) Post-process Answer
[2024-03-05 09:06:55,728 INFO generators.py gen_for_qa l.551] (30/47) * Start with LLM "command-nightly"
[2024-03-05 09:06:55,731 DEBUG generators.py generate l.352] (30/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:06:55,733 DEBUG generators.py generate l.361] (30/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:07:00,026 DEBUG generators.py generate l.373] (30/47) Post-process Answer
[2024-03-05 09:07:00,027 INFO generators.py generate l.480] (30/47) End question "Comment réformer le système de régulation des marchés des télécommunications ?  A) Réglementation stricte B) Déréglementation C) Neutralité du net"
[2024-03-05 09:07:00,032 INFO generators.py generate l.478] (31/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés des transports ?  A) Réglementation stricte B) Déréglementation C) Concurrence régulée"
[2024-03-05 09:07:00,034 INFO generators.py gen_for_qa l.551] (31/47) * Start with LLM "gpt-4"
[2024-03-05 09:07:00,037 DEBUG generators.py generate l.352] (31/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:07:00,039 DEBUG generators.py generate l.361] (31/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:07:05,535 DEBUG generators.py generate l.373] (31/47) Post-process Answer
[2024-03-05 09:07:05,537 INFO generators.py gen_for_qa l.551] (31/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 09:07:05,539 DEBUG generators.py generate l.352] (31/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:07:05,542 DEBUG generators.py generate l.361] (31/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:07:06,661 DEBUG generators.py generate l.373] (31/47) Post-process Answer
[2024-03-05 09:07:06,662 INFO generators.py gen_for_qa l.551] (31/47) * Start with LLM "gemini-pro"
[2024-03-05 09:07:06,665 DEBUG generators.py generate l.352] (31/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:07:06,666 DEBUG generators.py generate l.361] (31/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:07:10,783 DEBUG generators.py generate l.373] (31/47) Post-process Answer
[2024-03-05 09:07:10,787 INFO generators.py gen_for_qa l.551] (31/47) * Start with LLM "claude-2.1"
[2024-03-05 09:07:10,788 DEBUG generators.py generate l.352] (31/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:07:10,789 DEBUG generators.py generate l.361] (31/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:07:13,143 DEBUG generators.py generate l.373] (31/47) Post-process Answer
[2024-03-05 09:07:13,143 INFO generators.py gen_for_qa l.551] (31/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 09:07:13,147 DEBUG generators.py generate l.352] (31/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:07:13,149 DEBUG generators.py generate l.361] (31/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:07:15,258 DEBUG generators.py generate l.373] (31/47) Post-process Answer
[2024-03-05 09:07:15,262 INFO generators.py gen_for_qa l.551] (31/47) * Start with LLM "command-nightly"
[2024-03-05 09:07:15,265 DEBUG generators.py generate l.352] (31/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:07:15,268 DEBUG generators.py generate l.361] (31/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:07:17,479 DEBUG generators.py generate l.373] (31/47) Post-process Answer
[2024-03-05 09:07:17,490 INFO generators.py generate l.480] (31/47) End question "Comment réformer le système de régulation des marchés des transports ?  A) Réglementation stricte B) Déréglementation C) Concurrence régulée"
[2024-03-05 09:07:17,492 INFO generators.py generate l.478] (32/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés du logement ?  A) Réglementation stricte B) Déréglementation C) Encadrement des loyers"
[2024-03-05 09:07:17,494 INFO generators.py gen_for_qa l.551] (32/47) * Start with LLM "gpt-4"
[2024-03-05 09:07:17,496 DEBUG generators.py generate l.352] (32/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:07:17,497 DEBUG generators.py generate l.361] (32/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:07:24,085 DEBUG generators.py generate l.373] (32/47) Post-process Answer
[2024-03-05 09:07:24,085 INFO generators.py gen_for_qa l.551] (32/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 09:07:24,098 DEBUG generators.py generate l.352] (32/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:07:24,101 DEBUG generators.py generate l.361] (32/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:07:27,181 DEBUG generators.py generate l.373] (32/47) Post-process Answer
[2024-03-05 09:07:27,181 INFO generators.py gen_for_qa l.551] (32/47) * Start with LLM "gemini-pro"
[2024-03-05 09:07:27,184 DEBUG generators.py generate l.352] (32/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:07:27,185 DEBUG generators.py generate l.361] (32/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:07:30,494 DEBUG generators.py generate l.373] (32/47) Post-process Answer
[2024-03-05 09:07:30,499 INFO generators.py gen_for_qa l.551] (32/47) * Start with LLM "claude-2.1"
[2024-03-05 09:07:30,501 DEBUG generators.py generate l.352] (32/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:07:30,502 DEBUG generators.py generate l.361] (32/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:07:32,238 DEBUG generators.py generate l.373] (32/47) Post-process Answer
[2024-03-05 09:07:32,238 INFO generators.py gen_for_qa l.551] (32/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 09:07:32,248 DEBUG generators.py generate l.352] (32/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:07:32,249 DEBUG generators.py generate l.361] (32/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:07:36,305 DEBUG generators.py generate l.373] (32/47) Post-process Answer
[2024-03-05 09:07:36,319 INFO generators.py gen_for_qa l.551] (32/47) * Start with LLM "command-nightly"
[2024-03-05 09:07:36,322 DEBUG generators.py generate l.352] (32/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:07:36,324 DEBUG generators.py generate l.361] (32/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:07:39,052 DEBUG generators.py generate l.373] (32/47) Post-process Answer
[2024-03-05 09:07:39,052 INFO generators.py generate l.480] (32/47) End question "Comment réformer le système de régulation des marchés du logement ?  A) Réglementation stricte B) Déréglementation C) Encadrement des loyers"
[2024-03-05 09:07:39,062 INFO generators.py generate l.478] (33/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés de l'eau ?  A) Réglementation stricte B) Déréglementation C) Tarification sociale"
[2024-03-05 09:07:39,065 INFO generators.py gen_for_qa l.551] (33/47) * Start with LLM "gpt-4"
[2024-03-05 09:07:39,068 DEBUG generators.py generate l.352] (33/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:07:39,071 DEBUG generators.py generate l.361] (33/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:07:42,345 DEBUG generators.py generate l.373] (33/47) Post-process Answer
[2024-03-05 09:07:42,361 INFO generators.py gen_for_qa l.551] (33/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 09:07:42,363 DEBUG generators.py generate l.352] (33/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:07:42,365 DEBUG generators.py generate l.361] (33/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:07:47,908 DEBUG generators.py generate l.373] (33/47) Post-process Answer
[2024-03-05 09:07:47,909 INFO generators.py gen_for_qa l.551] (33/47) * Start with LLM "gemini-pro"
[2024-03-05 09:07:47,912 DEBUG generators.py generate l.352] (33/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:07:47,914 DEBUG generators.py generate l.361] (33/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:07:52,625 DEBUG generators.py generate l.373] (33/47) Post-process Answer
[2024-03-05 09:07:52,628 INFO generators.py gen_for_qa l.551] (33/47) * Start with LLM "claude-2.1"
[2024-03-05 09:07:52,629 DEBUG generators.py generate l.352] (33/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:07:52,630 DEBUG generators.py generate l.361] (33/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:07:53,891 DEBUG generators.py generate l.373] (33/47) Post-process Answer
[2024-03-05 09:07:53,894 INFO generators.py gen_for_qa l.551] (33/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 09:07:53,897 DEBUG generators.py generate l.352] (33/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:07:53,899 DEBUG generators.py generate l.361] (33/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:08:01,946 DEBUG generators.py generate l.373] (33/47) Post-process Answer
[2024-03-05 09:08:01,962 INFO generators.py gen_for_qa l.551] (33/47) * Start with LLM "command-nightly"
[2024-03-05 09:08:01,966 DEBUG generators.py generate l.352] (33/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:08:01,967 DEBUG generators.py generate l.361] (33/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:08:09,567 DEBUG generators.py generate l.373] (33/47) Post-process Answer
[2024-03-05 09:08:09,568 INFO generators.py generate l.480] (33/47) End question "Comment réformer le système de régulation des marchés de l'eau ?  A) Réglementation stricte B) Déréglementation C) Tarification sociale"
[2024-03-05 09:08:09,573 INFO generators.py generate l.478] (34/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés de l'alimentation ?  A) Réglementation stricte B) Déréglementation C) Agriculture biologique"
[2024-03-05 09:08:09,576 INFO generators.py gen_for_qa l.551] (34/47) * Start with LLM "gpt-4"
[2024-03-05 09:08:09,578 DEBUG generators.py generate l.352] (34/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:08:09,581 DEBUG generators.py generate l.361] (34/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:08:10,965 DEBUG generators.py generate l.373] (34/47) Post-process Answer
[2024-03-05 09:08:10,967 INFO generators.py gen_for_qa l.551] (34/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 09:08:10,968 DEBUG generators.py generate l.352] (34/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:08:10,970 DEBUG generators.py generate l.361] (34/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:08:11,434 DEBUG generators.py generate l.373] (34/47) Post-process Answer
[2024-03-05 09:08:11,439 INFO generators.py gen_for_qa l.551] (34/47) * Start with LLM "gemini-pro"
[2024-03-05 09:08:11,442 DEBUG generators.py generate l.352] (34/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:08:11,443 DEBUG generators.py generate l.361] (34/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:08:14,690 DEBUG generators.py generate l.373] (34/47) Post-process Answer
[2024-03-05 09:08:14,691 INFO generators.py gen_for_qa l.551] (34/47) * Start with LLM "claude-2.1"
[2024-03-05 09:08:14,692 DEBUG generators.py generate l.352] (34/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:08:14,695 DEBUG generators.py generate l.361] (34/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:08:16,229 DEBUG generators.py generate l.373] (34/47) Post-process Answer
[2024-03-05 09:08:16,233 INFO generators.py gen_for_qa l.551] (34/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 09:08:16,240 DEBUG generators.py generate l.352] (34/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:08:16,241 DEBUG generators.py generate l.361] (34/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:08:17,341 DEBUG generators.py generate l.373] (34/47) Post-process Answer
[2024-03-05 09:08:17,341 INFO generators.py gen_for_qa l.551] (34/47) * Start with LLM "command-nightly"
[2024-03-05 09:08:17,344 DEBUG generators.py generate l.352] (34/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:08:17,345 DEBUG generators.py generate l.361] (34/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:08:22,037 DEBUG generators.py generate l.373] (34/47) Post-process Answer
[2024-03-05 09:08:22,039 INFO generators.py generate l.480] (34/47) End question "Comment réformer le système de régulation des marchés de l'alimentation ?  A) Réglementation stricte B) Déréglementation C) Agriculture biologique"
[2024-03-05 09:08:22,042 INFO generators.py generate l.478] (35/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés de la santé ?  A) Réglementation stricte B) Déréglementation C) Médecine préventive"
[2024-03-05 09:08:22,043 INFO generators.py gen_for_qa l.551] (35/47) * Start with LLM "gpt-4"
[2024-03-05 09:08:22,046 DEBUG generators.py generate l.352] (35/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:08:22,048 DEBUG generators.py generate l.361] (35/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:08:23,727 DEBUG generators.py generate l.373] (35/47) Post-process Answer
[2024-03-05 09:08:23,727 INFO generators.py gen_for_qa l.551] (35/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 09:08:23,738 DEBUG generators.py generate l.352] (35/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:08:23,739 DEBUG generators.py generate l.361] (35/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:08:27,271 DEBUG generators.py generate l.373] (35/47) Post-process Answer
[2024-03-05 09:08:27,273 INFO generators.py gen_for_qa l.551] (35/47) * Start with LLM "gemini-pro"
[2024-03-05 09:08:27,276 DEBUG generators.py generate l.352] (35/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:08:27,277 DEBUG generators.py generate l.361] (35/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:08:33,449 DEBUG generators.py generate l.373] (35/47) Post-process Answer
[2024-03-05 09:08:33,453 INFO generators.py gen_for_qa l.551] (35/47) * Start with LLM "claude-2.1"
[2024-03-05 09:08:33,457 DEBUG generators.py generate l.352] (35/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:08:33,460 DEBUG generators.py generate l.361] (35/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:08:35,076 DEBUG generators.py generate l.373] (35/47) Post-process Answer
[2024-03-05 09:08:35,087 INFO generators.py gen_for_qa l.551] (35/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 09:08:35,089 DEBUG generators.py generate l.352] (35/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:08:35,090 DEBUG generators.py generate l.361] (35/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:08:38,545 DEBUG generators.py generate l.373] (35/47) Post-process Answer
[2024-03-05 09:08:38,548 INFO generators.py gen_for_qa l.551] (35/47) * Start with LLM "command-nightly"
[2024-03-05 09:08:38,552 DEBUG generators.py generate l.352] (35/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:08:38,554 DEBUG generators.py generate l.361] (35/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:08:40,945 DEBUG generators.py generate l.373] (35/47) Post-process Answer
[2024-03-05 09:08:40,946 INFO generators.py generate l.480] (35/47) End question "Comment réformer le système de régulation des marchés de la santé ?  A) Réglementation stricte B) Déréglementation C) Médecine préventive"
[2024-03-05 09:08:40,949 INFO generators.py generate l.478] (36/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés de l'éducation ?  A) Réglementation stricte B) Déréglementation C) Éducation inclusive"
[2024-03-05 09:08:40,951 INFO generators.py gen_for_qa l.551] (36/47) * Start with LLM "gpt-4"
[2024-03-05 09:08:40,953 DEBUG generators.py generate l.352] (36/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:08:40,954 DEBUG generators.py generate l.361] (36/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:08:44,291 DEBUG generators.py generate l.373] (36/47) Post-process Answer
[2024-03-05 09:08:44,291 INFO generators.py gen_for_qa l.551] (36/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 09:08:44,299 DEBUG generators.py generate l.352] (36/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:08:44,302 DEBUG generators.py generate l.361] (36/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:08:44,777 DEBUG generators.py generate l.373] (36/47) Post-process Answer
[2024-03-05 09:08:44,782 INFO generators.py gen_for_qa l.551] (36/47) * Start with LLM "gemini-pro"
[2024-03-05 09:08:44,784 DEBUG generators.py generate l.352] (36/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:08:44,786 DEBUG generators.py generate l.361] (36/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:08:47,112 DEBUG generators.py generate l.373] (36/47) Post-process Answer
[2024-03-05 09:08:47,116 INFO generators.py gen_for_qa l.551] (36/47) * Start with LLM "claude-2.1"
[2024-03-05 09:08:47,117 DEBUG generators.py generate l.352] (36/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:08:47,119 DEBUG generators.py generate l.361] (36/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:08:48,271 DEBUG generators.py generate l.373] (36/47) Post-process Answer
[2024-03-05 09:08:48,282 INFO generators.py gen_for_qa l.551] (36/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 09:08:48,283 DEBUG generators.py generate l.352] (36/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:08:48,286 DEBUG generators.py generate l.361] (36/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:08:58,582 DEBUG generators.py generate l.373] (36/47) Post-process Answer
[2024-03-05 09:08:58,595 INFO generators.py gen_for_qa l.551] (36/47) * Start with LLM "command-nightly"
[2024-03-05 09:08:58,597 DEBUG generators.py generate l.352] (36/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:08:58,598 DEBUG generators.py generate l.361] (36/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:08:59,002 DEBUG generators.py generate l.373] (36/47) Post-process Answer
[2024-03-05 09:08:59,002 INFO generators.py generate l.480] (36/47) End question "Comment réformer le système de régulation des marchés de l'éducation ?  A) Réglementation stricte B) Déréglementation C) Éducation inclusive"
[2024-03-05 09:08:59,011 INFO generators.py generate l.478] (37/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés de la culture ?  A) Réglementation stricte B) Déréglementation C) Diversité culturelle"
[2024-03-05 09:08:59,013 INFO generators.py gen_for_qa l.551] (37/47) * Start with LLM "gpt-4"
[2024-03-05 09:08:59,016 DEBUG generators.py generate l.352] (37/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:08:59,018 DEBUG generators.py generate l.361] (37/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:09:00,093 DEBUG generators.py generate l.373] (37/47) Post-process Answer
[2024-03-05 09:09:00,096 INFO generators.py gen_for_qa l.551] (37/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 09:09:00,101 DEBUG generators.py generate l.352] (37/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:09:00,104 DEBUG generators.py generate l.361] (37/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:09:00,832 DEBUG generators.py generate l.373] (37/47) Post-process Answer
[2024-03-05 09:09:00,834 INFO generators.py gen_for_qa l.551] (37/47) * Start with LLM "gemini-pro"
[2024-03-05 09:09:00,835 DEBUG generators.py generate l.352] (37/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:09:00,837 DEBUG generators.py generate l.361] (37/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:09:03,077 DEBUG generators.py generate l.373] (37/47) Post-process Answer
[2024-03-05 09:09:03,079 INFO generators.py gen_for_qa l.551] (37/47) * Start with LLM "claude-2.1"
[2024-03-05 09:09:03,081 DEBUG generators.py generate l.352] (37/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:09:03,083 DEBUG generators.py generate l.361] (37/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:09:04,025 DEBUG generators.py generate l.373] (37/47) Post-process Answer
[2024-03-05 09:09:04,027 INFO generators.py gen_for_qa l.551] (37/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 09:09:04,030 DEBUG generators.py generate l.352] (37/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:09:04,032 DEBUG generators.py generate l.361] (37/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:09:12,938 DEBUG generators.py generate l.373] (37/47) Post-process Answer
[2024-03-05 09:09:12,938 INFO generators.py gen_for_qa l.551] (37/47) * Start with LLM "command-nightly"
[2024-03-05 09:09:12,953 DEBUG generators.py generate l.352] (37/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:09:12,955 DEBUG generators.py generate l.361] (37/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:09:13,350 DEBUG generators.py generate l.373] (37/47) Post-process Answer
[2024-03-05 09:09:13,355 INFO generators.py generate l.480] (37/47) End question "Comment réformer le système de régulation des marchés de la culture ?  A) Réglementation stricte B) Déréglementation C) Diversité culturelle"
[2024-03-05 09:09:13,359 INFO generators.py generate l.478] (38/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés du sport ?  A) Réglementation stricte B) Déréglementation C) Sport pour tous"
[2024-03-05 09:09:13,362 INFO generators.py gen_for_qa l.551] (38/47) * Start with LLM "gpt-4"
[2024-03-05 09:09:13,364 DEBUG generators.py generate l.352] (38/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:09:13,367 DEBUG generators.py generate l.361] (38/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:09:14,063 DEBUG generators.py generate l.373] (38/47) Post-process Answer
[2024-03-05 09:09:14,065 INFO generators.py gen_for_qa l.551] (38/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 09:09:14,066 DEBUG generators.py generate l.352] (38/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:09:14,068 DEBUG generators.py generate l.361] (38/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:09:14,592 DEBUG generators.py generate l.373] (38/47) Post-process Answer
[2024-03-05 09:09:14,594 INFO generators.py gen_for_qa l.551] (38/47) * Start with LLM "gemini-pro"
[2024-03-05 09:09:14,597 DEBUG generators.py generate l.352] (38/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:09:14,598 DEBUG generators.py generate l.361] (38/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:09:16,848 DEBUG generators.py generate l.373] (38/47) Post-process Answer
[2024-03-05 09:09:16,850 INFO generators.py gen_for_qa l.551] (38/47) * Start with LLM "claude-2.1"
[2024-03-05 09:09:16,851 DEBUG generators.py generate l.352] (38/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:09:16,853 DEBUG generators.py generate l.361] (38/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:09:17,758 DEBUG generators.py generate l.373] (38/47) Post-process Answer
[2024-03-05 09:09:17,759 INFO generators.py gen_for_qa l.551] (38/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 09:09:17,765 DEBUG generators.py generate l.352] (38/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:09:17,768 DEBUG generators.py generate l.361] (38/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:09:18,277 DEBUG generators.py generate l.373] (38/47) Post-process Answer
[2024-03-05 09:09:18,281 INFO generators.py gen_for_qa l.551] (38/47) * Start with LLM "command-nightly"
[2024-03-05 09:09:18,284 DEBUG generators.py generate l.352] (38/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:09:18,286 DEBUG generators.py generate l.361] (38/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:09:18,649 DEBUG generators.py generate l.373] (38/47) Post-process Answer
[2024-03-05 09:09:18,662 INFO generators.py generate l.480] (38/47) End question "Comment réformer le système de régulation des marchés du sport ?  A) Réglementation stricte B) Déréglementation C) Sport pour tous"
[2024-03-05 09:09:18,664 INFO generators.py generate l.478] (39/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés de l'environnement ?  A) Réglementation stricte B) Déréglementation C) Économie verte"
[2024-03-05 09:09:18,665 INFO generators.py gen_for_qa l.551] (39/47) * Start with LLM "gpt-4"
[2024-03-05 09:09:18,667 DEBUG generators.py generate l.352] (39/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:09:18,668 DEBUG generators.py generate l.361] (39/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:09:19,598 DEBUG generators.py generate l.373] (39/47) Post-process Answer
[2024-03-05 09:09:19,600 INFO generators.py gen_for_qa l.551] (39/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 09:09:19,602 DEBUG generators.py generate l.352] (39/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:09:19,603 DEBUG generators.py generate l.361] (39/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:09:20,490 DEBUG generators.py generate l.373] (39/47) Post-process Answer
[2024-03-05 09:09:20,492 INFO generators.py gen_for_qa l.551] (39/47) * Start with LLM "gemini-pro"
[2024-03-05 09:09:20,495 DEBUG generators.py generate l.352] (39/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:09:20,496 DEBUG generators.py generate l.361] (39/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:09:22,910 DEBUG generators.py generate l.373] (39/47) Post-process Answer
[2024-03-05 09:09:22,913 INFO generators.py gen_for_qa l.551] (39/47) * Start with LLM "claude-2.1"
[2024-03-05 09:09:22,916 DEBUG generators.py generate l.352] (39/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:09:22,918 DEBUG generators.py generate l.361] (39/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:09:23,875 DEBUG generators.py generate l.373] (39/47) Post-process Answer
[2024-03-05 09:09:23,875 INFO generators.py gen_for_qa l.551] (39/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 09:09:23,888 DEBUG generators.py generate l.352] (39/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:09:23,891 DEBUG generators.py generate l.361] (39/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:09:24,590 DEBUG generators.py generate l.373] (39/47) Post-process Answer
[2024-03-05 09:09:24,594 INFO generators.py gen_for_qa l.551] (39/47) * Start with LLM "command-nightly"
[2024-03-05 09:09:24,597 DEBUG generators.py generate l.352] (39/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:09:24,599 DEBUG generators.py generate l.361] (39/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:09:25,026 DEBUG generators.py generate l.373] (39/47) Post-process Answer
[2024-03-05 09:09:25,032 INFO generators.py generate l.480] (39/47) End question "Comment réformer le système de régulation des marchés de l'environnement ?  A) Réglementation stricte B) Déréglementation C) Économie verte"
[2024-03-05 09:09:25,036 INFO generators.py generate l.478] (40/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés du numérique ?  A) Réglementation stricte B) Déréglementation C) Souveraineté numérique"
[2024-03-05 09:09:25,041 INFO generators.py gen_for_qa l.551] (40/47) * Start with LLM "gpt-4"
[2024-03-05 09:09:25,046 DEBUG generators.py generate l.352] (40/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:09:25,049 DEBUG generators.py generate l.361] (40/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:09:27,737 DEBUG generators.py generate l.373] (40/47) Post-process Answer
[2024-03-05 09:09:27,739 INFO generators.py gen_for_qa l.551] (40/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 09:09:27,742 DEBUG generators.py generate l.352] (40/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:09:27,743 DEBUG generators.py generate l.361] (40/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:09:30,865 DEBUG generators.py generate l.373] (40/47) Post-process Answer
[2024-03-05 09:09:30,868 INFO generators.py gen_for_qa l.551] (40/47) * Start with LLM "gemini-pro"
[2024-03-05 09:09:30,870 DEBUG generators.py generate l.352] (40/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:09:30,872 DEBUG generators.py generate l.361] (40/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:09:33,809 DEBUG generators.py generate l.373] (40/47) Post-process Answer
[2024-03-05 09:09:33,812 INFO generators.py gen_for_qa l.551] (40/47) * Start with LLM "claude-2.1"
[2024-03-05 09:09:33,814 DEBUG generators.py generate l.352] (40/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:09:33,815 DEBUG generators.py generate l.361] (40/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:09:35,205 DEBUG generators.py generate l.373] (40/47) Post-process Answer
[2024-03-05 09:09:35,210 INFO generators.py gen_for_qa l.551] (40/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 09:09:35,215 DEBUG generators.py generate l.352] (40/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:09:35,217 DEBUG generators.py generate l.361] (40/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:09:38,795 DEBUG generators.py generate l.373] (40/47) Post-process Answer
[2024-03-05 09:09:38,795 INFO generators.py gen_for_qa l.551] (40/47) * Start with LLM "command-nightly"
[2024-03-05 09:09:38,807 DEBUG generators.py generate l.352] (40/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:09:38,809 DEBUG generators.py generate l.361] (40/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:09:44,334 DEBUG generators.py generate l.373] (40/47) Post-process Answer
[2024-03-05 09:09:44,339 INFO generators.py generate l.480] (40/47) End question "Comment réformer le système de régulation des marchés du numérique ?  A) Réglementation stricte B) Déréglementation C) Souveraineté numérique"
[2024-03-05 09:09:44,345 INFO generators.py generate l.478] (41/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés de la finance solidaire ?  A) Réglementation stricte B) Déréglementation C) Finance éthique"
[2024-03-05 09:09:44,348 INFO generators.py gen_for_qa l.551] (41/47) * Start with LLM "gpt-4"
[2024-03-05 09:09:44,352 DEBUG generators.py generate l.352] (41/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:09:44,352 DEBUG generators.py generate l.361] (41/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:09:44,964 DEBUG generators.py generate l.373] (41/47) Post-process Answer
[2024-03-05 09:09:44,968 INFO generators.py gen_for_qa l.551] (41/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 09:09:44,971 DEBUG generators.py generate l.352] (41/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:09:44,972 DEBUG generators.py generate l.361] (41/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:09:45,769 DEBUG generators.py generate l.373] (41/47) Post-process Answer
[2024-03-05 09:09:45,771 INFO generators.py gen_for_qa l.551] (41/47) * Start with LLM "gemini-pro"
[2024-03-05 09:09:45,774 DEBUG generators.py generate l.352] (41/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:09:45,775 DEBUG generators.py generate l.361] (41/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:09:50,700 DEBUG generators.py generate l.373] (41/47) Post-process Answer
[2024-03-05 09:09:50,702 INFO generators.py gen_for_qa l.551] (41/47) * Start with LLM "claude-2.1"
[2024-03-05 09:09:50,703 DEBUG generators.py generate l.352] (41/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:09:50,704 DEBUG generators.py generate l.361] (41/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:09:52,180 DEBUG generators.py generate l.373] (41/47) Post-process Answer
[2024-03-05 09:09:52,183 INFO generators.py gen_for_qa l.551] (41/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 09:09:52,186 DEBUG generators.py generate l.352] (41/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:09:52,188 DEBUG generators.py generate l.361] (41/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:09:56,801 DEBUG generators.py generate l.373] (41/47) Post-process Answer
[2024-03-05 09:09:56,805 INFO generators.py gen_for_qa l.551] (41/47) * Start with LLM "command-nightly"
[2024-03-05 09:09:56,808 DEBUG generators.py generate l.352] (41/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:09:56,812 DEBUG generators.py generate l.361] (41/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:10:00,383 DEBUG generators.py generate l.373] (41/47) Post-process Answer
[2024-03-05 09:10:00,385 INFO generators.py generate l.480] (41/47) End question "Comment réformer le système de régulation des marchés de la finance solidaire ?  A) Réglementation stricte B) Déréglementation C) Finance éthique"
[2024-03-05 09:10:00,387 INFO generators.py generate l.478] (42/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés de l'économie sociale et solidaire ?  A) Réglementation stricte B) Déréglementation C) Économie collaborative"
[2024-03-05 09:10:00,389 INFO generators.py gen_for_qa l.551] (42/47) * Start with LLM "gpt-4"
[2024-03-05 09:10:00,391 DEBUG generators.py generate l.352] (42/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:10:00,394 DEBUG generators.py generate l.361] (42/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:10:04,504 DEBUG generators.py generate l.373] (42/47) Post-process Answer
[2024-03-05 09:10:04,509 INFO generators.py gen_for_qa l.551] (42/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 09:10:04,514 DEBUG generators.py generate l.352] (42/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:10:04,516 DEBUG generators.py generate l.361] (42/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:10:09,729 DEBUG generators.py generate l.373] (42/47) Post-process Answer
[2024-03-05 09:10:09,730 INFO generators.py gen_for_qa l.551] (42/47) * Start with LLM "gemini-pro"
[2024-03-05 09:10:09,732 DEBUG generators.py generate l.352] (42/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:10:09,734 DEBUG generators.py generate l.361] (42/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:10:12,783 DEBUG generators.py generate l.373] (42/47) Post-process Answer
[2024-03-05 09:10:12,787 INFO generators.py gen_for_qa l.551] (42/47) * Start with LLM "claude-2.1"
[2024-03-05 09:10:12,790 DEBUG generators.py generate l.352] (42/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:10:12,793 DEBUG generators.py generate l.361] (42/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:10:14,125 DEBUG generators.py generate l.373] (42/47) Post-process Answer
[2024-03-05 09:10:14,128 INFO generators.py gen_for_qa l.551] (42/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 09:10:14,133 DEBUG generators.py generate l.352] (42/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:10:14,135 DEBUG generators.py generate l.361] (42/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:10:18,095 DEBUG generators.py generate l.373] (42/47) Post-process Answer
[2024-03-05 09:10:18,100 INFO generators.py gen_for_qa l.551] (42/47) * Start with LLM "command-nightly"
[2024-03-05 09:10:18,104 DEBUG generators.py generate l.352] (42/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:10:18,106 DEBUG generators.py generate l.361] (42/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:10:22,024 DEBUG generators.py generate l.373] (42/47) Post-process Answer
[2024-03-05 09:10:22,024 INFO generators.py generate l.480] (42/47) End question "Comment réformer le système de régulation des marchés de l'économie sociale et solidaire ?  A) Réglementation stricte B) Déréglementation C) Économie collaborative"
[2024-03-05 09:10:22,036 INFO generators.py generate l.478] (43/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés de la propriété intellectuelle ?  A) Réglementation stricte B) Déréglementation C) Licences libres"
[2024-03-05 09:10:22,039 INFO generators.py gen_for_qa l.551] (43/47) * Start with LLM "gpt-4"
[2024-03-05 09:10:22,042 DEBUG generators.py generate l.352] (43/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:10:22,046 DEBUG generators.py generate l.361] (43/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:10:25,937 DEBUG generators.py generate l.373] (43/47) Post-process Answer
[2024-03-05 09:10:25,937 INFO generators.py gen_for_qa l.551] (43/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 09:10:25,944 DEBUG generators.py generate l.352] (43/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:10:25,947 DEBUG generators.py generate l.361] (43/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:10:29,626 DEBUG generators.py generate l.373] (43/47) Post-process Answer
[2024-03-05 09:10:29,628 INFO generators.py gen_for_qa l.551] (43/47) * Start with LLM "gemini-pro"
[2024-03-05 09:10:29,633 DEBUG generators.py generate l.352] (43/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:10:29,636 DEBUG generators.py generate l.361] (43/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:10:33,846 DEBUG generators.py generate l.373] (43/47) Post-process Answer
[2024-03-05 09:10:33,852 INFO generators.py gen_for_qa l.551] (43/47) * Start with LLM "claude-2.1"
[2024-03-05 09:10:33,855 DEBUG generators.py generate l.352] (43/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:10:33,857 DEBUG generators.py generate l.361] (43/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:10:35,452 DEBUG generators.py generate l.373] (43/47) Post-process Answer
[2024-03-05 09:10:35,454 INFO generators.py gen_for_qa l.551] (43/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 09:10:35,458 DEBUG generators.py generate l.352] (43/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:10:35,461 DEBUG generators.py generate l.361] (43/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:10:40,858 DEBUG generators.py generate l.373] (43/47) Post-process Answer
[2024-03-05 09:10:40,860 INFO generators.py gen_for_qa l.551] (43/47) * Start with LLM "command-nightly"
[2024-03-05 09:10:40,862 DEBUG generators.py generate l.352] (43/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:10:40,865 DEBUG generators.py generate l.361] (43/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:10:44,560 DEBUG generators.py generate l.373] (43/47) Post-process Answer
[2024-03-05 09:10:44,576 INFO generators.py generate l.480] (43/47) End question "Comment réformer le système de régulation des marchés de la propriété intellectuelle ?  A) Réglementation stricte B) Déréglementation C) Licences libres"
[2024-03-05 09:10:44,585 INFO generators.py generate l.478] (44/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés de la recherche et de l'innovation ?  A) Réglementation stricte B) Déréglementation C) Open innovation"
[2024-03-05 09:10:44,588 INFO generators.py gen_for_qa l.551] (44/47) * Start with LLM "gpt-4"
[2024-03-05 09:10:44,592 DEBUG generators.py generate l.352] (44/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:10:44,594 DEBUG generators.py generate l.361] (44/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:10:50,163 DEBUG generators.py generate l.373] (44/47) Post-process Answer
[2024-03-05 09:10:50,166 INFO generators.py gen_for_qa l.551] (44/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 09:10:50,168 DEBUG generators.py generate l.352] (44/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:10:50,170 DEBUG generators.py generate l.361] (44/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:10:53,272 DEBUG generators.py generate l.373] (44/47) Post-process Answer
[2024-03-05 09:10:53,276 INFO generators.py gen_for_qa l.551] (44/47) * Start with LLM "gemini-pro"
[2024-03-05 09:10:53,279 DEBUG generators.py generate l.352] (44/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:10:53,281 DEBUG generators.py generate l.361] (44/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:10:56,584 DEBUG generators.py generate l.373] (44/47) Post-process Answer
[2024-03-05 09:10:56,588 INFO generators.py gen_for_qa l.551] (44/47) * Start with LLM "claude-2.1"
[2024-03-05 09:10:56,592 DEBUG generators.py generate l.352] (44/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:10:56,594 DEBUG generators.py generate l.361] (44/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:10:58,024 DEBUG generators.py generate l.373] (44/47) Post-process Answer
[2024-03-05 09:10:58,027 INFO generators.py gen_for_qa l.551] (44/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 09:10:58,029 DEBUG generators.py generate l.352] (44/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:10:58,031 DEBUG generators.py generate l.361] (44/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:10:58,846 DEBUG generators.py generate l.373] (44/47) Post-process Answer
[2024-03-05 09:10:58,853 INFO generators.py gen_for_qa l.551] (44/47) * Start with LLM "command-nightly"
[2024-03-05 09:10:58,856 DEBUG generators.py generate l.352] (44/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:10:58,860 DEBUG generators.py generate l.361] (44/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:11:03,221 DEBUG generators.py generate l.373] (44/47) Post-process Answer
[2024-03-05 09:11:03,225 INFO generators.py generate l.480] (44/47) End question "Comment réformer le système de régulation des marchés de la recherche et de l'innovation ?  A) Réglementation stricte B) Déréglementation C) Open innovation"
[2024-03-05 09:11:03,230 INFO generators.py generate l.478] (45/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés de la défense et de la sécurité ?  A) Réglementation stricte B) Déréglementation C) Sécurité globale"
[2024-03-05 09:11:03,233 INFO generators.py gen_for_qa l.551] (45/47) * Start with LLM "gpt-4"
[2024-03-05 09:11:03,234 DEBUG generators.py generate l.352] (45/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:11:03,236 DEBUG generators.py generate l.361] (45/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:11:03,944 DEBUG generators.py generate l.373] (45/47) Post-process Answer
[2024-03-05 09:11:03,948 INFO generators.py gen_for_qa l.551] (45/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 09:11:03,951 DEBUG generators.py generate l.352] (45/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:11:03,954 DEBUG generators.py generate l.361] (45/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:11:06,878 DEBUG generators.py generate l.373] (45/47) Post-process Answer
[2024-03-05 09:11:06,879 INFO generators.py gen_for_qa l.551] (45/47) * Start with LLM "gemini-pro"
[2024-03-05 09:11:06,882 DEBUG generators.py generate l.352] (45/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:11:06,884 DEBUG generators.py generate l.361] (45/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:11:10,325 DEBUG generators.py generate l.373] (45/47) Post-process Answer
[2024-03-05 09:11:10,329 INFO generators.py gen_for_qa l.551] (45/47) * Start with LLM "claude-2.1"
[2024-03-05 09:11:10,334 DEBUG generators.py generate l.352] (45/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:11:10,337 DEBUG generators.py generate l.361] (45/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:11:11,702 DEBUG generators.py generate l.373] (45/47) Post-process Answer
[2024-03-05 09:11:11,705 INFO generators.py gen_for_qa l.551] (45/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 09:11:11,707 DEBUG generators.py generate l.352] (45/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:11:11,710 DEBUG generators.py generate l.361] (45/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:11:13,677 DEBUG generators.py generate l.373] (45/47) Post-process Answer
[2024-03-05 09:11:13,680 INFO generators.py gen_for_qa l.551] (45/47) * Start with LLM "command-nightly"
[2024-03-05 09:11:13,683 DEBUG generators.py generate l.352] (45/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:11:13,685 DEBUG generators.py generate l.361] (45/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:11:17,508 DEBUG generators.py generate l.373] (45/47) Post-process Answer
[2024-03-05 09:11:17,509 INFO generators.py generate l.480] (45/47) End question "Comment réformer le système de régulation des marchés de la défense et de la sécurité ?  A) Réglementation stricte B) Déréglementation C) Sécurité globale"
[2024-03-05 09:11:17,513 INFO generators.py generate l.478] (46/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés de la coopération internationale ?  A) Réglementation stricte B) Déréglementation C) Diplomatie économique"
[2024-03-05 09:11:17,517 INFO generators.py gen_for_qa l.551] (46/47) * Start with LLM "gpt-4"
[2024-03-05 09:11:17,520 DEBUG generators.py generate l.352] (46/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:11:17,523 DEBUG generators.py generate l.361] (46/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:11:18,170 DEBUG generators.py generate l.373] (46/47) Post-process Answer
[2024-03-05 09:11:18,181 INFO generators.py gen_for_qa l.551] (46/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 09:11:18,183 DEBUG generators.py generate l.352] (46/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:11:18,184 DEBUG generators.py generate l.361] (46/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:11:18,904 DEBUG generators.py generate l.373] (46/47) Post-process Answer
[2024-03-05 09:11:18,908 INFO generators.py gen_for_qa l.551] (46/47) * Start with LLM "gemini-pro"
[2024-03-05 09:11:18,910 DEBUG generators.py generate l.352] (46/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:11:18,913 DEBUG generators.py generate l.361] (46/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:11:22,415 DEBUG generators.py generate l.373] (46/47) Post-process Answer
[2024-03-05 09:11:22,417 INFO generators.py gen_for_qa l.551] (46/47) * Start with LLM "claude-2.1"
[2024-03-05 09:11:22,418 DEBUG generators.py generate l.352] (46/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:11:22,419 DEBUG generators.py generate l.361] (46/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:11:24,038 DEBUG generators.py generate l.373] (46/47) Post-process Answer
[2024-03-05 09:11:24,050 INFO generators.py gen_for_qa l.551] (46/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 09:11:24,053 DEBUG generators.py generate l.352] (46/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:11:24,055 DEBUG generators.py generate l.361] (46/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:11:32,809 DEBUG generators.py generate l.373] (46/47) Post-process Answer
[2024-03-05 09:11:32,814 INFO generators.py gen_for_qa l.551] (46/47) * Start with LLM "command-nightly"
[2024-03-05 09:11:32,816 DEBUG generators.py generate l.352] (46/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:11:32,818 DEBUG generators.py generate l.361] (46/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:11:35,379 DEBUG generators.py generate l.373] (46/47) Post-process Answer
[2024-03-05 09:11:35,381 INFO generators.py generate l.480] (46/47) End question "Comment réformer le système de régulation des marchés de la coopération internationale ?  A) Réglementation stricte B) Déréglementation C) Diplomatie économique"
[2024-03-05 09:11:35,384 INFO generators.py generate l.478] (47/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés de la gouvernance mondiale ?  A) Réglementation stricte B) Déréglementation C) Gouvernance multipartite"
[2024-03-05 09:11:35,387 INFO generators.py gen_for_qa l.551] (47/47) * Start with LLM "gpt-4"
[2024-03-05 09:11:35,389 DEBUG generators.py generate l.352] (47/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:11:35,390 DEBUG generators.py generate l.361] (47/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:11:39,683 DEBUG generators.py generate l.373] (47/47) Post-process Answer
[2024-03-05 09:11:39,686 INFO generators.py gen_for_qa l.551] (47/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 09:11:39,690 DEBUG generators.py generate l.352] (47/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:11:39,692 DEBUG generators.py generate l.361] (47/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:11:41,845 DEBUG generators.py generate l.373] (47/47) Post-process Answer
[2024-03-05 09:11:41,850 INFO generators.py gen_for_qa l.551] (47/47) * Start with LLM "gemini-pro"
[2024-03-05 09:11:41,852 DEBUG generators.py generate l.352] (47/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:11:41,855 DEBUG generators.py generate l.361] (47/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:11:46,388 DEBUG generators.py generate l.373] (47/47) Post-process Answer
[2024-03-05 09:11:46,390 INFO generators.py gen_for_qa l.551] (47/47) * Start with LLM "claude-2.1"
[2024-03-05 09:11:46,391 DEBUG generators.py generate l.352] (47/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:11:46,393 DEBUG generators.py generate l.361] (47/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:11:47,869 DEBUG generators.py generate l.373] (47/47) Post-process Answer
[2024-03-05 09:11:47,875 INFO generators.py gen_for_qa l.551] (47/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 09:11:47,879 DEBUG generators.py generate l.352] (47/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:11:47,883 DEBUG generators.py generate l.361] (47/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:11:48,696 DEBUG generators.py generate l.373] (47/47) Post-process Answer
[2024-03-05 09:11:48,713 INFO generators.py gen_for_qa l.551] (47/47) * Start with LLM "command-nightly"
[2024-03-05 09:11:48,716 DEBUG generators.py generate l.352] (47/47) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-03-05 09:11:48,720 DEBUG generators.py generate l.361] (47/47) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-03-05 09:11:53,494 DEBUG generators.py generate l.373] (47/47) Post-process Answer
[2024-03-05 09:11:53,494 INFO generators.py generate l.480] (47/47) End question "Comment réformer le système de régulation des marchés de la gouvernance mondiale ?  A) Réglementation stricte B) Déréglementation C) Gouvernance multipartite"
[2024-03-05 09:11:53,506 INFO expe.py save_to_json l.283] (47/47) Expe saved as JSON to expe\Answers\eco_fr_v1_gen_with_LeChat--47Q_0C_0F_6M_282A_0HE_0AE_2024-03-05_09,11,53.json
[2024-03-05 09:11:53,507 INFO main.py <module> l.96] (47/47) MAIN ENDS
[2024-03-05 09:17:20,785 INFO main.py <module> l.79] MAIN STARTS
[2024-03-05 09:17:20,888 INFO expe.py save_to_html l.296] Expe saved as HTML to expe\Answers\eco_fr_v1_gen_with_LeChat--47Q_0C_0F_6M_282A_0HE_0AE_2024-03-05_09,17,20.html
[2024-03-05 09:17:21,626 INFO expe.py save_to_spreadsheet l.376] Expe saved as Spreadsheet to expe\Answers\eco_fr_v1_gen_with_LeChat--47Q_0C_0F_6M_282A_0HE_0AE_2024-03-05_09,17,20.xlsx
[2024-03-05 09:17:21,626 INFO main.py <module> l.88] MAIN ENDS
[2024-03-05 09:38:32,306 INFO main.py <module> l.79] MAIN STARTS
[2024-03-05 09:38:32,852 INFO expe.py save_to_spreadsheet l.376] Expe saved as Spreadsheet to expe\Answers\eco_fr_v1_gen_with_LeChat--47Q_0C_0F_6M_282A_0HE_0AE_2024-03-05_09,38,32.xlsx
[2024-03-05 09:38:32,852 INFO main.py <module> l.89] MAIN ENDS
[2024-03-05 14:37:07,582 INFO main.py <module> l.24] MAIN STARTS
[2024-03-05 14:37:32,734 INFO main.py <module> l.24] MAIN STARTS
[2024-03-05 14:37:32,783 INFO expe.py save_to_json l.283] Expe saved as JSON to expe\Questions\eco_fr_v2_gen_with_LeChat--47Q_0C_0F_6M_282A_0HE_0AE_2024-03-05_14,37,32.json
[2024-03-05 14:37:32,790 INFO main.py <module> l.42] MAIN ENDS
[2024-03-05 15:13:03,453 INFO main.py <module> l.24] MAIN STARTS
[2024-03-05 15:13:03,476 INFO generators.py generate l.478] (1/47) *** AnsGenerator for question "Quel est le meilleur système économique ?  A) Capitalisme B) Socialisme démocratique C) Économie mixte"
[2024-03-05 15:13:03,487 INFO generators.py gen_for_qa l.551] (1/47) * Start with LLM "gpt-4"
[2024-03-05 15:13:03,490 DEBUG generators.py gen_for_qa l.557] (1/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:03,491 DEBUG generators.py generate l.355] (1/47) Reuse existing Prompt
[2024-03-05 15:13:03,493 DEBUG generators.py generate l.368] (1/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:03,493 DEBUG generators.py generate l.376] (1/47) Reuse post-processing
[2024-03-05 15:13:03,498 INFO generators.py gen_for_qa l.551] (1/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:13:03,498 DEBUG generators.py gen_for_qa l.557] (1/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:03,502 DEBUG generators.py generate l.355] (1/47) Reuse existing Prompt
[2024-03-05 15:13:03,508 DEBUG generators.py generate l.368] (1/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:03,511 DEBUG generators.py generate l.376] (1/47) Reuse post-processing
[2024-03-05 15:13:03,512 INFO generators.py gen_for_qa l.551] (1/47) * Start with LLM "gemini-pro"
[2024-03-05 15:13:03,514 DEBUG generators.py gen_for_qa l.557] (1/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:03,516 DEBUG generators.py generate l.355] (1/47) Reuse existing Prompt
[2024-03-05 15:13:03,519 DEBUG generators.py generate l.368] (1/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:03,522 DEBUG generators.py generate l.376] (1/47) Reuse post-processing
[2024-03-05 15:13:03,525 INFO generators.py gen_for_qa l.551] (1/47) * Start with LLM "claude-2.1"
[2024-03-05 15:13:03,528 DEBUG generators.py gen_for_qa l.557] (1/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:03,530 DEBUG generators.py generate l.355] (1/47) Reuse existing Prompt
[2024-03-05 15:13:03,531 DEBUG generators.py generate l.368] (1/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:03,532 DEBUG generators.py generate l.376] (1/47) Reuse post-processing
[2024-03-05 15:13:03,535 INFO generators.py gen_for_qa l.551] (1/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:13:03,538 DEBUG generators.py gen_for_qa l.557] (1/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:03,542 DEBUG generators.py generate l.355] (1/47) Reuse existing Prompt
[2024-03-05 15:13:03,544 DEBUG generators.py generate l.368] (1/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:03,546 DEBUG generators.py generate l.376] (1/47) Reuse post-processing
[2024-03-05 15:13:03,548 INFO generators.py gen_for_qa l.551] (1/47) * Start with LLM "command-nightly"
[2024-03-05 15:13:03,550 DEBUG generators.py gen_for_qa l.557] (1/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:03,552 DEBUG generators.py generate l.355] (1/47) Reuse existing Prompt
[2024-03-05 15:13:03,557 DEBUG generators.py generate l.368] (1/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:03,559 DEBUG generators.py generate l.376] (1/47) Reuse post-processing
[2024-03-05 15:13:03,561 INFO generators.py generate l.480] (1/47) End question "Quel est le meilleur système économique ?  A) Capitalisme B) Socialisme démocratique C) Économie mixte"
[2024-03-05 15:13:03,564 INFO generators.py generate l.478] (2/47) *** AnsGenerator for question "Quel est le rôle de l'État dans l'économie ?  A) Interventionnisme B) Libéralisme économique C) Néo-keynésianisme"
[2024-03-05 15:13:03,565 INFO generators.py gen_for_qa l.551] (2/47) * Start with LLM "gpt-4"
[2024-03-05 15:13:03,567 DEBUG generators.py gen_for_qa l.557] (2/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:03,570 DEBUG generators.py generate l.355] (2/47) Reuse existing Prompt
[2024-03-05 15:13:03,573 DEBUG generators.py generate l.368] (2/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:03,577 DEBUG generators.py generate l.376] (2/47) Reuse post-processing
[2024-03-05 15:13:03,579 INFO generators.py gen_for_qa l.551] (2/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:13:03,580 DEBUG generators.py gen_for_qa l.557] (2/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:03,580 DEBUG generators.py generate l.355] (2/47) Reuse existing Prompt
[2024-03-05 15:13:03,584 DEBUG generators.py generate l.368] (2/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:03,587 DEBUG generators.py generate l.376] (2/47) Reuse post-processing
[2024-03-05 15:13:03,590 INFO generators.py gen_for_qa l.551] (2/47) * Start with LLM "gemini-pro"
[2024-03-05 15:13:03,591 DEBUG generators.py gen_for_qa l.557] (2/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:03,595 DEBUG generators.py generate l.355] (2/47) Reuse existing Prompt
[2024-03-05 15:13:03,596 DEBUG generators.py generate l.368] (2/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:03,596 DEBUG generators.py generate l.376] (2/47) Reuse post-processing
[2024-03-05 15:13:03,598 INFO generators.py gen_for_qa l.551] (2/47) * Start with LLM "claude-2.1"
[2024-03-05 15:13:03,599 DEBUG generators.py gen_for_qa l.557] (2/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:03,604 DEBUG generators.py generate l.355] (2/47) Reuse existing Prompt
[2024-03-05 15:13:03,606 DEBUG generators.py generate l.368] (2/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:03,609 DEBUG generators.py generate l.376] (2/47) Reuse post-processing
[2024-03-05 15:13:03,611 INFO generators.py gen_for_qa l.551] (2/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:13:03,613 DEBUG generators.py gen_for_qa l.557] (2/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:03,615 DEBUG generators.py generate l.355] (2/47) Reuse existing Prompt
[2024-03-05 15:13:03,615 DEBUG generators.py generate l.368] (2/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:03,619 DEBUG generators.py generate l.376] (2/47) Reuse post-processing
[2024-03-05 15:13:03,622 INFO generators.py gen_for_qa l.551] (2/47) * Start with LLM "command-nightly"
[2024-03-05 15:13:03,624 DEBUG generators.py gen_for_qa l.557] (2/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:03,627 DEBUG generators.py generate l.355] (2/47) Reuse existing Prompt
[2024-03-05 15:13:03,629 DEBUG generators.py generate l.368] (2/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:03,630 DEBUG generators.py generate l.376] (2/47) Reuse post-processing
[2024-03-05 15:13:03,631 INFO generators.py generate l.480] (2/47) End question "Quel est le rôle de l'État dans l'économie ?  A) Interventionnisme B) Libéralisme économique C) Néo-keynésianisme"
[2024-03-05 15:13:03,633 INFO generators.py generate l.478] (3/47) *** AnsGenerator for question "Comment réduire les inégalités ?  A) Redistribution B) Croissance économique C) Investissement social"
[2024-03-05 15:13:03,636 INFO generators.py gen_for_qa l.551] (3/47) * Start with LLM "gpt-4"
[2024-03-05 15:13:03,639 DEBUG generators.py gen_for_qa l.557] (3/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:03,641 DEBUG generators.py generate l.355] (3/47) Reuse existing Prompt
[2024-03-05 15:13:03,642 DEBUG generators.py generate l.368] (3/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:03,644 DEBUG generators.py generate l.376] (3/47) Reuse post-processing
[2024-03-05 15:13:03,645 INFO generators.py gen_for_qa l.551] (3/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:13:03,646 DEBUG generators.py gen_for_qa l.557] (3/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:03,649 DEBUG generators.py generate l.355] (3/47) Reuse existing Prompt
[2024-03-05 15:13:03,650 DEBUG generators.py generate l.368] (3/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:03,652 DEBUG generators.py generate l.376] (3/47) Reuse post-processing
[2024-03-05 15:13:03,655 INFO generators.py gen_for_qa l.551] (3/47) * Start with LLM "gemini-pro"
[2024-03-05 15:13:03,657 DEBUG generators.py gen_for_qa l.557] (3/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:03,659 DEBUG generators.py generate l.355] (3/47) Reuse existing Prompt
[2024-03-05 15:13:03,659 DEBUG generators.py generate l.368] (3/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:03,663 DEBUG generators.py generate l.376] (3/47) Reuse post-processing
[2024-03-05 15:13:03,664 INFO generators.py gen_for_qa l.551] (3/47) * Start with LLM "claude-2.1"
[2024-03-05 15:13:03,667 DEBUG generators.py gen_for_qa l.557] (3/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:03,668 DEBUG generators.py generate l.355] (3/47) Reuse existing Prompt
[2024-03-05 15:13:03,671 DEBUG generators.py generate l.368] (3/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:03,674 DEBUG generators.py generate l.376] (3/47) Reuse post-processing
[2024-03-05 15:13:03,675 INFO generators.py gen_for_qa l.551] (3/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:13:03,678 DEBUG generators.py gen_for_qa l.557] (3/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:03,679 DEBUG generators.py generate l.355] (3/47) Reuse existing Prompt
[2024-03-05 15:13:03,679 DEBUG generators.py generate l.368] (3/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:03,682 DEBUG generators.py generate l.376] (3/47) Reuse post-processing
[2024-03-05 15:13:03,685 INFO generators.py gen_for_qa l.551] (3/47) * Start with LLM "command-nightly"
[2024-03-05 15:13:03,689 DEBUG generators.py gen_for_qa l.557] (3/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:03,691 DEBUG generators.py generate l.355] (3/47) Reuse existing Prompt
[2024-03-05 15:13:03,693 DEBUG generators.py generate l.368] (3/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:03,695 DEBUG generators.py generate l.376] (3/47) Reuse post-processing
[2024-03-05 15:13:03,697 INFO generators.py generate l.480] (3/47) End question "Comment réduire les inégalités ?  A) Redistribution B) Croissance économique C) Investissement social"
[2024-03-05 15:13:03,698 INFO generators.py generate l.478] (4/47) *** AnsGenerator for question "Comment stimuler la croissance économique ?  A) Investissement public B) Déréglementation C) Innovation technologique"
[2024-03-05 15:13:03,701 INFO generators.py gen_for_qa l.551] (4/47) * Start with LLM "gpt-4"
[2024-03-05 15:13:03,704 DEBUG generators.py gen_for_qa l.557] (4/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:03,707 DEBUG generators.py generate l.355] (4/47) Reuse existing Prompt
[2024-03-05 15:13:03,710 DEBUG generators.py generate l.368] (4/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:03,712 DEBUG generators.py generate l.376] (4/47) Reuse post-processing
[2024-03-05 15:13:03,713 INFO generators.py gen_for_qa l.551] (4/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:13:03,716 DEBUG generators.py gen_for_qa l.557] (4/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:03,718 DEBUG generators.py generate l.355] (4/47) Reuse existing Prompt
[2024-03-05 15:13:03,722 DEBUG generators.py generate l.368] (4/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:03,724 DEBUG generators.py generate l.376] (4/47) Reuse post-processing
[2024-03-05 15:13:03,727 INFO generators.py gen_for_qa l.551] (4/47) * Start with LLM "gemini-pro"
[2024-03-05 15:13:03,728 DEBUG generators.py gen_for_qa l.557] (4/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:03,728 DEBUG generators.py generate l.355] (4/47) Reuse existing Prompt
[2024-03-05 15:13:03,734 DEBUG generators.py generate l.368] (4/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:03,737 DEBUG generators.py generate l.376] (4/47) Reuse post-processing
[2024-03-05 15:13:03,740 INFO generators.py gen_for_qa l.551] (4/47) * Start with LLM "claude-2.1"
[2024-03-05 15:13:03,743 DEBUG generators.py gen_for_qa l.557] (4/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:03,746 DEBUG generators.py generate l.355] (4/47) Reuse existing Prompt
[2024-03-05 15:13:03,746 DEBUG generators.py generate l.368] (4/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:03,749 DEBUG generators.py generate l.376] (4/47) Reuse post-processing
[2024-03-05 15:13:03,752 INFO generators.py gen_for_qa l.551] (4/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:13:03,756 DEBUG generators.py gen_for_qa l.557] (4/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:03,759 DEBUG generators.py generate l.355] (4/47) Reuse existing Prompt
[2024-03-05 15:13:03,761 DEBUG generators.py generate l.368] (4/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:03,764 DEBUG generators.py generate l.376] (4/47) Reuse post-processing
[2024-03-05 15:13:03,764 INFO generators.py gen_for_qa l.551] (4/47) * Start with LLM "command-nightly"
[2024-03-05 15:13:03,769 DEBUG generators.py gen_for_qa l.557] (4/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:03,772 DEBUG generators.py generate l.355] (4/47) Reuse existing Prompt
[2024-03-05 15:13:03,775 DEBUG generators.py generate l.368] (4/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:03,776 DEBUG generators.py generate l.376] (4/47) Reuse post-processing
[2024-03-05 15:13:03,779 INFO generators.py generate l.480] (4/47) End question "Comment stimuler la croissance économique ?  A) Investissement public B) Déréglementation C) Innovation technologique"
[2024-03-05 15:13:03,780 INFO generators.py generate l.478] (5/47) *** AnsGenerator for question "Comment lutter contre l'inflation ?  A) Contrôle des prix et des salaires B) Politique monétaire restrictive C) Indexation des salaires"
[2024-03-05 15:13:03,783 INFO generators.py gen_for_qa l.551] (5/47) * Start with LLM "gpt-4"
[2024-03-05 15:13:03,788 DEBUG generators.py gen_for_qa l.557] (5/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:03,791 DEBUG generators.py generate l.355] (5/47) Reuse existing Prompt
[2024-03-05 15:13:03,793 DEBUG generators.py generate l.368] (5/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:03,796 DEBUG generators.py generate l.376] (5/47) Reuse post-processing
[2024-03-05 15:13:03,796 INFO generators.py gen_for_qa l.551] (5/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:13:03,802 DEBUG generators.py gen_for_qa l.557] (5/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:03,805 DEBUG generators.py generate l.355] (5/47) Reuse existing Prompt
[2024-03-05 15:13:03,810 DEBUG generators.py generate l.368] (5/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:03,811 DEBUG generators.py generate l.376] (5/47) Reuse post-processing
[2024-03-05 15:13:03,815 INFO generators.py gen_for_qa l.551] (5/47) * Start with LLM "gemini-pro"
[2024-03-05 15:13:03,819 DEBUG generators.py gen_for_qa l.557] (5/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:03,824 DEBUG generators.py generate l.355] (5/47) Reuse existing Prompt
[2024-03-05 15:13:03,828 DEBUG generators.py generate l.368] (5/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:03,831 DEBUG generators.py generate l.376] (5/47) Reuse post-processing
[2024-03-05 15:13:03,836 INFO generators.py gen_for_qa l.551] (5/47) * Start with LLM "claude-2.1"
[2024-03-05 15:13:03,840 DEBUG generators.py gen_for_qa l.557] (5/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:03,843 DEBUG generators.py generate l.355] (5/47) Reuse existing Prompt
[2024-03-05 15:13:03,846 DEBUG generators.py generate l.368] (5/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:03,849 DEBUG generators.py generate l.376] (5/47) Reuse post-processing
[2024-03-05 15:13:03,853 INFO generators.py gen_for_qa l.551] (5/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:13:03,856 DEBUG generators.py gen_for_qa l.557] (5/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:03,863 DEBUG generators.py generate l.355] (5/47) Reuse existing Prompt
[2024-03-05 15:13:03,863 DEBUG generators.py generate l.368] (5/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:03,863 DEBUG generators.py generate l.376] (5/47) Reuse post-processing
[2024-03-05 15:13:03,872 INFO generators.py gen_for_qa l.551] (5/47) * Start with LLM "command-nightly"
[2024-03-05 15:13:03,876 DEBUG generators.py gen_for_qa l.557] (5/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:03,878 DEBUG generators.py generate l.355] (5/47) Reuse existing Prompt
[2024-03-05 15:13:03,881 DEBUG generators.py generate l.368] (5/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:03,885 DEBUG generators.py generate l.376] (5/47) Reuse post-processing
[2024-03-05 15:13:03,889 INFO generators.py generate l.480] (5/47) End question "Comment lutter contre l'inflation ?  A) Contrôle des prix et des salaires B) Politique monétaire restrictive C) Indexation des salaires"
[2024-03-05 15:13:03,895 INFO generators.py generate l.478] (6/47) *** AnsGenerator for question "Comment lutter contre le chômage ?  A) Politique de l'emploi B) Flexibilisation du marché du travail C) Formation professionnelle"
[2024-03-05 15:13:03,896 INFO generators.py gen_for_qa l.551] (6/47) * Start with LLM "gpt-4"
[2024-03-05 15:13:03,902 DEBUG generators.py gen_for_qa l.557] (6/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:03,905 DEBUG generators.py generate l.355] (6/47) Reuse existing Prompt
[2024-03-05 15:13:03,911 DEBUG generators.py generate l.368] (6/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:03,915 DEBUG generators.py generate l.376] (6/47) Reuse post-processing
[2024-03-05 15:13:03,918 INFO generators.py gen_for_qa l.551] (6/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:13:03,923 DEBUG generators.py gen_for_qa l.557] (6/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:03,927 DEBUG generators.py generate l.355] (6/47) Reuse existing Prompt
[2024-03-05 15:13:03,928 DEBUG generators.py generate l.368] (6/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:03,933 DEBUG generators.py generate l.376] (6/47) Reuse post-processing
[2024-03-05 15:13:03,938 INFO generators.py gen_for_qa l.551] (6/47) * Start with LLM "gemini-pro"
[2024-03-05 15:13:03,944 DEBUG generators.py gen_for_qa l.557] (6/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:03,946 DEBUG generators.py generate l.355] (6/47) Reuse existing Prompt
[2024-03-05 15:13:03,953 DEBUG generators.py generate l.368] (6/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:03,959 DEBUG generators.py generate l.376] (6/47) Reuse post-processing
[2024-03-05 15:13:03,963 INFO generators.py gen_for_qa l.551] (6/47) * Start with LLM "claude-2.1"
[2024-03-05 15:13:03,970 DEBUG generators.py gen_for_qa l.557] (6/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:03,977 DEBUG generators.py generate l.355] (6/47) Reuse existing Prompt
[2024-03-05 15:13:03,982 DEBUG generators.py generate l.368] (6/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:03,988 DEBUG generators.py generate l.376] (6/47) Reuse post-processing
[2024-03-05 15:13:03,995 INFO generators.py gen_for_qa l.551] (6/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:13:03,997 DEBUG generators.py gen_for_qa l.557] (6/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,006 DEBUG generators.py generate l.355] (6/47) Reuse existing Prompt
[2024-03-05 15:13:04,011 DEBUG generators.py generate l.368] (6/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,014 DEBUG generators.py generate l.376] (6/47) Reuse post-processing
[2024-03-05 15:13:04,021 INFO generators.py gen_for_qa l.551] (6/47) * Start with LLM "command-nightly"
[2024-03-05 15:13:04,025 DEBUG generators.py gen_for_qa l.557] (6/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,029 DEBUG generators.py generate l.355] (6/47) Reuse existing Prompt
[2024-03-05 15:13:04,037 DEBUG generators.py generate l.368] (6/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,043 DEBUG generators.py generate l.376] (6/47) Reuse post-processing
[2024-03-05 15:13:04,045 INFO generators.py generate l.480] (6/47) End question "Comment lutter contre le chômage ?  A) Politique de l'emploi B) Flexibilisation du marché du travail C) Formation professionnelle"
[2024-03-05 15:13:04,055 INFO generators.py generate l.478] (7/47) *** AnsGenerator for question "Comment réformer le système fiscal ?  A) Impôt progressif B) Flat tax C) TVA sociale"
[2024-03-05 15:13:04,062 INFO generators.py gen_for_qa l.551] (7/47) * Start with LLM "gpt-4"
[2024-03-05 15:13:04,065 DEBUG generators.py gen_for_qa l.557] (7/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,071 DEBUG generators.py generate l.355] (7/47) Reuse existing Prompt
[2024-03-05 15:13:04,077 DEBUG generators.py generate l.368] (7/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,082 DEBUG generators.py generate l.376] (7/47) Reuse post-processing
[2024-03-05 15:13:04,086 INFO generators.py gen_for_qa l.551] (7/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:13:04,087 DEBUG generators.py gen_for_qa l.557] (7/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,093 DEBUG generators.py generate l.355] (7/47) Reuse existing Prompt
[2024-03-05 15:13:04,096 DEBUG generators.py generate l.368] (7/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,097 DEBUG generators.py generate l.376] (7/47) Reuse post-processing
[2024-03-05 15:13:04,101 INFO generators.py gen_for_qa l.551] (7/47) * Start with LLM "gemini-pro"
[2024-03-05 15:13:04,106 DEBUG generators.py gen_for_qa l.557] (7/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,108 DEBUG generators.py generate l.355] (7/47) Reuse existing Prompt
[2024-03-05 15:13:04,111 DEBUG generators.py generate l.368] (7/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,112 DEBUG generators.py generate l.376] (7/47) Reuse post-processing
[2024-03-05 15:13:04,115 INFO generators.py gen_for_qa l.551] (7/47) * Start with LLM "claude-2.1"
[2024-03-05 15:13:04,116 DEBUG generators.py gen_for_qa l.557] (7/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,119 DEBUG generators.py generate l.355] (7/47) Reuse existing Prompt
[2024-03-05 15:13:04,122 DEBUG generators.py generate l.368] (7/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,125 DEBUG generators.py generate l.376] (7/47) Reuse post-processing
[2024-03-05 15:13:04,127 INFO generators.py gen_for_qa l.551] (7/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:13:04,129 DEBUG generators.py gen_for_qa l.557] (7/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,130 DEBUG generators.py generate l.355] (7/47) Reuse existing Prompt
[2024-03-05 15:13:04,133 DEBUG generators.py generate l.368] (7/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,135 DEBUG generators.py generate l.376] (7/47) Reuse post-processing
[2024-03-05 15:13:04,138 INFO generators.py gen_for_qa l.551] (7/47) * Start with LLM "command-nightly"
[2024-03-05 15:13:04,141 DEBUG generators.py gen_for_qa l.557] (7/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,145 DEBUG generators.py generate l.355] (7/47) Reuse existing Prompt
[2024-03-05 15:13:04,148 DEBUG generators.py generate l.368] (7/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,150 DEBUG generators.py generate l.376] (7/47) Reuse post-processing
[2024-03-05 15:13:04,154 INFO generators.py generate l.480] (7/47) End question "Comment réformer le système fiscal ?  A) Impôt progressif B) Flat tax C) TVA sociale"
[2024-03-05 15:13:04,158 INFO generators.py generate l.478] (8/47) *** AnsGenerator for question "Comment réformer le système de protection sociale ?  A) Protection sociale universelle B) Responsabilisation individuelle C) Assurance privée"
[2024-03-05 15:13:04,158 INFO generators.py gen_for_qa l.551] (8/47) * Start with LLM "gpt-4"
[2024-03-05 15:13:04,167 DEBUG generators.py gen_for_qa l.557] (8/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,172 DEBUG generators.py generate l.355] (8/47) Reuse existing Prompt
[2024-03-05 15:13:04,176 DEBUG generators.py generate l.368] (8/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,176 DEBUG generators.py generate l.376] (8/47) Reuse post-processing
[2024-03-05 15:13:04,186 INFO generators.py gen_for_qa l.551] (8/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:13:04,194 DEBUG generators.py gen_for_qa l.557] (8/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,196 DEBUG generators.py generate l.355] (8/47) Reuse existing Prompt
[2024-03-05 15:13:04,204 DEBUG generators.py generate l.368] (8/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,210 DEBUG generators.py generate l.376] (8/47) Reuse post-processing
[2024-03-05 15:13:04,210 INFO generators.py gen_for_qa l.551] (8/47) * Start with LLM "gemini-pro"
[2024-03-05 15:13:04,215 DEBUG generators.py gen_for_qa l.557] (8/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,222 DEBUG generators.py generate l.355] (8/47) Reuse existing Prompt
[2024-03-05 15:13:04,225 DEBUG generators.py generate l.368] (8/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,226 DEBUG generators.py generate l.376] (8/47) Reuse post-processing
[2024-03-05 15:13:04,229 INFO generators.py gen_for_qa l.551] (8/47) * Start with LLM "claude-2.1"
[2024-03-05 15:13:04,230 DEBUG generators.py gen_for_qa l.557] (8/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,232 DEBUG generators.py generate l.355] (8/47) Reuse existing Prompt
[2024-03-05 15:13:04,235 DEBUG generators.py generate l.368] (8/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,238 DEBUG generators.py generate l.376] (8/47) Reuse post-processing
[2024-03-05 15:13:04,242 INFO generators.py gen_for_qa l.551] (8/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:13:04,244 DEBUG generators.py gen_for_qa l.557] (8/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,244 DEBUG generators.py generate l.355] (8/47) Reuse existing Prompt
[2024-03-05 15:13:04,244 DEBUG generators.py generate l.368] (8/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,249 DEBUG generators.py generate l.376] (8/47) Reuse post-processing
[2024-03-05 15:13:04,253 INFO generators.py gen_for_qa l.551] (8/47) * Start with LLM "command-nightly"
[2024-03-05 15:13:04,257 DEBUG generators.py gen_for_qa l.557] (8/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,260 DEBUG generators.py generate l.355] (8/47) Reuse existing Prompt
[2024-03-05 15:13:04,263 DEBUG generators.py generate l.368] (8/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,264 DEBUG generators.py generate l.376] (8/47) Reuse post-processing
[2024-03-05 15:13:04,269 INFO generators.py generate l.480] (8/47) End question "Comment réformer le système de protection sociale ?  A) Protection sociale universelle B) Responsabilisation individuelle C) Assurance privée"
[2024-03-05 15:13:04,272 INFO generators.py generate l.478] (9/47) *** AnsGenerator for question "Comment réformer le système de retraite ?  A) Retraite par répartition B) Retraite par capitalisation C) Retraite à points"
[2024-03-05 15:13:04,276 INFO generators.py gen_for_qa l.551] (9/47) * Start with LLM "gpt-4"
[2024-03-05 15:13:04,276 DEBUG generators.py gen_for_qa l.557] (9/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,276 DEBUG generators.py generate l.355] (9/47) Reuse existing Prompt
[2024-03-05 15:13:04,276 DEBUG generators.py generate l.368] (9/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,284 DEBUG generators.py generate l.376] (9/47) Reuse post-processing
[2024-03-05 15:13:04,288 INFO generators.py gen_for_qa l.551] (9/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:13:04,289 DEBUG generators.py gen_for_qa l.557] (9/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,291 DEBUG generators.py generate l.355] (9/47) Reuse existing Prompt
[2024-03-05 15:13:04,296 DEBUG generators.py generate l.368] (9/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,298 DEBUG generators.py generate l.376] (9/47) Reuse post-processing
[2024-03-05 15:13:04,304 INFO generators.py gen_for_qa l.551] (9/47) * Start with LLM "gemini-pro"
[2024-03-05 15:13:04,306 DEBUG generators.py gen_for_qa l.557] (9/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,308 DEBUG generators.py generate l.355] (9/47) Reuse existing Prompt
[2024-03-05 15:13:04,311 DEBUG generators.py generate l.368] (9/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,313 DEBUG generators.py generate l.376] (9/47) Reuse post-processing
[2024-03-05 15:13:04,315 INFO generators.py gen_for_qa l.551] (9/47) * Start with LLM "claude-2.1"
[2024-03-05 15:13:04,316 DEBUG generators.py gen_for_qa l.557] (9/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,321 DEBUG generators.py generate l.355] (9/47) Reuse existing Prompt
[2024-03-05 15:13:04,323 DEBUG generators.py generate l.368] (9/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,323 DEBUG generators.py generate l.376] (9/47) Reuse post-processing
[2024-03-05 15:13:04,328 INFO generators.py gen_for_qa l.551] (9/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:13:04,329 DEBUG generators.py gen_for_qa l.557] (9/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,331 DEBUG generators.py generate l.355] (9/47) Reuse existing Prompt
[2024-03-05 15:13:04,333 DEBUG generators.py generate l.368] (9/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,338 DEBUG generators.py generate l.376] (9/47) Reuse post-processing
[2024-03-05 15:13:04,339 INFO generators.py gen_for_qa l.551] (9/47) * Start with LLM "command-nightly"
[2024-03-05 15:13:04,342 DEBUG generators.py gen_for_qa l.557] (9/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,342 DEBUG generators.py generate l.355] (9/47) Reuse existing Prompt
[2024-03-05 15:13:04,344 DEBUG generators.py generate l.368] (9/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,348 DEBUG generators.py generate l.376] (9/47) Reuse post-processing
[2024-03-05 15:13:04,348 INFO generators.py generate l.480] (9/47) End question "Comment réformer le système de retraite ?  A) Retraite par répartition B) Retraite par capitalisation C) Retraite à points"
[2024-03-05 15:13:04,350 INFO generators.py generate l.478] (10/47) *** AnsGenerator for question "Comment réformer le marché du travail ?  A) Sécurisation de l'emploi B) Flexibilisation du marché du travail C) Compte personnel d'activité"
[2024-03-05 15:13:04,353 INFO generators.py gen_for_qa l.551] (10/47) * Start with LLM "gpt-4"
[2024-03-05 15:13:04,356 DEBUG generators.py gen_for_qa l.557] (10/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,356 DEBUG generators.py generate l.355] (10/47) Reuse existing Prompt
[2024-03-05 15:13:04,360 DEBUG generators.py generate l.368] (10/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,361 DEBUG generators.py generate l.376] (10/47) Reuse post-processing
[2024-03-05 15:13:04,361 INFO generators.py gen_for_qa l.551] (10/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:13:04,364 DEBUG generators.py gen_for_qa l.557] (10/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,368 DEBUG generators.py generate l.355] (10/47) Reuse existing Prompt
[2024-03-05 15:13:04,371 DEBUG generators.py generate l.368] (10/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,374 DEBUG generators.py generate l.376] (10/47) Reuse post-processing
[2024-03-05 15:13:04,376 INFO generators.py gen_for_qa l.551] (10/47) * Start with LLM "gemini-pro"
[2024-03-05 15:13:04,378 DEBUG generators.py gen_for_qa l.557] (10/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,378 DEBUG generators.py generate l.355] (10/47) Reuse existing Prompt
[2024-03-05 15:13:04,380 DEBUG generators.py generate l.368] (10/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,382 DEBUG generators.py generate l.376] (10/47) Reuse post-processing
[2024-03-05 15:13:04,385 INFO generators.py gen_for_qa l.551] (10/47) * Start with LLM "claude-2.1"
[2024-03-05 15:13:04,387 DEBUG generators.py gen_for_qa l.557] (10/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,391 DEBUG generators.py generate l.355] (10/47) Reuse existing Prompt
[2024-03-05 15:13:04,393 DEBUG generators.py generate l.368] (10/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,393 DEBUG generators.py generate l.376] (10/47) Reuse post-processing
[2024-03-05 15:13:04,394 INFO generators.py gen_for_qa l.551] (10/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:13:04,396 DEBUG generators.py gen_for_qa l.557] (10/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,396 DEBUG generators.py generate l.355] (10/47) Reuse existing Prompt
[2024-03-05 15:13:04,396 DEBUG generators.py generate l.368] (10/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,401 DEBUG generators.py generate l.376] (10/47) Reuse post-processing
[2024-03-05 15:13:04,405 INFO generators.py gen_for_qa l.551] (10/47) * Start with LLM "command-nightly"
[2024-03-05 15:13:04,408 DEBUG generators.py gen_for_qa l.557] (10/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,409 DEBUG generators.py generate l.355] (10/47) Reuse existing Prompt
[2024-03-05 15:13:04,409 DEBUG generators.py generate l.368] (10/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,409 DEBUG generators.py generate l.376] (10/47) Reuse post-processing
[2024-03-05 15:13:04,415 INFO generators.py generate l.480] (10/47) End question "Comment réformer le marché du travail ?  A) Sécurisation de l'emploi B) Flexibilisation du marché du travail C) Compte personnel d'activité"
[2024-03-05 15:13:04,417 INFO generators.py generate l.478] (11/47) *** AnsGenerator for question "Comment réformer le système éducatif ?  A) Éducation gratuite et obligatoire B) Libéralisation de l'éducation C) Formation professionnelle"
[2024-03-05 15:13:04,421 INFO generators.py gen_for_qa l.551] (11/47) * Start with LLM "gpt-4"
[2024-03-05 15:13:04,424 DEBUG generators.py gen_for_qa l.557] (11/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,426 DEBUG generators.py generate l.355] (11/47) Reuse existing Prompt
[2024-03-05 15:13:04,428 DEBUG generators.py generate l.368] (11/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,431 DEBUG generators.py generate l.376] (11/47) Reuse post-processing
[2024-03-05 15:13:04,433 INFO generators.py gen_for_qa l.551] (11/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:13:04,437 DEBUG generators.py gen_for_qa l.557] (11/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,440 DEBUG generators.py generate l.355] (11/47) Reuse existing Prompt
[2024-03-05 15:13:04,442 DEBUG generators.py generate l.368] (11/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,442 DEBUG generators.py generate l.376] (11/47) Reuse post-processing
[2024-03-05 15:13:04,442 INFO generators.py gen_for_qa l.551] (11/47) * Start with LLM "gemini-pro"
[2024-03-05 15:13:04,447 DEBUG generators.py gen_for_qa l.557] (11/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,447 DEBUG generators.py generate l.355] (11/47) Reuse existing Prompt
[2024-03-05 15:13:04,453 DEBUG generators.py generate l.368] (11/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,456 DEBUG generators.py generate l.376] (11/47) Reuse post-processing
[2024-03-05 15:13:04,458 INFO generators.py gen_for_qa l.551] (11/47) * Start with LLM "claude-2.1"
[2024-03-05 15:13:04,460 DEBUG generators.py gen_for_qa l.557] (11/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,460 DEBUG generators.py generate l.355] (11/47) Reuse existing Prompt
[2024-03-05 15:13:04,460 DEBUG generators.py generate l.368] (11/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,466 DEBUG generators.py generate l.376] (11/47) Reuse post-processing
[2024-03-05 15:13:04,469 INFO generators.py gen_for_qa l.551] (11/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:13:04,474 DEBUG generators.py gen_for_qa l.557] (11/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,476 DEBUG generators.py generate l.355] (11/47) Reuse existing Prompt
[2024-03-05 15:13:04,478 DEBUG generators.py generate l.368] (11/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,480 DEBUG generators.py generate l.376] (11/47) Reuse post-processing
[2024-03-05 15:13:04,482 INFO generators.py gen_for_qa l.551] (11/47) * Start with LLM "command-nightly"
[2024-03-05 15:13:04,485 DEBUG generators.py gen_for_qa l.557] (11/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,488 DEBUG generators.py generate l.355] (11/47) Reuse existing Prompt
[2024-03-05 15:13:04,489 DEBUG generators.py generate l.368] (11/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,493 DEBUG generators.py generate l.376] (11/47) Reuse post-processing
[2024-03-05 15:13:04,493 INFO generators.py generate l.480] (11/47) End question "Comment réformer le système éducatif ?  A) Éducation gratuite et obligatoire B) Libéralisation de l'éducation C) Formation professionnelle"
[2024-03-05 15:13:04,497 INFO generators.py generate l.478] (12/47) *** AnsGenerator for question "Comment réformer le système de santé ?  A) Système de santé public B) Système de santé privé C) Assurance maladie obligatoire"
[2024-03-05 15:13:04,498 INFO generators.py gen_for_qa l.551] (12/47) * Start with LLM "gpt-4"
[2024-03-05 15:13:04,501 DEBUG generators.py gen_for_qa l.557] (12/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,505 DEBUG generators.py generate l.355] (12/47) Reuse existing Prompt
[2024-03-05 15:13:04,507 DEBUG generators.py generate l.368] (12/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,507 DEBUG generators.py generate l.376] (12/47) Reuse post-processing
[2024-03-05 15:13:04,509 INFO generators.py gen_for_qa l.551] (12/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:13:04,512 DEBUG generators.py gen_for_qa l.557] (12/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,513 DEBUG generators.py generate l.355] (12/47) Reuse existing Prompt
[2024-03-05 15:13:04,513 DEBUG generators.py generate l.368] (12/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,513 DEBUG generators.py generate l.376] (12/47) Reuse post-processing
[2024-03-05 15:13:04,518 INFO generators.py gen_for_qa l.551] (12/47) * Start with LLM "gemini-pro"
[2024-03-05 15:13:04,521 DEBUG generators.py gen_for_qa l.557] (12/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,523 DEBUG generators.py generate l.355] (12/47) Reuse existing Prompt
[2024-03-05 15:13:04,526 DEBUG generators.py generate l.368] (12/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,528 DEBUG generators.py generate l.376] (12/47) Reuse post-processing
[2024-03-05 15:13:04,528 INFO generators.py gen_for_qa l.551] (12/47) * Start with LLM "claude-2.1"
[2024-03-05 15:13:04,532 DEBUG generators.py gen_for_qa l.557] (12/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,535 DEBUG generators.py generate l.355] (12/47) Reuse existing Prompt
[2024-03-05 15:13:04,538 DEBUG generators.py generate l.368] (12/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,540 DEBUG generators.py generate l.376] (12/47) Reuse post-processing
[2024-03-05 15:13:04,541 INFO generators.py gen_for_qa l.551] (12/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:13:04,541 DEBUG generators.py gen_for_qa l.557] (12/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,545 DEBUG generators.py generate l.355] (12/47) Reuse existing Prompt
[2024-03-05 15:13:04,547 DEBUG generators.py generate l.368] (12/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,549 DEBUG generators.py generate l.376] (12/47) Reuse post-processing
[2024-03-05 15:13:04,553 INFO generators.py gen_for_qa l.551] (12/47) * Start with LLM "command-nightly"
[2024-03-05 15:13:04,556 DEBUG generators.py gen_for_qa l.557] (12/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,558 DEBUG generators.py generate l.355] (12/47) Reuse existing Prompt
[2024-03-05 15:13:04,558 DEBUG generators.py generate l.368] (12/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,558 DEBUG generators.py generate l.376] (12/47) Reuse post-processing
[2024-03-05 15:13:04,564 INFO generators.py generate l.480] (12/47) End question "Comment réformer le système de santé ?  A) Système de santé public B) Système de santé privé C) Assurance maladie obligatoire"
[2024-03-05 15:13:04,565 INFO generators.py generate l.478] (13/47) *** AnsGenerator for question "Comment réformer le système bancaire ?  A) Banques publiques B) Déréglementation bancaire C) Régulation bancaire"
[2024-03-05 15:13:04,568 INFO generators.py gen_for_qa l.551] (13/47) * Start with LLM "gpt-4"
[2024-03-05 15:13:04,571 DEBUG generators.py gen_for_qa l.557] (13/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,574 DEBUG generators.py generate l.355] (13/47) Reuse existing Prompt
[2024-03-05 15:13:04,576 DEBUG generators.py generate l.368] (13/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,576 DEBUG generators.py generate l.376] (13/47) Reuse post-processing
[2024-03-05 15:13:04,576 INFO generators.py gen_for_qa l.551] (13/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:13:04,576 DEBUG generators.py gen_for_qa l.557] (13/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,576 DEBUG generators.py generate l.355] (13/47) Reuse existing Prompt
[2024-03-05 15:13:04,585 DEBUG generators.py generate l.368] (13/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,587 DEBUG generators.py generate l.376] (13/47) Reuse post-processing
[2024-03-05 15:13:04,590 INFO generators.py gen_for_qa l.551] (13/47) * Start with LLM "gemini-pro"
[2024-03-05 15:13:04,590 DEBUG generators.py gen_for_qa l.557] (13/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,593 DEBUG generators.py generate l.355] (13/47) Reuse existing Prompt
[2024-03-05 15:13:04,596 DEBUG generators.py generate l.368] (13/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,597 DEBUG generators.py generate l.376] (13/47) Reuse post-processing
[2024-03-05 15:13:04,598 INFO generators.py gen_for_qa l.551] (13/47) * Start with LLM "claude-2.1"
[2024-03-05 15:13:04,599 DEBUG generators.py gen_for_qa l.557] (13/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,603 DEBUG generators.py generate l.355] (13/47) Reuse existing Prompt
[2024-03-05 15:13:04,606 DEBUG generators.py generate l.368] (13/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,607 DEBUG generators.py generate l.376] (13/47) Reuse post-processing
[2024-03-05 15:13:04,609 INFO generators.py gen_for_qa l.551] (13/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:13:04,610 DEBUG generators.py gen_for_qa l.557] (13/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,610 DEBUG generators.py generate l.355] (13/47) Reuse existing Prompt
[2024-03-05 15:13:04,612 DEBUG generators.py generate l.368] (13/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,615 DEBUG generators.py generate l.376] (13/47) Reuse post-processing
[2024-03-05 15:13:04,615 INFO generators.py gen_for_qa l.551] (13/47) * Start with LLM "command-nightly"
[2024-03-05 15:13:04,618 DEBUG generators.py gen_for_qa l.557] (13/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,621 DEBUG generators.py generate l.355] (13/47) Reuse existing Prompt
[2024-03-05 15:13:04,623 DEBUG generators.py generate l.368] (13/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,623 DEBUG generators.py generate l.376] (13/47) Reuse post-processing
[2024-03-05 15:13:04,623 INFO generators.py generate l.480] (13/47) End question "Comment réformer le système bancaire ?  A) Banques publiques B) Déréglementation bancaire C) Régulation bancaire"
[2024-03-05 15:13:04,628 INFO generators.py generate l.478] (14/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés ?  A) Régulation étatique B) Déréglementation C) Autorégulation"
[2024-03-05 15:13:04,628 INFO generators.py gen_for_qa l.551] (14/47) * Start with LLM "gpt-4"
[2024-03-05 15:13:04,628 DEBUG generators.py gen_for_qa l.557] (14/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,632 DEBUG generators.py generate l.355] (14/47) Reuse existing Prompt
[2024-03-05 15:13:04,637 DEBUG generators.py generate l.368] (14/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,639 DEBUG generators.py generate l.376] (14/47) Reuse post-processing
[2024-03-05 15:13:04,643 INFO generators.py gen_for_qa l.551] (14/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:13:04,644 DEBUG generators.py gen_for_qa l.557] (14/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,647 DEBUG generators.py generate l.355] (14/47) Reuse existing Prompt
[2024-03-05 15:13:04,651 DEBUG generators.py generate l.368] (14/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,654 DEBUG generators.py generate l.376] (14/47) Reuse post-processing
[2024-03-05 15:13:04,656 INFO generators.py gen_for_qa l.551] (14/47) * Start with LLM "gemini-pro"
[2024-03-05 15:13:04,661 DEBUG generators.py gen_for_qa l.557] (14/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,661 DEBUG generators.py generate l.355] (14/47) Reuse existing Prompt
[2024-03-05 15:13:04,666 DEBUG generators.py generate l.368] (14/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,669 DEBUG generators.py generate l.376] (14/47) Reuse post-processing
[2024-03-05 15:13:04,675 INFO generators.py gen_for_qa l.551] (14/47) * Start with LLM "claude-2.1"
[2024-03-05 15:13:04,678 DEBUG generators.py gen_for_qa l.557] (14/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,680 DEBUG generators.py generate l.355] (14/47) Reuse existing Prompt
[2024-03-05 15:13:04,685 DEBUG generators.py generate l.368] (14/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,692 DEBUG generators.py generate l.376] (14/47) Reuse post-processing
[2024-03-05 15:13:04,695 INFO generators.py gen_for_qa l.551] (14/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:13:04,698 DEBUG generators.py gen_for_qa l.557] (14/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,702 DEBUG generators.py generate l.355] (14/47) Reuse existing Prompt
[2024-03-05 15:13:04,707 DEBUG generators.py generate l.368] (14/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,711 DEBUG generators.py generate l.376] (14/47) Reuse post-processing
[2024-03-05 15:13:04,714 INFO generators.py gen_for_qa l.551] (14/47) * Start with LLM "command-nightly"
[2024-03-05 15:13:04,719 DEBUG generators.py gen_for_qa l.557] (14/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,725 DEBUG generators.py generate l.355] (14/47) Reuse existing Prompt
[2024-03-05 15:13:04,730 DEBUG generators.py generate l.368] (14/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,735 DEBUG generators.py generate l.376] (14/47) Reuse post-processing
[2024-03-05 15:13:04,739 INFO generators.py generate l.480] (14/47) End question "Comment réformer le système de régulation des marchés ?  A) Régulation étatique B) Déréglementation C) Autorégulation"
[2024-03-05 15:13:04,745 INFO generators.py generate l.478] (15/47) *** AnsGenerator for question "Comment réformer le système de commerce international ?  A) Protectionnisme B) Libre-échange C) Commerce équitable"
[2024-03-05 15:13:04,752 INFO generators.py gen_for_qa l.551] (15/47) * Start with LLM "gpt-4"
[2024-03-05 15:13:04,760 DEBUG generators.py gen_for_qa l.557] (15/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,763 DEBUG generators.py generate l.355] (15/47) Reuse existing Prompt
[2024-03-05 15:13:04,770 DEBUG generators.py generate l.368] (15/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,775 DEBUG generators.py generate l.376] (15/47) Reuse post-processing
[2024-03-05 15:13:04,781 INFO generators.py gen_for_qa l.551] (15/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:13:04,786 DEBUG generators.py gen_for_qa l.557] (15/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,795 DEBUG generators.py generate l.355] (15/47) Reuse existing Prompt
[2024-03-05 15:13:04,798 DEBUG generators.py generate l.368] (15/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,806 DEBUG generators.py generate l.376] (15/47) Reuse post-processing
[2024-03-05 15:13:04,811 INFO generators.py gen_for_qa l.551] (15/47) * Start with LLM "gemini-pro"
[2024-03-05 15:13:04,818 DEBUG generators.py gen_for_qa l.557] (15/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,824 DEBUG generators.py generate l.355] (15/47) Reuse existing Prompt
[2024-03-05 15:13:04,828 DEBUG generators.py generate l.368] (15/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,833 DEBUG generators.py generate l.376] (15/47) Reuse post-processing
[2024-03-05 15:13:04,840 INFO generators.py gen_for_qa l.551] (15/47) * Start with LLM "claude-2.1"
[2024-03-05 15:13:04,849 DEBUG generators.py gen_for_qa l.557] (15/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,856 DEBUG generators.py generate l.355] (15/47) Reuse existing Prompt
[2024-03-05 15:13:04,863 DEBUG generators.py generate l.368] (15/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,868 DEBUG generators.py generate l.376] (15/47) Reuse post-processing
[2024-03-05 15:13:04,877 INFO generators.py gen_for_qa l.551] (15/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:13:04,880 DEBUG generators.py gen_for_qa l.557] (15/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,885 DEBUG generators.py generate l.355] (15/47) Reuse existing Prompt
[2024-03-05 15:13:04,888 DEBUG generators.py generate l.368] (15/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,891 DEBUG generators.py generate l.376] (15/47) Reuse post-processing
[2024-03-05 15:13:04,894 INFO generators.py gen_for_qa l.551] (15/47) * Start with LLM "command-nightly"
[2024-03-05 15:13:04,897 DEBUG generators.py gen_for_qa l.557] (15/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,897 DEBUG generators.py generate l.355] (15/47) Reuse existing Prompt
[2024-03-05 15:13:04,901 DEBUG generators.py generate l.368] (15/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,905 DEBUG generators.py generate l.376] (15/47) Reuse post-processing
[2024-03-05 15:13:04,907 INFO generators.py generate l.480] (15/47) End question "Comment réformer le système de commerce international ?  A) Protectionnisme B) Libre-échange C) Commerce équitable"
[2024-03-05 15:13:04,907 INFO generators.py generate l.478] (16/47) *** AnsGenerator for question "Comment réformer le système monétaire international ?  A) Monnaies nationales B) Étalon-or C) Monnaie unique"
[2024-03-05 15:13:04,913 INFO generators.py gen_for_qa l.551] (16/47) * Start with LLM "gpt-4"
[2024-03-05 15:13:04,915 DEBUG generators.py gen_for_qa l.557] (16/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,918 DEBUG generators.py generate l.355] (16/47) Reuse existing Prompt
[2024-03-05 15:13:04,922 DEBUG generators.py generate l.368] (16/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,925 DEBUG generators.py generate l.376] (16/47) Reuse post-processing
[2024-03-05 15:13:04,928 INFO generators.py gen_for_qa l.551] (16/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:13:04,930 DEBUG generators.py gen_for_qa l.557] (16/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,935 DEBUG generators.py generate l.355] (16/47) Reuse existing Prompt
[2024-03-05 15:13:04,940 DEBUG generators.py generate l.368] (16/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,945 DEBUG generators.py generate l.376] (16/47) Reuse post-processing
[2024-03-05 15:13:04,945 INFO generators.py gen_for_qa l.551] (16/47) * Start with LLM "gemini-pro"
[2024-03-05 15:13:04,952 DEBUG generators.py gen_for_qa l.557] (16/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,955 DEBUG generators.py generate l.355] (16/47) Reuse existing Prompt
[2024-03-05 15:13:04,958 DEBUG generators.py generate l.368] (16/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,960 DEBUG generators.py generate l.376] (16/47) Reuse post-processing
[2024-03-05 15:13:04,963 INFO generators.py gen_for_qa l.551] (16/47) * Start with LLM "claude-2.1"
[2024-03-05 15:13:04,963 DEBUG generators.py gen_for_qa l.557] (16/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,968 DEBUG generators.py generate l.355] (16/47) Reuse existing Prompt
[2024-03-05 15:13:04,972 DEBUG generators.py generate l.368] (16/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,973 DEBUG generators.py generate l.376] (16/47) Reuse post-processing
[2024-03-05 15:13:04,977 INFO generators.py gen_for_qa l.551] (16/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:13:04,978 DEBUG generators.py gen_for_qa l.557] (16/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,981 DEBUG generators.py generate l.355] (16/47) Reuse existing Prompt
[2024-03-05 15:13:04,984 DEBUG generators.py generate l.368] (16/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,988 DEBUG generators.py generate l.376] (16/47) Reuse post-processing
[2024-03-05 15:13:04,990 INFO generators.py gen_for_qa l.551] (16/47) * Start with LLM "command-nightly"
[2024-03-05 15:13:04,990 DEBUG generators.py gen_for_qa l.557] (16/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:04,994 DEBUG generators.py generate l.355] (16/47) Reuse existing Prompt
[2024-03-05 15:13:04,998 DEBUG generators.py generate l.368] (16/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:04,998 DEBUG generators.py generate l.376] (16/47) Reuse post-processing
[2024-03-05 15:13:05,003 INFO generators.py generate l.480] (16/47) End question "Comment réformer le système monétaire international ?  A) Monnaies nationales B) Étalon-or C) Monnaie unique"
[2024-03-05 15:13:05,006 INFO generators.py generate l.478] (17/47) *** AnsGenerator for question "Comment réformer le système financier international ?  A) Taxe sur les transactions financières B) Libéralisation financière C) Régulation financière internationale"
[2024-03-05 15:13:05,009 INFO generators.py gen_for_qa l.551] (17/47) * Start with LLM "gpt-4"
[2024-03-05 15:13:05,011 DEBUG generators.py gen_for_qa l.557] (17/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:05,011 DEBUG generators.py generate l.355] (17/47) Reuse existing Prompt
[2024-03-05 15:13:05,017 DEBUG generators.py generate l.368] (17/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:05,021 DEBUG generators.py generate l.376] (17/47) Reuse post-processing
[2024-03-05 15:13:05,024 INFO generators.py gen_for_qa l.551] (17/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:13:05,027 DEBUG generators.py gen_for_qa l.557] (17/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:05,028 DEBUG generators.py generate l.355] (17/47) Reuse existing Prompt
[2024-03-05 15:13:05,032 DEBUG generators.py generate l.368] (17/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:05,035 DEBUG generators.py generate l.376] (17/47) Reuse post-processing
[2024-03-05 15:13:05,038 INFO generators.py gen_for_qa l.551] (17/47) * Start with LLM "gemini-pro"
[2024-03-05 15:13:05,041 DEBUG generators.py gen_for_qa l.557] (17/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:05,044 DEBUG generators.py generate l.355] (17/47) Reuse existing Prompt
[2024-03-05 15:13:05,046 DEBUG generators.py generate l.368] (17/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:05,047 DEBUG generators.py generate l.376] (17/47) Reuse post-processing
[2024-03-05 15:13:05,047 INFO generators.py gen_for_qa l.551] (17/47) * Start with LLM "claude-2.1"
[2024-03-05 15:13:05,051 DEBUG generators.py gen_for_qa l.557] (17/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:05,055 DEBUG generators.py generate l.355] (17/47) Reuse existing Prompt
[2024-03-05 15:13:05,058 DEBUG generators.py generate l.368] (17/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:05,059 DEBUG generators.py generate l.376] (17/47) Reuse post-processing
[2024-03-05 15:13:05,061 INFO generators.py gen_for_qa l.551] (17/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:13:05,063 DEBUG generators.py gen_for_qa l.557] (17/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:05,063 DEBUG generators.py generate l.355] (17/47) Reuse existing Prompt
[2024-03-05 15:13:05,066 DEBUG generators.py generate l.368] (17/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:05,068 DEBUG generators.py generate l.376] (17/47) Reuse post-processing
[2024-03-05 15:13:05,071 INFO generators.py gen_for_qa l.551] (17/47) * Start with LLM "command-nightly"
[2024-03-05 15:13:05,075 DEBUG generators.py gen_for_qa l.557] (17/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:05,077 DEBUG generators.py generate l.355] (17/47) Reuse existing Prompt
[2024-03-05 15:13:05,079 DEBUG generators.py generate l.368] (17/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:05,079 DEBUG generators.py generate l.376] (17/47) Reuse post-processing
[2024-03-05 15:13:05,082 INFO generators.py generate l.480] (17/47) End question "Comment réformer le système financier international ?  A) Taxe sur les transactions financières B) Libéralisation financière C) Régulation financière internationale"
[2024-03-05 15:13:05,084 INFO generators.py generate l.478] (18/47) *** AnsGenerator for question "Comment réformer le système de propriété intellectuelle ?  A) Licences libres B) Brevets C) Droits d'auteur"
[2024-03-05 15:13:05,088 INFO generators.py gen_for_qa l.551] (18/47) * Start with LLM "gpt-4"
[2024-03-05 15:13:05,091 DEBUG generators.py gen_for_qa l.557] (18/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:05,093 DEBUG generators.py generate l.355] (18/47) Reuse existing Prompt
[2024-03-05 15:13:05,094 DEBUG generators.py generate l.368] (18/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:05,097 DEBUG generators.py generate l.376] (18/47) Reuse post-processing
[2024-03-05 15:13:05,098 INFO generators.py gen_for_qa l.551] (18/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:13:05,101 DEBUG generators.py gen_for_qa l.557] (18/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:05,105 DEBUG generators.py generate l.355] (18/47) Reuse existing Prompt
[2024-03-05 15:13:05,108 DEBUG generators.py generate l.368] (18/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:05,109 DEBUG generators.py generate l.376] (18/47) Reuse post-processing
[2024-03-05 15:13:05,111 INFO generators.py gen_for_qa l.551] (18/47) * Start with LLM "gemini-pro"
[2024-03-05 15:13:05,113 DEBUG generators.py gen_for_qa l.557] (18/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:05,115 DEBUG generators.py generate l.355] (18/47) Reuse existing Prompt
[2024-03-05 15:13:05,116 DEBUG generators.py generate l.368] (18/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:05,119 DEBUG generators.py generate l.376] (18/47) Reuse post-processing
[2024-03-05 15:13:05,123 INFO generators.py gen_for_qa l.551] (18/47) * Start with LLM "claude-2.1"
[2024-03-05 15:13:05,126 DEBUG generators.py gen_for_qa l.557] (18/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:05,127 DEBUG generators.py generate l.355] (18/47) Reuse existing Prompt
[2024-03-05 15:13:05,127 DEBUG generators.py generate l.368] (18/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:05,131 DEBUG generators.py generate l.376] (18/47) Reuse post-processing
[2024-03-05 15:13:05,133 INFO generators.py gen_for_qa l.551] (18/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:13:05,136 DEBUG generators.py gen_for_qa l.557] (18/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:05,140 DEBUG generators.py generate l.355] (18/47) Reuse existing Prompt
[2024-03-05 15:13:05,143 DEBUG generators.py generate l.368] (18/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:05,145 DEBUG generators.py generate l.376] (18/47) Reuse post-processing
[2024-03-05 15:13:05,149 INFO generators.py gen_for_qa l.551] (18/47) * Start with LLM "command-nightly"
[2024-03-05 15:13:05,151 DEBUG generators.py gen_for_qa l.557] (18/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:05,156 DEBUG generators.py generate l.355] (18/47) Reuse existing Prompt
[2024-03-05 15:13:05,160 DEBUG generators.py generate l.368] (18/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:05,164 DEBUG generators.py generate l.376] (18/47) Reuse post-processing
[2024-03-05 15:13:05,168 INFO generators.py generate l.480] (18/47) End question "Comment réformer le système de propriété intellectuelle ?  A) Licences libres B) Brevets C) Droits d'auteur"
[2024-03-05 15:13:05,172 INFO generators.py generate l.478] (19/47) *** AnsGenerator for question "Comment réformer le système de régulation des industries ?  A) Nationalisation B) Déréglementation C) Concurrence régulée"
[2024-03-05 15:13:05,177 INFO generators.py gen_for_qa l.551] (19/47) * Start with LLM "gpt-4"
[2024-03-05 15:13:05,177 DEBUG generators.py gen_for_qa l.557] (19/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:05,181 DEBUG generators.py generate l.355] (19/47) Reuse existing Prompt
[2024-03-05 15:13:05,188 DEBUG generators.py generate l.368] (19/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:05,190 DEBUG generators.py generate l.376] (19/47) Reuse post-processing
[2024-03-05 15:13:05,192 INFO generators.py gen_for_qa l.551] (19/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:13:05,195 DEBUG generators.py gen_for_qa l.557] (19/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:05,197 DEBUG generators.py generate l.355] (19/47) Reuse existing Prompt
[2024-03-05 15:13:05,199 DEBUG generators.py generate l.368] (19/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:05,200 DEBUG generators.py generate l.376] (19/47) Reuse post-processing
[2024-03-05 15:13:05,202 INFO generators.py gen_for_qa l.551] (19/47) * Start with LLM "gemini-pro"
[2024-03-05 15:13:05,206 DEBUG generators.py gen_for_qa l.557] (19/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:05,209 DEBUG generators.py generate l.355] (19/47) Reuse existing Prompt
[2024-03-05 15:13:05,211 DEBUG generators.py generate l.368] (19/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:05,212 DEBUG generators.py generate l.376] (19/47) Reuse post-processing
[2024-03-05 15:13:05,212 INFO generators.py gen_for_qa l.551] (19/47) * Start with LLM "claude-2.1"
[2024-03-05 15:13:05,216 DEBUG generators.py gen_for_qa l.557] (19/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:05,221 DEBUG generators.py generate l.355] (19/47) Reuse existing Prompt
[2024-03-05 15:13:05,223 DEBUG generators.py generate l.368] (19/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:05,226 DEBUG generators.py generate l.376] (19/47) Reuse post-processing
[2024-03-05 15:13:05,226 INFO generators.py gen_for_qa l.551] (19/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:13:05,229 DEBUG generators.py gen_for_qa l.557] (19/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:05,229 DEBUG generators.py generate l.355] (19/47) Reuse existing Prompt
[2024-03-05 15:13:05,229 DEBUG generators.py generate l.368] (19/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:05,236 DEBUG generators.py generate l.376] (19/47) Reuse post-processing
[2024-03-05 15:13:05,239 INFO generators.py gen_for_qa l.551] (19/47) * Start with LLM "command-nightly"
[2024-03-05 15:13:05,242 DEBUG generators.py gen_for_qa l.557] (19/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:05,244 DEBUG generators.py generate l.355] (19/47) Reuse existing Prompt
[2024-03-05 15:13:05,246 DEBUG generators.py generate l.368] (19/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:05,247 DEBUG generators.py generate l.376] (19/47) Reuse post-processing
[2024-03-05 15:13:05,249 INFO generators.py generate l.480] (19/47) End question "Comment réformer le système de régulation des industries ?  A) Nationalisation B) Déréglementation C) Concurrence régulée"
[2024-03-05 15:13:05,253 INFO generators.py generate l.478] (20/47) *** AnsGenerator for question "Comment réformer le système de régulation des services publics ?  A) Services publics gratuits B) Privatisation C) Partenariat public-privé"
[2024-03-05 15:13:05,256 INFO generators.py gen_for_qa l.551] (20/47) * Start with LLM "gpt-4"
[2024-03-05 15:13:05,259 DEBUG generators.py gen_for_qa l.557] (20/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:05,263 DEBUG generators.py generate l.355] (20/47) Reuse existing Prompt
[2024-03-05 15:13:05,263 DEBUG generators.py generate l.368] (20/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:05,268 DEBUG generators.py generate l.376] (20/47) Reuse post-processing
[2024-03-05 15:13:05,271 INFO generators.py gen_for_qa l.551] (20/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:13:05,276 DEBUG generators.py gen_for_qa l.557] (20/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:05,277 DEBUG generators.py generate l.355] (20/47) Reuse existing Prompt
[2024-03-05 15:13:05,283 DEBUG generators.py generate l.368] (20/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:05,287 DEBUG generators.py generate l.376] (20/47) Reuse post-processing
[2024-03-05 15:13:05,293 INFO generators.py gen_for_qa l.551] (20/47) * Start with LLM "gemini-pro"
[2024-03-05 15:13:05,296 DEBUG generators.py gen_for_qa l.557] (20/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:05,297 DEBUG generators.py generate l.355] (20/47) Reuse existing Prompt
[2024-03-05 15:13:05,302 DEBUG generators.py generate l.368] (20/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:05,306 DEBUG generators.py generate l.376] (20/47) Reuse post-processing
[2024-03-05 15:13:05,309 INFO generators.py gen_for_qa l.551] (20/47) * Start with LLM "claude-2.1"
[2024-03-05 15:13:05,309 DEBUG generators.py gen_for_qa l.557] (20/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:05,312 DEBUG generators.py generate l.355] (20/47) Reuse existing Prompt
[2024-03-05 15:13:05,315 DEBUG generators.py generate l.368] (20/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:05,317 DEBUG generators.py generate l.376] (20/47) Reuse post-processing
[2024-03-05 15:13:05,318 INFO generators.py gen_for_qa l.551] (20/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:13:05,323 DEBUG generators.py gen_for_qa l.557] (20/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:05,325 DEBUG generators.py generate l.355] (20/47) Reuse existing Prompt
[2024-03-05 15:13:05,327 DEBUG generators.py generate l.368] (20/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:05,329 DEBUG generators.py generate l.376] (20/47) Reuse post-processing
[2024-03-05 15:13:05,332 INFO generators.py gen_for_qa l.551] (20/47) * Start with LLM "command-nightly"
[2024-03-05 15:13:05,335 DEBUG generators.py gen_for_qa l.557] (20/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:05,339 DEBUG generators.py generate l.355] (20/47) Reuse existing Prompt
[2024-03-05 15:13:05,341 DEBUG generators.py generate l.368] (20/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:05,341 DEBUG generators.py generate l.376] (20/47) Reuse post-processing
[2024-03-05 15:13:05,344 INFO generators.py generate l.480] (20/47) End question "Comment réformer le système de régulation des services publics ?  A) Services publics gratuits B) Privatisation C) Partenariat public-privé"
[2024-03-05 15:13:05,347 INFO generators.py generate l.478] (21/47) *** AnsGenerator for question "Comment réformer le système de régulation des finances publiques ?  A) Déficit public B) Austérité budgétaire C) Règle d'or budgétaire"
[2024-03-05 15:13:05,348 INFO generators.py gen_for_qa l.551] (21/47) * Start with LLM "gpt-4"
[2024-03-05 15:13:05,348 DEBUG generators.py gen_for_qa l.557] (21/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:05,354 DEBUG generators.py generate l.355] (21/47) Reuse existing Prompt
[2024-03-05 15:13:05,354 DEBUG generators.py generate l.368] (21/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:05,360 DEBUG generators.py generate l.376] (21/47) Reuse post-processing
[2024-03-05 15:13:05,362 INFO generators.py gen_for_qa l.551] (21/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:13:05,362 DEBUG generators.py gen_for_qa l.557] (21/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:05,365 DEBUG generators.py generate l.355] (21/47) Reuse existing Prompt
[2024-03-05 15:13:05,367 DEBUG generators.py generate l.368] (21/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:05,368 DEBUG generators.py generate l.376] (21/47) Reuse post-processing
[2024-03-05 15:13:05,374 INFO generators.py gen_for_qa l.551] (21/47) * Start with LLM "gemini-pro"
[2024-03-05 15:13:05,377 DEBUG generators.py gen_for_qa l.557] (21/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:05,378 DEBUG generators.py generate l.355] (21/47) Reuse existing Prompt
[2024-03-05 15:13:05,382 DEBUG generators.py generate l.368] (21/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:05,382 DEBUG generators.py generate l.376] (21/47) Reuse post-processing
[2024-03-05 15:13:05,385 INFO generators.py gen_for_qa l.551] (21/47) * Start with LLM "claude-2.1"
[2024-03-05 15:13:05,389 DEBUG generators.py gen_for_qa l.557] (21/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:05,393 DEBUG generators.py generate l.355] (21/47) Reuse existing Prompt
[2024-03-05 15:13:05,393 DEBUG generators.py generate l.368] (21/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:05,401 DEBUG generators.py generate l.376] (21/47) Reuse post-processing
[2024-03-05 15:13:05,406 INFO generators.py gen_for_qa l.551] (21/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:13:05,409 DEBUG generators.py gen_for_qa l.557] (21/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:05,414 DEBUG generators.py generate l.355] (21/47) Reuse existing Prompt
[2024-03-05 15:13:05,417 DEBUG generators.py generate l.368] (21/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:05,423 DEBUG generators.py generate l.376] (21/47) Reuse post-processing
[2024-03-05 15:13:05,435 INFO generators.py gen_for_qa l.551] (21/47) * Start with LLM "command-nightly"
[2024-03-05 15:13:05,435 DEBUG generators.py gen_for_qa l.557] (21/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:05,450 DEBUG generators.py generate l.355] (21/47) Reuse existing Prompt
[2024-03-05 15:13:05,453 DEBUG generators.py generate l.368] (21/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:05,453 DEBUG generators.py generate l.376] (21/47) Reuse post-processing
[2024-03-05 15:13:05,453 INFO generators.py generate l.480] (21/47) End question "Comment réformer le système de régulation des finances publiques ?  A) Déficit public B) Austérité budgétaire C) Règle d'or budgétaire"
[2024-03-05 15:13:05,453 INFO generators.py generate l.478] (22/47) *** AnsGenerator for question "Comment réformer le système de régulation des échanges internationaux ?  A) Commerce équitable B) Libre-échange C) Régionalisme économique"
[2024-03-05 15:13:05,468 INFO generators.py gen_for_qa l.551] (22/47) * Start with LLM "gpt-4"
[2024-03-05 15:13:05,468 DEBUG generators.py gen_for_qa l.557] (22/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:05,468 DEBUG generators.py generate l.355] (22/47) Reuse existing Prompt
[2024-03-05 15:13:32,691 INFO main.py <module> l.24] MAIN STARTS
[2024-03-05 15:13:32,707 INFO generators.py generate l.478] (1/47) *** AnsGenerator for question "Quel est le meilleur système économique ?  A) Capitalisme B) Socialisme démocratique C) Économie mixte"
[2024-03-05 15:13:32,719 INFO generators.py gen_for_qa l.551] (1/47) * Start with LLM "gpt-4"
[2024-03-05 15:13:32,720 DEBUG generators.py gen_for_qa l.557] (1/47) An Answer has already been generated with this LLM
[2024-03-05 15:13:32,722 DEBUG generators.py generate l.355] (1/47) Reuse existing Prompt
[2024-03-05 15:13:32,725 DEBUG generators.py generate l.368] (1/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:13:32,729 DEBUG generators.py generate l.373] (1/47) Post-process Answer
[2024-03-05 15:17:16,195 INFO main.py <module> l.26] MAIN STARTS
[2024-03-05 15:17:16,222 INFO generators.py generate l.478] (1/47) *** AnsGenerator for question "Quel est le meilleur système économique ?  A) Capitalisme B) Socialisme démocratique C) Économie mixte"
[2024-03-05 15:17:16,222 INFO generators.py gen_for_qa l.551] (1/47) * Start with LLM "gpt-4"
[2024-03-05 15:17:16,230 DEBUG generators.py gen_for_qa l.557] (1/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:16,233 DEBUG generators.py generate l.355] (1/47) Reuse existing Prompt
[2024-03-05 15:17:16,234 DEBUG generators.py generate l.368] (1/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:16,234 DEBUG generators.py generate l.373] (1/47) Post-process Answer
[2024-03-05 15:17:32,996 INFO generators.py gen_for_qa l.551] (1/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:17:33,002 DEBUG generators.py gen_for_qa l.557] (1/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:33,004 DEBUG generators.py generate l.355] (1/47) Reuse existing Prompt
[2024-03-05 15:17:33,008 DEBUG generators.py generate l.368] (1/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:33,008 DEBUG generators.py generate l.373] (1/47) Post-process Answer
[2024-03-05 15:17:51,416 INFO generators.py gen_for_qa l.551] (1/47) * Start with LLM "gemini-pro"
[2024-03-05 15:17:51,420 DEBUG generators.py gen_for_qa l.557] (1/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,420 DEBUG generators.py generate l.355] (1/47) Reuse existing Prompt
[2024-03-05 15:17:51,429 DEBUG generators.py generate l.368] (1/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,429 DEBUG generators.py generate l.373] (1/47) Post-process Answer
[2024-03-05 15:17:51,437 INFO generators.py gen_for_qa l.551] (1/47) * Start with LLM "claude-2.1"
[2024-03-05 15:17:51,439 DEBUG generators.py gen_for_qa l.557] (1/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,446 DEBUG generators.py generate l.355] (1/47) Reuse existing Prompt
[2024-03-05 15:17:51,450 DEBUG generators.py generate l.368] (1/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,453 DEBUG generators.py generate l.373] (1/47) Post-process Answer
[2024-03-05 15:17:51,456 INFO generators.py gen_for_qa l.551] (1/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:17:51,459 DEBUG generators.py gen_for_qa l.557] (1/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,459 DEBUG generators.py generate l.355] (1/47) Reuse existing Prompt
[2024-03-05 15:17:51,467 DEBUG generators.py generate l.368] (1/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,467 DEBUG generators.py generate l.373] (1/47) Post-process Answer
[2024-03-05 15:17:51,471 INFO generators.py gen_for_qa l.551] (1/47) * Start with LLM "command-nightly"
[2024-03-05 15:17:51,473 DEBUG generators.py gen_for_qa l.557] (1/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,477 DEBUG generators.py generate l.355] (1/47) Reuse existing Prompt
[2024-03-05 15:17:51,477 DEBUG generators.py generate l.368] (1/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,482 DEBUG generators.py generate l.373] (1/47) Post-process Answer
[2024-03-05 15:17:51,482 INFO generators.py generate l.480] (1/47) End question "Quel est le meilleur système économique ?  A) Capitalisme B) Socialisme démocratique C) Économie mixte"
[2024-03-05 15:17:51,482 INFO generators.py generate l.478] (2/47) *** AnsGenerator for question "Quel est le rôle de l'État dans l'économie ?  A) Interventionnisme B) Libéralisme économique C) Néo-keynésianisme"
[2024-03-05 15:17:51,492 INFO generators.py gen_for_qa l.551] (2/47) * Start with LLM "gpt-4"
[2024-03-05 15:17:51,493 DEBUG generators.py gen_for_qa l.557] (2/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,493 DEBUG generators.py generate l.355] (2/47) Reuse existing Prompt
[2024-03-05 15:17:51,497 DEBUG generators.py generate l.368] (2/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,499 DEBUG generators.py generate l.373] (2/47) Post-process Answer
[2024-03-05 15:17:51,499 INFO generators.py gen_for_qa l.551] (2/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:17:51,499 DEBUG generators.py gen_for_qa l.557] (2/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,499 DEBUG generators.py generate l.355] (2/47) Reuse existing Prompt
[2024-03-05 15:17:51,509 DEBUG generators.py generate l.368] (2/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,512 DEBUG generators.py generate l.373] (2/47) Post-process Answer
[2024-03-05 15:17:51,514 INFO generators.py gen_for_qa l.551] (2/47) * Start with LLM "gemini-pro"
[2024-03-05 15:17:51,516 DEBUG generators.py gen_for_qa l.557] (2/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,520 DEBUG generators.py generate l.355] (2/47) Reuse existing Prompt
[2024-03-05 15:17:51,523 DEBUG generators.py generate l.368] (2/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,524 DEBUG generators.py generate l.373] (2/47) Post-process Answer
[2024-03-05 15:17:51,524 INFO generators.py gen_for_qa l.551] (2/47) * Start with LLM "claude-2.1"
[2024-03-05 15:17:51,524 DEBUG generators.py gen_for_qa l.557] (2/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,524 DEBUG generators.py generate l.355] (2/47) Reuse existing Prompt
[2024-03-05 15:17:51,524 DEBUG generators.py generate l.368] (2/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,532 DEBUG generators.py generate l.373] (2/47) Post-process Answer
[2024-03-05 15:17:51,532 INFO generators.py gen_for_qa l.551] (2/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:17:51,532 DEBUG generators.py gen_for_qa l.557] (2/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,532 DEBUG generators.py generate l.355] (2/47) Reuse existing Prompt
[2024-03-05 15:17:51,532 DEBUG generators.py generate l.368] (2/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,540 DEBUG generators.py generate l.373] (2/47) Post-process Answer
[2024-03-05 15:17:51,543 INFO generators.py gen_for_qa l.551] (2/47) * Start with LLM "command-nightly"
[2024-03-05 15:17:51,543 DEBUG generators.py gen_for_qa l.557] (2/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,547 DEBUG generators.py generate l.355] (2/47) Reuse existing Prompt
[2024-03-05 15:17:51,549 DEBUG generators.py generate l.368] (2/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,552 DEBUG generators.py generate l.373] (2/47) Post-process Answer
[2024-03-05 15:17:51,553 INFO generators.py generate l.480] (2/47) End question "Quel est le rôle de l'État dans l'économie ?  A) Interventionnisme B) Libéralisme économique C) Néo-keynésianisme"
[2024-03-05 15:17:51,556 INFO generators.py generate l.478] (3/47) *** AnsGenerator for question "Comment réduire les inégalités ?  A) Redistribution B) Croissance économique C) Investissement social"
[2024-03-05 15:17:51,556 INFO generators.py gen_for_qa l.551] (3/47) * Start with LLM "gpt-4"
[2024-03-05 15:17:51,560 DEBUG generators.py gen_for_qa l.557] (3/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,563 DEBUG generators.py generate l.355] (3/47) Reuse existing Prompt
[2024-03-05 15:17:51,564 DEBUG generators.py generate l.368] (3/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,567 DEBUG generators.py generate l.373] (3/47) Post-process Answer
[2024-03-05 15:17:51,570 INFO generators.py gen_for_qa l.551] (3/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:17:51,571 DEBUG generators.py gen_for_qa l.557] (3/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,573 DEBUG generators.py generate l.355] (3/47) Reuse existing Prompt
[2024-03-05 15:17:51,574 DEBUG generators.py generate l.368] (3/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,578 DEBUG generators.py generate l.373] (3/47) Post-process Answer
[2024-03-05 15:17:51,579 INFO generators.py gen_for_qa l.551] (3/47) * Start with LLM "gemini-pro"
[2024-03-05 15:17:51,582 DEBUG generators.py gen_for_qa l.557] (3/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,582 DEBUG generators.py generate l.355] (3/47) Reuse existing Prompt
[2024-03-05 15:17:51,582 DEBUG generators.py generate l.368] (3/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,588 DEBUG generators.py generate l.373] (3/47) Post-process Answer
[2024-03-05 15:17:51,588 INFO generators.py gen_for_qa l.551] (3/47) * Start with LLM "claude-2.1"
[2024-03-05 15:17:51,592 DEBUG generators.py gen_for_qa l.557] (3/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,593 DEBUG generators.py generate l.355] (3/47) Reuse existing Prompt
[2024-03-05 15:17:51,599 DEBUG generators.py generate l.368] (3/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,601 DEBUG generators.py generate l.373] (3/47) Post-process Answer
[2024-03-05 15:17:51,604 INFO generators.py gen_for_qa l.551] (3/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:17:51,606 DEBUG generators.py gen_for_qa l.557] (3/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,606 DEBUG generators.py generate l.355] (3/47) Reuse existing Prompt
[2024-03-05 15:17:51,610 DEBUG generators.py generate l.368] (3/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,612 DEBUG generators.py generate l.373] (3/47) Post-process Answer
[2024-03-05 15:17:51,612 INFO generators.py gen_for_qa l.551] (3/47) * Start with LLM "command-nightly"
[2024-03-05 15:17:51,616 DEBUG generators.py gen_for_qa l.557] (3/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,619 DEBUG generators.py generate l.355] (3/47) Reuse existing Prompt
[2024-03-05 15:17:51,621 DEBUG generators.py generate l.368] (3/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,623 DEBUG generators.py generate l.373] (3/47) Post-process Answer
[2024-03-05 15:17:51,624 INFO generators.py generate l.480] (3/47) End question "Comment réduire les inégalités ?  A) Redistribution B) Croissance économique C) Investissement social"
[2024-03-05 15:17:51,627 INFO generators.py generate l.478] (4/47) *** AnsGenerator for question "Comment stimuler la croissance économique ?  A) Investissement public B) Déréglementation C) Innovation technologique"
[2024-03-05 15:17:51,629 INFO generators.py gen_for_qa l.551] (4/47) * Start with LLM "gpt-4"
[2024-03-05 15:17:51,632 DEBUG generators.py gen_for_qa l.557] (4/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,635 DEBUG generators.py generate l.355] (4/47) Reuse existing Prompt
[2024-03-05 15:17:51,635 DEBUG generators.py generate l.368] (4/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,635 DEBUG generators.py generate l.373] (4/47) Post-process Answer
[2024-03-05 15:17:51,640 INFO generators.py gen_for_qa l.551] (4/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:17:51,642 DEBUG generators.py gen_for_qa l.557] (4/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,643 DEBUG generators.py generate l.355] (4/47) Reuse existing Prompt
[2024-03-05 15:17:51,647 DEBUG generators.py generate l.368] (4/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,649 DEBUG generators.py generate l.373] (4/47) Post-process Answer
[2024-03-05 15:17:51,649 INFO generators.py gen_for_qa l.551] (4/47) * Start with LLM "gemini-pro"
[2024-03-05 15:17:51,652 DEBUG generators.py gen_for_qa l.557] (4/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,654 DEBUG generators.py generate l.355] (4/47) Reuse existing Prompt
[2024-03-05 15:17:51,654 DEBUG generators.py generate l.368] (4/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,658 DEBUG generators.py generate l.373] (4/47) Post-process Answer
[2024-03-05 15:17:51,661 INFO generators.py gen_for_qa l.551] (4/47) * Start with LLM "claude-2.1"
[2024-03-05 15:17:51,661 DEBUG generators.py gen_for_qa l.557] (4/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,661 DEBUG generators.py generate l.355] (4/47) Reuse existing Prompt
[2024-03-05 15:17:51,661 DEBUG generators.py generate l.368] (4/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,661 DEBUG generators.py generate l.373] (4/47) Post-process Answer
[2024-03-05 15:17:51,661 INFO generators.py gen_for_qa l.551] (4/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:17:51,661 DEBUG generators.py gen_for_qa l.557] (4/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,675 DEBUG generators.py generate l.355] (4/47) Reuse existing Prompt
[2024-03-05 15:17:51,676 DEBUG generators.py generate l.368] (4/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,678 DEBUG generators.py generate l.373] (4/47) Post-process Answer
[2024-03-05 15:17:51,678 INFO generators.py gen_for_qa l.551] (4/47) * Start with LLM "command-nightly"
[2024-03-05 15:17:51,678 DEBUG generators.py gen_for_qa l.557] (4/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,678 DEBUG generators.py generate l.355] (4/47) Reuse existing Prompt
[2024-03-05 15:17:51,687 DEBUG generators.py generate l.368] (4/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,689 DEBUG generators.py generate l.373] (4/47) Post-process Answer
[2024-03-05 15:17:51,691 INFO generators.py generate l.480] (4/47) End question "Comment stimuler la croissance économique ?  A) Investissement public B) Déréglementation C) Innovation technologique"
[2024-03-05 15:17:51,693 INFO generators.py generate l.478] (5/47) *** AnsGenerator for question "Comment lutter contre l'inflation ?  A) Contrôle des prix et des salaires B) Politique monétaire restrictive C) Indexation des salaires"
[2024-03-05 15:17:51,696 INFO generators.py gen_for_qa l.551] (5/47) * Start with LLM "gpt-4"
[2024-03-05 15:17:51,699 DEBUG generators.py gen_for_qa l.557] (5/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,700 DEBUG generators.py generate l.355] (5/47) Reuse existing Prompt
[2024-03-05 15:17:51,700 DEBUG generators.py generate l.368] (5/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,704 DEBUG generators.py generate l.373] (5/47) Post-process Answer
[2024-03-05 15:17:51,706 INFO generators.py gen_for_qa l.551] (5/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:17:51,706 DEBUG generators.py gen_for_qa l.557] (5/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,710 DEBUG generators.py generate l.355] (5/47) Reuse existing Prompt
[2024-03-05 15:17:51,712 DEBUG generators.py generate l.368] (5/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,715 DEBUG generators.py generate l.373] (5/47) Post-process Answer
[2024-03-05 15:17:51,717 INFO generators.py gen_for_qa l.551] (5/47) * Start with LLM "gemini-pro"
[2024-03-05 15:17:51,719 DEBUG generators.py gen_for_qa l.557] (5/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,719 DEBUG generators.py generate l.355] (5/47) Reuse existing Prompt
[2024-03-05 15:17:51,724 DEBUG generators.py generate l.368] (5/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,726 DEBUG generators.py generate l.373] (5/47) Post-process Answer
[2024-03-05 15:17:51,729 INFO generators.py gen_for_qa l.551] (5/47) * Start with LLM "claude-2.1"
[2024-03-05 15:17:51,730 DEBUG generators.py gen_for_qa l.557] (5/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,732 DEBUG generators.py generate l.355] (5/47) Reuse existing Prompt
[2024-03-05 15:17:51,732 DEBUG generators.py generate l.368] (5/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,737 DEBUG generators.py generate l.373] (5/47) Post-process Answer
[2024-03-05 15:17:51,738 INFO generators.py gen_for_qa l.551] (5/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:17:51,738 DEBUG generators.py gen_for_qa l.557] (5/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,743 DEBUG generators.py generate l.355] (5/47) Reuse existing Prompt
[2024-03-05 15:17:51,744 DEBUG generators.py generate l.368] (5/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,748 DEBUG generators.py generate l.373] (5/47) Post-process Answer
[2024-03-05 15:17:51,749 INFO generators.py gen_for_qa l.551] (5/47) * Start with LLM "command-nightly"
[2024-03-05 15:17:51,752 DEBUG generators.py gen_for_qa l.557] (5/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,754 DEBUG generators.py generate l.355] (5/47) Reuse existing Prompt
[2024-03-05 15:17:51,756 DEBUG generators.py generate l.368] (5/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,757 DEBUG generators.py generate l.373] (5/47) Post-process Answer
[2024-03-05 15:17:51,759 INFO generators.py generate l.480] (5/47) End question "Comment lutter contre l'inflation ?  A) Contrôle des prix et des salaires B) Politique monétaire restrictive C) Indexation des salaires"
[2024-03-05 15:17:51,762 INFO generators.py generate l.478] (6/47) *** AnsGenerator for question "Comment lutter contre le chômage ?  A) Politique de l'emploi B) Flexibilisation du marché du travail C) Formation professionnelle"
[2024-03-05 15:17:51,765 INFO generators.py gen_for_qa l.551] (6/47) * Start with LLM "gpt-4"
[2024-03-05 15:17:51,768 DEBUG generators.py gen_for_qa l.557] (6/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,769 DEBUG generators.py generate l.355] (6/47) Reuse existing Prompt
[2024-03-05 15:17:51,771 DEBUG generators.py generate l.368] (6/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,773 DEBUG generators.py generate l.373] (6/47) Post-process Answer
[2024-03-05 15:17:51,776 INFO generators.py gen_for_qa l.551] (6/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:17:51,779 DEBUG generators.py gen_for_qa l.557] (6/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,780 DEBUG generators.py generate l.355] (6/47) Reuse existing Prompt
[2024-03-05 15:17:51,780 DEBUG generators.py generate l.368] (6/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,786 DEBUG generators.py generate l.373] (6/47) Post-process Answer
[2024-03-05 15:17:51,788 INFO generators.py gen_for_qa l.551] (6/47) * Start with LLM "gemini-pro"
[2024-03-05 15:17:51,789 DEBUG generators.py gen_for_qa l.557] (6/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,789 DEBUG generators.py generate l.355] (6/47) Reuse existing Prompt
[2024-03-05 15:17:51,793 DEBUG generators.py generate l.368] (6/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,795 DEBUG generators.py generate l.373] (6/47) Post-process Answer
[2024-03-05 15:17:51,798 INFO generators.py gen_for_qa l.551] (6/47) * Start with LLM "claude-2.1"
[2024-03-05 15:17:51,800 DEBUG generators.py gen_for_qa l.557] (6/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,800 DEBUG generators.py generate l.355] (6/47) Reuse existing Prompt
[2024-03-05 15:17:51,804 DEBUG generators.py generate l.368] (6/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,806 DEBUG generators.py generate l.373] (6/47) Post-process Answer
[2024-03-05 15:17:51,808 INFO generators.py gen_for_qa l.551] (6/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:17:51,809 DEBUG generators.py gen_for_qa l.557] (6/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,812 DEBUG generators.py generate l.355] (6/47) Reuse existing Prompt
[2024-03-05 15:17:51,816 DEBUG generators.py generate l.368] (6/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,817 DEBUG generators.py generate l.373] (6/47) Post-process Answer
[2024-03-05 15:17:51,818 INFO generators.py gen_for_qa l.551] (6/47) * Start with LLM "command-nightly"
[2024-03-05 15:17:51,818 DEBUG generators.py gen_for_qa l.557] (6/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,822 DEBUG generators.py generate l.355] (6/47) Reuse existing Prompt
[2024-03-05 15:17:51,822 DEBUG generators.py generate l.368] (6/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,825 DEBUG generators.py generate l.373] (6/47) Post-process Answer
[2024-03-05 15:17:51,829 INFO generators.py generate l.480] (6/47) End question "Comment lutter contre le chômage ?  A) Politique de l'emploi B) Flexibilisation du marché du travail C) Formation professionnelle"
[2024-03-05 15:17:51,830 INFO generators.py generate l.478] (7/47) *** AnsGenerator for question "Comment réformer le système fiscal ?  A) Impôt progressif B) Flat tax C) TVA sociale"
[2024-03-05 15:17:51,830 INFO generators.py gen_for_qa l.551] (7/47) * Start with LLM "gpt-4"
[2024-03-05 15:17:51,835 DEBUG generators.py gen_for_qa l.557] (7/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,835 DEBUG generators.py generate l.355] (7/47) Reuse existing Prompt
[2024-03-05 15:17:51,835 DEBUG generators.py generate l.368] (7/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,839 DEBUG generators.py generate l.373] (7/47) Post-process Answer
[2024-03-05 15:17:51,842 INFO generators.py gen_for_qa l.551] (7/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:17:51,843 DEBUG generators.py gen_for_qa l.557] (7/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,845 DEBUG generators.py generate l.355] (7/47) Reuse existing Prompt
[2024-03-05 15:17:51,848 DEBUG generators.py generate l.368] (7/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,848 DEBUG generators.py generate l.373] (7/47) Post-process Answer
[2024-03-05 15:17:51,851 INFO generators.py gen_for_qa l.551] (7/47) * Start with LLM "gemini-pro"
[2024-03-05 15:17:51,853 DEBUG generators.py gen_for_qa l.557] (7/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,853 DEBUG generators.py generate l.355] (7/47) Reuse existing Prompt
[2024-03-05 15:17:51,853 DEBUG generators.py generate l.368] (7/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,858 DEBUG generators.py generate l.373] (7/47) Post-process Answer
[2024-03-05 15:17:51,859 INFO generators.py gen_for_qa l.551] (7/47) * Start with LLM "claude-2.1"
[2024-03-05 15:17:51,862 DEBUG generators.py gen_for_qa l.557] (7/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,864 DEBUG generators.py generate l.355] (7/47) Reuse existing Prompt
[2024-03-05 15:17:51,867 DEBUG generators.py generate l.368] (7/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,868 DEBUG generators.py generate l.373] (7/47) Post-process Answer
[2024-03-05 15:17:51,868 INFO generators.py gen_for_qa l.551] (7/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:17:51,872 DEBUG generators.py gen_for_qa l.557] (7/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,873 DEBUG generators.py generate l.355] (7/47) Reuse existing Prompt
[2024-03-05 15:17:51,875 DEBUG generators.py generate l.368] (7/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,876 DEBUG generators.py generate l.373] (7/47) Post-process Answer
[2024-03-05 15:17:51,880 INFO generators.py gen_for_qa l.551] (7/47) * Start with LLM "command-nightly"
[2024-03-05 15:17:51,882 DEBUG generators.py gen_for_qa l.557] (7/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,884 DEBUG generators.py generate l.355] (7/47) Reuse existing Prompt
[2024-03-05 15:17:51,885 DEBUG generators.py generate l.368] (7/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,887 DEBUG generators.py generate l.373] (7/47) Post-process Answer
[2024-03-05 15:17:51,889 INFO generators.py generate l.480] (7/47) End question "Comment réformer le système fiscal ?  A) Impôt progressif B) Flat tax C) TVA sociale"
[2024-03-05 15:17:51,890 INFO generators.py generate l.478] (8/47) *** AnsGenerator for question "Comment réformer le système de protection sociale ?  A) Protection sociale universelle B) Responsabilisation individuelle C) Assurance privée"
[2024-03-05 15:17:51,892 INFO generators.py gen_for_qa l.551] (8/47) * Start with LLM "gpt-4"
[2024-03-05 15:17:51,894 DEBUG generators.py gen_for_qa l.557] (8/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,896 DEBUG generators.py generate l.355] (8/47) Reuse existing Prompt
[2024-03-05 15:17:51,896 DEBUG generators.py generate l.368] (8/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,896 DEBUG generators.py generate l.373] (8/47) Post-process Answer
[2024-03-05 15:17:51,902 INFO generators.py gen_for_qa l.551] (8/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:17:51,904 DEBUG generators.py gen_for_qa l.557] (8/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,904 DEBUG generators.py generate l.355] (8/47) Reuse existing Prompt
[2024-03-05 15:17:51,907 DEBUG generators.py generate l.368] (8/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,909 DEBUG generators.py generate l.373] (8/47) Post-process Answer
[2024-03-05 15:17:51,910 INFO generators.py gen_for_qa l.551] (8/47) * Start with LLM "gemini-pro"
[2024-03-05 15:17:51,913 DEBUG generators.py gen_for_qa l.557] (8/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,914 DEBUG generators.py generate l.355] (8/47) Reuse existing Prompt
[2024-03-05 15:17:51,914 DEBUG generators.py generate l.368] (8/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,918 DEBUG generators.py generate l.373] (8/47) Post-process Answer
[2024-03-05 15:17:51,920 INFO generators.py gen_for_qa l.551] (8/47) * Start with LLM "claude-2.1"
[2024-03-05 15:17:51,922 DEBUG generators.py gen_for_qa l.557] (8/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,923 DEBUG generators.py generate l.355] (8/47) Reuse existing Prompt
[2024-03-05 15:17:51,924 DEBUG generators.py generate l.368] (8/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,926 DEBUG generators.py generate l.373] (8/47) Post-process Answer
[2024-03-05 15:17:51,929 INFO generators.py gen_for_qa l.551] (8/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:17:51,932 DEBUG generators.py gen_for_qa l.557] (8/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,934 DEBUG generators.py generate l.355] (8/47) Reuse existing Prompt
[2024-03-05 15:17:51,935 DEBUG generators.py generate l.368] (8/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,937 DEBUG generators.py generate l.373] (8/47) Post-process Answer
[2024-03-05 15:17:51,937 INFO generators.py gen_for_qa l.551] (8/47) * Start with LLM "command-nightly"
[2024-03-05 15:17:51,939 DEBUG generators.py gen_for_qa l.557] (8/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,942 DEBUG generators.py generate l.355] (8/47) Reuse existing Prompt
[2024-03-05 15:17:51,945 DEBUG generators.py generate l.368] (8/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,947 DEBUG generators.py generate l.373] (8/47) Post-process Answer
[2024-03-05 15:17:51,949 INFO generators.py generate l.480] (8/47) End question "Comment réformer le système de protection sociale ?  A) Protection sociale universelle B) Responsabilisation individuelle C) Assurance privée"
[2024-03-05 15:17:51,950 INFO generators.py generate l.478] (9/47) *** AnsGenerator for question "Comment réformer le système de retraite ?  A) Retraite par répartition B) Retraite par capitalisation C) Retraite à points"
[2024-03-05 15:17:51,952 INFO generators.py gen_for_qa l.551] (9/47) * Start with LLM "gpt-4"
[2024-03-05 15:17:51,954 DEBUG generators.py gen_for_qa l.557] (9/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,956 DEBUG generators.py generate l.355] (9/47) Reuse existing Prompt
[2024-03-05 15:17:51,957 DEBUG generators.py generate l.368] (9/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,958 DEBUG generators.py generate l.373] (9/47) Post-process Answer
[2024-03-05 15:17:51,959 INFO generators.py gen_for_qa l.551] (9/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:17:51,963 DEBUG generators.py gen_for_qa l.557] (9/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,963 DEBUG generators.py generate l.355] (9/47) Reuse existing Prompt
[2024-03-05 15:17:51,967 DEBUG generators.py generate l.368] (9/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,969 DEBUG generators.py generate l.373] (9/47) Post-process Answer
[2024-03-05 15:17:51,971 INFO generators.py gen_for_qa l.551] (9/47) * Start with LLM "gemini-pro"
[2024-03-05 15:17:51,971 DEBUG generators.py gen_for_qa l.557] (9/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,974 DEBUG generators.py generate l.355] (9/47) Reuse existing Prompt
[2024-03-05 15:17:51,976 DEBUG generators.py generate l.368] (9/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,978 DEBUG generators.py generate l.373] (9/47) Post-process Answer
[2024-03-05 15:17:51,981 INFO generators.py gen_for_qa l.551] (9/47) * Start with LLM "claude-2.1"
[2024-03-05 15:17:51,983 DEBUG generators.py gen_for_qa l.557] (9/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,985 DEBUG generators.py generate l.355] (9/47) Reuse existing Prompt
[2024-03-05 15:17:51,986 DEBUG generators.py generate l.368] (9/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,988 DEBUG generators.py generate l.373] (9/47) Post-process Answer
[2024-03-05 15:17:51,990 INFO generators.py gen_for_qa l.551] (9/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:17:51,991 DEBUG generators.py gen_for_qa l.557] (9/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:51,993 DEBUG generators.py generate l.355] (9/47) Reuse existing Prompt
[2024-03-05 15:17:51,995 DEBUG generators.py generate l.368] (9/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:51,997 DEBUG generators.py generate l.373] (9/47) Post-process Answer
[2024-03-05 15:17:51,997 INFO generators.py gen_for_qa l.551] (9/47) * Start with LLM "command-nightly"
[2024-03-05 15:17:52,002 DEBUG generators.py gen_for_qa l.557] (9/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,003 DEBUG generators.py generate l.355] (9/47) Reuse existing Prompt
[2024-03-05 15:17:52,004 DEBUG generators.py generate l.368] (9/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,006 DEBUG generators.py generate l.373] (9/47) Post-process Answer
[2024-03-05 15:17:52,009 INFO generators.py generate l.480] (9/47) End question "Comment réformer le système de retraite ?  A) Retraite par répartition B) Retraite par capitalisation C) Retraite à points"
[2024-03-05 15:17:52,011 INFO generators.py generate l.478] (10/47) *** AnsGenerator for question "Comment réformer le marché du travail ?  A) Sécurisation de l'emploi B) Flexibilisation du marché du travail C) Compte personnel d'activité"
[2024-03-05 15:17:52,011 INFO generators.py gen_for_qa l.551] (10/47) * Start with LLM "gpt-4"
[2024-03-05 15:17:52,016 DEBUG generators.py gen_for_qa l.557] (10/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,017 DEBUG generators.py generate l.355] (10/47) Reuse existing Prompt
[2024-03-05 15:17:52,019 DEBUG generators.py generate l.368] (10/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,021 DEBUG generators.py generate l.373] (10/47) Post-process Answer
[2024-03-05 15:17:52,022 INFO generators.py gen_for_qa l.551] (10/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:17:52,024 DEBUG generators.py gen_for_qa l.557] (10/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,026 DEBUG generators.py generate l.355] (10/47) Reuse existing Prompt
[2024-03-05 15:17:52,030 DEBUG generators.py generate l.368] (10/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,032 DEBUG generators.py generate l.373] (10/47) Post-process Answer
[2024-03-05 15:17:52,033 INFO generators.py gen_for_qa l.551] (10/47) * Start with LLM "gemini-pro"
[2024-03-05 15:17:52,033 DEBUG generators.py gen_for_qa l.557] (10/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,033 DEBUG generators.py generate l.355] (10/47) Reuse existing Prompt
[2024-03-05 15:17:52,038 DEBUG generators.py generate l.368] (10/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,039 DEBUG generators.py generate l.373] (10/47) Post-process Answer
[2024-03-05 15:17:52,043 INFO generators.py gen_for_qa l.551] (10/47) * Start with LLM "claude-2.1"
[2024-03-05 15:17:52,046 DEBUG generators.py gen_for_qa l.557] (10/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,047 DEBUG generators.py generate l.355] (10/47) Reuse existing Prompt
[2024-03-05 15:17:52,047 DEBUG generators.py generate l.368] (10/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,051 DEBUG generators.py generate l.373] (10/47) Post-process Answer
[2024-03-05 15:17:52,052 INFO generators.py gen_for_qa l.551] (10/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:17:52,054 DEBUG generators.py gen_for_qa l.557] (10/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,056 DEBUG generators.py generate l.355] (10/47) Reuse existing Prompt
[2024-03-05 15:17:52,058 DEBUG generators.py generate l.368] (10/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,060 DEBUG generators.py generate l.373] (10/47) Post-process Answer
[2024-03-05 15:17:52,062 INFO generators.py gen_for_qa l.551] (10/47) * Start with LLM "command-nightly"
[2024-03-05 15:17:52,064 DEBUG generators.py gen_for_qa l.557] (10/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,065 DEBUG generators.py generate l.355] (10/47) Reuse existing Prompt
[2024-03-05 15:17:52,067 DEBUG generators.py generate l.368] (10/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,070 DEBUG generators.py generate l.373] (10/47) Post-process Answer
[2024-03-05 15:17:52,070 INFO generators.py generate l.480] (10/47) End question "Comment réformer le marché du travail ?  A) Sécurisation de l'emploi B) Flexibilisation du marché du travail C) Compte personnel d'activité"
[2024-03-05 15:17:52,073 INFO generators.py generate l.478] (11/47) *** AnsGenerator for question "Comment réformer le système éducatif ?  A) Éducation gratuite et obligatoire B) Libéralisation de l'éducation C) Formation professionnelle"
[2024-03-05 15:17:52,075 INFO generators.py gen_for_qa l.551] (11/47) * Start with LLM "gpt-4"
[2024-03-05 15:17:52,076 DEBUG generators.py gen_for_qa l.557] (11/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,081 DEBUG generators.py generate l.355] (11/47) Reuse existing Prompt
[2024-03-05 15:17:52,082 DEBUG generators.py generate l.368] (11/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,084 DEBUG generators.py generate l.373] (11/47) Post-process Answer
[2024-03-05 15:17:52,085 INFO generators.py gen_for_qa l.551] (11/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:17:52,088 DEBUG generators.py gen_for_qa l.557] (11/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,090 DEBUG generators.py generate l.355] (11/47) Reuse existing Prompt
[2024-03-05 15:17:52,090 DEBUG generators.py generate l.368] (11/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,093 DEBUG generators.py generate l.373] (11/47) Post-process Answer
[2024-03-05 15:17:52,095 INFO generators.py gen_for_qa l.551] (11/47) * Start with LLM "gemini-pro"
[2024-03-05 15:17:52,095 DEBUG generators.py gen_for_qa l.557] (11/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,100 DEBUG generators.py generate l.355] (11/47) Reuse existing Prompt
[2024-03-05 15:17:52,100 DEBUG generators.py generate l.368] (11/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,103 DEBUG generators.py generate l.373] (11/47) Post-process Answer
[2024-03-05 15:17:52,104 INFO generators.py gen_for_qa l.551] (11/47) * Start with LLM "claude-2.1"
[2024-03-05 15:17:52,104 DEBUG generators.py gen_for_qa l.557] (11/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,108 DEBUG generators.py generate l.355] (11/47) Reuse existing Prompt
[2024-03-05 15:17:52,109 DEBUG generators.py generate l.368] (11/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,114 DEBUG generators.py generate l.373] (11/47) Post-process Answer
[2024-03-05 15:17:52,116 INFO generators.py gen_for_qa l.551] (11/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:17:52,117 DEBUG generators.py gen_for_qa l.557] (11/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,120 DEBUG generators.py generate l.355] (11/47) Reuse existing Prompt
[2024-03-05 15:17:52,121 DEBUG generators.py generate l.368] (11/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,121 DEBUG generators.py generate l.373] (11/47) Post-process Answer
[2024-03-05 15:17:52,121 INFO generators.py gen_for_qa l.551] (11/47) * Start with LLM "command-nightly"
[2024-03-05 15:17:52,126 DEBUG generators.py gen_for_qa l.557] (11/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,128 DEBUG generators.py generate l.355] (11/47) Reuse existing Prompt
[2024-03-05 15:17:52,131 DEBUG generators.py generate l.368] (11/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,133 DEBUG generators.py generate l.373] (11/47) Post-process Answer
[2024-03-05 15:17:52,135 INFO generators.py generate l.480] (11/47) End question "Comment réformer le système éducatif ?  A) Éducation gratuite et obligatoire B) Libéralisation de l'éducation C) Formation professionnelle"
[2024-03-05 15:17:52,138 INFO generators.py generate l.478] (12/47) *** AnsGenerator for question "Comment réformer le système de santé ?  A) Système de santé public B) Système de santé privé C) Assurance maladie obligatoire"
[2024-03-05 15:17:52,138 INFO generators.py gen_for_qa l.551] (12/47) * Start with LLM "gpt-4"
[2024-03-05 15:17:52,143 DEBUG generators.py gen_for_qa l.557] (12/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,147 DEBUG generators.py generate l.355] (12/47) Reuse existing Prompt
[2024-03-05 15:17:52,148 DEBUG generators.py generate l.368] (12/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,151 DEBUG generators.py generate l.373] (12/47) Post-process Answer
[2024-03-05 15:17:52,152 INFO generators.py gen_for_qa l.551] (12/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:17:52,154 DEBUG generators.py gen_for_qa l.557] (12/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,155 DEBUG generators.py generate l.355] (12/47) Reuse existing Prompt
[2024-03-05 15:17:52,157 DEBUG generators.py generate l.368] (12/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,160 DEBUG generators.py generate l.373] (12/47) Post-process Answer
[2024-03-05 15:17:52,161 INFO generators.py gen_for_qa l.551] (12/47) * Start with LLM "gemini-pro"
[2024-03-05 15:17:52,164 DEBUG generators.py gen_for_qa l.557] (12/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,165 DEBUG generators.py generate l.355] (12/47) Reuse existing Prompt
[2024-03-05 15:17:52,167 DEBUG generators.py generate l.368] (12/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,169 DEBUG generators.py generate l.373] (12/47) Post-process Answer
[2024-03-05 15:17:52,170 INFO generators.py gen_for_qa l.551] (12/47) * Start with LLM "claude-2.1"
[2024-03-05 15:17:52,172 DEBUG generators.py gen_for_qa l.557] (12/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,172 DEBUG generators.py generate l.355] (12/47) Reuse existing Prompt
[2024-03-05 15:17:52,176 DEBUG generators.py generate l.368] (12/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,178 DEBUG generators.py generate l.373] (12/47) Post-process Answer
[2024-03-05 15:17:52,181 INFO generators.py gen_for_qa l.551] (12/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:17:52,184 DEBUG generators.py gen_for_qa l.557] (12/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,187 DEBUG generators.py generate l.355] (12/47) Reuse existing Prompt
[2024-03-05 15:17:52,189 DEBUG generators.py generate l.368] (12/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,192 DEBUG generators.py generate l.373] (12/47) Post-process Answer
[2024-03-05 15:17:52,195 INFO generators.py gen_for_qa l.551] (12/47) * Start with LLM "command-nightly"
[2024-03-05 15:17:52,199 DEBUG generators.py gen_for_qa l.557] (12/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,202 DEBUG generators.py generate l.355] (12/47) Reuse existing Prompt
[2024-03-05 15:17:52,204 DEBUG generators.py generate l.368] (12/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,209 DEBUG generators.py generate l.373] (12/47) Post-process Answer
[2024-03-05 15:17:52,213 INFO generators.py generate l.480] (12/47) End question "Comment réformer le système de santé ?  A) Système de santé public B) Système de santé privé C) Assurance maladie obligatoire"
[2024-03-05 15:17:52,216 INFO generators.py generate l.478] (13/47) *** AnsGenerator for question "Comment réformer le système bancaire ?  A) Banques publiques B) Déréglementation bancaire C) Régulation bancaire"
[2024-03-05 15:17:52,218 INFO generators.py gen_for_qa l.551] (13/47) * Start with LLM "gpt-4"
[2024-03-05 15:17:52,221 DEBUG generators.py gen_for_qa l.557] (13/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,224 DEBUG generators.py generate l.355] (13/47) Reuse existing Prompt
[2024-03-05 15:17:52,226 DEBUG generators.py generate l.368] (13/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,229 DEBUG generators.py generate l.373] (13/47) Post-process Answer
[2024-03-05 15:17:52,233 INFO generators.py gen_for_qa l.551] (13/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:17:52,234 DEBUG generators.py gen_for_qa l.557] (13/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,237 DEBUG generators.py generate l.355] (13/47) Reuse existing Prompt
[2024-03-05 15:17:52,239 DEBUG generators.py generate l.368] (13/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,242 DEBUG generators.py generate l.373] (13/47) Post-process Answer
[2024-03-05 15:17:52,246 INFO generators.py gen_for_qa l.551] (13/47) * Start with LLM "gemini-pro"
[2024-03-05 15:17:52,248 DEBUG generators.py gen_for_qa l.557] (13/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,248 DEBUG generators.py generate l.355] (13/47) Reuse existing Prompt
[2024-03-05 15:17:52,253 DEBUG generators.py generate l.368] (13/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,255 DEBUG generators.py generate l.373] (13/47) Post-process Answer
[2024-03-05 15:17:52,258 INFO generators.py gen_for_qa l.551] (13/47) * Start with LLM "claude-2.1"
[2024-03-05 15:17:52,260 DEBUG generators.py gen_for_qa l.557] (13/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,265 DEBUG generators.py generate l.355] (13/47) Reuse existing Prompt
[2024-03-05 15:17:52,266 DEBUG generators.py generate l.368] (13/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,269 DEBUG generators.py generate l.373] (13/47) Post-process Answer
[2024-03-05 15:17:52,271 INFO generators.py gen_for_qa l.551] (13/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:17:52,273 DEBUG generators.py gen_for_qa l.557] (13/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,276 DEBUG generators.py generate l.355] (13/47) Reuse existing Prompt
[2024-03-05 15:17:52,279 DEBUG generators.py generate l.368] (13/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,283 DEBUG generators.py generate l.373] (13/47) Post-process Answer
[2024-03-05 15:17:52,285 INFO generators.py gen_for_qa l.551] (13/47) * Start with LLM "command-nightly"
[2024-03-05 15:17:52,288 DEBUG generators.py gen_for_qa l.557] (13/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,293 DEBUG generators.py generate l.355] (13/47) Reuse existing Prompt
[2024-03-05 15:17:52,296 DEBUG generators.py generate l.368] (13/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,299 DEBUG generators.py generate l.373] (13/47) Post-process Answer
[2024-03-05 15:17:52,301 INFO generators.py generate l.480] (13/47) End question "Comment réformer le système bancaire ?  A) Banques publiques B) Déréglementation bancaire C) Régulation bancaire"
[2024-03-05 15:17:52,303 INFO generators.py generate l.478] (14/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés ?  A) Régulation étatique B) Déréglementation C) Autorégulation"
[2024-03-05 15:17:52,306 INFO generators.py gen_for_qa l.551] (14/47) * Start with LLM "gpt-4"
[2024-03-05 15:17:52,308 DEBUG generators.py gen_for_qa l.557] (14/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,313 DEBUG generators.py generate l.355] (14/47) Reuse existing Prompt
[2024-03-05 15:17:52,315 DEBUG generators.py generate l.368] (14/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,317 DEBUG generators.py generate l.373] (14/47) Post-process Answer
[2024-03-05 15:17:52,319 INFO generators.py gen_for_qa l.551] (14/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:17:52,319 DEBUG generators.py gen_for_qa l.557] (14/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,324 DEBUG generators.py generate l.355] (14/47) Reuse existing Prompt
[2024-03-05 15:17:52,327 DEBUG generators.py generate l.368] (14/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,329 DEBUG generators.py generate l.373] (14/47) Post-process Answer
[2024-03-05 15:17:52,329 INFO generators.py gen_for_qa l.551] (14/47) * Start with LLM "gemini-pro"
[2024-03-05 15:17:52,335 DEBUG generators.py gen_for_qa l.557] (14/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,337 DEBUG generators.py generate l.355] (14/47) Reuse existing Prompt
[2024-03-05 15:17:52,339 DEBUG generators.py generate l.368] (14/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,342 DEBUG generators.py generate l.373] (14/47) Post-process Answer
[2024-03-05 15:17:52,343 INFO generators.py gen_for_qa l.551] (14/47) * Start with LLM "claude-2.1"
[2024-03-05 15:17:52,348 DEBUG generators.py gen_for_qa l.557] (14/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,349 DEBUG generators.py generate l.355] (14/47) Reuse existing Prompt
[2024-03-05 15:17:52,352 DEBUG generators.py generate l.368] (14/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,354 DEBUG generators.py generate l.373] (14/47) Post-process Answer
[2024-03-05 15:17:52,356 INFO generators.py gen_for_qa l.551] (14/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:17:52,360 DEBUG generators.py gen_for_qa l.557] (14/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,362 DEBUG generators.py generate l.355] (14/47) Reuse existing Prompt
[2024-03-05 15:17:52,362 DEBUG generators.py generate l.368] (14/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,367 DEBUG generators.py generate l.373] (14/47) Post-process Answer
[2024-03-05 15:17:52,370 INFO generators.py gen_for_qa l.551] (14/47) * Start with LLM "command-nightly"
[2024-03-05 15:17:52,371 DEBUG generators.py gen_for_qa l.557] (14/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,371 DEBUG generators.py generate l.355] (14/47) Reuse existing Prompt
[2024-03-05 15:17:52,376 DEBUG generators.py generate l.368] (14/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,380 DEBUG generators.py generate l.373] (14/47) Post-process Answer
[2024-03-05 15:17:52,383 INFO generators.py generate l.480] (14/47) End question "Comment réformer le système de régulation des marchés ?  A) Régulation étatique B) Déréglementation C) Autorégulation"
[2024-03-05 15:17:52,383 INFO generators.py generate l.478] (15/47) *** AnsGenerator for question "Comment réformer le système de commerce international ?  A) Protectionnisme B) Libre-échange C) Commerce équitable"
[2024-03-05 15:17:52,387 INFO generators.py gen_for_qa l.551] (15/47) * Start with LLM "gpt-4"
[2024-03-05 15:17:52,390 DEBUG generators.py gen_for_qa l.557] (15/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,392 DEBUG generators.py generate l.355] (15/47) Reuse existing Prompt
[2024-03-05 15:17:52,396 DEBUG generators.py generate l.368] (15/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,398 DEBUG generators.py generate l.373] (15/47) Post-process Answer
[2024-03-05 15:17:52,398 INFO generators.py gen_for_qa l.551] (15/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:17:52,403 DEBUG generators.py gen_for_qa l.557] (15/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,405 DEBUG generators.py generate l.355] (15/47) Reuse existing Prompt
[2024-03-05 15:17:52,408 DEBUG generators.py generate l.368] (15/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,410 DEBUG generators.py generate l.373] (15/47) Post-process Answer
[2024-03-05 15:17:52,412 INFO generators.py gen_for_qa l.551] (15/47) * Start with LLM "gemini-pro"
[2024-03-05 15:17:52,417 DEBUG generators.py gen_for_qa l.557] (15/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,418 DEBUG generators.py generate l.355] (15/47) Reuse existing Prompt
[2024-03-05 15:17:52,418 DEBUG generators.py generate l.368] (15/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,423 DEBUG generators.py generate l.373] (15/47) Post-process Answer
[2024-03-05 15:17:52,426 INFO generators.py gen_for_qa l.551] (15/47) * Start with LLM "claude-2.1"
[2024-03-05 15:17:52,430 DEBUG generators.py gen_for_qa l.557] (15/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,431 DEBUG generators.py generate l.355] (15/47) Reuse existing Prompt
[2024-03-05 15:17:52,435 DEBUG generators.py generate l.368] (15/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,437 DEBUG generators.py generate l.373] (15/47) Post-process Answer
[2024-03-05 15:17:52,439 INFO generators.py gen_for_qa l.551] (15/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:17:52,442 DEBUG generators.py gen_for_qa l.557] (15/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,447 DEBUG generators.py generate l.355] (15/47) Reuse existing Prompt
[2024-03-05 15:17:52,450 DEBUG generators.py generate l.368] (15/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,454 DEBUG generators.py generate l.373] (15/47) Post-process Answer
[2024-03-05 15:17:52,455 INFO generators.py gen_for_qa l.551] (15/47) * Start with LLM "command-nightly"
[2024-03-05 15:17:52,460 DEBUG generators.py gen_for_qa l.557] (15/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,464 DEBUG generators.py generate l.355] (15/47) Reuse existing Prompt
[2024-03-05 15:17:52,466 DEBUG generators.py generate l.368] (15/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,466 DEBUG generators.py generate l.373] (15/47) Post-process Answer
[2024-03-05 15:17:52,472 INFO generators.py generate l.480] (15/47) End question "Comment réformer le système de commerce international ?  A) Protectionnisme B) Libre-échange C) Commerce équitable"
[2024-03-05 15:17:52,473 INFO generators.py generate l.478] (16/47) *** AnsGenerator for question "Comment réformer le système monétaire international ?  A) Monnaies nationales B) Étalon-or C) Monnaie unique"
[2024-03-05 15:17:52,476 INFO generators.py gen_for_qa l.551] (16/47) * Start with LLM "gpt-4"
[2024-03-05 15:17:52,480 DEBUG generators.py gen_for_qa l.557] (16/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,484 DEBUG generators.py generate l.355] (16/47) Reuse existing Prompt
[2024-03-05 15:17:52,484 DEBUG generators.py generate l.368] (16/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,488 DEBUG generators.py generate l.373] (16/47) Post-process Answer
[2024-03-05 15:17:52,488 INFO generators.py gen_for_qa l.551] (16/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:17:52,493 DEBUG generators.py gen_for_qa l.557] (16/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,496 DEBUG generators.py generate l.355] (16/47) Reuse existing Prompt
[2024-03-05 15:17:52,500 DEBUG generators.py generate l.368] (16/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,500 DEBUG generators.py generate l.373] (16/47) Post-process Answer
[2024-03-05 15:17:52,504 INFO generators.py gen_for_qa l.551] (16/47) * Start with LLM "gemini-pro"
[2024-03-05 15:17:52,504 DEBUG generators.py gen_for_qa l.557] (16/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,510 DEBUG generators.py generate l.355] (16/47) Reuse existing Prompt
[2024-03-05 15:17:52,513 DEBUG generators.py generate l.368] (16/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,516 DEBUG generators.py generate l.373] (16/47) Post-process Answer
[2024-03-05 15:17:52,516 INFO generators.py gen_for_qa l.551] (16/47) * Start with LLM "claude-2.1"
[2024-03-05 15:17:52,516 DEBUG generators.py gen_for_qa l.557] (16/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,526 DEBUG generators.py generate l.355] (16/47) Reuse existing Prompt
[2024-03-05 15:17:52,530 DEBUG generators.py generate l.368] (16/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,530 DEBUG generators.py generate l.373] (16/47) Post-process Answer
[2024-03-05 15:17:52,535 INFO generators.py gen_for_qa l.551] (16/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:17:52,537 DEBUG generators.py gen_for_qa l.557] (16/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,539 DEBUG generators.py generate l.355] (16/47) Reuse existing Prompt
[2024-03-05 15:17:52,542 DEBUG generators.py generate l.368] (16/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,543 DEBUG generators.py generate l.373] (16/47) Post-process Answer
[2024-03-05 15:17:52,547 INFO generators.py gen_for_qa l.551] (16/47) * Start with LLM "command-nightly"
[2024-03-05 15:17:52,550 DEBUG generators.py gen_for_qa l.557] (16/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,552 DEBUG generators.py generate l.355] (16/47) Reuse existing Prompt
[2024-03-05 15:17:52,554 DEBUG generators.py generate l.368] (16/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,554 DEBUG generators.py generate l.373] (16/47) Post-process Answer
[2024-03-05 15:17:52,558 INFO generators.py generate l.480] (16/47) End question "Comment réformer le système monétaire international ?  A) Monnaies nationales B) Étalon-or C) Monnaie unique"
[2024-03-05 15:17:52,562 INFO generators.py generate l.478] (17/47) *** AnsGenerator for question "Comment réformer le système financier international ?  A) Taxe sur les transactions financières B) Libéralisation financière C) Régulation financière internationale"
[2024-03-05 15:17:52,565 INFO generators.py gen_for_qa l.551] (17/47) * Start with LLM "gpt-4"
[2024-03-05 15:17:52,565 DEBUG generators.py gen_for_qa l.557] (17/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,570 DEBUG generators.py generate l.355] (17/47) Reuse existing Prompt
[2024-03-05 15:17:52,573 DEBUG generators.py generate l.368] (17/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,575 DEBUG generators.py generate l.373] (17/47) Post-process Answer
[2024-03-05 15:17:52,578 INFO generators.py gen_for_qa l.551] (17/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:17:52,581 DEBUG generators.py gen_for_qa l.557] (17/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,584 DEBUG generators.py generate l.355] (17/47) Reuse existing Prompt
[2024-03-05 15:17:52,587 DEBUG generators.py generate l.368] (17/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,587 DEBUG generators.py generate l.373] (17/47) Post-process Answer
[2024-03-05 15:17:52,592 INFO generators.py gen_for_qa l.551] (17/47) * Start with LLM "gemini-pro"
[2024-03-05 15:17:52,593 DEBUG generators.py gen_for_qa l.557] (17/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,597 DEBUG generators.py generate l.355] (17/47) Reuse existing Prompt
[2024-03-05 15:17:52,599 DEBUG generators.py generate l.368] (17/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,601 DEBUG generators.py generate l.373] (17/47) Post-process Answer
[2024-03-05 15:17:52,603 INFO generators.py gen_for_qa l.551] (17/47) * Start with LLM "claude-2.1"
[2024-03-05 15:17:52,606 DEBUG generators.py gen_for_qa l.557] (17/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,608 DEBUG generators.py generate l.355] (17/47) Reuse existing Prompt
[2024-03-05 15:17:52,611 DEBUG generators.py generate l.368] (17/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,614 DEBUG generators.py generate l.373] (17/47) Post-process Answer
[2024-03-05 15:17:52,615 INFO generators.py gen_for_qa l.551] (17/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:17:52,615 DEBUG generators.py gen_for_qa l.557] (17/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,621 DEBUG generators.py generate l.355] (17/47) Reuse existing Prompt
[2024-03-05 15:17:52,623 DEBUG generators.py generate l.368] (17/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,626 DEBUG generators.py generate l.373] (17/47) Post-process Answer
[2024-03-05 15:17:52,628 INFO generators.py gen_for_qa l.551] (17/47) * Start with LLM "command-nightly"
[2024-03-05 15:17:52,629 DEBUG generators.py gen_for_qa l.557] (17/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,633 DEBUG generators.py generate l.355] (17/47) Reuse existing Prompt
[2024-03-05 15:17:52,634 DEBUG generators.py generate l.368] (17/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,638 DEBUG generators.py generate l.373] (17/47) Post-process Answer
[2024-03-05 15:17:52,643 INFO generators.py generate l.480] (17/47) End question "Comment réformer le système financier international ?  A) Taxe sur les transactions financières B) Libéralisation financière C) Régulation financière internationale"
[2024-03-05 15:17:52,646 INFO generators.py generate l.478] (18/47) *** AnsGenerator for question "Comment réformer le système de propriété intellectuelle ?  A) Licences libres B) Brevets C) Droits d'auteur"
[2024-03-05 15:17:52,649 INFO generators.py gen_for_qa l.551] (18/47) * Start with LLM "gpt-4"
[2024-03-05 15:17:52,652 DEBUG generators.py gen_for_qa l.557] (18/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,654 DEBUG generators.py generate l.355] (18/47) Reuse existing Prompt
[2024-03-05 15:17:52,658 DEBUG generators.py generate l.368] (18/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,660 DEBUG generators.py generate l.373] (18/47) Post-process Answer
[2024-03-05 15:17:52,661 INFO generators.py gen_for_qa l.551] (18/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:17:52,664 DEBUG generators.py gen_for_qa l.557] (18/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,667 DEBUG generators.py generate l.355] (18/47) Reuse existing Prompt
[2024-03-05 15:17:52,670 DEBUG generators.py generate l.368] (18/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,672 DEBUG generators.py generate l.373] (18/47) Post-process Answer
[2024-03-05 15:17:52,673 INFO generators.py gen_for_qa l.551] (18/47) * Start with LLM "gemini-pro"
[2024-03-05 15:17:52,676 DEBUG generators.py gen_for_qa l.557] (18/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,679 DEBUG generators.py generate l.355] (18/47) Reuse existing Prompt
[2024-03-05 15:17:52,681 DEBUG generators.py generate l.368] (18/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,685 DEBUG generators.py generate l.373] (18/47) Post-process Answer
[2024-03-05 15:17:52,685 INFO generators.py gen_for_qa l.551] (18/47) * Start with LLM "claude-2.1"
[2024-03-05 15:17:52,689 DEBUG generators.py gen_for_qa l.557] (18/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,692 DEBUG generators.py generate l.355] (18/47) Reuse existing Prompt
[2024-03-05 15:17:52,694 DEBUG generators.py generate l.368] (18/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,696 DEBUG generators.py generate l.373] (18/47) Post-process Answer
[2024-03-05 15:17:52,698 INFO generators.py gen_for_qa l.551] (18/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:17:52,702 DEBUG generators.py gen_for_qa l.557] (18/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,704 DEBUG generators.py generate l.355] (18/47) Reuse existing Prompt
[2024-03-05 15:17:52,704 DEBUG generators.py generate l.368] (18/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,709 DEBUG generators.py generate l.373] (18/47) Post-process Answer
[2024-03-05 15:17:52,712 INFO generators.py gen_for_qa l.551] (18/47) * Start with LLM "command-nightly"
[2024-03-05 15:17:52,714 DEBUG generators.py gen_for_qa l.557] (18/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,717 DEBUG generators.py generate l.355] (18/47) Reuse existing Prompt
[2024-03-05 15:17:52,720 DEBUG generators.py generate l.368] (18/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,722 DEBUG generators.py generate l.373] (18/47) Post-process Answer
[2024-03-05 15:17:52,724 INFO generators.py generate l.480] (18/47) End question "Comment réformer le système de propriété intellectuelle ?  A) Licences libres B) Brevets C) Droits d'auteur"
[2024-03-05 15:17:52,726 INFO generators.py generate l.478] (19/47) *** AnsGenerator for question "Comment réformer le système de régulation des industries ?  A) Nationalisation B) Déréglementation C) Concurrence régulée"
[2024-03-05 15:17:52,731 INFO generators.py gen_for_qa l.551] (19/47) * Start with LLM "gpt-4"
[2024-03-05 15:17:52,732 DEBUG generators.py gen_for_qa l.557] (19/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,734 DEBUG generators.py generate l.355] (19/47) Reuse existing Prompt
[2024-03-05 15:17:52,737 DEBUG generators.py generate l.368] (19/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,737 DEBUG generators.py generate l.373] (19/47) Post-process Answer
[2024-03-05 15:17:52,742 INFO generators.py gen_for_qa l.551] (19/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:17:52,746 DEBUG generators.py gen_for_qa l.557] (19/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,746 DEBUG generators.py generate l.355] (19/47) Reuse existing Prompt
[2024-03-05 15:17:52,750 DEBUG generators.py generate l.368] (19/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,752 DEBUG generators.py generate l.373] (19/47) Post-process Answer
[2024-03-05 15:17:52,755 INFO generators.py gen_for_qa l.551] (19/47) * Start with LLM "gemini-pro"
[2024-03-05 15:17:52,757 DEBUG generators.py gen_for_qa l.557] (19/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,759 DEBUG generators.py generate l.355] (19/47) Reuse existing Prompt
[2024-03-05 15:17:52,763 DEBUG generators.py generate l.368] (19/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,765 DEBUG generators.py generate l.373] (19/47) Post-process Answer
[2024-03-05 15:17:52,768 INFO generators.py gen_for_qa l.551] (19/47) * Start with LLM "claude-2.1"
[2024-03-05 15:17:52,771 DEBUG generators.py gen_for_qa l.557] (19/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,773 DEBUG generators.py generate l.355] (19/47) Reuse existing Prompt
[2024-03-05 15:17:52,776 DEBUG generators.py generate l.368] (19/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,778 DEBUG generators.py generate l.373] (19/47) Post-process Answer
[2024-03-05 15:17:52,783 INFO generators.py gen_for_qa l.551] (19/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:17:52,786 DEBUG generators.py gen_for_qa l.557] (19/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,789 DEBUG generators.py generate l.355] (19/47) Reuse existing Prompt
[2024-03-05 15:17:52,792 DEBUG generators.py generate l.368] (19/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,796 DEBUG generators.py generate l.373] (19/47) Post-process Answer
[2024-03-05 15:17:52,798 INFO generators.py gen_for_qa l.551] (19/47) * Start with LLM "command-nightly"
[2024-03-05 15:17:52,801 DEBUG generators.py gen_for_qa l.557] (19/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,803 DEBUG generators.py generate l.355] (19/47) Reuse existing Prompt
[2024-03-05 15:17:52,805 DEBUG generators.py generate l.368] (19/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,808 DEBUG generators.py generate l.373] (19/47) Post-process Answer
[2024-03-05 15:17:52,810 INFO generators.py generate l.480] (19/47) End question "Comment réformer le système de régulation des industries ?  A) Nationalisation B) Déréglementation C) Concurrence régulée"
[2024-03-05 15:17:52,813 INFO generators.py generate l.478] (20/47) *** AnsGenerator for question "Comment réformer le système de régulation des services publics ?  A) Services publics gratuits B) Privatisation C) Partenariat public-privé"
[2024-03-05 15:17:52,816 INFO generators.py gen_for_qa l.551] (20/47) * Start with LLM "gpt-4"
[2024-03-05 15:17:52,818 DEBUG generators.py gen_for_qa l.557] (20/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,821 DEBUG generators.py generate l.355] (20/47) Reuse existing Prompt
[2024-03-05 15:17:52,824 DEBUG generators.py generate l.368] (20/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,826 DEBUG generators.py generate l.373] (20/47) Post-process Answer
[2024-03-05 15:17:52,828 INFO generators.py gen_for_qa l.551] (20/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:17:52,831 DEBUG generators.py gen_for_qa l.557] (20/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,834 DEBUG generators.py generate l.355] (20/47) Reuse existing Prompt
[2024-03-05 15:17:52,837 DEBUG generators.py generate l.368] (20/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,837 DEBUG generators.py generate l.373] (20/47) Post-process Answer
[2024-03-05 15:17:52,840 INFO generators.py gen_for_qa l.551] (20/47) * Start with LLM "gemini-pro"
[2024-03-05 15:17:52,843 DEBUG generators.py gen_for_qa l.557] (20/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,847 DEBUG generators.py generate l.355] (20/47) Reuse existing Prompt
[2024-03-05 15:17:52,850 DEBUG generators.py generate l.368] (20/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,851 DEBUG generators.py generate l.373] (20/47) Post-process Answer
[2024-03-05 15:17:52,854 INFO generators.py gen_for_qa l.551] (20/47) * Start with LLM "claude-2.1"
[2024-03-05 15:17:52,854 DEBUG generators.py gen_for_qa l.557] (20/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,858 DEBUG generators.py generate l.355] (20/47) Reuse existing Prompt
[2024-03-05 15:17:52,862 DEBUG generators.py generate l.368] (20/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,864 DEBUG generators.py generate l.373] (20/47) Post-process Answer
[2024-03-05 15:17:52,868 INFO generators.py gen_for_qa l.551] (20/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:17:52,870 DEBUG generators.py gen_for_qa l.557] (20/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,873 DEBUG generators.py generate l.355] (20/47) Reuse existing Prompt
[2024-03-05 15:17:52,875 DEBUG generators.py generate l.368] (20/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,876 DEBUG generators.py generate l.373] (20/47) Post-process Answer
[2024-03-05 15:17:52,881 INFO generators.py gen_for_qa l.551] (20/47) * Start with LLM "command-nightly"
[2024-03-05 15:17:52,883 DEBUG generators.py gen_for_qa l.557] (20/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,885 DEBUG generators.py generate l.355] (20/47) Reuse existing Prompt
[2024-03-05 15:17:52,888 DEBUG generators.py generate l.368] (20/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,890 DEBUG generators.py generate l.373] (20/47) Post-process Answer
[2024-03-05 15:17:52,893 INFO generators.py generate l.480] (20/47) End question "Comment réformer le système de régulation des services publics ?  A) Services publics gratuits B) Privatisation C) Partenariat public-privé"
[2024-03-05 15:17:52,897 INFO generators.py generate l.478] (21/47) *** AnsGenerator for question "Comment réformer le système de régulation des finances publiques ?  A) Déficit public B) Austérité budgétaire C) Règle d'or budgétaire"
[2024-03-05 15:17:52,900 INFO generators.py gen_for_qa l.551] (21/47) * Start with LLM "gpt-4"
[2024-03-05 15:17:52,903 DEBUG generators.py gen_for_qa l.557] (21/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,903 DEBUG generators.py generate l.355] (21/47) Reuse existing Prompt
[2024-03-05 15:17:52,903 DEBUG generators.py generate l.368] (21/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,908 DEBUG generators.py generate l.373] (21/47) Post-process Answer
[2024-03-05 15:17:52,912 INFO generators.py gen_for_qa l.551] (21/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:17:52,914 DEBUG generators.py gen_for_qa l.557] (21/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,917 DEBUG generators.py generate l.355] (21/47) Reuse existing Prompt
[2024-03-05 15:17:52,920 DEBUG generators.py generate l.368] (21/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,922 DEBUG generators.py generate l.373] (21/47) Post-process Answer
[2024-03-05 15:17:52,923 INFO generators.py gen_for_qa l.551] (21/47) * Start with LLM "gemini-pro"
[2024-03-05 15:17:52,926 DEBUG generators.py gen_for_qa l.557] (21/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,929 DEBUG generators.py generate l.355] (21/47) Reuse existing Prompt
[2024-03-05 15:17:52,933 DEBUG generators.py generate l.368] (21/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,935 DEBUG generators.py generate l.373] (21/47) Post-process Answer
[2024-03-05 15:17:52,937 INFO generators.py gen_for_qa l.551] (21/47) * Start with LLM "claude-2.1"
[2024-03-05 15:17:52,940 DEBUG generators.py gen_for_qa l.557] (21/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,943 DEBUG generators.py generate l.355] (21/47) Reuse existing Prompt
[2024-03-05 15:17:52,947 DEBUG generators.py generate l.368] (21/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,948 DEBUG generators.py generate l.373] (21/47) Post-process Answer
[2024-03-05 15:17:52,951 INFO generators.py gen_for_qa l.551] (21/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:17:52,954 DEBUG generators.py gen_for_qa l.557] (21/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,956 DEBUG generators.py generate l.355] (21/47) Reuse existing Prompt
[2024-03-05 15:17:52,960 DEBUG generators.py generate l.368] (21/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,960 DEBUG generators.py generate l.373] (21/47) Post-process Answer
[2024-03-05 15:17:52,960 INFO generators.py gen_for_qa l.551] (21/47) * Start with LLM "command-nightly"
[2024-03-05 15:17:52,971 DEBUG generators.py gen_for_qa l.557] (21/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,973 DEBUG generators.py generate l.355] (21/47) Reuse existing Prompt
[2024-03-05 15:17:52,976 DEBUG generators.py generate l.368] (21/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:52,982 DEBUG generators.py generate l.373] (21/47) Post-process Answer
[2024-03-05 15:17:52,982 INFO generators.py generate l.480] (21/47) End question "Comment réformer le système de régulation des finances publiques ?  A) Déficit public B) Austérité budgétaire C) Règle d'or budgétaire"
[2024-03-05 15:17:52,986 INFO generators.py generate l.478] (22/47) *** AnsGenerator for question "Comment réformer le système de régulation des échanges internationaux ?  A) Commerce équitable B) Libre-échange C) Régionalisme économique"
[2024-03-05 15:17:52,989 INFO generators.py gen_for_qa l.551] (22/47) * Start with LLM "gpt-4"
[2024-03-05 15:17:52,991 DEBUG generators.py gen_for_qa l.557] (22/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:52,994 DEBUG generators.py generate l.355] (22/47) Reuse existing Prompt
[2024-03-05 15:17:52,997 DEBUG generators.py generate l.368] (22/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,000 DEBUG generators.py generate l.373] (22/47) Post-process Answer
[2024-03-05 15:17:53,003 INFO generators.py gen_for_qa l.551] (22/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:17:53,005 DEBUG generators.py gen_for_qa l.557] (22/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,008 DEBUG generators.py generate l.355] (22/47) Reuse existing Prompt
[2024-03-05 15:17:53,011 DEBUG generators.py generate l.368] (22/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,015 DEBUG generators.py generate l.373] (22/47) Post-process Answer
[2024-03-05 15:17:53,018 INFO generators.py gen_for_qa l.551] (22/47) * Start with LLM "gemini-pro"
[2024-03-05 15:17:53,020 DEBUG generators.py gen_for_qa l.557] (22/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,021 DEBUG generators.py generate l.355] (22/47) Reuse existing Prompt
[2024-03-05 15:17:53,024 DEBUG generators.py generate l.368] (22/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,026 DEBUG generators.py generate l.373] (22/47) Post-process Answer
[2024-03-05 15:17:53,028 INFO generators.py gen_for_qa l.551] (22/47) * Start with LLM "claude-2.1"
[2024-03-05 15:17:53,032 DEBUG generators.py gen_for_qa l.557] (22/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,035 DEBUG generators.py generate l.355] (22/47) Reuse existing Prompt
[2024-03-05 15:17:53,037 DEBUG generators.py generate l.368] (22/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,039 DEBUG generators.py generate l.373] (22/47) Post-process Answer
[2024-03-05 15:17:53,042 INFO generators.py gen_for_qa l.551] (22/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:17:53,045 DEBUG generators.py gen_for_qa l.557] (22/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,046 DEBUG generators.py generate l.355] (22/47) Reuse existing Prompt
[2024-03-05 15:17:53,052 DEBUG generators.py generate l.368] (22/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,053 DEBUG generators.py generate l.373] (22/47) Post-process Answer
[2024-03-05 15:17:53,056 INFO generators.py gen_for_qa l.551] (22/47) * Start with LLM "command-nightly"
[2024-03-05 15:17:53,058 DEBUG generators.py gen_for_qa l.557] (22/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,060 DEBUG generators.py generate l.355] (22/47) Reuse existing Prompt
[2024-03-05 15:17:53,065 DEBUG generators.py generate l.368] (22/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,066 DEBUG generators.py generate l.373] (22/47) Post-process Answer
[2024-03-05 15:17:53,069 INFO generators.py generate l.480] (22/47) End question "Comment réformer le système de régulation des échanges internationaux ?  A) Commerce équitable B) Libre-échange C) Régionalisme économique"
[2024-03-05 15:17:53,070 INFO generators.py generate l.478] (23/47) *** AnsGenerator for question "Comment réformer le système de régulation des migrations internationales ?  A) Politique migratoire ouverte B) Politique migratoire restrictive C) Politique migratoire sélective"
[2024-03-05 15:17:53,074 INFO generators.py gen_for_qa l.551] (23/47) * Start with LLM "gpt-4"
[2024-03-05 15:17:53,076 DEBUG generators.py gen_for_qa l.557] (23/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,082 DEBUG generators.py generate l.355] (23/47) Reuse existing Prompt
[2024-03-05 15:17:53,085 DEBUG generators.py generate l.368] (23/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,085 DEBUG generators.py generate l.373] (23/47) Post-process Answer
[2024-03-05 15:17:53,093 INFO generators.py gen_for_qa l.551] (23/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:17:53,099 DEBUG generators.py gen_for_qa l.557] (23/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,100 DEBUG generators.py generate l.355] (23/47) Reuse existing Prompt
[2024-03-05 15:17:53,103 DEBUG generators.py generate l.368] (23/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,105 DEBUG generators.py generate l.373] (23/47) Post-process Answer
[2024-03-05 15:17:53,107 INFO generators.py gen_for_qa l.551] (23/47) * Start with LLM "gemini-pro"
[2024-03-05 15:17:53,109 DEBUG generators.py gen_for_qa l.557] (23/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,113 DEBUG generators.py generate l.355] (23/47) Reuse existing Prompt
[2024-03-05 15:17:53,115 DEBUG generators.py generate l.368] (23/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,117 DEBUG generators.py generate l.373] (23/47) Post-process Answer
[2024-03-05 15:17:53,118 INFO generators.py gen_for_qa l.551] (23/47) * Start with LLM "claude-2.1"
[2024-03-05 15:17:53,120 DEBUG generators.py gen_for_qa l.557] (23/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,121 DEBUG generators.py generate l.355] (23/47) Reuse existing Prompt
[2024-03-05 15:17:53,124 DEBUG generators.py generate l.368] (23/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,126 DEBUG generators.py generate l.373] (23/47) Post-process Answer
[2024-03-05 15:17:53,129 INFO generators.py gen_for_qa l.551] (23/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:17:53,131 DEBUG generators.py gen_for_qa l.557] (23/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,133 DEBUG generators.py generate l.355] (23/47) Reuse existing Prompt
[2024-03-05 15:17:53,134 DEBUG generators.py generate l.368] (23/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,136 DEBUG generators.py generate l.373] (23/47) Post-process Answer
[2024-03-05 15:17:53,138 INFO generators.py gen_for_qa l.551] (23/47) * Start with LLM "command-nightly"
[2024-03-05 15:17:53,140 DEBUG generators.py gen_for_qa l.557] (23/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,141 DEBUG generators.py generate l.355] (23/47) Reuse existing Prompt
[2024-03-05 15:17:53,145 DEBUG generators.py generate l.368] (23/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,147 DEBUG generators.py generate l.373] (23/47) Post-process Answer
[2024-03-05 15:17:53,149 INFO generators.py generate l.480] (23/47) End question "Comment réformer le système de régulation des migrations internationales ?  A) Politique migratoire ouverte B) Politique migratoire restrictive C) Politique migratoire sélective"
[2024-03-05 15:17:53,152 INFO generators.py generate l.478] (24/47) *** AnsGenerator for question "Comment réformer le système de régulation de l'environnement ?  A) Écologie politique B) Marché du carbone C) Développement durable"
[2024-03-05 15:17:53,154 INFO generators.py gen_for_qa l.551] (24/47) * Start with LLM "gpt-4"
[2024-03-05 15:17:53,155 DEBUG generators.py gen_for_qa l.557] (24/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,157 DEBUG generators.py generate l.355] (24/47) Reuse existing Prompt
[2024-03-05 15:17:53,159 DEBUG generators.py generate l.368] (24/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,163 DEBUG generators.py generate l.373] (24/47) Post-process Answer
[2024-03-05 15:17:53,165 INFO generators.py gen_for_qa l.551] (24/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:17:53,168 DEBUG generators.py gen_for_qa l.557] (24/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,170 DEBUG generators.py generate l.355] (24/47) Reuse existing Prompt
[2024-03-05 15:17:53,171 DEBUG generators.py generate l.368] (24/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,171 DEBUG generators.py generate l.373] (24/47) Post-process Answer
[2024-03-05 15:17:53,176 INFO generators.py gen_for_qa l.551] (24/47) * Start with LLM "gemini-pro"
[2024-03-05 15:17:53,179 DEBUG generators.py gen_for_qa l.557] (24/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,181 DEBUG generators.py generate l.355] (24/47) Reuse existing Prompt
[2024-03-05 15:17:53,184 DEBUG generators.py generate l.368] (24/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,186 DEBUG generators.py generate l.373] (24/47) Post-process Answer
[2024-03-05 15:17:53,188 INFO generators.py gen_for_qa l.551] (24/47) * Start with LLM "claude-2.1"
[2024-03-05 15:17:53,189 DEBUG generators.py gen_for_qa l.557] (24/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,193 DEBUG generators.py generate l.355] (24/47) Reuse existing Prompt
[2024-03-05 15:17:53,196 DEBUG generators.py generate l.368] (24/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,198 DEBUG generators.py generate l.373] (24/47) Post-process Answer
[2024-03-05 15:17:53,201 INFO generators.py gen_for_qa l.551] (24/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:17:53,202 DEBUG generators.py gen_for_qa l.557] (24/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,204 DEBUG generators.py generate l.355] (24/47) Reuse existing Prompt
[2024-03-05 15:17:53,206 DEBUG generators.py generate l.368] (24/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,208 DEBUG generators.py generate l.373] (24/47) Post-process Answer
[2024-03-05 15:17:53,211 INFO generators.py gen_for_qa l.551] (24/47) * Start with LLM "command-nightly"
[2024-03-05 15:17:53,213 DEBUG generators.py gen_for_qa l.557] (24/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,215 DEBUG generators.py generate l.355] (24/47) Reuse existing Prompt
[2024-03-05 15:17:53,218 DEBUG generators.py generate l.368] (24/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,218 DEBUG generators.py generate l.373] (24/47) Post-process Answer
[2024-03-05 15:17:53,221 INFO generators.py generate l.480] (24/47) End question "Comment réformer le système de régulation de l'environnement ?  A) Écologie politique B) Marché du carbone C) Développement durable"
[2024-03-05 15:17:53,223 INFO generators.py generate l.478] (25/47) *** AnsGenerator for question "Comment réduire la pauvreté ?  A) Redistribution des richesses B) Croissance économique C) Filets de sécurité sociaux"
[2024-03-05 15:17:53,226 INFO generators.py gen_for_qa l.551] (25/47) * Start with LLM "gpt-4"
[2024-03-05 15:17:53,228 DEBUG generators.py gen_for_qa l.557] (25/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,231 DEBUG generators.py generate l.355] (25/47) Reuse existing Prompt
[2024-03-05 15:17:53,234 DEBUG generators.py generate l.368] (25/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,236 DEBUG generators.py generate l.373] (25/47) Post-process Answer
[2024-03-05 15:17:53,237 INFO generators.py gen_for_qa l.551] (25/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:17:53,237 DEBUG generators.py gen_for_qa l.557] (25/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,241 DEBUG generators.py generate l.355] (25/47) Reuse existing Prompt
[2024-03-05 15:17:53,244 DEBUG generators.py generate l.368] (25/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,247 DEBUG generators.py generate l.373] (25/47) Post-process Answer
[2024-03-05 15:17:53,249 INFO generators.py gen_for_qa l.551] (25/47) * Start with LLM "gemini-pro"
[2024-03-05 15:17:53,251 DEBUG generators.py gen_for_qa l.557] (25/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,252 DEBUG generators.py generate l.355] (25/47) Reuse existing Prompt
[2024-03-05 15:17:53,254 DEBUG generators.py generate l.368] (25/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,254 DEBUG generators.py generate l.373] (25/47) Post-process Answer
[2024-03-05 15:17:53,257 INFO generators.py gen_for_qa l.551] (25/47) * Start with LLM "claude-2.1"
[2024-03-05 15:17:53,259 DEBUG generators.py gen_for_qa l.557] (25/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,262 DEBUG generators.py generate l.355] (25/47) Reuse existing Prompt
[2024-03-05 15:17:53,263 DEBUG generators.py generate l.368] (25/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,266 DEBUG generators.py generate l.373] (25/47) Post-process Answer
[2024-03-05 15:17:53,268 INFO generators.py gen_for_qa l.551] (25/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:17:53,269 DEBUG generators.py gen_for_qa l.557] (25/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,270 DEBUG generators.py generate l.355] (25/47) Reuse existing Prompt
[2024-03-05 15:17:53,270 DEBUG generators.py generate l.368] (25/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,274 DEBUG generators.py generate l.373] (25/47) Post-process Answer
[2024-03-05 15:17:53,275 INFO generators.py gen_for_qa l.551] (25/47) * Start with LLM "command-nightly"
[2024-03-05 15:17:53,280 DEBUG generators.py gen_for_qa l.557] (25/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,281 DEBUG generators.py generate l.355] (25/47) Reuse existing Prompt
[2024-03-05 15:17:53,284 DEBUG generators.py generate l.368] (25/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,284 DEBUG generators.py generate l.373] (25/47) Post-process Answer
[2024-03-05 15:17:53,286 INFO generators.py generate l.480] (25/47) End question "Comment réduire la pauvreté ?  A) Redistribution des richesses B) Croissance économique C) Filets de sécurité sociaux"
[2024-03-05 15:17:53,290 INFO generators.py generate l.478] (26/47) *** AnsGenerator for question "Comment réformer le système de protection des consommateurs ?  A) Réglementation stricte B) Déréglementation C) Autorégulation"
[2024-03-05 15:17:53,290 INFO generators.py gen_for_qa l.551] (26/47) * Start with LLM "gpt-4"
[2024-03-05 15:17:53,294 DEBUG generators.py gen_for_qa l.557] (26/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,297 DEBUG generators.py generate l.355] (26/47) Reuse existing Prompt
[2024-03-05 15:17:53,299 DEBUG generators.py generate l.368] (26/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,301 DEBUG generators.py generate l.373] (26/47) Post-process Answer
[2024-03-05 15:17:53,302 INFO generators.py gen_for_qa l.551] (26/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:17:53,304 DEBUG generators.py gen_for_qa l.557] (26/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,307 DEBUG generators.py generate l.355] (26/47) Reuse existing Prompt
[2024-03-05 15:17:53,309 DEBUG generators.py generate l.368] (26/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,311 DEBUG generators.py generate l.373] (26/47) Post-process Answer
[2024-03-05 15:17:53,313 INFO generators.py gen_for_qa l.551] (26/47) * Start with LLM "gemini-pro"
[2024-03-05 15:17:53,315 DEBUG generators.py gen_for_qa l.557] (26/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,317 DEBUG generators.py generate l.355] (26/47) Reuse existing Prompt
[2024-03-05 15:17:53,320 DEBUG generators.py generate l.368] (26/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,322 DEBUG generators.py generate l.373] (26/47) Post-process Answer
[2024-03-05 15:17:53,323 INFO generators.py gen_for_qa l.551] (26/47) * Start with LLM "claude-2.1"
[2024-03-05 15:17:53,326 DEBUG generators.py gen_for_qa l.557] (26/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,329 DEBUG generators.py generate l.355] (26/47) Reuse existing Prompt
[2024-03-05 15:17:53,332 DEBUG generators.py generate l.368] (26/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,333 DEBUG generators.py generate l.373] (26/47) Post-process Answer
[2024-03-05 15:17:53,335 INFO generators.py gen_for_qa l.551] (26/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:17:53,338 DEBUG generators.py gen_for_qa l.557] (26/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,339 DEBUG generators.py generate l.355] (26/47) Reuse existing Prompt
[2024-03-05 15:17:53,341 DEBUG generators.py generate l.368] (26/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,344 DEBUG generators.py generate l.373] (26/47) Post-process Answer
[2024-03-05 15:17:53,346 INFO generators.py gen_for_qa l.551] (26/47) * Start with LLM "command-nightly"
[2024-03-05 15:17:53,348 DEBUG generators.py gen_for_qa l.557] (26/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,351 DEBUG generators.py generate l.355] (26/47) Reuse existing Prompt
[2024-03-05 15:17:53,353 DEBUG generators.py generate l.368] (26/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,354 DEBUG generators.py generate l.373] (26/47) Post-process Answer
[2024-03-05 15:17:53,356 INFO generators.py generate l.480] (26/47) End question "Comment réformer le système de protection des consommateurs ?  A) Réglementation stricte B) Déréglementation C) Autorégulation"
[2024-03-05 15:17:53,357 INFO generators.py generate l.478] (27/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés financiers ?  A) Réglementation stricte B) Déréglementation C) Régulation prudentielle"
[2024-03-05 15:17:53,359 INFO generators.py gen_for_qa l.551] (27/47) * Start with LLM "gpt-4"
[2024-03-05 15:17:53,363 DEBUG generators.py gen_for_qa l.557] (27/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,364 DEBUG generators.py generate l.355] (27/47) Reuse existing Prompt
[2024-03-05 15:17:53,364 DEBUG generators.py generate l.368] (27/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,368 DEBUG generators.py generate l.373] (27/47) Post-process Answer
[2024-03-05 15:17:53,371 INFO generators.py gen_for_qa l.551] (27/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:17:53,371 DEBUG generators.py gen_for_qa l.557] (27/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,374 DEBUG generators.py generate l.355] (27/47) Reuse existing Prompt
[2024-03-05 15:17:53,376 DEBUG generators.py generate l.368] (27/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,378 DEBUG generators.py generate l.373] (27/47) Post-process Answer
[2024-03-05 15:17:53,379 INFO generators.py gen_for_qa l.551] (27/47) * Start with LLM "gemini-pro"
[2024-03-05 15:17:53,382 DEBUG generators.py gen_for_qa l.557] (27/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,383 DEBUG generators.py generate l.355] (27/47) Reuse existing Prompt
[2024-03-05 15:17:53,385 DEBUG generators.py generate l.368] (27/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,386 DEBUG generators.py generate l.373] (27/47) Post-process Answer
[2024-03-05 15:17:53,386 INFO generators.py gen_for_qa l.551] (27/47) * Start with LLM "claude-2.1"
[2024-03-05 15:17:53,390 DEBUG generators.py gen_for_qa l.557] (27/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,392 DEBUG generators.py generate l.355] (27/47) Reuse existing Prompt
[2024-03-05 15:17:53,394 DEBUG generators.py generate l.368] (27/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,397 DEBUG generators.py generate l.373] (27/47) Post-process Answer
[2024-03-05 15:17:53,398 INFO generators.py gen_for_qa l.551] (27/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:17:53,398 DEBUG generators.py gen_for_qa l.557] (27/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,401 DEBUG generators.py generate l.355] (27/47) Reuse existing Prompt
[2024-03-05 15:17:53,403 DEBUG generators.py generate l.368] (27/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,403 DEBUG generators.py generate l.373] (27/47) Post-process Answer
[2024-03-05 15:17:53,405 INFO generators.py gen_for_qa l.551] (27/47) * Start with LLM "command-nightly"
[2024-03-05 15:17:53,406 DEBUG generators.py gen_for_qa l.557] (27/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,409 DEBUG generators.py generate l.355] (27/47) Reuse existing Prompt
[2024-03-05 15:17:53,411 DEBUG generators.py generate l.368] (27/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,413 DEBUG generators.py generate l.373] (27/47) Post-process Answer
[2024-03-05 15:17:53,415 INFO generators.py generate l.480] (27/47) End question "Comment réformer le système de régulation des marchés financiers ?  A) Réglementation stricte B) Déréglementation C) Régulation prudentielle"
[2024-03-05 15:17:53,417 INFO generators.py generate l.478] (28/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés du travail ?  A) Réglementation stricte B) Déréglementation C) Flexisécurité"
[2024-03-05 15:17:53,419 INFO generators.py gen_for_qa l.551] (28/47) * Start with LLM "gpt-4"
[2024-03-05 15:17:53,420 DEBUG generators.py gen_for_qa l.557] (28/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,420 DEBUG generators.py generate l.355] (28/47) Reuse existing Prompt
[2024-03-05 15:17:53,420 DEBUG generators.py generate l.368] (28/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,425 DEBUG generators.py generate l.373] (28/47) Post-process Answer
[2024-03-05 15:17:53,427 INFO generators.py gen_for_qa l.551] (28/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:17:53,430 DEBUG generators.py gen_for_qa l.557] (28/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,431 DEBUG generators.py generate l.355] (28/47) Reuse existing Prompt
[2024-03-05 15:17:53,431 DEBUG generators.py generate l.368] (28/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,435 DEBUG generators.py generate l.373] (28/47) Post-process Answer
[2024-03-05 15:17:53,437 INFO generators.py gen_for_qa l.551] (28/47) * Start with LLM "gemini-pro"
[2024-03-05 15:17:53,437 DEBUG generators.py gen_for_qa l.557] (28/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,440 DEBUG generators.py generate l.355] (28/47) Reuse existing Prompt
[2024-03-05 15:17:53,440 DEBUG generators.py generate l.368] (28/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,443 DEBUG generators.py generate l.373] (28/47) Post-process Answer
[2024-03-05 15:17:53,446 INFO generators.py gen_for_qa l.551] (28/47) * Start with LLM "claude-2.1"
[2024-03-05 15:17:53,447 DEBUG generators.py gen_for_qa l.557] (28/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,449 DEBUG generators.py generate l.355] (28/47) Reuse existing Prompt
[2024-03-05 15:17:53,451 DEBUG generators.py generate l.368] (28/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,452 DEBUG generators.py generate l.373] (28/47) Post-process Answer
[2024-03-05 15:17:53,453 INFO generators.py gen_for_qa l.551] (28/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:17:53,456 DEBUG generators.py gen_for_qa l.557] (28/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,457 DEBUG generators.py generate l.355] (28/47) Reuse existing Prompt
[2024-03-05 15:17:53,459 DEBUG generators.py generate l.368] (28/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,459 DEBUG generators.py generate l.373] (28/47) Post-process Answer
[2024-03-05 15:17:53,463 INFO generators.py gen_for_qa l.551] (28/47) * Start with LLM "command-nightly"
[2024-03-05 15:17:53,464 DEBUG generators.py gen_for_qa l.557] (28/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,467 DEBUG generators.py generate l.355] (28/47) Reuse existing Prompt
[2024-03-05 15:17:53,469 DEBUG generators.py generate l.368] (28/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,469 DEBUG generators.py generate l.373] (28/47) Post-process Answer
[2024-03-05 15:17:53,472 INFO generators.py generate l.480] (28/47) End question "Comment réformer le système de régulation des marchés du travail ?  A) Réglementation stricte B) Déréglementation C) Flexisécurité"
[2024-03-05 15:17:53,472 INFO generators.py generate l.478] (29/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés de l'énergie ?  A) Réglementation stricte B) Déréglementation C) Tarification progressive"
[2024-03-05 15:17:53,475 INFO generators.py gen_for_qa l.551] (29/47) * Start with LLM "gpt-4"
[2024-03-05 15:17:53,479 DEBUG generators.py gen_for_qa l.557] (29/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,480 DEBUG generators.py generate l.355] (29/47) Reuse existing Prompt
[2024-03-05 15:17:53,483 DEBUG generators.py generate l.368] (29/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,483 DEBUG generators.py generate l.373] (29/47) Post-process Answer
[2024-03-05 15:17:53,486 INFO generators.py gen_for_qa l.551] (29/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:17:53,486 DEBUG generators.py gen_for_qa l.557] (29/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,486 DEBUG generators.py generate l.355] (29/47) Reuse existing Prompt
[2024-03-05 15:17:53,491 DEBUG generators.py generate l.368] (29/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,494 DEBUG generators.py generate l.373] (29/47) Post-process Answer
[2024-03-05 15:17:53,496 INFO generators.py gen_for_qa l.551] (29/47) * Start with LLM "gemini-pro"
[2024-03-05 15:17:53,498 DEBUG generators.py gen_for_qa l.557] (29/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,500 DEBUG generators.py generate l.355] (29/47) Reuse existing Prompt
[2024-03-05 15:17:53,501 DEBUG generators.py generate l.368] (29/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,501 DEBUG generators.py generate l.373] (29/47) Post-process Answer
[2024-03-05 15:17:53,501 INFO generators.py gen_for_qa l.551] (29/47) * Start with LLM "claude-2.1"
[2024-03-05 15:17:53,507 DEBUG generators.py gen_for_qa l.557] (29/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,509 DEBUG generators.py generate l.355] (29/47) Reuse existing Prompt
[2024-03-05 15:17:53,511 DEBUG generators.py generate l.368] (29/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,512 DEBUG generators.py generate l.373] (29/47) Post-process Answer
[2024-03-05 15:17:53,512 INFO generators.py gen_for_qa l.551] (29/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:17:53,516 DEBUG generators.py gen_for_qa l.557] (29/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,519 DEBUG generators.py generate l.355] (29/47) Reuse existing Prompt
[2024-03-05 15:17:53,521 DEBUG generators.py generate l.368] (29/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,522 DEBUG generators.py generate l.373] (29/47) Post-process Answer
[2024-03-05 15:17:53,523 INFO generators.py gen_for_qa l.551] (29/47) * Start with LLM "command-nightly"
[2024-03-05 15:17:53,526 DEBUG generators.py gen_for_qa l.557] (29/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,527 DEBUG generators.py generate l.355] (29/47) Reuse existing Prompt
[2024-03-05 15:17:53,530 DEBUG generators.py generate l.368] (29/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,531 DEBUG generators.py generate l.373] (29/47) Post-process Answer
[2024-03-05 15:17:53,533 INFO generators.py generate l.480] (29/47) End question "Comment réformer le système de régulation des marchés de l'énergie ?  A) Réglementation stricte B) Déréglementation C) Tarification progressive"
[2024-03-05 15:17:53,535 INFO generators.py generate l.478] (30/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés des télécommunications ?  A) Réglementation stricte B) Déréglementation C) Neutralité du net"
[2024-03-05 15:17:53,537 INFO generators.py gen_for_qa l.551] (30/47) * Start with LLM "gpt-4"
[2024-03-05 15:17:53,537 DEBUG generators.py gen_for_qa l.557] (30/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,537 DEBUG generators.py generate l.355] (30/47) Reuse existing Prompt
[2024-03-05 15:17:53,537 DEBUG generators.py generate l.368] (30/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,542 DEBUG generators.py generate l.373] (30/47) Post-process Answer
[2024-03-05 15:17:53,544 INFO generators.py gen_for_qa l.551] (30/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:17:53,547 DEBUG generators.py gen_for_qa l.557] (30/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,549 DEBUG generators.py generate l.355] (30/47) Reuse existing Prompt
[2024-03-05 15:17:53,549 DEBUG generators.py generate l.368] (30/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,552 DEBUG generators.py generate l.373] (30/47) Post-process Answer
[2024-03-05 15:17:53,553 INFO generators.py gen_for_qa l.551] (30/47) * Start with LLM "gemini-pro"
[2024-03-05 15:17:53,553 DEBUG generators.py gen_for_qa l.557] (30/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,557 DEBUG generators.py generate l.355] (30/47) Reuse existing Prompt
[2024-03-05 15:17:53,559 DEBUG generators.py generate l.368] (30/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,560 DEBUG generators.py generate l.373] (30/47) Post-process Answer
[2024-03-05 15:17:53,563 INFO generators.py gen_for_qa l.551] (30/47) * Start with LLM "claude-2.1"
[2024-03-05 15:17:53,565 DEBUG generators.py gen_for_qa l.557] (30/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,567 DEBUG generators.py generate l.355] (30/47) Reuse existing Prompt
[2024-03-05 15:17:53,568 DEBUG generators.py generate l.368] (30/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,568 DEBUG generators.py generate l.373] (30/47) Post-process Answer
[2024-03-05 15:17:53,571 INFO generators.py gen_for_qa l.551] (30/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:17:53,573 DEBUG generators.py gen_for_qa l.557] (30/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,574 DEBUG generators.py generate l.355] (30/47) Reuse existing Prompt
[2024-03-05 15:17:53,576 DEBUG generators.py generate l.368] (30/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,579 DEBUG generators.py generate l.373] (30/47) Post-process Answer
[2024-03-05 15:17:53,580 INFO generators.py gen_for_qa l.551] (30/47) * Start with LLM "command-nightly"
[2024-03-05 15:17:53,582 DEBUG generators.py gen_for_qa l.557] (30/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,584 DEBUG generators.py generate l.355] (30/47) Reuse existing Prompt
[2024-03-05 15:17:53,585 DEBUG generators.py generate l.368] (30/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,585 DEBUG generators.py generate l.373] (30/47) Post-process Answer
[2024-03-05 15:17:53,589 INFO generators.py generate l.480] (30/47) End question "Comment réformer le système de régulation des marchés des télécommunications ?  A) Réglementation stricte B) Déréglementation C) Neutralité du net"
[2024-03-05 15:17:53,590 INFO generators.py generate l.478] (31/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés des transports ?  A) Réglementation stricte B) Déréglementation C) Concurrence régulée"
[2024-03-05 15:17:53,592 INFO generators.py gen_for_qa l.551] (31/47) * Start with LLM "gpt-4"
[2024-03-05 15:17:53,595 DEBUG generators.py gen_for_qa l.557] (31/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,596 DEBUG generators.py generate l.355] (31/47) Reuse existing Prompt
[2024-03-05 15:17:53,599 DEBUG generators.py generate l.368] (31/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,600 DEBUG generators.py generate l.373] (31/47) Post-process Answer
[2024-03-05 15:17:53,602 INFO generators.py gen_for_qa l.551] (31/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:17:53,604 DEBUG generators.py gen_for_qa l.557] (31/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,605 DEBUG generators.py generate l.355] (31/47) Reuse existing Prompt
[2024-03-05 15:17:53,607 DEBUG generators.py generate l.368] (31/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,609 DEBUG generators.py generate l.373] (31/47) Post-process Answer
[2024-03-05 15:17:53,610 INFO generators.py gen_for_qa l.551] (31/47) * Start with LLM "gemini-pro"
[2024-03-05 15:17:53,612 DEBUG generators.py gen_for_qa l.557] (31/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,615 DEBUG generators.py generate l.355] (31/47) Reuse existing Prompt
[2024-03-05 15:17:53,616 DEBUG generators.py generate l.368] (31/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,616 DEBUG generators.py generate l.373] (31/47) Post-process Answer
[2024-03-05 15:17:53,620 INFO generators.py gen_for_qa l.551] (31/47) * Start with LLM "claude-2.1"
[2024-03-05 15:17:53,621 DEBUG generators.py gen_for_qa l.557] (31/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,621 DEBUG generators.py generate l.355] (31/47) Reuse existing Prompt
[2024-03-05 15:17:53,625 DEBUG generators.py generate l.368] (31/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,625 DEBUG generators.py generate l.373] (31/47) Post-process Answer
[2024-03-05 15:17:53,629 INFO generators.py gen_for_qa l.551] (31/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:17:53,629 DEBUG generators.py gen_for_qa l.557] (31/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,632 DEBUG generators.py generate l.355] (31/47) Reuse existing Prompt
[2024-03-05 15:17:53,634 DEBUG generators.py generate l.368] (31/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,634 DEBUG generators.py generate l.373] (31/47) Post-process Answer
[2024-03-05 15:17:53,637 INFO generators.py gen_for_qa l.551] (31/47) * Start with LLM "command-nightly"
[2024-03-05 15:17:53,638 DEBUG generators.py gen_for_qa l.557] (31/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,640 DEBUG generators.py generate l.355] (31/47) Reuse existing Prompt
[2024-03-05 15:17:53,640 DEBUG generators.py generate l.368] (31/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,642 DEBUG generators.py generate l.373] (31/47) Post-process Answer
[2024-03-05 15:17:53,646 INFO generators.py generate l.480] (31/47) End question "Comment réformer le système de régulation des marchés des transports ?  A) Réglementation stricte B) Déréglementation C) Concurrence régulée"
[2024-03-05 15:17:53,648 INFO generators.py generate l.478] (32/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés du logement ?  A) Réglementation stricte B) Déréglementation C) Encadrement des loyers"
[2024-03-05 15:17:53,651 INFO generators.py gen_for_qa l.551] (32/47) * Start with LLM "gpt-4"
[2024-03-05 15:17:53,651 DEBUG generators.py gen_for_qa l.557] (32/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,653 DEBUG generators.py generate l.355] (32/47) Reuse existing Prompt
[2024-03-05 15:17:53,656 DEBUG generators.py generate l.368] (32/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,658 DEBUG generators.py generate l.373] (32/47) Post-process Answer
[2024-03-05 15:17:53,661 INFO generators.py gen_for_qa l.551] (32/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:17:53,662 DEBUG generators.py gen_for_qa l.557] (32/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,665 DEBUG generators.py generate l.355] (32/47) Reuse existing Prompt
[2024-03-05 15:17:53,666 DEBUG generators.py generate l.368] (32/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,666 DEBUG generators.py generate l.373] (32/47) Post-process Answer
[2024-03-05 15:17:53,671 INFO generators.py gen_for_qa l.551] (32/47) * Start with LLM "gemini-pro"
[2024-03-05 15:17:53,672 DEBUG generators.py gen_for_qa l.557] (32/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,675 DEBUG generators.py generate l.355] (32/47) Reuse existing Prompt
[2024-03-05 15:17:53,675 DEBUG generators.py generate l.368] (32/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,679 DEBUG generators.py generate l.373] (32/47) Post-process Answer
[2024-03-05 15:17:53,682 INFO generators.py gen_for_qa l.551] (32/47) * Start with LLM "claude-2.1"
[2024-03-05 15:17:53,683 DEBUG generators.py gen_for_qa l.557] (32/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,686 DEBUG generators.py generate l.355] (32/47) Reuse existing Prompt
[2024-03-05 15:17:53,688 DEBUG generators.py generate l.368] (32/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,690 DEBUG generators.py generate l.373] (32/47) Post-process Answer
[2024-03-05 15:17:53,692 INFO generators.py gen_for_qa l.551] (32/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:17:53,692 DEBUG generators.py gen_for_qa l.557] (32/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,699 DEBUG generators.py generate l.355] (32/47) Reuse existing Prompt
[2024-03-05 15:17:53,700 DEBUG generators.py generate l.368] (32/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,701 DEBUG generators.py generate l.373] (32/47) Post-process Answer
[2024-03-05 15:17:53,701 INFO generators.py gen_for_qa l.551] (32/47) * Start with LLM "command-nightly"
[2024-03-05 15:17:53,705 DEBUG generators.py gen_for_qa l.557] (32/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,705 DEBUG generators.py generate l.355] (32/47) Reuse existing Prompt
[2024-03-05 15:17:53,705 DEBUG generators.py generate l.368] (32/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,710 DEBUG generators.py generate l.373] (32/47) Post-process Answer
[2024-03-05 15:17:53,713 INFO generators.py generate l.480] (32/47) End question "Comment réformer le système de régulation des marchés du logement ?  A) Réglementation stricte B) Déréglementation C) Encadrement des loyers"
[2024-03-05 15:17:53,713 INFO generators.py generate l.478] (33/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés de l'eau ?  A) Réglementation stricte B) Déréglementation C) Tarification sociale"
[2024-03-05 15:17:53,716 INFO generators.py gen_for_qa l.551] (33/47) * Start with LLM "gpt-4"
[2024-03-05 15:17:53,718 DEBUG generators.py gen_for_qa l.557] (33/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,718 DEBUG generators.py generate l.355] (33/47) Reuse existing Prompt
[2024-03-05 15:17:53,721 DEBUG generators.py generate l.368] (33/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,722 DEBUG generators.py generate l.373] (33/47) Post-process Answer
[2024-03-05 15:17:53,722 INFO generators.py gen_for_qa l.551] (33/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:17:53,725 DEBUG generators.py gen_for_qa l.557] (33/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,729 DEBUG generators.py generate l.355] (33/47) Reuse existing Prompt
[2024-03-05 15:17:53,729 DEBUG generators.py generate l.368] (33/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,729 DEBUG generators.py generate l.373] (33/47) Post-process Answer
[2024-03-05 15:17:53,734 INFO generators.py gen_for_qa l.551] (33/47) * Start with LLM "gemini-pro"
[2024-03-05 15:17:53,736 DEBUG generators.py gen_for_qa l.557] (33/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,737 DEBUG generators.py generate l.355] (33/47) Reuse existing Prompt
[2024-03-05 15:17:53,738 DEBUG generators.py generate l.368] (33/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,739 DEBUG generators.py generate l.373] (33/47) Post-process Answer
[2024-03-05 15:17:53,741 INFO generators.py gen_for_qa l.551] (33/47) * Start with LLM "claude-2.1"
[2024-03-05 15:17:53,742 DEBUG generators.py gen_for_qa l.557] (33/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,746 DEBUG generators.py generate l.355] (33/47) Reuse existing Prompt
[2024-03-05 15:17:53,747 DEBUG generators.py generate l.368] (33/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,750 DEBUG generators.py generate l.373] (33/47) Post-process Answer
[2024-03-05 15:17:53,751 INFO generators.py gen_for_qa l.551] (33/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:17:53,753 DEBUG generators.py gen_for_qa l.557] (33/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,754 DEBUG generators.py generate l.355] (33/47) Reuse existing Prompt
[2024-03-05 15:17:53,756 DEBUG generators.py generate l.368] (33/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,756 DEBUG generators.py generate l.373] (33/47) Post-process Answer
[2024-03-05 15:17:53,760 INFO generators.py gen_for_qa l.551] (33/47) * Start with LLM "command-nightly"
[2024-03-05 15:17:53,762 DEBUG generators.py gen_for_qa l.557] (33/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,765 DEBUG generators.py generate l.355] (33/47) Reuse existing Prompt
[2024-03-05 15:17:53,766 DEBUG generators.py generate l.368] (33/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,768 DEBUG generators.py generate l.373] (33/47) Post-process Answer
[2024-03-05 15:17:53,769 INFO generators.py generate l.480] (33/47) End question "Comment réformer le système de régulation des marchés de l'eau ?  A) Réglementation stricte B) Déréglementation C) Tarification sociale"
[2024-03-05 15:17:53,769 INFO generators.py generate l.478] (34/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés de l'alimentation ?  A) Réglementation stricte B) Déréglementation C) Agriculture biologique"
[2024-03-05 15:17:53,773 INFO generators.py gen_for_qa l.551] (34/47) * Start with LLM "gpt-4"
[2024-03-05 15:17:53,774 DEBUG generators.py gen_for_qa l.557] (34/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,775 DEBUG generators.py generate l.355] (34/47) Reuse existing Prompt
[2024-03-05 15:17:53,779 DEBUG generators.py generate l.368] (34/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,780 DEBUG generators.py generate l.373] (34/47) Post-process Answer
[2024-03-05 15:17:53,782 INFO generators.py gen_for_qa l.551] (34/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:17:53,784 DEBUG generators.py gen_for_qa l.557] (34/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,786 DEBUG generators.py generate l.355] (34/47) Reuse existing Prompt
[2024-03-05 15:17:53,786 DEBUG generators.py generate l.368] (34/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,789 DEBUG generators.py generate l.373] (34/47) Post-process Answer
[2024-03-05 15:17:53,790 INFO generators.py gen_for_qa l.551] (34/47) * Start with LLM "gemini-pro"
[2024-03-05 15:17:53,792 DEBUG generators.py gen_for_qa l.557] (34/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,794 DEBUG generators.py generate l.355] (34/47) Reuse existing Prompt
[2024-03-05 15:17:53,796 DEBUG generators.py generate l.368] (34/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,799 DEBUG generators.py generate l.373] (34/47) Post-process Answer
[2024-03-05 15:17:53,799 INFO generators.py gen_for_qa l.551] (34/47) * Start with LLM "claude-2.1"
[2024-03-05 15:17:53,802 DEBUG generators.py gen_for_qa l.557] (34/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,804 DEBUG generators.py generate l.355] (34/47) Reuse existing Prompt
[2024-03-05 15:17:53,805 DEBUG generators.py generate l.368] (34/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,807 DEBUG generators.py generate l.373] (34/47) Post-process Answer
[2024-03-05 15:17:53,808 INFO generators.py gen_for_qa l.551] (34/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:17:53,811 DEBUG generators.py gen_for_qa l.557] (34/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,811 DEBUG generators.py generate l.355] (34/47) Reuse existing Prompt
[2024-03-05 15:17:53,815 DEBUG generators.py generate l.368] (34/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,816 DEBUG generators.py generate l.373] (34/47) Post-process Answer
[2024-03-05 15:17:53,817 INFO generators.py gen_for_qa l.551] (34/47) * Start with LLM "command-nightly"
[2024-03-05 15:17:53,820 DEBUG generators.py gen_for_qa l.557] (34/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,822 DEBUG generators.py generate l.355] (34/47) Reuse existing Prompt
[2024-03-05 15:17:53,823 DEBUG generators.py generate l.368] (34/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,824 DEBUG generators.py generate l.373] (34/47) Post-process Answer
[2024-03-05 15:17:53,828 INFO generators.py generate l.480] (34/47) End question "Comment réformer le système de régulation des marchés de l'alimentation ?  A) Réglementation stricte B) Déréglementation C) Agriculture biologique"
[2024-03-05 15:17:53,830 INFO generators.py generate l.478] (35/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés de la santé ?  A) Réglementation stricte B) Déréglementation C) Médecine préventive"
[2024-03-05 15:17:53,832 INFO generators.py gen_for_qa l.551] (35/47) * Start with LLM "gpt-4"
[2024-03-05 15:17:53,833 DEBUG generators.py gen_for_qa l.557] (35/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,833 DEBUG generators.py generate l.355] (35/47) Reuse existing Prompt
[2024-03-05 15:17:53,836 DEBUG generators.py generate l.368] (35/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,836 DEBUG generators.py generate l.373] (35/47) Post-process Answer
[2024-03-05 15:17:53,836 INFO generators.py gen_for_qa l.551] (35/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:17:53,841 DEBUG generators.py gen_for_qa l.557] (35/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,843 DEBUG generators.py generate l.355] (35/47) Reuse existing Prompt
[2024-03-05 15:17:53,846 DEBUG generators.py generate l.368] (35/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,847 DEBUG generators.py generate l.373] (35/47) Post-process Answer
[2024-03-05 15:17:53,849 INFO generators.py gen_for_qa l.551] (35/47) * Start with LLM "gemini-pro"
[2024-03-05 15:17:53,850 DEBUG generators.py gen_for_qa l.557] (35/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,852 DEBUG generators.py generate l.355] (35/47) Reuse existing Prompt
[2024-03-05 15:17:53,854 DEBUG generators.py generate l.368] (35/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,854 DEBUG generators.py generate l.373] (35/47) Post-process Answer
[2024-03-05 15:17:53,857 INFO generators.py gen_for_qa l.551] (35/47) * Start with LLM "claude-2.1"
[2024-03-05 15:17:53,859 DEBUG generators.py gen_for_qa l.557] (35/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,861 DEBUG generators.py generate l.355] (35/47) Reuse existing Prompt
[2024-03-05 15:17:53,864 DEBUG generators.py generate l.368] (35/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,865 DEBUG generators.py generate l.373] (35/47) Post-process Answer
[2024-03-05 15:17:53,866 INFO generators.py gen_for_qa l.551] (35/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:17:53,866 DEBUG generators.py gen_for_qa l.557] (35/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,869 DEBUG generators.py generate l.355] (35/47) Reuse existing Prompt
[2024-03-05 15:17:53,869 DEBUG generators.py generate l.368] (35/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,873 DEBUG generators.py generate l.373] (35/47) Post-process Answer
[2024-03-05 15:17:53,874 INFO generators.py gen_for_qa l.551] (35/47) * Start with LLM "command-nightly"
[2024-03-05 15:17:53,875 DEBUG generators.py gen_for_qa l.557] (35/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,878 DEBUG generators.py generate l.355] (35/47) Reuse existing Prompt
[2024-03-05 15:17:53,880 DEBUG generators.py generate l.368] (35/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,881 DEBUG generators.py generate l.373] (35/47) Post-process Answer
[2024-03-05 15:17:53,883 INFO generators.py generate l.480] (35/47) End question "Comment réformer le système de régulation des marchés de la santé ?  A) Réglementation stricte B) Déréglementation C) Médecine préventive"
[2024-03-05 15:17:53,885 INFO generators.py generate l.478] (36/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés de l'éducation ?  A) Réglementation stricte B) Déréglementation C) Éducation inclusive"
[2024-03-05 15:17:53,890 INFO generators.py gen_for_qa l.551] (36/47) * Start with LLM "gpt-4"
[2024-03-05 15:17:53,892 DEBUG generators.py gen_for_qa l.557] (36/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,894 DEBUG generators.py generate l.355] (36/47) Reuse existing Prompt
[2024-03-05 15:17:53,894 DEBUG generators.py generate l.368] (36/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,898 DEBUG generators.py generate l.373] (36/47) Post-process Answer
[2024-03-05 15:17:53,899 INFO generators.py gen_for_qa l.551] (36/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:17:53,901 DEBUG generators.py gen_for_qa l.557] (36/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,903 DEBUG generators.py generate l.355] (36/47) Reuse existing Prompt
[2024-03-05 15:17:53,903 DEBUG generators.py generate l.368] (36/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,903 DEBUG generators.py generate l.373] (36/47) Post-process Answer
[2024-03-05 15:17:53,903 INFO generators.py gen_for_qa l.551] (36/47) * Start with LLM "gemini-pro"
[2024-03-05 15:17:53,909 DEBUG generators.py gen_for_qa l.557] (36/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,912 DEBUG generators.py generate l.355] (36/47) Reuse existing Prompt
[2024-03-05 15:17:53,914 DEBUG generators.py generate l.368] (36/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,915 DEBUG generators.py generate l.373] (36/47) Post-process Answer
[2024-03-05 15:17:53,917 INFO generators.py gen_for_qa l.551] (36/47) * Start with LLM "claude-2.1"
[2024-03-05 15:17:53,919 DEBUG generators.py gen_for_qa l.557] (36/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,920 DEBUG generators.py generate l.355] (36/47) Reuse existing Prompt
[2024-03-05 15:17:53,922 DEBUG generators.py generate l.368] (36/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,922 DEBUG generators.py generate l.373] (36/47) Post-process Answer
[2024-03-05 15:17:53,925 INFO generators.py gen_for_qa l.551] (36/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:17:53,928 DEBUG generators.py gen_for_qa l.557] (36/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,930 DEBUG generators.py generate l.355] (36/47) Reuse existing Prompt
[2024-03-05 15:17:53,933 DEBUG generators.py generate l.368] (36/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,933 DEBUG generators.py generate l.373] (36/47) Post-process Answer
[2024-03-05 15:17:53,935 INFO generators.py gen_for_qa l.551] (36/47) * Start with LLM "command-nightly"
[2024-03-05 15:17:53,935 DEBUG generators.py gen_for_qa l.557] (36/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,938 DEBUG generators.py generate l.355] (36/47) Reuse existing Prompt
[2024-03-05 15:17:53,940 DEBUG generators.py generate l.368] (36/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,942 DEBUG generators.py generate l.373] (36/47) Post-process Answer
[2024-03-05 15:17:53,944 INFO generators.py generate l.480] (36/47) End question "Comment réformer le système de régulation des marchés de l'éducation ?  A) Réglementation stricte B) Déréglementation C) Éducation inclusive"
[2024-03-05 15:17:53,945 INFO generators.py generate l.478] (37/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés de la culture ?  A) Réglementation stricte B) Déréglementation C) Diversité culturelle"
[2024-03-05 15:17:53,945 INFO generators.py gen_for_qa l.551] (37/47) * Start with LLM "gpt-4"
[2024-03-05 15:17:53,945 DEBUG generators.py gen_for_qa l.557] (37/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,952 DEBUG generators.py generate l.355] (37/47) Reuse existing Prompt
[2024-03-05 15:17:53,952 DEBUG generators.py generate l.368] (37/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,955 DEBUG generators.py generate l.373] (37/47) Post-process Answer
[2024-03-05 15:17:53,955 INFO generators.py gen_for_qa l.551] (37/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:17:53,958 DEBUG generators.py gen_for_qa l.557] (37/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,959 DEBUG generators.py generate l.355] (37/47) Reuse existing Prompt
[2024-03-05 15:17:53,959 DEBUG generators.py generate l.368] (37/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,964 DEBUG generators.py generate l.373] (37/47) Post-process Answer
[2024-03-05 15:17:53,966 INFO generators.py gen_for_qa l.551] (37/47) * Start with LLM "gemini-pro"
[2024-03-05 15:17:53,966 DEBUG generators.py gen_for_qa l.557] (37/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,969 DEBUG generators.py generate l.355] (37/47) Reuse existing Prompt
[2024-03-05 15:17:53,969 DEBUG generators.py generate l.368] (37/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,969 DEBUG generators.py generate l.373] (37/47) Post-process Answer
[2024-03-05 15:17:53,972 INFO generators.py gen_for_qa l.551] (37/47) * Start with LLM "claude-2.1"
[2024-03-05 15:17:53,974 DEBUG generators.py gen_for_qa l.557] (37/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,978 DEBUG generators.py generate l.355] (37/47) Reuse existing Prompt
[2024-03-05 15:17:53,980 DEBUG generators.py generate l.368] (37/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,981 DEBUG generators.py generate l.373] (37/47) Post-process Answer
[2024-03-05 15:17:53,981 INFO generators.py gen_for_qa l.551] (37/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:17:53,986 DEBUG generators.py gen_for_qa l.557] (37/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,988 DEBUG generators.py generate l.355] (37/47) Reuse existing Prompt
[2024-03-05 15:17:53,988 DEBUG generators.py generate l.368] (37/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,990 DEBUG generators.py generate l.373] (37/47) Post-process Answer
[2024-03-05 15:17:53,992 INFO generators.py gen_for_qa l.551] (37/47) * Start with LLM "command-nightly"
[2024-03-05 15:17:53,994 DEBUG generators.py gen_for_qa l.557] (37/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:53,997 DEBUG generators.py generate l.355] (37/47) Reuse existing Prompt
[2024-03-05 15:17:53,998 DEBUG generators.py generate l.368] (37/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:53,998 DEBUG generators.py generate l.373] (37/47) Post-process Answer
[2024-03-05 15:17:54,001 INFO generators.py generate l.480] (37/47) End question "Comment réformer le système de régulation des marchés de la culture ?  A) Réglementation stricte B) Déréglementation C) Diversité culturelle"
[2024-03-05 15:17:54,003 INFO generators.py generate l.478] (38/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés du sport ?  A) Réglementation stricte B) Déréglementation C) Sport pour tous"
[2024-03-05 15:17:54,003 INFO generators.py gen_for_qa l.551] (38/47) * Start with LLM "gpt-4"
[2024-03-05 15:17:54,003 DEBUG generators.py gen_for_qa l.557] (38/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,009 DEBUG generators.py generate l.355] (38/47) Reuse existing Prompt
[2024-03-05 15:17:54,010 DEBUG generators.py generate l.368] (38/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,013 DEBUG generators.py generate l.373] (38/47) Post-process Answer
[2024-03-05 15:17:54,013 INFO generators.py gen_for_qa l.551] (38/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:17:54,018 DEBUG generators.py gen_for_qa l.557] (38/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,019 DEBUG generators.py generate l.355] (38/47) Reuse existing Prompt
[2024-03-05 15:17:54,021 DEBUG generators.py generate l.368] (38/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,021 DEBUG generators.py generate l.373] (38/47) Post-process Answer
[2024-03-05 15:17:54,024 INFO generators.py gen_for_qa l.551] (38/47) * Start with LLM "gemini-pro"
[2024-03-05 15:17:54,025 DEBUG generators.py gen_for_qa l.557] (38/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,030 DEBUG generators.py generate l.355] (38/47) Reuse existing Prompt
[2024-03-05 15:17:54,031 DEBUG generators.py generate l.368] (38/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,031 DEBUG generators.py generate l.373] (38/47) Post-process Answer
[2024-03-05 15:17:54,034 INFO generators.py gen_for_qa l.551] (38/47) * Start with LLM "claude-2.1"
[2024-03-05 15:17:54,037 DEBUG generators.py gen_for_qa l.557] (38/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,037 DEBUG generators.py generate l.355] (38/47) Reuse existing Prompt
[2024-03-05 15:17:54,037 DEBUG generators.py generate l.368] (38/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,041 DEBUG generators.py generate l.373] (38/47) Post-process Answer
[2024-03-05 15:17:54,043 INFO generators.py gen_for_qa l.551] (38/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:17:54,043 DEBUG generators.py gen_for_qa l.557] (38/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,048 DEBUG generators.py generate l.355] (38/47) Reuse existing Prompt
[2024-03-05 15:17:54,050 DEBUG generators.py generate l.368] (38/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,050 DEBUG generators.py generate l.373] (38/47) Post-process Answer
[2024-03-05 15:17:54,054 INFO generators.py gen_for_qa l.551] (38/47) * Start with LLM "command-nightly"
[2024-03-05 15:17:54,054 DEBUG generators.py gen_for_qa l.557] (38/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,054 DEBUG generators.py generate l.355] (38/47) Reuse existing Prompt
[2024-03-05 15:17:54,059 DEBUG generators.py generate l.368] (38/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,061 DEBUG generators.py generate l.373] (38/47) Post-process Answer
[2024-03-05 15:17:54,063 INFO generators.py generate l.480] (38/47) End question "Comment réformer le système de régulation des marchés du sport ?  A) Réglementation stricte B) Déréglementation C) Sport pour tous"
[2024-03-05 15:17:54,063 INFO generators.py generate l.478] (39/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés de l'environnement ?  A) Réglementation stricte B) Déréglementation C) Économie verte"
[2024-03-05 15:17:54,067 INFO generators.py gen_for_qa l.551] (39/47) * Start with LLM "gpt-4"
[2024-03-05 15:17:54,068 DEBUG generators.py gen_for_qa l.557] (39/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,068 DEBUG generators.py generate l.355] (39/47) Reuse existing Prompt
[2024-03-05 15:17:54,072 DEBUG generators.py generate l.368] (39/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,074 DEBUG generators.py generate l.373] (39/47) Post-process Answer
[2024-03-05 15:17:54,075 INFO generators.py gen_for_qa l.551] (39/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:17:54,078 DEBUG generators.py gen_for_qa l.557] (39/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,080 DEBUG generators.py generate l.355] (39/47) Reuse existing Prompt
[2024-03-05 15:17:54,082 DEBUG generators.py generate l.368] (39/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,085 DEBUG generators.py generate l.373] (39/47) Post-process Answer
[2024-03-05 15:17:54,087 INFO generators.py gen_for_qa l.551] (39/47) * Start with LLM "gemini-pro"
[2024-03-05 15:17:54,089 DEBUG generators.py gen_for_qa l.557] (39/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,092 DEBUG generators.py generate l.355] (39/47) Reuse existing Prompt
[2024-03-05 15:17:54,095 DEBUG generators.py generate l.368] (39/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,099 DEBUG generators.py generate l.373] (39/47) Post-process Answer
[2024-03-05 15:17:54,101 INFO generators.py gen_for_qa l.551] (39/47) * Start with LLM "claude-2.1"
[2024-03-05 15:17:54,102 DEBUG generators.py gen_for_qa l.557] (39/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,105 DEBUG generators.py generate l.355] (39/47) Reuse existing Prompt
[2024-03-05 15:17:54,107 DEBUG generators.py generate l.368] (39/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,109 DEBUG generators.py generate l.373] (39/47) Post-process Answer
[2024-03-05 15:17:54,113 INFO generators.py gen_for_qa l.551] (39/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:17:54,114 DEBUG generators.py gen_for_qa l.557] (39/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,116 DEBUG generators.py generate l.355] (39/47) Reuse existing Prompt
[2024-03-05 15:17:54,118 DEBUG generators.py generate l.368] (39/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,118 DEBUG generators.py generate l.373] (39/47) Post-process Answer
[2024-03-05 15:17:54,122 INFO generators.py gen_for_qa l.551] (39/47) * Start with LLM "command-nightly"
[2024-03-05 15:17:54,124 DEBUG generators.py gen_for_qa l.557] (39/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,126 DEBUG generators.py generate l.355] (39/47) Reuse existing Prompt
[2024-03-05 15:17:54,128 DEBUG generators.py generate l.368] (39/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,130 DEBUG generators.py generate l.373] (39/47) Post-process Answer
[2024-03-05 15:17:54,132 INFO generators.py generate l.480] (39/47) End question "Comment réformer le système de régulation des marchés de l'environnement ?  A) Réglementation stricte B) Déréglementation C) Économie verte"
[2024-03-05 15:17:54,135 INFO generators.py generate l.478] (40/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés du numérique ?  A) Réglementation stricte B) Déréglementation C) Souveraineté numérique"
[2024-03-05 15:17:54,136 INFO generators.py gen_for_qa l.551] (40/47) * Start with LLM "gpt-4"
[2024-03-05 15:17:54,138 DEBUG generators.py gen_for_qa l.557] (40/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,140 DEBUG generators.py generate l.355] (40/47) Reuse existing Prompt
[2024-03-05 15:17:54,142 DEBUG generators.py generate l.368] (40/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,142 DEBUG generators.py generate l.373] (40/47) Post-process Answer
[2024-03-05 15:17:54,147 INFO generators.py gen_for_qa l.551] (40/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:17:54,148 DEBUG generators.py gen_for_qa l.557] (40/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,148 DEBUG generators.py generate l.355] (40/47) Reuse existing Prompt
[2024-03-05 15:17:54,152 DEBUG generators.py generate l.368] (40/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,154 DEBUG generators.py generate l.373] (40/47) Post-process Answer
[2024-03-05 15:17:54,155 INFO generators.py gen_for_qa l.551] (40/47) * Start with LLM "gemini-pro"
[2024-03-05 15:17:54,157 DEBUG generators.py gen_for_qa l.557] (40/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,159 DEBUG generators.py generate l.355] (40/47) Reuse existing Prompt
[2024-03-05 15:17:54,161 DEBUG generators.py generate l.368] (40/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,164 DEBUG generators.py generate l.373] (40/47) Post-process Answer
[2024-03-05 15:17:54,166 INFO generators.py gen_for_qa l.551] (40/47) * Start with LLM "claude-2.1"
[2024-03-05 15:17:54,166 DEBUG generators.py gen_for_qa l.557] (40/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,169 DEBUG generators.py generate l.355] (40/47) Reuse existing Prompt
[2024-03-05 15:17:54,170 DEBUG generators.py generate l.368] (40/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,170 DEBUG generators.py generate l.373] (40/47) Post-process Answer
[2024-03-05 15:17:54,174 INFO generators.py gen_for_qa l.551] (40/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:17:54,175 DEBUG generators.py gen_for_qa l.557] (40/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,178 DEBUG generators.py generate l.355] (40/47) Reuse existing Prompt
[2024-03-05 15:17:54,181 DEBUG generators.py generate l.368] (40/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,181 DEBUG generators.py generate l.373] (40/47) Post-process Answer
[2024-03-05 15:17:54,181 INFO generators.py gen_for_qa l.551] (40/47) * Start with LLM "command-nightly"
[2024-03-05 15:17:54,186 DEBUG generators.py gen_for_qa l.557] (40/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,188 DEBUG generators.py generate l.355] (40/47) Reuse existing Prompt
[2024-03-05 15:17:54,188 DEBUG generators.py generate l.368] (40/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,191 DEBUG generators.py generate l.373] (40/47) Post-process Answer
[2024-03-05 15:17:54,194 INFO generators.py generate l.480] (40/47) End question "Comment réformer le système de régulation des marchés du numérique ?  A) Réglementation stricte B) Déréglementation C) Souveraineté numérique"
[2024-03-05 15:17:54,196 INFO generators.py generate l.478] (41/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés de la finance solidaire ?  A) Réglementation stricte B) Déréglementation C) Finance éthique"
[2024-03-05 15:17:54,198 INFO generators.py gen_for_qa l.551] (41/47) * Start with LLM "gpt-4"
[2024-03-05 15:17:54,201 DEBUG generators.py gen_for_qa l.557] (41/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,203 DEBUG generators.py generate l.355] (41/47) Reuse existing Prompt
[2024-03-05 15:17:54,204 DEBUG generators.py generate l.368] (41/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,206 DEBUG generators.py generate l.373] (41/47) Post-process Answer
[2024-03-05 15:17:54,208 INFO generators.py gen_for_qa l.551] (41/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:17:54,210 DEBUG generators.py gen_for_qa l.557] (41/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,213 DEBUG generators.py generate l.355] (41/47) Reuse existing Prompt
[2024-03-05 15:17:54,215 DEBUG generators.py generate l.368] (41/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,215 DEBUG generators.py generate l.373] (41/47) Post-process Answer
[2024-03-05 15:17:54,218 INFO generators.py gen_for_qa l.551] (41/47) * Start with LLM "gemini-pro"
[2024-03-05 15:17:54,220 DEBUG generators.py gen_for_qa l.557] (41/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,221 DEBUG generators.py generate l.355] (41/47) Reuse existing Prompt
[2024-03-05 15:17:54,223 DEBUG generators.py generate l.368] (41/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,224 DEBUG generators.py generate l.373] (41/47) Post-process Answer
[2024-03-05 15:17:54,227 INFO generators.py gen_for_qa l.551] (41/47) * Start with LLM "claude-2.1"
[2024-03-05 15:17:54,229 DEBUG generators.py gen_for_qa l.557] (41/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,229 DEBUG generators.py generate l.355] (41/47) Reuse existing Prompt
[2024-03-05 15:17:54,229 DEBUG generators.py generate l.368] (41/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,235 DEBUG generators.py generate l.373] (41/47) Post-process Answer
[2024-03-05 15:17:54,235 INFO generators.py gen_for_qa l.551] (41/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:17:54,235 DEBUG generators.py gen_for_qa l.557] (41/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,239 DEBUG generators.py generate l.355] (41/47) Reuse existing Prompt
[2024-03-05 15:17:54,241 DEBUG generators.py generate l.368] (41/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,242 DEBUG generators.py generate l.373] (41/47) Post-process Answer
[2024-03-05 15:17:54,245 INFO generators.py gen_for_qa l.551] (41/47) * Start with LLM "command-nightly"
[2024-03-05 15:17:54,247 DEBUG generators.py gen_for_qa l.557] (41/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,249 DEBUG generators.py generate l.355] (41/47) Reuse existing Prompt
[2024-03-05 15:17:54,250 DEBUG generators.py generate l.368] (41/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,252 DEBUG generators.py generate l.373] (41/47) Post-process Answer
[2024-03-05 15:17:54,252 INFO generators.py generate l.480] (41/47) End question "Comment réformer le système de régulation des marchés de la finance solidaire ?  A) Réglementation stricte B) Déréglementation C) Finance éthique"
[2024-03-05 15:17:54,252 INFO generators.py generate l.478] (42/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés de l'économie sociale et solidaire ?  A) Réglementation stricte B) Déréglementation C) Économie collaborative"
[2024-03-05 15:17:54,256 INFO generators.py gen_for_qa l.551] (42/47) * Start with LLM "gpt-4"
[2024-03-05 15:17:54,258 DEBUG generators.py gen_for_qa l.557] (42/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,260 DEBUG generators.py generate l.355] (42/47) Reuse existing Prompt
[2024-03-05 15:17:54,263 DEBUG generators.py generate l.368] (42/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,264 DEBUG generators.py generate l.373] (42/47) Post-process Answer
[2024-03-05 15:17:54,264 INFO generators.py gen_for_qa l.551] (42/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:17:54,268 DEBUG generators.py gen_for_qa l.557] (42/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,269 DEBUG generators.py generate l.355] (42/47) Reuse existing Prompt
[2024-03-05 15:17:54,270 DEBUG generators.py generate l.368] (42/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,271 DEBUG generators.py generate l.373] (42/47) Post-process Answer
[2024-03-05 15:17:54,274 INFO generators.py gen_for_qa l.551] (42/47) * Start with LLM "gemini-pro"
[2024-03-05 15:17:54,275 DEBUG generators.py gen_for_qa l.557] (42/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,278 DEBUG generators.py generate l.355] (42/47) Reuse existing Prompt
[2024-03-05 15:17:54,281 DEBUG generators.py generate l.368] (42/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,281 DEBUG generators.py generate l.373] (42/47) Post-process Answer
[2024-03-05 15:17:54,283 INFO generators.py gen_for_qa l.551] (42/47) * Start with LLM "claude-2.1"
[2024-03-05 15:17:54,285 DEBUG generators.py gen_for_qa l.557] (42/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,287 DEBUG generators.py generate l.355] (42/47) Reuse existing Prompt
[2024-03-05 15:17:54,287 DEBUG generators.py generate l.368] (42/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,290 DEBUG generators.py generate l.373] (42/47) Post-process Answer
[2024-03-05 15:17:54,291 INFO generators.py gen_for_qa l.551] (42/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:17:54,294 DEBUG generators.py gen_for_qa l.557] (42/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,296 DEBUG generators.py generate l.355] (42/47) Reuse existing Prompt
[2024-03-05 15:17:54,298 DEBUG generators.py generate l.368] (42/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,299 DEBUG generators.py generate l.373] (42/47) Post-process Answer
[2024-03-05 15:17:54,301 INFO generators.py gen_for_qa l.551] (42/47) * Start with LLM "command-nightly"
[2024-03-05 15:17:54,302 DEBUG generators.py gen_for_qa l.557] (42/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,304 DEBUG generators.py generate l.355] (42/47) Reuse existing Prompt
[2024-03-05 15:17:54,304 DEBUG generators.py generate l.368] (42/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,307 DEBUG generators.py generate l.373] (42/47) Post-process Answer
[2024-03-05 15:17:54,309 INFO generators.py generate l.480] (42/47) End question "Comment réformer le système de régulation des marchés de l'économie sociale et solidaire ?  A) Réglementation stricte B) Déréglementation C) Économie collaborative"
[2024-03-05 15:17:54,310 INFO generators.py generate l.478] (43/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés de la propriété intellectuelle ?  A) Réglementation stricte B) Déréglementation C) Licences libres"
[2024-03-05 15:17:54,314 INFO generators.py gen_for_qa l.551] (43/47) * Start with LLM "gpt-4"
[2024-03-05 15:17:54,315 DEBUG generators.py gen_for_qa l.557] (43/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,315 DEBUG generators.py generate l.355] (43/47) Reuse existing Prompt
[2024-03-05 15:17:54,318 DEBUG generators.py generate l.368] (43/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,318 DEBUG generators.py generate l.373] (43/47) Post-process Answer
[2024-03-05 15:17:54,321 INFO generators.py gen_for_qa l.551] (43/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:17:54,323 DEBUG generators.py gen_for_qa l.557] (43/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,324 DEBUG generators.py generate l.355] (43/47) Reuse existing Prompt
[2024-03-05 15:17:54,326 DEBUG generators.py generate l.368] (43/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,328 DEBUG generators.py generate l.373] (43/47) Post-process Answer
[2024-03-05 15:17:54,330 INFO generators.py gen_for_qa l.551] (43/47) * Start with LLM "gemini-pro"
[2024-03-05 15:17:54,330 DEBUG generators.py gen_for_qa l.557] (43/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,334 DEBUG generators.py generate l.355] (43/47) Reuse existing Prompt
[2024-03-05 15:17:54,342 DEBUG generators.py generate l.368] (43/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,350 DEBUG generators.py generate l.373] (43/47) Post-process Answer
[2024-03-05 15:17:54,353 INFO generators.py gen_for_qa l.551] (43/47) * Start with LLM "claude-2.1"
[2024-03-05 15:17:54,358 DEBUG generators.py gen_for_qa l.557] (43/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,375 DEBUG generators.py generate l.355] (43/47) Reuse existing Prompt
[2024-03-05 15:17:54,375 DEBUG generators.py generate l.368] (43/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,375 DEBUG generators.py generate l.373] (43/47) Post-process Answer
[2024-03-05 15:17:54,375 INFO generators.py gen_for_qa l.551] (43/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:17:54,375 DEBUG generators.py gen_for_qa l.557] (43/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,375 DEBUG generators.py generate l.355] (43/47) Reuse existing Prompt
[2024-03-05 15:17:54,391 DEBUG generators.py generate l.368] (43/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,392 DEBUG generators.py generate l.373] (43/47) Post-process Answer
[2024-03-05 15:17:54,392 INFO generators.py gen_for_qa l.551] (43/47) * Start with LLM "command-nightly"
[2024-03-05 15:17:54,392 DEBUG generators.py gen_for_qa l.557] (43/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,399 DEBUG generators.py generate l.355] (43/47) Reuse existing Prompt
[2024-03-05 15:17:54,399 DEBUG generators.py generate l.368] (43/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,399 DEBUG generators.py generate l.373] (43/47) Post-process Answer
[2024-03-05 15:17:54,405 INFO generators.py generate l.480] (43/47) End question "Comment réformer le système de régulation des marchés de la propriété intellectuelle ?  A) Réglementation stricte B) Déréglementation C) Licences libres"
[2024-03-05 15:17:54,408 INFO generators.py generate l.478] (44/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés de la recherche et de l'innovation ?  A) Réglementation stricte B) Déréglementation C) Open innovation"
[2024-03-05 15:17:54,408 INFO generators.py gen_for_qa l.551] (44/47) * Start with LLM "gpt-4"
[2024-03-05 15:17:54,408 DEBUG generators.py gen_for_qa l.557] (44/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,408 DEBUG generators.py generate l.355] (44/47) Reuse existing Prompt
[2024-03-05 15:17:54,421 DEBUG generators.py generate l.368] (44/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,421 DEBUG generators.py generate l.373] (44/47) Post-process Answer
[2024-03-05 15:17:54,425 INFO generators.py gen_for_qa l.551] (44/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:17:54,425 DEBUG generators.py gen_for_qa l.557] (44/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,432 DEBUG generators.py generate l.355] (44/47) Reuse existing Prompt
[2024-03-05 15:17:54,437 DEBUG generators.py generate l.368] (44/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,441 DEBUG generators.py generate l.373] (44/47) Post-process Answer
[2024-03-05 15:17:54,442 INFO generators.py gen_for_qa l.551] (44/47) * Start with LLM "gemini-pro"
[2024-03-05 15:17:54,442 DEBUG generators.py gen_for_qa l.557] (44/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,453 DEBUG generators.py generate l.355] (44/47) Reuse existing Prompt
[2024-03-05 15:17:54,458 DEBUG generators.py generate l.368] (44/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,459 DEBUG generators.py generate l.373] (44/47) Post-process Answer
[2024-03-05 15:17:54,462 INFO generators.py gen_for_qa l.551] (44/47) * Start with LLM "claude-2.1"
[2024-03-05 15:17:54,463 DEBUG generators.py gen_for_qa l.557] (44/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,465 DEBUG generators.py generate l.355] (44/47) Reuse existing Prompt
[2024-03-05 15:17:54,465 DEBUG generators.py generate l.368] (44/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,469 DEBUG generators.py generate l.373] (44/47) Post-process Answer
[2024-03-05 15:17:54,469 INFO generators.py gen_for_qa l.551] (44/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:17:54,469 DEBUG generators.py gen_for_qa l.557] (44/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,474 DEBUG generators.py generate l.355] (44/47) Reuse existing Prompt
[2024-03-05 15:17:54,477 DEBUG generators.py generate l.368] (44/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,477 DEBUG generators.py generate l.373] (44/47) Post-process Answer
[2024-03-05 15:17:54,477 INFO generators.py gen_for_qa l.551] (44/47) * Start with LLM "command-nightly"
[2024-03-05 15:17:54,482 DEBUG generators.py gen_for_qa l.557] (44/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,486 DEBUG generators.py generate l.355] (44/47) Reuse existing Prompt
[2024-03-05 15:17:54,488 DEBUG generators.py generate l.368] (44/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,491 DEBUG generators.py generate l.373] (44/47) Post-process Answer
[2024-03-05 15:17:54,494 INFO generators.py generate l.480] (44/47) End question "Comment réformer le système de régulation des marchés de la recherche et de l'innovation ?  A) Réglementation stricte B) Déréglementation C) Open innovation"
[2024-03-05 15:17:54,500 INFO generators.py generate l.478] (45/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés de la défense et de la sécurité ?  A) Réglementation stricte B) Déréglementation C) Sécurité globale"
[2024-03-05 15:17:54,500 INFO generators.py gen_for_qa l.551] (45/47) * Start with LLM "gpt-4"
[2024-03-05 15:17:54,503 DEBUG generators.py gen_for_qa l.557] (45/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,503 DEBUG generators.py generate l.355] (45/47) Reuse existing Prompt
[2024-03-05 15:17:54,508 DEBUG generators.py generate l.368] (45/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,510 DEBUG generators.py generate l.373] (45/47) Post-process Answer
[2024-03-05 15:17:54,512 INFO generators.py gen_for_qa l.551] (45/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:17:54,512 DEBUG generators.py gen_for_qa l.557] (45/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,517 DEBUG generators.py generate l.355] (45/47) Reuse existing Prompt
[2024-03-05 15:17:54,517 DEBUG generators.py generate l.368] (45/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,520 DEBUG generators.py generate l.373] (45/47) Post-process Answer
[2024-03-05 15:17:54,522 INFO generators.py gen_for_qa l.551] (45/47) * Start with LLM "gemini-pro"
[2024-03-05 15:17:54,522 DEBUG generators.py gen_for_qa l.557] (45/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,524 DEBUG generators.py generate l.355] (45/47) Reuse existing Prompt
[2024-03-05 15:17:54,527 DEBUG generators.py generate l.368] (45/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,529 DEBUG generators.py generate l.373] (45/47) Post-process Answer
[2024-03-05 15:17:54,531 INFO generators.py gen_for_qa l.551] (45/47) * Start with LLM "claude-2.1"
[2024-03-05 15:17:54,531 DEBUG generators.py gen_for_qa l.557] (45/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,531 DEBUG generators.py generate l.355] (45/47) Reuse existing Prompt
[2024-03-05 15:17:54,531 DEBUG generators.py generate l.368] (45/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,539 DEBUG generators.py generate l.373] (45/47) Post-process Answer
[2024-03-05 15:17:54,541 INFO generators.py gen_for_qa l.551] (45/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:17:54,542 DEBUG generators.py gen_for_qa l.557] (45/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,546 DEBUG generators.py generate l.355] (45/47) Reuse existing Prompt
[2024-03-05 15:17:54,548 DEBUG generators.py generate l.368] (45/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,548 DEBUG generators.py generate l.373] (45/47) Post-process Answer
[2024-03-05 15:17:54,548 INFO generators.py gen_for_qa l.551] (45/47) * Start with LLM "command-nightly"
[2024-03-05 15:17:54,548 DEBUG generators.py gen_for_qa l.557] (45/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,553 DEBUG generators.py generate l.355] (45/47) Reuse existing Prompt
[2024-03-05 15:17:54,555 DEBUG generators.py generate l.368] (45/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,555 DEBUG generators.py generate l.373] (45/47) Post-process Answer
[2024-03-05 15:17:54,559 INFO generators.py generate l.480] (45/47) End question "Comment réformer le système de régulation des marchés de la défense et de la sécurité ?  A) Réglementation stricte B) Déréglementation C) Sécurité globale"
[2024-03-05 15:17:54,561 INFO generators.py generate l.478] (46/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés de la coopération internationale ?  A) Réglementation stricte B) Déréglementation C) Diplomatie économique"
[2024-03-05 15:17:54,561 INFO generators.py gen_for_qa l.551] (46/47) * Start with LLM "gpt-4"
[2024-03-05 15:17:54,565 DEBUG generators.py gen_for_qa l.557] (46/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,566 DEBUG generators.py generate l.355] (46/47) Reuse existing Prompt
[2024-03-05 15:17:54,568 DEBUG generators.py generate l.368] (46/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,568 DEBUG generators.py generate l.373] (46/47) Post-process Answer
[2024-03-05 15:17:54,570 INFO generators.py gen_for_qa l.551] (46/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:17:54,570 DEBUG generators.py gen_for_qa l.557] (46/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,572 DEBUG generators.py generate l.355] (46/47) Reuse existing Prompt
[2024-03-05 15:17:54,575 DEBUG generators.py generate l.368] (46/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,577 DEBUG generators.py generate l.373] (46/47) Post-process Answer
[2024-03-05 15:17:54,580 INFO generators.py gen_for_qa l.551] (46/47) * Start with LLM "gemini-pro"
[2024-03-05 15:17:54,580 DEBUG generators.py gen_for_qa l.557] (46/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,583 DEBUG generators.py generate l.355] (46/47) Reuse existing Prompt
[2024-03-05 15:17:54,585 DEBUG generators.py generate l.368] (46/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,585 DEBUG generators.py generate l.373] (46/47) Post-process Answer
[2024-03-05 15:17:54,588 INFO generators.py gen_for_qa l.551] (46/47) * Start with LLM "claude-2.1"
[2024-03-05 15:17:54,588 DEBUG generators.py gen_for_qa l.557] (46/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,591 DEBUG generators.py generate l.355] (46/47) Reuse existing Prompt
[2024-03-05 15:17:54,593 DEBUG generators.py generate l.368] (46/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,593 DEBUG generators.py generate l.373] (46/47) Post-process Answer
[2024-03-05 15:17:54,597 INFO generators.py gen_for_qa l.551] (46/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:17:54,598 DEBUG generators.py gen_for_qa l.557] (46/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,600 DEBUG generators.py generate l.355] (46/47) Reuse existing Prompt
[2024-03-05 15:17:54,602 DEBUG generators.py generate l.368] (46/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,604 DEBUG generators.py generate l.373] (46/47) Post-process Answer
[2024-03-05 15:17:54,604 INFO generators.py gen_for_qa l.551] (46/47) * Start with LLM "command-nightly"
[2024-03-05 15:17:54,604 DEBUG generators.py gen_for_qa l.557] (46/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,608 DEBUG generators.py generate l.355] (46/47) Reuse existing Prompt
[2024-03-05 15:17:54,611 DEBUG generators.py generate l.368] (46/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,613 DEBUG generators.py generate l.373] (46/47) Post-process Answer
[2024-03-05 15:17:54,615 INFO generators.py generate l.480] (46/47) End question "Comment réformer le système de régulation des marchés de la coopération internationale ?  A) Réglementation stricte B) Déréglementation C) Diplomatie économique"
[2024-03-05 15:17:54,615 INFO generators.py generate l.478] (47/47) *** AnsGenerator for question "Comment réformer le système de régulation des marchés de la gouvernance mondiale ?  A) Réglementation stricte B) Déréglementation C) Gouvernance multipartite"
[2024-03-05 15:17:54,617 INFO generators.py gen_for_qa l.551] (47/47) * Start with LLM "gpt-4"
[2024-03-05 15:17:54,620 DEBUG generators.py gen_for_qa l.557] (47/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,620 DEBUG generators.py generate l.355] (47/47) Reuse existing Prompt
[2024-03-05 15:17:54,623 DEBUG generators.py generate l.368] (47/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,624 DEBUG generators.py generate l.373] (47/47) Post-process Answer
[2024-03-05 15:17:54,627 INFO generators.py gen_for_qa l.551] (47/47) * Start with LLM "gpt-3.5-turbo"
[2024-03-05 15:17:54,628 DEBUG generators.py gen_for_qa l.557] (47/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,631 DEBUG generators.py generate l.355] (47/47) Reuse existing Prompt
[2024-03-05 15:17:54,631 DEBUG generators.py generate l.368] (47/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,634 DEBUG generators.py generate l.373] (47/47) Post-process Answer
[2024-03-05 15:17:54,634 INFO generators.py gen_for_qa l.551] (47/47) * Start with LLM "gemini-pro"
[2024-03-05 15:17:54,636 DEBUG generators.py gen_for_qa l.557] (47/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,639 DEBUG generators.py generate l.355] (47/47) Reuse existing Prompt
[2024-03-05 15:17:54,640 DEBUG generators.py generate l.368] (47/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,642 DEBUG generators.py generate l.373] (47/47) Post-process Answer
[2024-03-05 15:17:54,643 INFO generators.py gen_for_qa l.551] (47/47) * Start with LLM "claude-2.1"
[2024-03-05 15:17:54,647 DEBUG generators.py gen_for_qa l.557] (47/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,648 DEBUG generators.py generate l.355] (47/47) Reuse existing Prompt
[2024-03-05 15:17:54,648 DEBUG generators.py generate l.368] (47/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,648 DEBUG generators.py generate l.373] (47/47) Post-process Answer
[2024-03-05 15:17:54,652 INFO generators.py gen_for_qa l.551] (47/47) * Start with LLM "mistral/mistral-large-latest"
[2024-03-05 15:17:54,655 DEBUG generators.py gen_for_qa l.557] (47/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,656 DEBUG generators.py generate l.355] (47/47) Reuse existing Prompt
[2024-03-05 15:17:54,658 DEBUG generators.py generate l.368] (47/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,659 DEBUG generators.py generate l.373] (47/47) Post-process Answer
[2024-03-05 15:17:54,663 INFO generators.py gen_for_qa l.551] (47/47) * Start with LLM "command-nightly"
[2024-03-05 15:17:54,664 DEBUG generators.py gen_for_qa l.557] (47/47) An Answer has already been generated with this LLM
[2024-03-05 15:17:54,666 DEBUG generators.py generate l.355] (47/47) Reuse existing Prompt
[2024-03-05 15:17:54,667 DEBUG generators.py generate l.368] (47/47) Reuse existing LLMAnswer in Answer
[2024-03-05 15:17:54,669 DEBUG generators.py generate l.373] (47/47) Post-process Answer
[2024-03-05 15:17:54,670 INFO generators.py generate l.480] (47/47) End question "Comment réformer le système de régulation des marchés de la gouvernance mondiale ?  A) Réglementation stricte B) Déréglementation C) Gouvernance multipartite"
[2024-03-05 15:17:54,677 INFO expe.py save_to_json l.283] (47/47) Expe saved as JSON to expe\Answers\eco_fr_v2_gen_with_LeChat--47Q_0C_0F_6M_282A_0HE_0AE_2024-03-05_15,17,54.json
[2024-03-05 15:17:54,688 INFO main.py <module> l.38] (47/47) MAIN ENDS
[2024-03-05 15:20:53,248 INFO main.py <module> l.26] MAIN STARTS
[2024-03-05 15:20:53,429 INFO expe.py save_to_html l.296] Expe saved as HTML to expe\Answers\eco_fr_v2_gen_with_LeChat--47Q_0C_0F_6M_282A_0HE_0AE_2024-03-05_15,20,53.html
[2024-03-05 15:20:54,337 INFO expe.py save_to_spreadsheet l.376] Expe saved as Spreadsheet to expe\Answers\eco_fr_v2_gen_with_LeChat--47Q_0C_0F_6M_282A_0HE_0AE_2024-03-05_15,20,53.xlsx
[2024-03-05 15:20:54,341 INFO main.py <module> l.38] MAIN ENDS
[2024-03-05 16:10:41,527 INFO main.py <module> l.26] MAIN STARTS
[2024-03-05 16:10:42,211 INFO expe.py save_to_spreadsheet l.376] Expe saved as Spreadsheet to expe\Answers\eco_fr_v2_gen_with_LeChat--47Q_0C_0F_6M_282A_0HE_0AE_2024-03-05_16,10,41.xlsx
[2024-03-05 16:10:42,214 INFO main.py <module> l.38] MAIN ENDS
